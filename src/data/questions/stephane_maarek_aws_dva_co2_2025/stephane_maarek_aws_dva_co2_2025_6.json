[
  {
    "id": 1,
    "question": "<p>A developer is configuring the redirect actions for an Application Load Balancer. The developer stumbled upon the following snippet of code.</p>\n\n<p>Which of the following is an example of a query string condition that the developer can use on AWS CLI?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"PathPatternConfig\": {\n          \"Values\": [\"/img/*\"]\n      }\n  }\n]\n</code></pre>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"StringHeaderConfig\": {\n          \"Values\": [\"*.example.com\"]\n      }\n  }\n]\n</code></pre>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<pre><code>[\n  {\n      \"Type\": \"redirect\",\n      \"RedirectConfig\": {\n          \"Protocol\": \"HTTPS\",\n          \"Port\": \"443\",\n          \"Host\": \"#{host}\",\n          \"Path\": \"/#{path}\",\n          \"Query\": \"#{query}\",\n          \"StatusCode\": \"HTTP_301\"\n      }\n  }\n]\n\n</code></pre>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"QueryStringConfig\": {\n          \"Values\": [\n            {\n                \"Key\": \"version\",\n                \"Value\": \"v1\"\n            },\n            {\n                \"Value\": \"*example*\"\n            }\n          ]\n      }\n  }\n]\n\n</code></pre>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p>**</p>\n\n<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"QueryStringConfig\": {\n          \"Values\": [\n            {\n                \"Key\": \"version\",\n                \"Value\": \"v1\"\n            },\n            {\n                \"Value\": \"*example*\"\n            }\n          ]\n      }\n  }\n]\n</code></pre>\n\n<p>**</p>\n\n<p>You can use query string conditions to configure rules that route requests based on key/value pairs or values in the query string. The match evaluation is not case-sensitive. The following wildcard characters are supported: * (matches 0 or more characters) and ? (matches exactly 1 character). You can specify conditions when you create or modify a rule.</p>\n\n<p>Query parameters are often used along with the path component of the URL for applying a special logic to the resource being fetched.</p>\n\n<p>The query string component starts after the first \"?\" in a URI. Typically query strings contain key-value pairs separated by a delimiter \"&amp;\". Example: http://example.com/path/to/page?version=A&amp;gender=female</p>\n\n<p>The example condition given in the question is satisfied by requests with a query string that includes either a key/value pair of \"version=v1\" or any key set to \"example\".</p>\n\n<p>Incorrect options:</p>\n\n<p>**</p>\n\n<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"PathPatternConfig\": {\n          \"Values\": [\"/img/*\"]\n      }\n  }\n]\n</code></pre>\n\n<p>**</p>\n\n<p>**</p>\n\n<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"StringHeaderConfig\": {\n          \"Values\": [\"*.example.com\"]\n      }\n  }\n]\n</code></pre>\n\n<p>**</p>\n\n<p>These two options are malformed and are incorrect.</p>\n\n<p>**</p>\n\n<pre><code>[\n  {\n      \"Type\": \"redirect\",\n      \"RedirectConfig\": {\n          \"Protocol\": \"HTTPS\",\n          \"Port\": \"443\",\n          \"Host\": \"#{host}\",\n          \"Path\": \"/#{path}\",\n          \"Query\": \"#{query}\",\n          \"StatusCode\": \"HTTP_301\"\n      }\n  }\n]\n</code></pre>\n\n<p>** - This action redirects an HTTP request to an HTTPS request on port 443, with the same hostname, path, and query string as the HTTP request.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#query-string-conditions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#query-string-conditions</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "**"
      },
      {
        "answer": "",
        "explanation": "<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"QueryStringConfig\": {\n          \"Values\": [\n            {\n                \"Key\": \"version\",\n                \"Value\": \"v1\"\n            },\n            {\n                \"Value\": \"*example*\"\n            }\n          ]\n      }\n  }\n]\n</code></pre>"
      },
      {
        "answer": "",
        "explanation": "**"
      },
      {
        "answer": "",
        "explanation": "You can use query string conditions to configure rules that route requests based on key/value pairs or values in the query string. The match evaluation is not case-sensitive. The following wildcard characters are supported: * (matches 0 or more characters) and ? (matches exactly 1 character). You can specify conditions when you create or modify a rule."
      },
      {
        "answer": "",
        "explanation": "Query parameters are often used along with the path component of the URL for applying a special logic to the resource being fetched."
      },
      {
        "answer": "",
        "explanation": "The query string component starts after the first \"?\" in a URI. Typically query strings contain key-value pairs separated by a delimiter \"&amp;\". Example: http://example.com/path/to/page?version=A&amp;gender=female"
      },
      {
        "answer": "",
        "explanation": "The example condition given in the question is satisfied by requests with a query string that includes either a key/value pair of \"version=v1\" or any key set to \"example\"."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "**"
      },
      {
        "answer": "",
        "explanation": "<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"PathPatternConfig\": {\n          \"Values\": [\"/img/*\"]\n      }\n  }\n]\n</code></pre>"
      },
      {
        "answer": "",
        "explanation": "**"
      },
      {
        "answer": "",
        "explanation": "**"
      },
      {
        "answer": "",
        "explanation": "<pre><code>[\n  {\n      \"Field\": \"query-string\",\n      \"StringHeaderConfig\": {\n          \"Values\": [\"*.example.com\"]\n      }\n  }\n]\n</code></pre>"
      },
      {
        "answer": "",
        "explanation": "**"
      },
      {
        "answer": "",
        "explanation": "These two options are malformed and are incorrect."
      },
      {
        "answer": "",
        "explanation": "**"
      },
      {
        "answer": "",
        "explanation": "<pre><code>[\n  {\n      \"Type\": \"redirect\",\n      \"RedirectConfig\": {\n          \"Protocol\": \"HTTPS\",\n          \"Port\": \"443\",\n          \"Host\": \"#{host}\",\n          \"Path\": \"/#{path}\",\n          \"Query\": \"#{query}\",\n          \"StatusCode\": \"HTTP_301\"\n      }\n  }\n]\n</code></pre>"
      },
      {
        "answer": "",
        "explanation": "** - This action redirects an HTTP request to an HTTPS request on port 443, with the same hostname, path, and query string as the HTTP request."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#query-string-conditions"
    ]
  },
  {
    "id": 2,
    "question": "<p>A new recruit is trying to understand the nuances of EC2 Auto Scaling. As an AWS Certified Developer Associate, you have been asked to mentor the new recruit.</p>\n\n<p>Can you identify and explain the correct statements about Auto Scaling to the new recruit? (Select two).</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>You cannot use Amazon EC2 Auto Scaling for health checks (to replace unhealthy instances) if you are not using Elastic Load Balancing (ELB)</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Every time you create an Auto Scaling group from an existing instance, it creates a new AMI (Amazon Machine Image)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>EC2 Auto Scaling groups are regional constructs. They span across Availability Zones and AWS regions</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon EC2 Auto Scaling cannot add a volume to an existing instance if the existing volume is approaching capacity</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Amazon EC2 Auto Scaling works with both Application Load Balancers and Network Load Balancers</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct options:</p>\n\n<p>Amazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application.</p>\n\n<p><strong>Amazon EC2 Auto Scaling cannot add a volume to an existing instance if the existing volume is approaching capacity</strong> - A volume is attached to a new instance when it is added. Amazon EC2 Auto Scaling doesn't automatically add a volume when the existing one is approaching capacity. You can use the EC2 API to add a volume to an existing instance.</p>\n\n<p><strong>Amazon EC2 Auto Scaling works with both Application Load Balancers and Network Load Balancers</strong> - Amazon EC2 Auto Scaling works with Application Load Balancers and Network Load Balancers including their health check feature.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EC2 Auto Scaling groups are regional constructs. They span across Availability Zones and AWS regions</strong> - This is an incorrect statement. EC2 Auto Scaling groups are regional constructs. They can span Availability Zones, but not AWS regions.</p>\n\n<p><strong>Every time you create an Auto Scaling group from an existing instance, it creates a new AMI (Amazon Machine Image)</strong> - This is an incorrect statement. When you create an Auto Scaling group from an existing instance, it does not create a new AMI.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q17-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q17-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html</a><p></p>\n\n<p><strong>You cannot use Amazon EC2 Auto Scaling for health checks (to replace unhealthy instances) if you are not using Elastic Load Balancing (ELB)</strong> - This is an incorrect statement. You don't have to use ELB to use Auto Scaling. You can use the EC2 health check to identify and replace unhealthy instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/autoscaling/faqs/\">https://aws.amazon.com/ec2/autoscaling/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 Auto Scaling cannot add a volume to an existing instance if the existing volume is approaching capacity</strong> - A volume is attached to a new instance when it is added. Amazon EC2 Auto Scaling doesn't automatically add a volume when the existing one is approaching capacity. You can use the EC2 API to add a volume to an existing instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 Auto Scaling works with both Application Load Balancers and Network Load Balancers</strong> - Amazon EC2 Auto Scaling works with Application Load Balancers and Network Load Balancers including their health check feature."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>EC2 Auto Scaling groups are regional constructs. They span across Availability Zones and AWS regions</strong> - This is an incorrect statement. EC2 Auto Scaling groups are regional constructs. They can span Availability Zones, but not AWS regions."
      },
      {
        "answer": "",
        "explanation": "<strong>Every time you create an Auto Scaling group from an existing instance, it creates a new AMI (Amazon Machine Image)</strong> - This is an incorrect statement. When you create an Auto Scaling group from an existing instance, it does not create a new AMI."
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html"
      },
      {
        "answer": "",
        "explanation": "<strong>You cannot use Amazon EC2 Auto Scaling for health checks (to replace unhealthy instances) if you are not using Elastic Load Balancing (ELB)</strong> - This is an incorrect statement. You don't have to use ELB to use Auto Scaling. You can use the EC2 health check to identify and replace unhealthy instances."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html",
      "https://aws.amazon.com/ec2/autoscaling/faqs/"
    ]
  },
  {
    "id": 3,
    "question": "<p>An analytics company is using Kinesis Data Streams (KDS) to process automobile health-status data from the taxis managed by a taxi ride-hailing service. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams.</p>\n\n<p>As a Developer Associate, which of the following options would you suggest for improving the performance for the given use-case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Enhanced Fanout feature of Kinesis Data Streams</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Swap out Kinesis Data Streams with SQS Standard queues</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Swap out Kinesis Data Streams with Kinesis Data Firehose</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Swap out Kinesis Data Streams with SQS FIFO queues</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Enhanced Fanout feature of Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream. You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream.</p>\n\n<p>Kinesis Data Streams Fanout\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q33-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q33-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Swap out Kinesis Data Streams with Kinesis Data Firehose</strong> -  Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Kinesis Data Firehose can only write to S3, Redshift, Elasticsearch or Splunk. You can't have applications consuming data streams from Kinesis Data Firehose, that's the job of Kinesis Data Streams. Therefore this option is not correct.</p>\n\n<p><strong>Swap out Kinesis Data Streams with SQS Standard queues</strong></p>\n\n<p><strong>Swap out Kinesis Data Streams with SQS FIFO queues</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both SQS Standard and SQS FIFO are not the right fit for the given use-case.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please understand the differences between the capabilities of Kinesis Data Streams vs SQS, as you may be asked scenario-based questions on this topic in the exam.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q33-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q33-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a><p></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use Enhanced Fanout feature of Kinesis Data Streams</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events."
      },
      {
        "answer": "",
        "explanation": "By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream. You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q33-i1.jpg",
        "answer": "",
        "explanation": "Kinesis Data Streams Fanout"
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Swap out Kinesis Data Streams with Kinesis Data Firehose</strong> -  Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Kinesis Data Firehose can only write to S3, Redshift, Elasticsearch or Splunk. You can't have applications consuming data streams from Kinesis Data Firehose, that's the job of Kinesis Data Streams. Therefore this option is not correct."
      },
      {
        "answer": "<strong>Swap out Kinesis Data Streams with SQS Standard queues</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Swap out Kinesis Data Streams with SQS FIFO queues</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both SQS Standard and SQS FIFO are not the right fit for the given use-case."
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "answer": "",
        "explanation": "Please understand the differences between the capabilities of Kinesis Data Streams vs SQS, as you may be asked scenario-based questions on this topic in the exam."
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/faqs/"
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/",
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 4,
    "question": "<p>An e-commerce company uses Amazon SQS queues to decouple their application architecture. The development team has observed message processing failures for an edge case scenario when a user places an order for a particular product ID, but the product ID is deleted, thereby causing the application code to fail.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to address such message failures?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a temporary queue to handle message processing failures</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use a dead-letter queue to handle message processing failures</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use long polling to handle message processing failures</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use short polling to handle message processing failures</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use a dead-letter queue to handle message processing failures</strong></p>\n\n<p>Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.</p>\n\n<p>Sometimes, messages can’t be processed because of a variety of possible issues, such as when a user comments on a story but it remains unprocessed because the original story itself is deleted by the author while the comments were being posted. In such a case, the dead-letter queue can be used to handle message processing failures.</p>\n\n<p>How do dead-letter queues work?\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q31-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q31-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a><p></p>\n\n<p>Use-cases for dead-letter queues:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q31-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q31-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a temporary queue to handle message processing failures</strong> - The most common use case for temporary queues is the request-response messaging pattern (for example, processing a login request), where a requester creates a temporary queue for receiving each response message. To avoid creating an Amazon SQS queue for each response message, the Temporary Queue Client lets you create and delete multiple temporary queues without making any Amazon SQS API calls. Temporary queues cannot be used to handle message processing failures.</p>\n\n<p><strong>Use short polling to handle message processing failures</strong></p>\n\n<p><strong>Use long polling to handle message processing failures</strong></p>\n\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.\nNeither short polling nor long polling can be used to handle message processing failures.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use a dead-letter queue to handle message processing failures</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed."
      },
      {
        "answer": "",
        "explanation": "Sometimes, messages can’t be processed because of a variety of possible issues, such as when a user comments on a story but it remains unprocessed because the original story itself is deleted by the author while the comments were being posted. In such a case, the dead-letter queue can be used to handle message processing failures."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q31-i1.jpg",
        "answer": "",
        "explanation": "How do dead-letter queues work?"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q31-i2.jpg",
        "answer": "",
        "explanation": "Use-cases for dead-letter queues:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use a temporary queue to handle message processing failures</strong> - The most common use case for temporary queues is the request-response messaging pattern (for example, processing a login request), where a requester creates a temporary queue for receiving each response message. To avoid creating an Amazon SQS queue for each response message, the Temporary Queue Client lets you create and delete multiple temporary queues without making any Amazon SQS API calls. Temporary queues cannot be used to handle message processing failures."
      },
      {
        "answer": "<strong>Use short polling to handle message processing failures</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Use long polling to handle message processing failures</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.\nNeither short polling nor long polling can be used to handle message processing failures."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
    ]
  },
  {
    "id": 5,
    "question": "<p>As a Developer, you are working on a mobile application that utilizes Amazon Simple Queue Service (SQS) for sending messages to downstream systems for further processing. One of the requirements is that the messages should be stored in the queue for a period of 12 days.</p>\n\n<p>How will you configure this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The maximum retention period of SQS messages is 7 days, therefore retention period of 12 days is not possible</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use a FIFO SQS queue</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable Long Polling for the SQS queue</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Change the queue message retention setting</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Change the queue message retention setting</strong> - Amazon SQS automatically deletes messages that have been in a queue for more than the maximum message retention period. The default message retention period is 4 days. However, you can set the message retention period to a value from 60 seconds to 1,209,600 seconds (14 days) using the SetQueueAttributes action.</p>\n\n<p>More info here:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q21-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q21-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Long Polling for the SQS queue</strong> - Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). This feature is not useful for the current use case.</p>\n\n<p><strong>The maximum retention period of SQS messages is 7 days, therefore retention period of 12 days is not possible</strong> - This is an incorrect statement. Retention period of up to 14 days is possible.</p>\n\n<p><strong>Use a FIFO SQS queue</strong> - FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated. This is not useful for the current scenario.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Change the queue message retention setting</strong> - Amazon SQS automatically deletes messages that have been in a queue for more than the maximum message retention period. The default message retention period is 4 days. However, you can set the message retention period to a value from 60 seconds to 1,209,600 seconds (14 days) using the SetQueueAttributes action."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q21-i1.jpg",
        "answer": "",
        "explanation": "More info here:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable Long Polling for the SQS queue</strong> - Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). This feature is not useful for the current use case."
      },
      {
        "answer": "",
        "explanation": "<strong>The maximum retention period of SQS messages is 7 days, therefore retention period of 12 days is not possible</strong> - This is an incorrect statement. Retention period of up to 14 days is possible."
      },
      {
        "answer": "",
        "explanation": "<strong>Use a FIFO SQS queue</strong> - FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated. This is not useful for the current scenario."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html"
    ]
  },
  {
    "id": 6,
    "question": "<p>A development team has noticed that one of the EC2 instances has been wrongly configured with the 'DeleteOnTermination' attribute set to True for its root EBS volume.</p>\n\n<p>As a developer associate, can you suggest a way to disable this flag while the instance is still running?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set the <code>DeleteOnTermination</code> attribute to False using the command line</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set the <code>DisableApiTermination</code> attribute of the instance using the API</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p>When an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume and is set to False for all other volume types.</p>\n\n<p><strong>Set the <code>DeleteOnTermination</code> attribute to False using the command line</strong> - If the instance is already running, you can set <code>DeleteOnTermination</code> to False using the command line.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume</strong> - You can set the <code>DeleteOnTermination</code> attribute to False when you launch a new instance. It is not possible to update this attribute of a running instance from the AWS console.</p>\n\n<p><strong>Set the <code>DisableApiTermination</code> attribute of the instance using the API</strong> - By default, you can terminate your instance using the Amazon EC2 console, command-line interface, or API. To prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination protection for the instance. The <code>DisableApiTermination</code> attribute controls whether the instance can be terminated using the console, CLI, or API. This option cannot be used to control the delete status for the EBS volume when the instance terminates.</p>\n\n<p><strong>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</strong> - This statement is wrong and given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/\">https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "When an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume and is set to False for all other volume types."
      },
      {
        "answer": "",
        "explanation": "<strong>Set the <code>DeleteOnTermination</code> attribute to False using the command line</strong> - If the instance is already running, you can set <code>DeleteOnTermination</code> to False using the command line."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume</strong> - You can set the <code>DeleteOnTermination</code> attribute to False when you launch a new instance. It is not possible to update this attribute of a running instance from the AWS console."
      },
      {
        "answer": "",
        "explanation": "<strong>Set the <code>DisableApiTermination</code> attribute of the instance using the API</strong> - By default, you can terminate your instance using the Amazon EC2 console, command-line interface, or API. To prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination protection for the instance. The <code>DisableApiTermination</code> attribute controls whether the instance can be terminated using the console, CLI, or API. This option cannot be used to control the delete status for the EBS volume when the instance terminates."
      },
      {
        "answer": "",
        "explanation": "<strong>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</strong> - This statement is wrong and given only as a distractor."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance"
    ]
  },
  {
    "id": 7,
    "question": "<p>As a site reliability engineer, you are responsible for improving the company’s deployment by scaling and automating applications. As new application versions are ready for production you ensure that the application gets deployed to different sets of EC2 instances at different times allowing for a smooth transition.</p>\n\n<p>Using AWS CodeDeploy, which of the following options will allow you to do this?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>CodeDeploy Agent</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>CodeDeploy Deployment Groups</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>CodeDeploy Hooks</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Define multiple CodeDeploy Applications</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>CodeDeploy Deployment Groups</strong></p>\n\n<p>You can specify one or more deployment groups for a CodeDeploy application. The deployment group contains settings and configurations used during the deployment. Most deployment group settings depend on the compute platform used by your application. Some settings, such as rollbacks, triggers, and alarms can be configured for deployment groups for any compute platform.</p>\n\n<p>In an EC2/On-Premises deployment, a deployment group is a set of individual instances targeted for deployment. A deployment group contains individually tagged instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CodeDeploy Agent</strong> - The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The agent connects the EC2 instances to the CodeDeploy service.</p>\n\n<p><strong>CodeDeploy Hooks</strong> - Hooks are found in the AppSec file used by AWS CodeDeploy to manage deployment. Hooks correspond to lifecycle events such as ApplicationStart, ApplicationStop, etc. to which you can assign a script.</p>\n\n<p><strong>Define multiple CodeDeploy Applications</strong> - This option has been added as a distractor. Instead, you want to use deployment groups to use the same deployment and maybe separate the times when a group of instances receives the software updates.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>CodeDeploy Deployment Groups</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can specify one or more deployment groups for a CodeDeploy application. The deployment group contains settings and configurations used during the deployment. Most deployment group settings depend on the compute platform used by your application. Some settings, such as rollbacks, triggers, and alarms can be configured for deployment groups for any compute platform."
      },
      {
        "answer": "",
        "explanation": "In an EC2/On-Premises deployment, a deployment group is a set of individual instances targeted for deployment. A deployment group contains individually tagged instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>CodeDeploy Agent</strong> - The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The agent connects the EC2 instances to the CodeDeploy service."
      },
      {
        "answer": "",
        "explanation": "<strong>CodeDeploy Hooks</strong> - Hooks are found in the AppSec file used by AWS CodeDeploy to manage deployment. Hooks correspond to lifecycle events such as ApplicationStart, ApplicationStop, etc. to which you can assign a script."
      },
      {
        "answer": "",
        "explanation": "<strong>Define multiple CodeDeploy Applications</strong> - This option has been added as a distractor. Instead, you want to use deployment groups to use the same deployment and maybe separate the times when a group of instances receives the software updates."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html"
    ]
  },
  {
    "id": 8,
    "question": "<p>A development team has been using Amazon S3 service as an object store. With Amazon S3 turning strongly consistent, the team wants to understand the impact of this change on its data storage practices.</p>\n\n<p>As a developer associate, can you identify the key characteristics of the strongly consistent data model followed by S3? (Select two)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A process deletes an existing object and immediately lists keys within its bucket. The object could still be visible for few more minutes till the change propagates</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>A process deletes an existing object and immediately tries to read it. Amazon S3 can return data as the object deletion has not yet propagated completely</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>A process deletes an existing object and immediately tries to read it. Amazon S3 will not return any data as the object has been deleted</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>A process replaces an existing object and immediately tries to read it. Amazon S3 might return the old data</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list</strong> - Bucket configurations have an eventual consistency model. If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list.</p>\n\n<p><strong>A process deletes an existing object and immediately tries to read it. Amazon S3 will not return any data as the object has been deleted</strong> - Amazon S3 provides strong read-after-write consistency for PUTs and DELETEs of objects in your Amazon S3 bucket in all AWS Regions. This applies to both writes to new objects as well as PUTs that overwrite existing objects and DELETEs.</p>\n\n<p>Amazon S3 data consistency model:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q44-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q44-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A process deletes an existing object and immediately lists keys within its bucket. The object could still be visible for few more minutes till the change propagates</strong></p>\n\n<p><strong>A process deletes an existing object and immediately tries to read it. Amazon S3 can return data as the object deletion has not yet propagated completely</strong> -</p>\n\n<p>These two options highlight an eventually consistent behavior. Amazon S3 is now strongly consistent and will not return any data as the object has been deleted. So both these options are incorrect.</p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Amazon S3 might return the old data</strong> - Amazon S3 will return the new data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list</strong> - Bucket configurations have an eventual consistency model. If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list."
      },
      {
        "answer": "",
        "explanation": "<strong>A process deletes an existing object and immediately tries to read it. Amazon S3 will not return any data as the object has been deleted</strong> - Amazon S3 provides strong read-after-write consistency for PUTs and DELETEs of objects in your Amazon S3 bucket in all AWS Regions. This applies to both writes to new objects as well as PUTs that overwrite existing objects and DELETEs."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q44-i1.jpg",
        "answer": "",
        "explanation": "Amazon S3 data consistency model:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>A process deletes an existing object and immediately lists keys within its bucket. The object could still be visible for few more minutes till the change propagates</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "<strong>A process deletes an existing object and immediately tries to read it. Amazon S3 can return data as the object deletion has not yet propagated completely</strong> -"
      },
      {
        "answer": "",
        "explanation": "These two options highlight an eventually consistent behavior. Amazon S3 is now strongly consistent and will not return any data as the object has been deleted. So both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>A process replaces an existing object and immediately tries to read it. Amazon S3 might return the old data</strong> - Amazon S3 will return the new data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel"
    ]
  },
  {
    "id": 9,
    "question": "<p>A development team is configuring Kinesis Data Streams for ingesting real-time data from various appliances. The team has declared a shard capacity of one to test the configuration.</p>\n\n<p>What happens if the capacity limits of an Amazon Kinesis data stream are exceeded while the data producer adds data to the data stream?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The put data calls will be rejected with a <code>ProvisionedThroughputExceeded</code> exception</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Contact AWS support to request an increase in the number of shards</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The put data calls will be rejected with a <code>AccessDeniedException</code> exception once the limit is reached</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Data is lost unless the partition key of the data records is changed in order to write data to a different shard in the stream</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The put data calls will be rejected with a <code>ProvisionedThroughputExceeded</code> exception</strong></p>\n\n<p>The capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception. If this is due to a temporary rise of the data stream’s input data rate, retry by the data producer will eventually lead to completion of the requests. If this is due to a sustained rise of the data stream’s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The put data calls will be rejected with a <code>AccessDeniedException</code> exception once the limit is reached</strong> - Access Denied error is thrown when the accessing system does not have enough permissions. Since data was getting ingested into Data Streams before reaching the capacity, this error is not possible.</p>\n\n<p><strong>Data is lost unless the partition key of the data records is changed in order to write data to a different shard in the stream</strong> - Partition key is used to segregate and route records to different shards of a data stream. A partition key is specified by your data producer while adding data to an Amazon Kinesis data stream. The use case talks about provisioning only one shard. It is not possible to set up more shards by simply changing the partition key. Hence, this choice is incorrect.</p>\n\n<p><strong>Contact AWS support to request an increase in the number of shards</strong> - This is a made-up option that acts as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>The put data calls will be rejected with a <code>ProvisionedThroughputExceeded</code> exception</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception. If this is due to a temporary rise of the data stream’s input data rate, retry by the data producer will eventually lead to completion of the requests. If this is due to a sustained rise of the data stream’s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The put data calls will be rejected with a <code>AccessDeniedException</code> exception once the limit is reached</strong> - Access Denied error is thrown when the accessing system does not have enough permissions. Since data was getting ingested into Data Streams before reaching the capacity, this error is not possible."
      },
      {
        "answer": "",
        "explanation": "<strong>Data is lost unless the partition key of the data records is changed in order to write data to a different shard in the stream</strong> - Partition key is used to segregate and route records to different shards of a data stream. A partition key is specified by your data producer while adding data to an Amazon Kinesis data stream. The use case talks about provisioning only one shard. It is not possible to set up more shards by simply changing the partition key. Hence, this choice is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Contact AWS support to request an increase in the number of shards</strong> - This is a made-up option that acts as a distractor."
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 10,
    "question": "<p>A video streaming application uses Amazon CloudFront for its data distribution. The development team has decided to use CloudFront with origin failover for high availability.</p>\n\n<p>Which of the following options are correct while configuring CloudFront with Origin Groups? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>In the Origin Group of your distribution, all the origins are defined as primary for automatic failover in case an origin fails</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>To set up origin failover, you must have a distribution with at least three origins</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>When there’s a cache hit, CloudFront routes the request to the primary origin in the origin group</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>CloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, HEAD or OPTIONS</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin</strong></p>\n\n<p>CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin. CloudFront only sends requests to the secondary origin after a request to the primary origin fails.</p>\n\n<p><strong>CloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, HEAD or OPTIONS</strong></p>\n\n<p>CloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, HEAD, or OPTIONS. CloudFront does not failover when the viewer sends a different HTTP method (for example POST, PUT, and so on).</p>\n\n<p>How origin failover works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q41-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q41-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When there’s a cache hit, CloudFront routes the request to the primary origin in the origin group</strong> - When there’s a cache miss, CloudFront routes the request to the primary origin in the origin group. When there’s a cache hit, CloudFront returns the requested file.</p>\n\n<p><strong>To set up origin failover, you must have a distribution with at least three origins</strong> - Two origins are enough to set up an origin failover.</p>\n\n<p><strong>In the Origin Group of your distribution, all the origins are defined as primary for automatic failover in case an origin fails</strong> - To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Only one origin can be set as primary.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin. CloudFront only sends requests to the secondary origin after a request to the primary origin fails."
      },
      {
        "answer": "<strong>CloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, HEAD or OPTIONS</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "CloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, HEAD, or OPTIONS. CloudFront does not failover when the viewer sends a different HTTP method (for example POST, PUT, and so on)."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q41-i1.jpg",
        "answer": "",
        "explanation": "How origin failover works:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>When there’s a cache hit, CloudFront routes the request to the primary origin in the origin group</strong> - When there’s a cache miss, CloudFront routes the request to the primary origin in the origin group. When there’s a cache hit, CloudFront returns the requested file."
      },
      {
        "answer": "",
        "explanation": "<strong>To set up origin failover, you must have a distribution with at least three origins</strong> - Two origins are enough to set up an origin failover."
      },
      {
        "answer": "",
        "explanation": "<strong>In the Origin Group of your distribution, all the origins are defined as primary for automatic failover in case an origin fails</strong> - To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Only one origin can be set as primary."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html"
    ]
  },
  {
    "id": 11,
    "question": "<p>A retail company manages its IT infrastructure on AWS Cloud via Elastic Beanstalk. The development team at the company is planning to deploy the next version with MINIMUM application downtime and the ability to rollback quickly in case deployment goes wrong.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend to the development team?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy the new version to a separate environment via Blue/Green Deployment, and then swap Route 53 records of the two environments to redirect traffic to the new version</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Deploy the new application version using 'Rolling with additional batch' deployment policy</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy the new application version using 'All at once' deployment policy</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy the new application version using 'Rolling' deployment policy</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the new version to a separate environment via Blue/Green Deployment, and then swap Route 53 records of the two environments to redirect traffic to the new version</strong></p>\n\n<p>With deployment policies such as 'All at once', AWS Elastic Beanstalk performs an in-place update when you update your application versions and your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs (via Route 53) of the two environments to redirect traffic to the new version instantly. In case of any deployment issues, the rollback process is very quick via swapping the URLs for the two environments.</p>\n\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q10-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q10-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the new application version using 'All at once' deployment policy</strong> - Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time. So this option is not correct.</p>\n\n<p><strong>Deploy the new application version using 'Rolling' deployment policy</strong> - This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. However rollback process is via manual redeploy, so it's not as quick as the Blue/Green deployment.</p>\n\n<p><strong>Deploy the new application version using 'Rolling with additional batch' deployment policy</strong> - This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. However rollback process is via manual redeploy, so it's not as quick as the Blue/Green deployment.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Deploy the new version to a separate environment via Blue/Green Deployment, and then swap Route 53 records of the two environments to redirect traffic to the new version</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "With deployment policies such as 'All at once', AWS Elastic Beanstalk performs an in-place update when you update your application versions and your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs (via Route 53) of the two environments to redirect traffic to the new version instantly. In case of any deployment issues, the rollback process is very quick via swapping the URLs for the two environments."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q10-i1.jpg",
        "answer": "",
        "explanation": "Overview of Elastic Beanstalk Deployment Policies:"
      },
      {
        "link": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy the new application version using 'All at once' deployment policy</strong> - Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time. So this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy the new application version using 'Rolling' deployment policy</strong> - This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. However rollback process is via manual redeploy, so it's not as quick as the Blue/Green deployment."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy the new application version using 'Rolling with additional batch' deployment policy</strong> - This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. However rollback process is via manual redeploy, so it's not as quick as the Blue/Green deployment."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html"
    ]
  },
  {
    "id": 12,
    "question": "<p>Your e-commerce company needs to improve its software delivery process and is moving away from the waterfall methodology. You decided that every application should be built using the best CI/CD practices and every application should be packaged and deployed as a Docker container. The Docker images should be stored in ECR and pushed with AWS CodePipeline and AWS CodeBuild.</p>\n\n<p>When you attempt to do this, the last step fails with an authorization issue. What is the most likely issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The ECS instances are misconfigured and must contain additional data in /etc/ecs/ecs.config</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The IAM permissions are wrong for the CodeBuild service</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The ECR repository is stale, you must delete and re-create it</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>CodeBuild cannot talk to ECR because of security group issues</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The IAM permissions are wrong for the CodeBuild service</strong></p>\n\n<p>You can push your Docker or Open Container Initiative (OCI) images to an Amazon ECR repository with the docker push command.</p>\n\n<p>Amazon ECR users require permission to call ecr:GetAuthorizationToken before they can authenticate to a registry and push or pull any images from any Amazon ECR repository. Amazon ECR provides several managed policies to control user access at varying levels</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The ECR repository is stale, you must delete and re-create it</strong> - You can delete a repository when you are done using it, stale is not a concept within ECR. This option has been added as a distractor.</p>\n\n<p><strong>CodeBuild cannot talk to ECR because of security group issues</strong> - A security group acts as a virtual firewall at the instance level and it is not related to pushing Docker images, so this option does not fit the given use-case.</p>\n\n<p><strong>The ECS instances are misconfigured and must contain additional data in /etc/ecs/ecs.config</strong> - The error Authorization is an indication that there is an access issue, therefore you should not look at your configuration first but rather permissions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html\">https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>The IAM permissions are wrong for the CodeBuild service</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can push your Docker or Open Container Initiative (OCI) images to an Amazon ECR repository with the docker push command."
      },
      {
        "answer": "",
        "explanation": "Amazon ECR users require permission to call ecr:GetAuthorizationToken before they can authenticate to a registry and push or pull any images from any Amazon ECR repository. Amazon ECR provides several managed policies to control user access at varying levels"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The ECR repository is stale, you must delete and re-create it</strong> - You can delete a repository when you are done using it, stale is not a concept within ECR. This option has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>CodeBuild cannot talk to ECR because of security group issues</strong> - A security group acts as a virtual firewall at the instance level and it is not related to pushing Docker images, so this option does not fit the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>The ECS instances are misconfigured and must contain additional data in /etc/ecs/ecs.config</strong> - The error Authorization is an indication that there is an access issue, therefore you should not look at your configuration first but rather permissions."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html",
      "https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html"
    ]
  },
  {
    "id": 13,
    "question": "<p>You're a developer for 'Movie Gallery', a company that just migrated to the cloud. A database must be created using NoSQL technology to hold movies that are listed for public viewing. You are taking an important step in designing the database with DynamoDB and need to choose the appropriate partition key.</p>\n\n<p>Which of the following unique attributes satisfies this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p><code>producer_name</code></p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p><code>movie_language</code></p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p><code>lead_actor_name</code></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p><code>movie_id</code></p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>DynamoDB stores data as groups of attributes, known as items. Items are similar to rows or records in other database systems. DynamoDB stores and retrieves each item based on the primary key value, which must be unique. Items are distributed across 10-GB storage units, called partitions (physical storage internal to DynamoDB).</p>\n\n<p>DynamoDB uses the partition key’s value as an input to an internal hash function. The output from the hash function determines the partition in which the item is stored. Each item’s location is determined by the hash value of its partition key.</p>\n\n<p>Please see these details for the DynamoDB Partition Keys:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q11-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q11-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q11-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q11-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a><p></p>\n\n<p><strong><code>movie_id</code></strong></p>\n\n<p>The <code>movie_id</code> attribute has high-cardinality across the entire collection of the movie database, hence it is the most suitable candidate for the partition key in this use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>producer_name</code></strong>  - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)</p>\n\n<p><strong><code>lead_actor_name</code></strong> - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)</p>\n\n<p><strong><code>movie_language</code></strong> - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "DynamoDB stores data as groups of attributes, known as items. Items are similar to rows or records in other database systems. DynamoDB stores and retrieves each item based on the primary key value, which must be unique. Items are distributed across 10-GB storage units, called partitions (physical storage internal to DynamoDB)."
      },
      {
        "answer": "",
        "explanation": "DynamoDB uses the partition key’s value as an input to an internal hash function. The output from the hash function determines the partition in which the item is stored. Each item’s location is determined by the hash value of its partition key."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q11-i1.jpg",
        "answer": "",
        "explanation": "Please see these details for the DynamoDB Partition Keys:"
      },
      {
        "link": "https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/"
      },
      {
        "link": "https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/"
      },
      {
        "answer": "<strong><code>movie_id</code></strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The <code>movie_id</code> attribute has high-cardinality across the entire collection of the movie database, hence it is the most suitable candidate for the partition key in this use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong><code>producer_name</code></strong>  - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)"
      },
      {
        "answer": "",
        "explanation": "<strong><code>lead_actor_name</code></strong> - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)"
      },
      {
        "answer": "",
        "explanation": "<strong><code>movie_language</code></strong> - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)"
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/"
    ]
  },
  {
    "id": 14,
    "question": "<p>An e-commerce application writes log files into Amazon S3. The application also reads these log files in parallel on a near real-time basis. The development team wants to address any data discrepancies that might arise when the application overwrites an existing log file and then tries to read that specific log file.</p>\n\n<p>Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object</strong></p>\n\n<p>Amazon S3 delivers strong read-after-write consistency automatically, without changes to performance or availability, without sacrificing regional isolation for applications, and at no additional cost.</p>\n\n<p>After a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected.</p>\n\n<p>Strong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects.</p>\n\n<p>To summarize, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are strongly consistent. What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data</strong></p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data</strong></p>\n\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data</strong></p>\n\n<p>These three options contradict the earlier details provided in the explanation.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 delivers strong read-after-write consistency automatically, without changes to performance or availability, without sacrificing regional isolation for applications, and at no additional cost."
      },
      {
        "answer": "",
        "explanation": "After a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected."
      },
      {
        "answer": "",
        "explanation": "Strong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects."
      },
      {
        "answer": "",
        "explanation": "To summarize, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are strongly consistent. What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the earlier details provided in the explanation."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel",
      "https://aws.amazon.com/s3/faqs/"
    ]
  },
  {
    "id": 15,
    "question": "<p>Your team-mate has configured an Amazon S3 event notification for an S3 bucket that holds sensitive audit data of a firm. As the Team Lead, you are receiving the SNS notifications for every event in this bucket. After validating the event data, you realized that few events are missing.</p>\n\n<p>What could be the reason for this behavior and how to avoid this in the future?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Someone could have created a new notification configuration and that has overridden your existing configuration</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Your notification action is writing to the same bucket that triggers the notification</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Versioning is enabled on the S3 bucket and event notifications are getting fired for only one version</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent</strong> - Amazon S3 event notifications are designed to be delivered at least once. Typically, event notifications are delivered in seconds but can sometimes take a minute or longer.</p>\n\n<p>If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent. If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket. With versioning, every successful write will create a new version of your object and will also send event notification.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Someone could have created a new notification configuration and that has overridden your existing configuration</strong> - It is possible that the configuration can be overridden. But, in the current scenario, the team lead is receiving notifications for most of the events, which nullifies the claim that the configuration is overridden.</p>\n\n<p><strong>Versioning is enabled on the S3 bucket and event notifications are getting fired for only one version</strong> - This is an incorrect statement. If you want to ensure that an event notification is sent for every successful write, you should enable versioning on your bucket. With versioning, every successful write will create a new version of your object and will also send event notification.</p>\n\n<p><strong>Your notification action is writing to the same bucket that triggers the notification</strong> - If your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. But it will not result in missing events.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent</strong> - Amazon S3 event notifications are designed to be delivered at least once. Typically, event notifications are delivered in seconds but can sometimes take a minute or longer."
      },
      {
        "answer": "",
        "explanation": "If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent. If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket. With versioning, every successful write will create a new version of your object and will also send event notification."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Someone could have created a new notification configuration and that has overridden your existing configuration</strong> - It is possible that the configuration can be overridden. But, in the current scenario, the team lead is receiving notifications for most of the events, which nullifies the claim that the configuration is overridden."
      },
      {
        "answer": "",
        "explanation": "<strong>Versioning is enabled on the S3 bucket and event notifications are getting fired for only one version</strong> - This is an incorrect statement. If you want to ensure that an event notification is sent for every successful write, you should enable versioning on your bucket. With versioning, every successful write will create a new version of your object and will also send event notification."
      },
      {
        "answer": "",
        "explanation": "<strong>Your notification action is writing to the same bucket that triggers the notification</strong> - If your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. But it will not result in missing events."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html"
    ]
  },
  {
    "id": 16,
    "question": "<p>A company has AWS Lambda functions where each is invoked by other AWS services such as Amazon Kinesis Data Firehose, Amazon API Gateway, Amazon Simple Storage Service, or Amazon CloudWatch Events. What these Lambda functions have in common is that they process heavy workloads such as big data analysis, large file processing, and statistical computations.</p>\n\n<p>What should you do to improve the performance of your AWS Lambda functions without changing your code?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Change the instance type for your Lambda function</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Change your Lambda function runtime to use Golang</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Increase the Lambda function timeout</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Increase the RAM assigned to your Lambda function</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Increase the RAM assigned to your Lambda function</strong></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>In the AWS Lambda resource model, you choose the amount of memory you want for your function which allocates proportional CPU power and other resources. This means you will have access to more compute power when you choose one of the new larger settings. To configure the memory for your function, set a value between 128 MB and 10,240 MB in 1-MB increments. At 1,769 MB, a function has the equivalent of one vCPU (one vCPU-second of credits per second). You access these settings when you create a function or update its configuration. The settings are available using the AWS Management Console, AWS CLI, or SDKs.</p>\n\n<p>Therefore, by increasing the amount of memory available to the Lambda functions, you can run the compute-heavy workflows.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the instance type for your Lambda function</strong> - Instance types apply to the EC2 service and not to Lambda function as its a serverless service.</p>\n\n<p><strong>Change your Lambda function runtime to use Golang</strong> - This changes programming language which requires code changes, so this option is not correct. Besides, changing the runtime may not even address the performance issues.</p>\n\n<p><strong>Increase the Lambda function timeout</strong> - This option would increase the amount of time for which the Lambda function executes, which may help in case you have some heavy processing, but won't help with the actual performance of your Lambda function.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Increase the RAM assigned to your Lambda function</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume."
      },
      {
        "answer": "",
        "explanation": "In the AWS Lambda resource model, you choose the amount of memory you want for your function which allocates proportional CPU power and other resources. This means you will have access to more compute power when you choose one of the new larger settings. To configure the memory for your function, set a value between 128 MB and 10,240 MB in 1-MB increments. At 1,769 MB, a function has the equivalent of one vCPU (one vCPU-second of credits per second). You access these settings when you create a function or update its configuration. The settings are available using the AWS Management Console, AWS CLI, or SDKs."
      },
      {
        "answer": "",
        "explanation": "Therefore, by increasing the amount of memory available to the Lambda functions, you can run the compute-heavy workflows."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Change the instance type for your Lambda function</strong> - Instance types apply to the EC2 service and not to Lambda function as its a serverless service."
      },
      {
        "answer": "",
        "explanation": "<strong>Change your Lambda function runtime to use Golang</strong> - This changes programming language which requires code changes, so this option is not correct. Besides, changing the runtime may not even address the performance issues."
      },
      {
        "answer": "",
        "explanation": "<strong>Increase the Lambda function timeout</strong> - This option would increase the amount of time for which the Lambda function executes, which may help in case you have some heavy processing, but won't help with the actual performance of your Lambda function."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html"
    ]
  },
  {
    "id": 17,
    "question": "<p>The development team at an IT company uses CloudFormation to manage its AWS infrastructure. The team has created a network stack containing a VPC with subnets and a web application stack with EC2 instances and an RDS instance. The team wants to reference the VPC created in the network stack into its web application stack.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</strong></p>\n\n<p>AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a><p></p>\n\n<p>You can create a cross-stack reference to export resources from one AWS CloudFormation stack to another. For example, you might have a network stack with a VPC and subnets and a separate public web application stack. To use the security group and subnet from the network stack, you can create a cross-stack reference that allows the web application stack to reference resource outputs from the network stack. With a cross-stack reference, owners of the web application stacks don't need to create or maintain networking rules or assets.</p>\n\n<p>To create a cross-stack reference, use the Export output field to flag the value of a resource output for export. Then, use the Fn::ImportValue intrinsic function to import the value.</p>\n\n<p>You cannot use the Ref intrinsic function to import the value.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q3-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q3-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</strong></p>\n\n<p><strong>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</strong></p>\n\n<p><strong>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these options are not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png",
        "answer": "",
        "explanation": "How CloudFormation Works:"
      },
      {
        "link": "https://aws.amazon.com/cloudformation/"
      },
      {
        "answer": "",
        "explanation": "You can create a cross-stack reference to export resources from one AWS CloudFormation stack to another. For example, you might have a network stack with a VPC and subnets and a separate public web application stack. To use the security group and subnet from the network stack, you can create a cross-stack reference that allows the web application stack to reference resource outputs from the network stack. With a cross-stack reference, owners of the web application stacks don't need to create or maintain networking rules or assets."
      },
      {
        "answer": "",
        "explanation": "To create a cross-stack reference, use the Export output field to flag the value of a resource output for export. Then, use the Fn::ImportValue intrinsic function to import the value."
      },
      {
        "answer": "",
        "explanation": "You cannot use the Ref intrinsic function to import the value."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the details provided in the explanation above, so these options are not correct."
      }
    ],
    "references": [
      "https://aws.amazon.com/cloudformation/",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html"
    ]
  },
  {
    "id": 18,
    "question": "<p>A multi-national company maintains separate AWS accounts for different verticals in their organization. The project manager of a team wants to migrate the Elastic Beanstalk environment from Team A's AWS account into Team B's AWS account. As a Developer, you have been roped in to help him in this process.</p>\n\n<p>Which of the following will you suggest?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a saved configuration in Team A's account and configure it to Export. Now, log into Team B's account and choose the Import option. Here, you need to specify the name of the saved configuration and allow the system to create the new application. This takes a little time based on the Regions the two accounts belong to</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a saved configuration in Team A's account and download it to your local machine. Make the account-specific parameter changes and upload to the S3 bucket in Team B's account. From Elastic Beanstalk console, create an application from 'Saved Configurations'</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>It is not possible to migrate Elastic Beanstalk environment from one AWS account to the other</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an export configuration from the Elastic Beanstalk console from Team A's account. This configuration has to be shared with the IAM Role of Team B's account. The import option of Team B's account will show the saved configuration, that can be used to create a new Beanstalk application</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a saved configuration in Team A's account and download it to your local machine. Make the account-specific parameter changes and upload to the S3 bucket in Team B's account. From Elastic Beanstalk console, create an application from 'Saved Configurations</strong> - You must use saved configurations to migrate an Elastic Beanstalk environment between AWS accounts.\nYou can save your environment's configuration as an object in Amazon Simple Storage Service (Amazon S3) that can be applied to other environments during environment creation, or applied to a running environment. Saved configurations are YAML formatted templates that define an environment's platform version, tier, configuration option settings, and tags.</p>\n\n<p>Download the saved configuration to your local machine. Change your account-specific parameters in the downloaded configuration file, and then save the changes. For example, change the key pair name, subnet ID, or application name (such as application-b-name). Upload the saved configuration from your local machine to an S3 bucket in Team B's account. From this account, create a new Beanstalk application by choosing 'Saved Configurations' from the navigation panel.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a saved configuration in Team A's account and configure it to Export. Now, log into Team B's account and choose the Import option. Here, you need to specify the name of the saved configuration and allow the system to create the new application. This takes a little time based on the Regions the two accounts belong to</strong> - There is no direct Export and Import\noption for migrating Elastic Beanstalk configurations.</p>\n\n<p><strong>It is not possible to migrate Elastic Beanstalk environment from one AWS account to the other</strong> - This is an incorrect statement.</p>\n\n<p><strong>Create an export configuration from the Elastic Beanstalk console from Team A's account. This configuration has to be shared with the IAM Role of Team B's account. The import option of the Team B's account will show the saved configuration, that can be used to create a new Beanstalk application</strong> - This contradicts the explanation provided earlier.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-migration-accounts/\">https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-migration-accounts/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-savedconfig.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-savedconfig.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a saved configuration in Team A's account and download it to your local machine. Make the account-specific parameter changes and upload to the S3 bucket in Team B's account. From Elastic Beanstalk console, create an application from 'Saved Configurations</strong> - You must use saved configurations to migrate an Elastic Beanstalk environment between AWS accounts.\nYou can save your environment's configuration as an object in Amazon Simple Storage Service (Amazon S3) that can be applied to other environments during environment creation, or applied to a running environment. Saved configurations are YAML formatted templates that define an environment's platform version, tier, configuration option settings, and tags."
      },
      {
        "answer": "",
        "explanation": "Download the saved configuration to your local machine. Change your account-specific parameters in the downloaded configuration file, and then save the changes. For example, change the key pair name, subnet ID, or application name (such as application-b-name). Upload the saved configuration from your local machine to an S3 bucket in Team B's account. From this account, create a new Beanstalk application by choosing 'Saved Configurations' from the navigation panel."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a saved configuration in Team A's account and configure it to Export. Now, log into Team B's account and choose the Import option. Here, you need to specify the name of the saved configuration and allow the system to create the new application. This takes a little time based on the Regions the two accounts belong to</strong> - There is no direct Export and Import\noption for migrating Elastic Beanstalk configurations."
      },
      {
        "answer": "",
        "explanation": "<strong>It is not possible to migrate Elastic Beanstalk environment from one AWS account to the other</strong> - This is an incorrect statement."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an export configuration from the Elastic Beanstalk console from Team A's account. This configuration has to be shared with the IAM Role of Team B's account. The import option of the Team B's account will show the saved configuration, that can be used to create a new Beanstalk application</strong> - This contradicts the explanation provided earlier."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-migration-accounts/",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-savedconfig.html"
    ]
  },
  {
    "id": 19,
    "question": "<p>An AWS CodePipeline was configured to be triggered by Amazon CloudWatch Events. Recently the pipeline failed and upon investigation, the Team Lead noticed that the source was changed from AWS CodeCommit to Amazon Simple Storage Service (S3). The Team Lead has requested you to find the user who had made the changes.</p>\n\n<p>Which service will help you solve this?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS CloudTrail</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon CloudWatch</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS X-Ray</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Inspector</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS CloudTrail</strong></p>\n\n<p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides an event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services.</p>\n\n<p>AWS CloudTrail increases visibility into your user and resource activity by recording AWS Management Console actions and API calls. You can identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred.</p>\n\n<p>How CloudTrail works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q25-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q25-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon CloudWatch</strong> - Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. CloudWatch can collect numbers and respond to AWS service-related events, but it does not help in user activity logging.</p>\n\n<p><strong>AWS X-Ray</strong> - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray is a very important tool in troubleshooting but is not useful in logging user activity.</p>\n\n<p><strong>Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. This does not log User activity at the account level.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/\">https://aws.amazon.com/cloudwatch/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>AWS CloudTrail</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides an event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services."
      },
      {
        "answer": "",
        "explanation": "AWS CloudTrail increases visibility into your user and resource activity by recording AWS Management Console actions and API calls. You can identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q25-i1.jpg",
        "answer": "",
        "explanation": "How CloudTrail works:"
      },
      {
        "link": "https://aws.amazon.com/cloudtrail/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon CloudWatch</strong> - Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. CloudWatch can collect numbers and respond to AWS service-related events, but it does not help in user activity logging."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS X-Ray</strong> - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray is a very important tool in troubleshooting but is not useful in logging user activity."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. This does not log User activity at the account level."
      }
    ],
    "references": [
      "https://aws.amazon.com/cloudtrail/",
      "https://aws.amazon.com/xray/",
      "https://aws.amazon.com/inspector/",
      "https://aws.amazon.com/cloudwatch/"
    ]
  },
  {
    "id": 20,
    "question": "<p>A company wants to automate the creation of ECS clusters using CloudFormation. The process has worked for a while, but after creating task definitions and assigning roles, the development team discovers that the tasks for containers are not using the permissions assigned to them.</p>\n\n<p>Which ECS config must be set in <code>/etc/ecs/ecs.config</code> to allow ECS tasks to use IAM roles?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p><code>ECS_ENABLE_TASK_IAM_ROLE</code></p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p><code>ECS_CLUSTER</code></p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p><code>ECS_AVAILABLE_LOGGING_DRIVERS</code></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p><code>ECS_ENGINE_AUTH_DATA</code></p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong><code>ECS_ENABLE_TASK_IAM_ROLE</code></strong></p>\n\n<p>This configuration item is used to enable IAM roles for tasks for containers with the bridge and default network modes.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>ECS_ENGINE_AUTH_DATA</code></strong> - This refers to the authentication data within a Docker configuration file, so this is not the correct option.</p>\n\n<p><strong><code>ECS_AVAILABLE_LOGGING_DRIVERS</code></strong> - The Amazon ECS container agent running on a container instance must register the logging drivers available on that instance with this variable. This configuration item refers to the logging driver.</p>\n\n<p><strong><code>ECS_CLUSTER</code></strong> - This refers to the ECS cluster that the ECS agent should check into. This is passed to the container instance at launch through Amazon EC2 user data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong><code>ECS_ENABLE_TASK_IAM_ROLE</code></strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This configuration item is used to enable IAM roles for tasks for containers with the bridge and default network modes."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong><code>ECS_ENGINE_AUTH_DATA</code></strong> - This refers to the authentication data within a Docker configuration file, so this is not the correct option."
      },
      {
        "answer": "",
        "explanation": "<strong><code>ECS_AVAILABLE_LOGGING_DRIVERS</code></strong> - The Amazon ECS container agent running on a container instance must register the logging drivers available on that instance with this variable. This configuration item refers to the logging driver."
      },
      {
        "answer": "",
        "explanation": "<strong><code>ECS_CLUSTER</code></strong> - This refers to the ECS cluster that the ECS agent should check into. This is passed to the container instance at launch through Amazon EC2 user data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html"
    ]
  },
  {
    "id": 21,
    "question": "<p>A media application uses Amazon CloudFront distribution to distribute static content configured on an Amazon S3 bucket. The application is used across different countries and various AWS Regions. Some regions have been experiencing latency when there is a cache miss on CloudFront.</p>\n\n<p>Which of the following configuration changes will you suggest to decrease latency and improve user performance by redirecting requests on cache misses to the S3 bucket in the Region that is nearest to the user's country?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a Lambda@Edge function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the Lambda@Edge function with the distribution's viewer request event</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a CloudFront function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the CloudFront function with the distribution's viewer request event</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a CloudFront function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the CloudFront function with the distribution's origin request event</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a Lambda@Edge function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the Lambda@Edge function with the distribution's origin request event</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a Lambda@Edge function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the Lambda@Edge function with the distribution's origin request event</strong></p>\n\n<p>When a viewer request to CloudFront results in a cache miss (the requested object is not cached at the edge location), CloudFront sends a request to the origin to retrieve the object. This is called an origin request. The origin request always includes the following information from the viewer request:</p>\n\n<p>The URL path (the path only, without URL query strings or the domain name)</p>\n\n<p>The request body (if there is one)</p>\n\n<p>The HTTP headers that CloudFront automatically includes in every origin request, including Host, User-Agent, and X-Amz-Cf-Id</p>\n\n<p>The example below shows a Lambda@Edge function with a handler that is used to change response based on user country:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q32-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q32-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-content-based-S3-origin-request-trigger\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-content-based-S3-origin-request-trigger</a><p></p>\n\n<p>Choosing between CloudFront Functions and Lambda@Edge:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q32-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q32-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html</a><p></p>\n\n<p>We use the value of the CloudFront-Viewer-Country header to update the S3 bucket domain name to a bucket in a Region that is closer to the viewer. This can be useful in several ways:</p>\n\n<ol>\n<li><p>It reduces latencies when the Region specified is nearer to the viewer's country.</p></li>\n<li><p>It provides data sovereignty by making sure that data is served from an origin that's in the same country that the request came from.</p></li>\n</ol>\n\n<p>So, for the given use case, we need to leverage the origin request event (to address the cache miss) and trigger the Lambda@Edge function to redirect the requests on cache miss to the S3 bucket nearest to the user country.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a Lambda@Edge function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the Lambda@Edge function with the distribution's viewer request event</strong></p>\n\n<p>Consider the following events:</p>\n\n<p>Viewer request event: The function executes when CloudFront receives a request from a viewer before it checks to see whether the requested object is in the CloudFront cache.</p>\n\n<p>Origin request event: The function executes only when CloudFront forwards a request to your origin. When the requested object is in the CloudFront cache, the function doesn't execute.</p>\n\n<p>If you use the viewer request event, the function would be triggered on all user requests rather than only when you have a cache miss. So, this option is incorrect.</p>\n\n<p><strong>Redirect requests on cache misses to the Amazon S3 bucket nearest to the user country. Create a CloudFront function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the CloudFront function with the distribution's viewer request event</strong></p>\n\n<p>With CloudFront Functions in Amazon CloudFront, you can write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. Your functions can manipulate the requests and responses that flow through CloudFront, perform basic authentication and authorization, generate HTTP responses at the edge, and more.</p>\n\n<p>When you associate a CloudFront function with a CloudFront distribution, CloudFront intercepts requests and responses at CloudFront edge locations and passes them to your function. You can invoke CloudFront functions when the following events occur:</p>\n\n<ol>\n<li><p>When CloudFront receives a request from a viewer (viewer request): The function executes when CloudFront receives a request from a viewer before it checks to see whether the requested object is in the CloudFront cache.</p></li>\n<li><p>Before CloudFront returns the response to the viewer (viewer response): The function executes before returning the requested file to the viewer. Note that the function executes regardless of whether the file is already in the CloudFront cache.</p></li>\n</ol>\n\n<p>If you use the viewer request event, the function would be triggered on all user requests rather than only when you have a cache miss. So, this option is incorrect.</p>\n\n<p><strong>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a CloudFront function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the CloudFront function with the distribution's origin request event</strong> - This option is incorrect. You can invoke CloudFront functions for only two events: When CloudFront receives a request from a viewer (viewer request) and Before CloudFront returns the response to the viewer (viewer response). Origin request and Origin response events are not supported for the CloudFront function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-redirecting-examples\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-redirecting-examples</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-cloudfront-trigger-events.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-cloudfront-trigger-events.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/controlling-origin-requests.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/controlling-origin-requests.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a Lambda@Edge function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the Lambda@Edge function with the distribution's origin request event</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "When a viewer request to CloudFront results in a cache miss (the requested object is not cached at the edge location), CloudFront sends a request to the origin to retrieve the object. This is called an origin request. The origin request always includes the following information from the viewer request:"
      },
      {
        "answer": "",
        "explanation": "The URL path (the path only, without URL query strings or the domain name)"
      },
      {
        "answer": "",
        "explanation": "The request body (if there is one)"
      },
      {
        "answer": "",
        "explanation": "The HTTP headers that CloudFront automatically includes in every origin request, including Host, User-Agent, and X-Amz-Cf-Id"
      },
      {
        "answer": "",
        "explanation": "The example below shows a Lambda@Edge function with a handler that is used to change response based on user country:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-content-based-S3-origin-request-trigger"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q32-i2.jpg",
        "answer": "",
        "explanation": "Choosing between CloudFront Functions and Lambda@Edge:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html"
      },
      {
        "answer": "",
        "explanation": "We use the value of the CloudFront-Viewer-Country header to update the S3 bucket domain name to a bucket in a Region that is closer to the viewer. This can be useful in several ways:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li><p>It reduces latencies when the Region specified is nearer to the viewer's country.</p></li>\n<li><p>It provides data sovereignty by making sure that data is served from an origin that's in the same country that the request came from.</p></li>\n</ol>"
      },
      {
        "answer": "",
        "explanation": "So, for the given use case, we need to leverage the origin request event (to address the cache miss) and trigger the Lambda@Edge function to redirect the requests on cache miss to the S3 bucket nearest to the user country."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a Lambda@Edge function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the Lambda@Edge function with the distribution's viewer request event</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Consider the following events:"
      },
      {
        "answer": "",
        "explanation": "Viewer request event: The function executes when CloudFront receives a request from a viewer before it checks to see whether the requested object is in the CloudFront cache."
      },
      {
        "answer": "",
        "explanation": "Origin request event: The function executes only when CloudFront forwards a request to your origin. When the requested object is in the CloudFront cache, the function doesn't execute."
      },
      {
        "answer": "",
        "explanation": "If you use the viewer request event, the function would be triggered on all user requests rather than only when you have a cache miss. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Redirect requests on cache misses to the Amazon S3 bucket nearest to the user country. Create a CloudFront function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the CloudFront function with the distribution's viewer request event</strong>"
      },
      {
        "answer": "",
        "explanation": "With CloudFront Functions in Amazon CloudFront, you can write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. Your functions can manipulate the requests and responses that flow through CloudFront, perform basic authentication and authorization, generate HTTP responses at the edge, and more."
      },
      {
        "answer": "",
        "explanation": "When you associate a CloudFront function with a CloudFront distribution, CloudFront intercepts requests and responses at CloudFront edge locations and passes them to your function. You can invoke CloudFront functions when the following events occur:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li><p>When CloudFront receives a request from a viewer (viewer request): The function executes when CloudFront receives a request from a viewer before it checks to see whether the requested object is in the CloudFront cache.</p></li>\n<li><p>Before CloudFront returns the response to the viewer (viewer response): The function executes before returning the requested file to the viewer. Note that the function executes regardless of whether the file is already in the CloudFront cache.</p></li>\n</ol>"
      },
      {
        "answer": "",
        "explanation": "If you use the viewer request event, the function would be triggered on all user requests rather than only when you have a cache miss. So, this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Redirect requests on cache misses to the S3 bucket nearest to the user country. Create a CloudFront function to redirect requests based on the value of the CloudFront-Viewer-Country header. Associate the CloudFront function with the distribution's origin request event</strong> - This option is incorrect. You can invoke CloudFront functions for only two events: When CloudFront receives a request from a viewer (viewer request) and Before CloudFront returns the response to the viewer (viewer response). Origin request and Origin response events are not supported for the CloudFront function."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-content-based-S3-origin-request-trigger",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-redirecting-examples",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-cloudfront-trigger-events.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/controlling-origin-requests.html"
    ]
  },
  {
    "id": 22,
    "question": "<p>As an AWS Certified Developer Associate, you are writing a CloudFormation template in YAML. The template consists of an EC2 instance creation and one RDS resource. Once your resources are created you would like to output the connection endpoint for the RDS database.</p>\n\n<p>Which intrinsic function returns the value needed?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p><code>!FindInMap</code></p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p><code>!GetAtt</code></p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p><code>!Ref</code></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p><code>!Sub</code></p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation provides several built-in functions that help you manage your stacks. Intrinsic functions are used in templates to assign values to properties that are not available until runtime.</p>\n\n<p><strong><code>!GetAtt</code></strong> - The Fn::GetAtt intrinsic function returns the value of an attribute from a resource in the template. This example snippet returns a string containing the DNS name of the load balancer with the logical name myELB -\nYML :   !GetAtt myELB.DNSName\nJSON :   \"Fn::GetAtt\" : [ \"myELB\" , \"DNSName\" ]</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>!Sub</code></strong> - The intrinsic function Fn::Sub substitutes variables in an input string with values that you specify. In your templates, you can use this function to construct commands or outputs that include values that aren't available until you create or update a stack.</p>\n\n<p><strong><code>!Ref</code></strong> - The intrinsic function Ref returns the value of the specified parameter or resource.</p>\n\n<p><strong><code>!FindInMap</code></strong> - The intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that is declared in the Mappings section. For example, you can use this in the Mappings section that contains a single map, RegionMap, that associates AMIs with AWS regions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS CloudFormation provides several built-in functions that help you manage your stacks. Intrinsic functions are used in templates to assign values to properties that are not available until runtime."
      },
      {
        "answer": "",
        "explanation": "<strong><code>!GetAtt</code></strong> - The Fn::GetAtt intrinsic function returns the value of an attribute from a resource in the template. This example snippet returns a string containing the DNS name of the load balancer with the logical name myELB -\nYML :   !GetAtt myELB.DNSName\nJSON :   \"Fn::GetAtt\" : [ \"myELB\" , \"DNSName\" ]"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong><code>!Sub</code></strong> - The intrinsic function Fn::Sub substitutes variables in an input string with values that you specify. In your templates, you can use this function to construct commands or outputs that include values that aren't available until you create or update a stack."
      },
      {
        "answer": "",
        "explanation": "<strong><code>!Ref</code></strong> - The intrinsic function Ref returns the value of the specified parameter or resource."
      },
      {
        "answer": "",
        "explanation": "<strong><code>!FindInMap</code></strong> - The intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that is declared in the Mappings section. For example, you can use this in the Mappings section that contains a single map, RegionMap, that associates AMIs with AWS regions."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html"
    ]
  },
  {
    "id": 23,
    "question": "<p>The development team at an e-commerce company wants to run a serverless data store service on two docker containers that share resources.</p>\n\n<p>Which of the following ECS configurations can be used to facilitate this use-case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Put the two containers into a single task definition using an EC2 Launch Type</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Put the two containers into a single task definition using a Fargate Launch Type</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Put the two containers into two separate task definitions using an EC2 Launch Type</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Put the two containers into two separate task definitions using a Fargate Launch Type</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Put the two containers into a single task definition using a Fargate Launch Type</strong></p>\n\n<p>Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control over your infrastructure, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.</p>\n\n<p><img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/ecs/\">https://aws.amazon.com/ecs/</a><p></p>\n\n<p>As the development team is looking for a serverless data store service, therefore the two containers should be launched into a single task definition using a Fargate Launch Type. Using a single task definition allows the two containers to share resources. Please see these use-cases for Fargate Launch type when you should put multiple containers into the same task definition:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q2-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q2-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html</a><p></p>\n\n<p>For a deep-dive on understanding how Amazon ECS manages CPU and memory resources, please review this excellent blog-\n<a href=\"https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/\">https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Put the two containers into two separate task definitions using a Fargate Launch Type</strong> - This option contradicts the details provided in the explanation above, so this option is ruled out.</p>\n\n<p><strong>Put the two containers into two separate task definitions using an EC2 Launch Type</strong></p>\n\n<p><strong>Put the two containers into a single task definition using an EC2 Launch Type</strong></p>\n\n<p>As the development team is looking for a serverless data store service, therefore EC2 Launch Type is ruled out. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/\">https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Put the two containers into a single task definition using a Fargate Launch Type</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control over your infrastructure, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type."
      },
      {
        "link": "https://aws.amazon.com/ecs/"
      },
      {
        "answer": "",
        "explanation": "As the development team is looking for a serverless data store service, therefore the two containers should be launched into a single task definition using a Fargate Launch Type. Using a single task definition allows the two containers to share resources. Please see these use-cases for Fargate Launch type when you should put multiple containers into the same task definition:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html"
      },
      {
        "link": "https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/",
        "answer": "",
        "explanation": "For a deep-dive on understanding how Amazon ECS manages CPU and memory resources, please review this excellent blog-\n<a href=\"https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/\">https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/</a>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Put the two containers into two separate task definitions using a Fargate Launch Type</strong> - This option contradicts the details provided in the explanation above, so this option is ruled out."
      },
      {
        "answer": "<strong>Put the two containers into two separate task definitions using an EC2 Launch Type</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Put the two containers into a single task definition using an EC2 Launch Type</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As the development team is looking for a serverless data store service, therefore EC2 Launch Type is ruled out. So both these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/ecs/",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html",
      "https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/"
    ]
  },
  {
    "id": 24,
    "question": "<p>A development team has created AWS CloudFormation templates that are reusable by taking advantage of input parameters to name resources based on client names.</p>\n\n<p>You would like to save your templates on the cloud, which storage option should you choose?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>S3</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>ECR</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>EFS</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>EBS</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>S3</strong></p>\n\n<p>If you upload a local template file, AWS CloudFormation uploads it to an Amazon Simple Storage Service (Amazon S3) bucket in your AWS account. If you don't already have an S3 bucket that was created by AWS CloudFormation, it creates a unique bucket for each region in which you upload a template file. If you already have an S3 bucket that was created by AWS CloudFormation in your AWS account, AWS CloudFormation adds the template to that bucket.</p>\n\n<p>Selecting a stack template for CloudFormation:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q56-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q56-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EBS</strong> - An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. Amazon EBS is a recommended storage option when data must be quickly accessible and requires long-term persistence. EBS cannot be used for selecting a stack template for CloudFormation.</p>\n\n<p><strong>EFS</strong> - EFS is a file storage service where you mount the file system on an Amazon EC2 Linux-based instance which is not an option for CloudFormation.</p>\n\n<p><strong>ECR</strong> - Amazon ECR eliminates the need to operate your container repositories or worry about scaling the underlying infrastructure which does not apply to CloudFormation.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>S3</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "If you upload a local template file, AWS CloudFormation uploads it to an Amazon Simple Storage Service (Amazon S3) bucket in your AWS account. If you don't already have an S3 bucket that was created by AWS CloudFormation, it creates a unique bucket for each region in which you upload a template file. If you already have an S3 bucket that was created by AWS CloudFormation in your AWS account, AWS CloudFormation adds the template to that bucket."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q56-i1.jpg",
        "answer": "",
        "explanation": "Selecting a stack template for CloudFormation:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>EBS</strong> - An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. Amazon EBS is a recommended storage option when data must be quickly accessible and requires long-term persistence. EBS cannot be used for selecting a stack template for CloudFormation."
      },
      {
        "answer": "",
        "explanation": "<strong>EFS</strong> - EFS is a file storage service where you mount the file system on an Amazon EC2 Linux-based instance which is not an option for CloudFormation."
      },
      {
        "answer": "",
        "explanation": "<strong>ECR</strong> - Amazon ECR eliminates the need to operate your container repositories or worry about scaling the underlying infrastructure which does not apply to CloudFormation."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html"
    ]
  },
  {
    "id": 25,
    "question": "<p>As a site reliability engineer, you work on building and running large-scale, distributed, fault-tolerant systems in the cloud using automation. You have just replaced the company's Jenkins based CI/CD platform with AWS CodeBuild and would like to programmatically define your build steps.</p>\n\n<p>Which of the following options should you choose?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Define a <code>buildspec.yml</code> file in the codebuild/ directory</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Define an <code>appspec.yml</code> file in the codebuild/ directory</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Define a <code>buildspec.yml</code> file in the root directory</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Define an <code>appspec.yml</code> file in the root directory</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Define a <code>buildspec.yml</code> file in the root directory</strong></p>\n\n<p>AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.</p>\n\n<p>A build spec is a collection of build commands and related settings, in YAML format, that AWS CodeBuild uses to run a build. You can include a build spec as part of the source code or you can define a build spec when you create a build project.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q37-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q37-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Define an <code>appspec.yml</code> file in the root directory</strong> - The AppSpec file is used for deployment in the CodeDeploy service.</p>\n\n<p><strong>Define a <code>buildspec.yml</code> file in the codebuild/ directory</strong> - The file is correct but must be in the root directory.</p>\n\n<p><strong>Define an <code>appspec.yml</code> file in the codebuild/ directory</strong> - The AppSpec file is used for deployment in the CodeDeploy service.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Define a <code>buildspec.yml</code> file in the root directory</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications."
      },
      {
        "answer": "",
        "explanation": "A build spec is a collection of build commands and related settings, in YAML format, that AWS CodeBuild uses to run a build. You can include a build spec as part of the source code or you can define a build spec when you create a build project."
      },
      {
        "link": "https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Define an <code>appspec.yml</code> file in the root directory</strong> - The AppSpec file is used for deployment in the CodeDeploy service."
      },
      {
        "answer": "",
        "explanation": "<strong>Define a <code>buildspec.yml</code> file in the codebuild/ directory</strong> - The file is correct but must be in the root directory."
      },
      {
        "answer": "",
        "explanation": "<strong>Define an <code>appspec.yml</code> file in the codebuild/ directory</strong> - The AppSpec file is used for deployment in the CodeDeploy service."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html"
    ]
  },
  {
    "id": 26,
    "question": "<p>You are a DynamoDB developer for an aerospace company that requires you to write 6 objects per second of 4.5KB in size each.</p>\n\n<p>What write capacity unit is needed for your project?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>30</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>24</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>46</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>15</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>Before proceeding with the calculations, please review the following:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q58-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q58-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q58-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q58-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p><strong>30</strong></p>\n\n<p>A write capacity unit represents one write per second, for an item up to 1 KB in size.</p>\n\n<p>Item sizes for writes are rounded up to the next 1 KB multiple. For example, writing a 500-byte item consumes the same throughput as writing a 1 KB item. So, for the given use-case, each object is of size 4.5 KB, which will be rounded up to 5KB.</p>\n\n<p>Therefore, for 6 objects, you need 6x5 = 30 WCUs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>24</strong></p>\n\n<p><strong>15</strong></p>\n\n<p><strong>46</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Before proceeding with the calculations, please review the following:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html",
        "answer": "",
        "explanation": "via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a>"
      },
      {
        "answer": "<strong>30</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A write capacity unit represents one write per second, for an item up to 1 KB in size."
      },
      {
        "answer": "",
        "explanation": "Item sizes for writes are rounded up to the next 1 KB multiple. For example, writing a 500-byte item consumes the same throughput as writing a 1 KB item. So, for the given use-case, each object is of size 4.5 KB, which will be rounded up to 5KB."
      },
      {
        "answer": "",
        "explanation": "Therefore, for 6 objects, you need 6x5 = 30 WCUs."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>24</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>15</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>46</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the details provided in the explanation above, so these are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html"
    ]
  },
  {
    "id": 27,
    "question": "<p>Your web application front end consists of 5 EC2 instances behind an Application Load Balancer. You have configured your web application to capture the IP address of the client making requests. When viewing the data captured you notice that every IP address being captured is the same, which also happens to be the IP address of the Application Load Balancer.</p>\n\n<p>What should you do to identify the true IP address of the client?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Look into the X-Forwarded-For header in the backend</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Look into the X-Forwarded-Proto header in the backend</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Modify the front-end of the website so that the users send their IP in the requests</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Look into the client's cookie</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Look into the X-Forwarded-For header in the backend</strong></p>\n\n<p>The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Modify the front-end of the website so that the users send their IP in the requests</strong> - When a user makes a request the IP address is sent with the request to the server and the load balancer intercepts it. There is no need to modify the application.</p>\n\n<p><strong>Look into the X-Forwarded-Proto header in the backend</strong> - The X-Forwarded-Proto request header helps you identify the protocol (HTTP or HTTPS) that a client used to connect to your load balancer.</p>\n\n<p><strong>Look into the client's cookie</strong> - For this, we would need to modify the client-side logic and server-side logic, which would not be efficient.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Look into the X-Forwarded-For header in the backend</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Modify the front-end of the website so that the users send their IP in the requests</strong> - When a user makes a request the IP address is sent with the request to the server and the load balancer intercepts it. There is no need to modify the application."
      },
      {
        "answer": "",
        "explanation": "<strong>Look into the X-Forwarded-Proto header in the backend</strong> - The X-Forwarded-Proto request header helps you identify the protocol (HTTP or HTTPS) that a client used to connect to your load balancer."
      },
      {
        "answer": "",
        "explanation": "<strong>Look into the client's cookie</strong> - For this, we would need to modify the client-side logic and server-side logic, which would not be efficient."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html"
    ]
  },
  {
    "id": 28,
    "question": "<p>A company has configured an Auto Scaling group with health checks. The configuration is set to the desired capacity value of 3 and maximum capacity value of 3. The EC2 instances of your Auto Scaling group are configured to scale when CPU utilization is at 60 percent and is now running at 80 percent utilization.</p>\n\n<p>Which of the following will take place?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The desired capacity will go up to 4 and the maximum capacity will also go up to 4</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>System will keep running as is</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>System will trigger CloudWatch alarms to AWS support</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The desired capacity will go up to 4 and the maximum capacity will stay at 3</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>System will keep running as is</strong></p>\n\n<p>You are already running at max capacity. After you have created your Auto Scaling group, the Auto Scaling group starts by launching enough EC2 instances to meet its minimum capacity (or its desired capacity, if specified). If there are no other scaling conditions attached to the Auto Scaling group, the Auto Scaling group maintains this number of running instances even if an instance becomes unhealthy.</p>\n\n<p>Setting Capacity Limits for Your Auto Scaling Group:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q34-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q34-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The desired capacity will go up to 4 and the maximum capacity will stay at 3</strong> - The desired capacity cannot go over the maximum capacity.</p>\n\n<p><strong>The desired capacity will go up to 4 and the maximum capacity will also go up to 4</strong> - The maximum capacity cannot change on its own just because the desired capacity has been set to a higher value. You will have to make those changes to the maximum capacity manually.</p>\n\n<p><strong>System will trigger CloudWatch alarms to AWS support</strong> - This option has been added as a distractor. You already have alarms configured based on rules but AWS support will not intervene for you.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>System will keep running as is</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You are already running at max capacity. After you have created your Auto Scaling group, the Auto Scaling group starts by launching enough EC2 instances to meet its minimum capacity (or its desired capacity, if specified). If there are no other scaling conditions attached to the Auto Scaling group, the Auto Scaling group maintains this number of running instances even if an instance becomes unhealthy."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q34-i1.jpg",
        "answer": "",
        "explanation": "Setting Capacity Limits for Your Auto Scaling Group:"
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The desired capacity will go up to 4 and the maximum capacity will stay at 3</strong> - The desired capacity cannot go over the maximum capacity."
      },
      {
        "answer": "",
        "explanation": "<strong>The desired capacity will go up to 4 and the maximum capacity will also go up to 4</strong> - The maximum capacity cannot change on its own just because the desired capacity has been set to a higher value. You will have to make those changes to the maximum capacity manually."
      },
      {
        "answer": "",
        "explanation": "<strong>System will trigger CloudWatch alarms to AWS support</strong> - This option has been added as a distractor. You already have alarms configured based on rules but AWS support will not intervene for you."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html"
    ]
  },
  {
    "id": 29,
    "question": "<p>A Company uses a large set of EBS volumes for their fleet of Amazon EC2 instances. As an AWS Certified Developer Associate, your help has been requested to understand the security features of the EBS volumes. The company does not want to build or maintain their own encryption key management infrastructure.</p>\n\n<p>Can you help them understand what works for Amazon EBS encryption? (Select two)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A snapshot of an encrypted volume can be encrypted or unencrypted</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>You can encrypt an existing unencrypted volume or snapshot by using AWS Key Management Service (KMS) AWS SDKs</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Encryption by default is an AZ specific setting. If you enable it for an AZ, you cannot disable it for individual volumes or snapshots in that AZ</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>A volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region</strong> - You can configure your AWS account to enforce the encryption of the new EBS volumes and snapshot copies that you create. Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q23-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q23-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a><p></p>\n\n<p><strong>A volume restored from an encrypted snapshot, or a copy of an encrypted snapshot, is always encrypted</strong> - By default, the CMK that you selected when creating a volume encrypts the snapshots that you make from the volume and the volumes that you restore from those encrypted snapshots. You cannot remove encryption from an encrypted volume or snapshot, which means that a volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q23-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q23-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can encrypt an existing unencrypted volume or snapshot by using AWS Key Management Service (KMS) AWS SDKs</strong> - This is an incorrect statement. There is no direct way to encrypt an existing unencrypted volume or snapshot. You can encrypt an unencrypted snapshot by copying and enabling encryption while copying the snapshot. To encrypt an EBS volume, you need to create a snapshot and then encrypt the snapshot as described earlier. From this new encrypted snapshot, you can then create an encrypted volume.</p>\n\n<p><strong>A snapshot of an encrypted volume can be encrypted or unencrypted</strong> - This is an incorrect statement. You cannot remove encryption from an encrypted volume or snapshot, which means that a volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted.</p>\n\n<p><strong>Encryption by default is an AZ specific setting. If you enable it for an AZ, you cannot disable it for individual volumes or snapshots in that AZ</strong> - This is an incorrect statement. Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encrypt-unencrypted\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encrypt-unencrypted</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region</strong> - You can configure your AWS account to enforce the encryption of the new EBS volumes and snapshot copies that you create. Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html"
      },
      {
        "answer": "",
        "explanation": "<strong>A volume restored from an encrypted snapshot, or a copy of an encrypted snapshot, is always encrypted</strong> - By default, the CMK that you selected when creating a volume encrypts the snapshots that you make from the volume and the volumes that you restore from those encrypted snapshots. You cannot remove encryption from an encrypted volume or snapshot, which means that a volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You can encrypt an existing unencrypted volume or snapshot by using AWS Key Management Service (KMS) AWS SDKs</strong> - This is an incorrect statement. There is no direct way to encrypt an existing unencrypted volume or snapshot. You can encrypt an unencrypted snapshot by copying and enabling encryption while copying the snapshot. To encrypt an EBS volume, you need to create a snapshot and then encrypt the snapshot as described earlier. From this new encrypted snapshot, you can then create an encrypted volume."
      },
      {
        "answer": "",
        "explanation": "<strong>A snapshot of an encrypted volume can be encrypted or unencrypted</strong> - This is an incorrect statement. You cannot remove encryption from an encrypted volume or snapshot, which means that a volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted."
      },
      {
        "answer": "",
        "explanation": "<strong>Encryption by default is an AZ specific setting. If you enable it for an AZ, you cannot disable it for individual volumes or snapshots in that AZ</strong> - This is an incorrect statement. Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encrypt-unencrypted"
    ]
  },
  {
    "id": 30,
    "question": "<p>The development team at a health-care company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon RDS as the database tier for its flagship application.</p>\n\n<p>Which of the following would you identify as correct for RDS Multi-AZ? (Select two)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary, and finally performing maintenance on the old primary, which becomes the new standby</strong></p>\n\n<p>Running a DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event because Amazon RDS applies operating system updates by following these steps:</p>\n\n<p>Perform maintenance on the standby.</p>\n\n<p>Promote the standby to primary.</p>\n\n<p>Perform maintenance on the old primary, which becomes the new standby.</p>\n\n<p>When you modify the database engine for your DB instance in a Multi-AZ deployment, then Amazon RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade.</p>\n\n<p><strong>Amazon RDS automatically initiates a failover to the standby, in case the primary database fails for any reason</strong> - You also benefit from enhanced database availability when running your DB instance as a Multi-AZ deployment. If an Availability Zone failure or DB instance failure occurs, your availability impact is limited to the time automatic failover takes to complete.</p>\n\n<p>Another implied benefit of running your DB instance as a Multi-AZ deployment is that DB instance failover is automatic and requires no administration. In an Amazon RDS context, this means you are not required to monitor DB instance events and initiate manual DB instance recovery in the event of an Availability Zone failure or DB instance failure.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB</strong> - The availability benefits of Multi-AZ also extend to planned maintenance. For example, with automated backups, I/O activity is no longer suspended on your primary during your preferred backup window, since backups are taken from the standby.</p>\n\n<p><strong>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</strong> - A Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. As such, the feature uses synchronous replication between primary and standby. AWS implementation makes sure the primary and the standby are constantly in sync, but precludes using the standby for read or write operations.</p>\n\n<p><strong>Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</strong> - When you create your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across the Availability Zone to the standby in order to keep both in sync and protect your latest database updates against DB instance failure.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary, and finally performing maintenance on the old primary, which becomes the new standby</strong>"
      },
      {
        "answer": "",
        "explanation": "Running a DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event because Amazon RDS applies operating system updates by following these steps:"
      },
      {
        "answer": "",
        "explanation": "Perform maintenance on the standby."
      },
      {
        "answer": "",
        "explanation": "Promote the standby to primary."
      },
      {
        "answer": "",
        "explanation": "Perform maintenance on the old primary, which becomes the new standby."
      },
      {
        "answer": "",
        "explanation": "When you modify the database engine for your DB instance in a Multi-AZ deployment, then Amazon RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon RDS automatically initiates a failover to the standby, in case the primary database fails for any reason</strong> - You also benefit from enhanced database availability when running your DB instance as a Multi-AZ deployment. If an Availability Zone failure or DB instance failure occurs, your availability impact is limited to the time automatic failover takes to complete."
      },
      {
        "answer": "",
        "explanation": "Another implied benefit of running your DB instance as a Multi-AZ deployment is that DB instance failover is automatic and requires no administration. In an Amazon RDS context, this means you are not required to monitor DB instance events and initiate manual DB instance recovery in the event of an Availability Zone failure or DB instance failure."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB</strong> - The availability benefits of Multi-AZ also extend to planned maintenance. For example, with automated backups, I/O activity is no longer suspended on your primary during your preferred backup window, since backups are taken from the standby."
      },
      {
        "answer": "",
        "explanation": "<strong>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</strong> - A Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. As such, the feature uses synchronous replication between primary and standby. AWS implementation makes sure the primary and the standby are constantly in sync, but precludes using the standby for read or write operations."
      },
      {
        "answer": "",
        "explanation": "<strong>Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</strong> - When you create your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across the Availability Zone to the standby in order to keep both in sync and protect your latest database updates against DB instance failure."
      }
    ],
    "references": [
      "https://aws.amazon.com/rds/faqs/"
    ]
  },
  {
    "id": 31,
    "question": "<p>A developer is creating a RESTful API service using an Amazon API Gateway with AWS Lambda integration. The service must support different API versions for testing purposes.</p>\n\n<p>As a Developer Associate, which of the following would you suggest as the best way to accomplish this?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an API Gateway Lambda authorizer to route API clients to the correct API version</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an API Gateway resource policy to identify the API versions and provide context to the Lambda function</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an X-Version header to identify which version is being called and pass that header to the Lambda function</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy the API versions as unique stages with unique endpoints and use stage variables to provide the context to identify the API versions</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the API versions as unique stages with unique endpoints and use stage variables to provide the context to identify the API versions</strong> - A stage is a named reference to a deployment, which is a snapshot of the API. You use a stage to manage and optimize a particular deployment. For example, you can configure stage settings to enable caching, customize request throttling, configure logging, define stage variables, or attach a canary release for testing.</p>\n\n<p>Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates.</p>\n\n<p>With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.</p>\n\n<p>For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example, beta.example.com). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an X-Version header to identify which version is being called and pass that header to the Lambda function</strong> - This is an incorrect option and has been added as a distractor.</p>\n\n<p><strong>Use an API Gateway Lambda authorizer to route API clients to the correct API version</strong> - A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity.</p>\n\n<p><strong>Set up an API Gateway resource policy to identify the API versions and provide context to the Lambda function</strong> - Amazon API Gateway resource policies are JSON policy documents that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. You can use API Gateway resource policies to allow your API to be securely invoked requestors. They are not meant for choosing the version of APIs.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy the API versions as unique stages with unique endpoints and use stage variables to provide the context to identify the API versions</strong> - A stage is a named reference to a deployment, which is a snapshot of the API. You use a stage to manage and optimize a particular deployment. For example, you can configure stage settings to enable caching, customize request throttling, configure logging, define stage variables, or attach a canary release for testing."
      },
      {
        "answer": "",
        "explanation": "Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates."
      },
      {
        "answer": "",
        "explanation": "With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints."
      },
      {
        "answer": "",
        "explanation": "For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example, beta.example.com). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use an X-Version header to identify which version is being called and pass that header to the Lambda function</strong> - This is an incorrect option and has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Use an API Gateway Lambda authorizer to route API clients to the correct API version</strong> - A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an API Gateway resource policy to identify the API versions and provide context to the Lambda function</strong> - Amazon API Gateway resource policies are JSON policy documents that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. You can use API Gateway resource policies to allow your API to be securely invoked requestors. They are not meant for choosing the version of APIs."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html"
    ]
  },
  {
    "id": 32,
    "question": "<p>An e-commerce company has a fleet of EC2 based web servers running into very high CPU utilization issues. The development team has determined that serving secure traffic via HTTPS is a major contributor to the high CPU load.</p>\n\n<p>Which of the following steps can take the high CPU load off the web servers? (Select two)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an HTTPS listener on the Application Load Balancer with SSL pass-through</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an HTTP listener on the Application Load Balancer with SSL termination</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an HTTPS listener on the Application Load Balancer with SSL termination</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure an SSL/TLS certificate on an Application Load Balancer via AWS Certificate Manager (ACM)</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Create an HTTP listener on the Application Load Balancer with SSL pass-through</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p>\"Configure an SSL/TLS certificate on an Application Load Balancer via AWS Certificate Manager (ACM)\"</p>\n\n<p>\"Create an HTTPS listener on the Application Load Balancer with SSL termination\"</p>\n\n<p>An Application load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/images/component_architecture.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/images/component_architecture.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a><p></p>\n\n<p>To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. You can create an HTTPS listener, which uses encrypted connections (also known as SSL offload). This feature enables traffic encryption between your load balancer and the clients that initiate SSL or TLS sessions. As the EC2 instances are under heavy CPU load, the load balancer will use the server certificate to terminate the front-end connection and then decrypt requests from clients before sending them to the EC2 instances.</p>\n\n<p>Please review this resource to understand how to associate an ACM SSL/TLS certificate with an Application Load Balancer:\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/\">https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Create an HTTPS listener on the Application Load Balancer with SSL pass-through\" - If you use an HTTPS listener with SSL pass-through, then the EC2 instances would continue to be under heavy CPU load as they would still need to decrypt the secure traffic\nat the instance level. Hence this option is incorrect.</p>\n\n<p>\"Create an HTTP listener on the Application Load Balancer with SSL termination\"</p>\n\n<p>\"Create an HTTP listener on the Application Load Balancer with SSL pass-through\"</p>\n\n<p>You cannot have an HTTP listener for an Application Load Balancer to support SSL termination or SSL pass-through, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/\">https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure an SSL/TLS certificate on an Application Load Balancer via AWS Certificate Manager (ACM)</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Create an HTTPS listener on the Application Load Balancer with SSL termination</strong>"
      },
      {
        "answer": "",
        "explanation": "An Application load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions."
      },
      {
        "link": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html"
      },
      {
        "answer": "",
        "explanation": "To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. You can create an HTTPS listener, which uses encrypted connections (also known as SSL offload). This feature enables traffic encryption between your load balancer and the clients that initiate SSL or TLS sessions. As the EC2 instances are under heavy CPU load, the load balancer will use the server certificate to terminate the front-end connection and then decrypt requests from clients before sending them to the EC2 instances."
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/",
        "answer": "",
        "explanation": "Please review this resource to understand how to associate an ACM SSL/TLS certificate with an Application Load Balancer:\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/\">https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/</a>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an HTTPS listener on the Application Load Balancer with SSL pass-through</strong> - If you use an HTTPS listener with SSL pass-through, then the EC2 instances would continue to be under heavy CPU load as they would still need to decrypt the secure traffic\nat the instance level. Hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an HTTP listener on the Application Load Balancer with SSL termination</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Create an HTTP listener on the Application Load Balancer with SSL pass-through</strong>"
      },
      {
        "answer": "",
        "explanation": "You cannot have an HTTP listener for an Application Load Balancer to support SSL termination or SSL pass-through, so both these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/"
    ]
  },
  {
    "id": 33,
    "question": "<p>A multi-national company runs its technology operations on AWS Cloud. As part of their storage solution, they use a large number of EBS volumes, with AWS Config and CloudTrail activated. A manager has tried to find the user name that created an EBS volume by searching CloudTrail events logs but wasn't successful.</p>\n\n<p>As a Developer Associate, which of the following would you recommend as the correct solution?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon EBS CloudWatch metrics are disabled</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon EC2 launch</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS CloudTrail event logs for 'ManageVolume' aren't available for EBS volumes created during an Amazon EC2 launch</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>EBS volume status checks are disabled</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon EC2 launch</strong> - AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon Elastic Compute Cloud (Amazon EC2) launch.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS CloudTrail event logs for 'ManageVolume' aren't available for EBS volumes created during an Amazon EC2 launch</strong> - Event 'ManageVolume' is a made-up option and has been added as a distractor.</p>\n\n<p><strong>Amazon EBS CloudWatch metrics are disabled</strong> - Amazon Elastic Block Store (Amazon EBS) sends data points to CloudWatch for several metrics. Data is only reported to CloudWatch when the volume is attached to an instance. CloudWatch metrics are useful in tracking the status or life cycle changes of an EBS volume, they are not useful in knowing about the metadata of EBS volumes.</p>\n\n<p><strong>EBS volume status checks are disabled</strong> - Volume status checks enable you to better understand, track and manage potential inconsistencies in the data on an Amazon EBS volume. They are designed to provide you with the information that you need to determine whether your Amazon EBS volumes are impaired, and to help you control how a potentially inconsistent volume is handled. Our current use case requires us to pull data about EBS volume metadata, which is not possible with this feature.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/find-ebs-user-config-cloudtrail/\">https://aws.amazon.com/premiumsupport/knowledge-center/find-ebs-user-config-cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cloudwatch_ebs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cloudwatch_ebs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon EC2 launch</strong> - AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon Elastic Compute Cloud (Amazon EC2) launch."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS CloudTrail event logs for 'ManageVolume' aren't available for EBS volumes created during an Amazon EC2 launch</strong> - Event 'ManageVolume' is a made-up option and has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EBS CloudWatch metrics are disabled</strong> - Amazon Elastic Block Store (Amazon EBS) sends data points to CloudWatch for several metrics. Data is only reported to CloudWatch when the volume is attached to an instance. CloudWatch metrics are useful in tracking the status or life cycle changes of an EBS volume, they are not useful in knowing about the metadata of EBS volumes."
      },
      {
        "answer": "",
        "explanation": "<strong>EBS volume status checks are disabled</strong> - Volume status checks enable you to better understand, track and manage potential inconsistencies in the data on an Amazon EBS volume. They are designed to provide you with the information that you need to determine whether your Amazon EBS volumes are impaired, and to help you control how a potentially inconsistent volume is handled. Our current use case requires us to pull data about EBS volume metadata, which is not possible with this feature."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/find-ebs-user-config-cloudtrail/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cloudwatch_ebs.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html"
    ]
  },
  {
    "id": 34,
    "question": "<p>The development team at a retail organization wants to allow a Lambda function in its AWS Account A to access a DynamoDB table in another AWS Account B.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an IAM role in Account B with access to DynamoDB. Modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Update the Lambda function code to add the AssumeRole API call</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create an IAM role in Account B with access to DynamoDB. Modify the trust policy of the execution role in Account A to allow the execution role of Lambda to assume the IAM role in Account B. Update the Lambda function code to add the AssumeRole API call</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a clone of the Lambda function in AWS Account B so that it can access the DynamoDB table in the same account</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Add a resource policy to the DynamoDB table in AWS Account B to give access to the Lambda function in Account A</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM role in account B with access to DynamoDB. Modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Update the Lambda function code to add the AssumeRole API call</strong></p>\n\n<p>You can give a Lambda function created in one account (\"account A\") permissions to assume a role from another account (\"account B\") to access resources such as DynamoDB or S3 bucket. You need to create an execution role in Account A that gives the Lambda function permission to do its work. Then you need to create a role in account B that the Lambda function in account A assumes to gain access to the cross-account DynamoDB table. Make sure that you modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Finally, update the Lambda function code to add the AssumeRole API call.</p>\n\n<p>Sample use-case to configure a Lambda function to assume a role from another AWS account:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q1-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q1-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a clone of the Lambda function in AWS Account B so that it can access the DynamoDB table in the same account</strong> - Creating a clone of the Lambda function is a distractor as this does not solve the use-case outlined in the problem statement.</p>\n\n<p><strong>Add a resource policy to the DynamoDB table in AWS Account B to give access to the Lambda function in Account A</strong> - You cannot attach a resource policy to a DynamoDB table, so this option is incorrect.</p>\n\n<p><strong>Create an IAM role in Account B with access to DynamoDB. Modify the trust policy of the execution role in Account A to allow the execution role of Lambda to assume the IAM role in Account B. Update the Lambda function code to add the AssumeRole API call</strong> - As mentioned in the explanation above, you need to modify the trust policy of the IAM role in Account B so that it allows the execution role of Lambda function in account A to assume the IAM role in Account B.</p>\n\n<p>Reference:\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an IAM role in account B with access to DynamoDB. Modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Update the Lambda function code to add the AssumeRole API call</strong>"
      },
      {
        "answer": "",
        "explanation": "You can give a Lambda function created in one account (\"account A\") permissions to assume a role from another account (\"account B\") to access resources such as DynamoDB or S3 bucket. You need to create an execution role in Account A that gives the Lambda function permission to do its work. Then you need to create a role in account B that the Lambda function in account A assumes to gain access to the cross-account DynamoDB table. Make sure that you modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Finally, update the Lambda function code to add the AssumeRole API call."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q1-i1.jpg",
        "answer": "",
        "explanation": "Sample use-case to configure a Lambda function to assume a role from another AWS account:"
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a clone of the Lambda function in AWS Account B so that it can access the DynamoDB table in the same account</strong> - Creating a clone of the Lambda function is a distractor as this does not solve the use-case outlined in the problem statement."
      },
      {
        "answer": "",
        "explanation": "<strong>Add a resource policy to the DynamoDB table in AWS Account B to give access to the Lambda function in Account A</strong> - You cannot attach a resource policy to a DynamoDB table, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an IAM role in Account B with access to DynamoDB. Modify the trust policy of the execution role in Account A to allow the execution role of Lambda to assume the IAM role in Account B. Update the Lambda function code to add the AssumeRole API call</strong> - As mentioned in the explanation above, you need to modify the trust policy of the IAM role in Account B so that it allows the execution role of Lambda function in account A to assume the IAM role in Account B."
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/",
        "answer": "",
        "explanation": "Reference:\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/</a>"
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/"
    ]
  },
  {
    "id": 35,
    "question": "<p>Your organization has developers that merge code changes regularly to an AWS CodeCommit repository. Your pipeline has AWS CodeCommit as the source and you would like to configure a rule that reacts to changes in CodeCommit.</p>\n\n<p>Which of the following options do you choose for this type of integration?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Lambda Event Rules</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use CloudTrail Event rules with Amazon Simple Email Service (SES)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use CloudWatch Event Rules</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Lambda function with Amazon Simple Notification Service (SNS)</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use CloudWatch Event Rules</strong></p>\n\n<p>Amazon CloudWatch Events is a web service that monitors your AWS resources and the applications you run on AWS. You can use Amazon CloudWatch Events to detect and react to changes in the state of a pipeline, stage, or action. Then, based on rules you create, CloudWatch Events invokes one or more target actions when a pipeline, stage, or action enters the state you specify in a rule.\nExamples of Amazon CloudWatch Events rules and targets:</p>\n\n<ol>\n<li><p>A rule that sends a notification when the instance state changes, where an EC2 instance is the event source, and Amazon SNS is the event target.</p></li>\n<li><p>A rule that sends a notification when the build phase changes, where a CodeBuild configuration is the event source, and Amazon SNS is the event target.</p></li>\n<li><p>A rule that detects pipeline changes and invokes an AWS Lambda function.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudTrail Event rules with Amazon Simple Email Service (SES)</strong> - This is an incorrect statement. There is no such thing as CloudTrail Event Rule.</p>\n\n<p><strong>Use Lambda function with Amazon Simple Notification Service (SNS)</strong> - Lambda functions can be triggered by the use of CloudWatch Event Rules as discussed above. AWS CodePipeline does not trigger Lambda functions directly.</p>\n\n<p><strong>Use Lambda Event Rules</strong> - This is an incorrect statement. There is no such thing as Lambda Event Rule.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use CloudWatch Event Rules</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon CloudWatch Events is a web service that monitors your AWS resources and the applications you run on AWS. You can use Amazon CloudWatch Events to detect and react to changes in the state of a pipeline, stage, or action. Then, based on rules you create, CloudWatch Events invokes one or more target actions when a pipeline, stage, or action enters the state you specify in a rule.\nExamples of Amazon CloudWatch Events rules and targets:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li><p>A rule that sends a notification when the instance state changes, where an EC2 instance is the event source, and Amazon SNS is the event target.</p></li>\n<li><p>A rule that sends a notification when the build phase changes, where a CodeBuild configuration is the event source, and Amazon SNS is the event target.</p></li>\n<li><p>A rule that detects pipeline changes and invokes an AWS Lambda function.</p></li>\n</ol>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use CloudTrail Event rules with Amazon Simple Email Service (SES)</strong> - This is an incorrect statement. There is no such thing as CloudTrail Event Rule."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Lambda function with Amazon Simple Notification Service (SNS)</strong> - Lambda functions can be triggered by the use of CloudWatch Event Rules as discussed above. AWS CodePipeline does not trigger Lambda functions directly."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Lambda Event Rules</strong> - This is an incorrect statement. There is no such thing as Lambda Event Rule."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html"
    ]
  },
  {
    "id": 36,
    "question": "<p>Your team has just signed up an year-long contract with a client maintaining a three-tier web application, that needs to be moved to AWS Cloud. The application has steady traffic throughout the day and needs to be on a reliable system with no down-time or access issues. The solution needs to be cost-optimal for this startup.</p>\n\n<p>Which of the following options should you choose?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon EC2 On Demand Instances</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon EC2 Spot Instances</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon EC2 Reserved Instances</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>On-premise EC2 instance</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EC2 Reserved Instances</strong> - Reserved instances can provide a capacity reservation, offering additional confidence in your ability to launch the number of instances you have reserved when you need them. You save money going with Reserved instances vs on-demand especially in a year's worth of time.</p>\n\n<p>Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account. These On-Demand Instances must match certain attributes, such as instance type and Region, to benefit from the billing discount. So, there is no performance difference between an On-Demand instance or a Reserved instance.</p>\n\n<p>How RIs work:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q60-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q60-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon EC2 Spot Instances</strong> - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot instances are useful if your applications can be interrupted, like data analysis, batch jobs, background processing, and optional tasks. Spot instances can be pulled down anytime without prior notice. Hence, not the right choice for the current scenario.</p>\n\n<p><strong>Amazon EC2 On-Demand Instances</strong> - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. But, On-Demand instances cost a lot more than Reserved instances. Here, in our use case, we already know that the systems are required for a complete year, so making use of Reserved Instances discount makes a lot more sense.</p>\n\n<p><strong>On-premise EC2 instance</strong> - On-premise implies the client has to maintain the physical machines, their capacity provisioning and maintenance. Not an option when the client is planning to move to AWS Cloud.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/pricing/reserved-instances/\">https://aws.amazon.com/ec2/pricing/reserved-instances/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 Reserved Instances</strong> - Reserved instances can provide a capacity reservation, offering additional confidence in your ability to launch the number of instances you have reserved when you need them. You save money going with Reserved instances vs on-demand especially in a year's worth of time."
      },
      {
        "answer": "",
        "explanation": "Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account. These On-Demand Instances must match certain attributes, such as instance type and Region, to benefit from the billing discount. So, there is no performance difference between an On-Demand instance or a Reserved instance."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q60-i1.jpg",
        "answer": "",
        "explanation": "How RIs work:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 Spot Instances</strong> - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot instances are useful if your applications can be interrupted, like data analysis, batch jobs, background processing, and optional tasks. Spot instances can be pulled down anytime without prior notice. Hence, not the right choice for the current scenario."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 On-Demand Instances</strong> - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. But, On-Demand instances cost a lot more than Reserved instances. Here, in our use case, we already know that the systems are required for a complete year, so making use of Reserved Instances discount makes a lot more sense."
      },
      {
        "answer": "",
        "explanation": "<strong>On-premise EC2 instance</strong> - On-premise implies the client has to maintain the physical machines, their capacity provisioning and maintenance. Not an option when the client is planning to move to AWS Cloud."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html",
      "https://aws.amazon.com/ec2/pricing/reserved-instances/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html"
    ]
  },
  {
    "id": 37,
    "question": "<p>Your company has a load balancer in a VPC configured to be internet facing. The public DNS name assigned to the load balancer is <code>myDns-1234567890.us-east-1.elb.amazonaws.com</code>. When your client applications first load they capture the load balancer DNS name and then resolve the IP address for the load balancer so that they can directly reference the underlying IP.</p>\n\n<p>It is observed that the client applications work well but unexpectedly stop working after a while. What is the reason for this?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The load balancer is highly available and its public IP may change. The DNS name is constant</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>You need to enable stickiness</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Your security groups are not stable</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>You need to disable multi-AZ deployments</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The load balancer is highly available and its public IP may change. The DNS name is constant</strong></p>\n\n<p>When your load balancer is created, it receives a public DNS name that clients can use to send requests. The DNS servers resolve the DNS name of your load balancer to the public IP addresses of the load balancer nodes for your load balancer. Never resolve the IP of a load balancer as it can change with time. You should always use the DNS name.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Your security groups are not stable</strong> - You security groups to allow your load balancer to work with registered instances. It is stable if set correctly. If your application is working and stops after a while, the issue is not with the security groups.</p>\n\n<p><strong>You need to enable stickiness</strong> - This enables the load balancer to bind a user's session to a specific instance, so this has no impact on the issue described in the given use-case.</p>\n\n<p><strong>You need to disable multi-AZ deployments</strong> - This has been added as a distractor and this has no bearing on the use-case. The change is happening with the IP of the load balancer.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>The load balancer is highly available and its public IP may change. The DNS name is constant</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "When your load balancer is created, it receives a public DNS name that clients can use to send requests. The DNS servers resolve the DNS name of your load balancer to the public IP addresses of the load balancer nodes for your load balancer. Never resolve the IP of a load balancer as it can change with time. You should always use the DNS name."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Your security groups are not stable</strong> - You security groups to allow your load balancer to work with registered instances. It is stable if set correctly. If your application is working and stops after a while, the issue is not with the security groups."
      },
      {
        "answer": "",
        "explanation": "<strong>You need to enable stickiness</strong> - This enables the load balancer to bind a user's session to a specific instance, so this has no impact on the issue described in the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>You need to disable multi-AZ deployments</strong> - This has been added as a distractor and this has no bearing on the use-case. The change is happening with the IP of the load balancer."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html"
    ]
  },
  {
    "id": 38,
    "question": "<p>The development team at an IT company has configured an Application Load Balancer (ALB) with a Lambda function A as the target but the Lambda function A is not able to process any request from the ALB. Upon investigation, the team finds that there is another Lambda function B in the AWS account that is exceeding the concurrency limits.</p>\n\n<p>How can the development team address this issue?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a Cloudfront Distribution instead of an Application Load Balancer (ALB) for Lambda function A</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use an API Gateway instead of an Application Load Balancer (ALB) for Lambda function A</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up provisioned concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up reserved concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up reserved concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</strong></p>\n\n<p>Concurrency is the number of requests that a Lambda function is serving at any given time. If a Lambda function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.</p>\n\n<p>To ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency. When a function has reserved concurrency, no other function can use that concurrency. More importantly, reserved concurrency also limits the maximum concurrency for the function, and applies to the function as a whole, including versions and aliases.</p>\n\n<p>Please review this note to understand how reserved concurrency works:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q6-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q6-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved</a><p></p>\n\n<p>Therefore using reserved concurrency for Lambda function B would limit its maximum concurrency and allow Lambda function A to execute without getting throttled.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up provisioned concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</strong> - You should use provisioned concurrency to enable your function to scale without fluctuations in latency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency. Provisioned concurrency is not used to limit the maximum concurrency for a given Lambda function, so this option is incorrect.</p>\n\n<p><strong>Use an API Gateway instead of an Application Load Balancer (ALB) for Lambda function A</strong> - This has been added as a distractor as using an API Gateway for Lambda function A has no bearing on limiting the concurrency of Lambda function B, so this option is incorrect.</p>\n\n<p><strong>Use a Cloudfront Distribution instead of an Application Load Balancer (ALB) for Lambda function A</strong> - When you associate a CloudFront distribution with a Lambda function (known as Lambda@Edge), CloudFront intercepts requests and responses at CloudFront edge locations and runs the function. Again, this has no bearing on limiting the concurrency of Lambda function B, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/lambda-functions-as-targets-for-application-load-balancers/\">https://aws.amazon.com/blogs/networking-and-content-delivery/lambda-functions-as-targets-for-application-load-balancers/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Set up reserved concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Concurrency is the number of requests that a Lambda function is serving at any given time. If a Lambda function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency."
      },
      {
        "answer": "",
        "explanation": "To ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency. When a function has reserved concurrency, no other function can use that concurrency. More importantly, reserved concurrency also limits the maximum concurrency for the function, and applies to the function as a whole, including versions and aliases."
      },
      {
        "answer": "",
        "explanation": "Please review this note to understand how reserved concurrency works:"
      },
      {
        "link": "https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved"
      },
      {
        "answer": "",
        "explanation": "Therefore using reserved concurrency for Lambda function B would limit its maximum concurrency and allow Lambda function A to execute without getting throttled."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up provisioned concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</strong> - You should use provisioned concurrency to enable your function to scale without fluctuations in latency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency. Provisioned concurrency is not used to limit the maximum concurrency for a given Lambda function, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use an API Gateway instead of an Application Load Balancer (ALB) for Lambda function A</strong> - This has been added as a distractor as using an API Gateway for Lambda function A has no bearing on limiting the concurrency of Lambda function B, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use a Cloudfront Distribution instead of an Application Load Balancer (ALB) for Lambda function A</strong> - When you associate a CloudFront distribution with a Lambda function (known as Lambda@Edge), CloudFront intercepts requests and responses at CloudFront edge locations and runs the function. Again, this has no bearing on limiting the concurrency of Lambda function B, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved",
      "https://aws.amazon.com/blogs/networking-and-content-delivery/lambda-functions-as-targets-for-application-load-balancers/"
    ]
  },
  {
    "id": 39,
    "question": "<p>A developer at a university is encrypting a large XML payload transferred over the network using AWS KMS and wants to test the application before going to production.</p>\n\n<p>What is the maximum data size supported by AWS KMS?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>16KB</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>4KB</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>10MB</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>1MB</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>4 KB</strong></p>\n\n<p>You can encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information.</p>\n\n<p>While AWS KMS does support sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>1MB</strong> - For anything over 4 KB, you may want to look at envelope encryption</p>\n\n<p><strong>10MB</strong> - For anything over 4 KB, you may want to look at envelope encryption</p>\n\n<p><strong>16KB</strong> - For anything over 4 KB, you may want to look at envelope encryption</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kms/faqs/\">https://aws.amazon.com/kms/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>4 KB</strong>"
      },
      {
        "answer": "",
        "explanation": "You can encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information."
      },
      {
        "answer": "",
        "explanation": "While AWS KMS does support sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>1MB</strong> - For anything over 4 KB, you may want to look at envelope encryption"
      },
      {
        "answer": "",
        "explanation": "<strong>10MB</strong> - For anything over 4 KB, you may want to look at envelope encryption"
      },
      {
        "answer": "",
        "explanation": "<strong>16KB</strong> - For anything over 4 KB, you may want to look at envelope encryption"
      }
    ],
    "references": [
      "https://aws.amazon.com/kms/faqs/"
    ]
  },
  {
    "id": 40,
    "question": "<p>A company wants to implement authentication for its new RESTful API service that uses Amazon API Gateway. To authenticate the calls, each request must include HTTP headers with a client ID and user ID. These credentials must be compared to the authentication data in a DynamoDB table.</p>\n\n<p>As an AWS Certified Developer Associate, which of the following would you recommend for implementing this authentication in API Gateway? </p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an API Gateway Model that requires the credentials, then grant API Gateway access to the authentication table in DynamoDB</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Develop an AWS Lambda authorizer that references the authentication data in the DynamoDB table</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Authorize using Amazon Cognito that will reference the authentication table of DynamoDB</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Update the API Gateway integration requests to require the credentials, then grant API Gateway access to the authentication table in DynamoDB</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Develop an AWS Lambda authorizer that references the DynamoDB authentication table</strong> - A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API.</p>\n\n<p>A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity.</p>\n\n<p>When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output.</p>\n\n<p>There are two types of Lambda authorizers:</p>\n\n<ol>\n<li><p>A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.</p></li>\n<li><p>A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, state variables, and $context variables.</p></li>\n</ol>\n\n<p>API Gateway Lambda authorization workflow:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q55-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q55-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an API Gateway Model that requires the credentials, then grant API Gateway access to the authentication table in DynamoDB</strong> - In API Gateway, a model defines the data structure of a payload. In API Gateway models are defined using the JSON schema draft 4. Models are not mandatory.</p>\n\n<p><strong>Update the API Gateway integration requests to require the credentials, then grant API Gateway access to the authentication table in DynamoDB</strong> - After setting up an API method, you must integrate it with an endpoint in the backend. A backend endpoint is also referred to as an integration endpoint and can be a Lambda function, an HTTP webpage, or an AWS service action.</p>\n\n<p>An integration request is an HTTP request that API Gateway submits to the backend, passing along the client-submitted request data, and transforming the data, if necessary. The HTTP method (or verb) and URI of the integration request are dictated by the backend (that is, the integration endpoint). They can be the same as or different from the method request's HTTP method and URI, respectively.</p>\n\n<p><strong>Authorize using Amazon Cognito that will reference the authentication table of DynamoDB</strong> - As an alternative to using IAM roles and policies or Lambda authorizers (formerly known as custom authorizers), you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway.</p>\n\n<p>To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user in to the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-integration-settings.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-integration-settings.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Develop an AWS Lambda authorizer that references the DynamoDB authentication table</strong> - A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API."
      },
      {
        "answer": "",
        "explanation": "A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity."
      },
      {
        "answer": "",
        "explanation": "When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output."
      },
      {
        "answer": "",
        "explanation": "There are two types of Lambda authorizers:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li><p>A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.</p></li>\n<li><p>A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, state variables, and $context variables.</p></li>\n</ol>"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q55-i1.jpg",
        "answer": "",
        "explanation": "API Gateway Lambda authorization workflow:"
      },
      {
        "link": "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an API Gateway Model that requires the credentials, then grant API Gateway access to the authentication table in DynamoDB</strong> - In API Gateway, a model defines the data structure of a payload. In API Gateway models are defined using the JSON schema draft 4. Models are not mandatory."
      },
      {
        "answer": "",
        "explanation": "<strong>Update the API Gateway integration requests to require the credentials, then grant API Gateway access to the authentication table in DynamoDB</strong> - After setting up an API method, you must integrate it with an endpoint in the backend. A backend endpoint is also referred to as an integration endpoint and can be a Lambda function, an HTTP webpage, or an AWS service action."
      },
      {
        "answer": "",
        "explanation": "An integration request is an HTTP request that API Gateway submits to the backend, passing along the client-submitted request data, and transforming the data, if necessary. The HTTP method (or verb) and URI of the integration request are dictated by the backend (that is, the integration endpoint). They can be the same as or different from the method request's HTTP method and URI, respectively."
      },
      {
        "answer": "",
        "explanation": "<strong>Authorize using Amazon Cognito that will reference the authentication table of DynamoDB</strong> - As an alternative to using IAM roles and policies or Lambda authorizers (formerly known as custom authorizers), you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway."
      },
      {
        "answer": "",
        "explanation": "To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user in to the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-integration-settings.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html"
    ]
  },
  {
    "id": 41,
    "question": "<p>An application runs on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. The company's audit requirements mandate that logging and storing of application log data must be done centrally on AWS.</p>\n\n<p>How will you configure this requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the awslogs log driver to send log information to CloudWatch Logs. To turn on the awslogs log driver, your Amazon ECS container instances require at least version 1.9.0 of the container agent</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon ECS metric data is automatically sent to CloudWatch in 1-minute periods. Amazon ECS service using the Fargate launch type has CloudWatch CPU and memory utilization metrics that can be enabled from the ECS console</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the awslogs log driver to configure the containers in your tasks to send log information to CloudWatch Logs. Add the required <code>logConfiguration</code> parameters to your task definition</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Download and install the unified CloudWatch agent on the ECS instances to collect internal system-level metrics and application logs from the instances. The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch logs and can be queried for report generation</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the awslogs log driver to configure the containers in your tasks to send log information to CloudWatch Logs. Add the required <code>logConfiguration</code> parameters to your task definition</strong></p>\n\n<p>Using the awslogs log driver you can configure the containers in your tasks to send log information to CloudWatch Logs. If you're using the Fargate launch type for your tasks, you need to add the required logConfiguration parameters to your task definition to turn on the awslogs log driver.</p>\n\n<p>Before your containers can send logs to CloudWatch, you must specify the awslogs log driver for containers in your task definition. The example task definition JSON that follows has a logConfiguration object specified for each container. One is for the WordPress container that sends logs to a log group called awslogs-wordpress. The other is for a MySQL container that sends logs to a log group that's called awslogs-mysql. Both containers use the awslogs-example log stream. prefix.</p>\n\n<p>Specifying a log configuration in your task definition:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q26-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q26-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html#specify-log-config\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html#specify-log-config</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the awslogs log driver to send log information to CloudWatch Logs. To turn on the awslogs log driver, your Amazon ECS container instances require at least version 1.9.0 of the container agent</strong> - This statement is incorrect. If you're using the EC2 launch type (and not Fargate) for your tasks and want to turn on the awslogs log driver, your Amazon ECS container instances require at least version 1.9.0 of the container agent.</p>\n\n<p><strong>Amazon ECS metric data is automatically sent to CloudWatch in 1-minute periods. Amazon ECS service using the Fargate launch type has CloudWatch CPU and memory utilization metrics that can be enabled from the ECS console</strong> - Indeed, Amazon ECS metric data is automatically sent to CloudWatch in 1-minute periods. Also, any Amazon ECS service using the Fargate launch type has CloudWatch CPU and memory utilization metrics automatically, so you don't need to take any manual steps. But, these are system logs and not the application logs as is needed in the current use case.</p>\n\n<p><strong>Download and install the unified CloudWatch agent on the ECS instances to collect internal system-level metrics and application logs from the instances. The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch logs and can be queried for report generation</strong> - This statement is incorrect and given only as a distractor. ECS Fargate is serverless and hence the scope of downloading and installing software on the instance does not arise.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/userguide/cloudwatch-metrics.html#available_cloudwatch_metrics\">https://docs.aws.amazon.com/AmazonECS/latest/userguide/cloudwatch-metrics.html#available_cloudwatch_metrics</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use the awslogs log driver to configure the containers in your tasks to send log information to CloudWatch Logs. Add the required <code>logConfiguration</code> parameters to your task definition</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Using the awslogs log driver you can configure the containers in your tasks to send log information to CloudWatch Logs. If you're using the Fargate launch type for your tasks, you need to add the required logConfiguration parameters to your task definition to turn on the awslogs log driver."
      },
      {
        "answer": "",
        "explanation": "Before your containers can send logs to CloudWatch, you must specify the awslogs log driver for containers in your task definition. The example task definition JSON that follows has a logConfiguration object specified for each container. One is for the WordPress container that sends logs to a log group called awslogs-wordpress. The other is for a MySQL container that sends logs to a log group that's called awslogs-mysql. Both containers use the awslogs-example log stream. prefix."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q26-i2.jpg",
        "answer": "",
        "explanation": "Specifying a log configuration in your task definition:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html#specify-log-config"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the awslogs log driver to send log information to CloudWatch Logs. To turn on the awslogs log driver, your Amazon ECS container instances require at least version 1.9.0 of the container agent</strong> - This statement is incorrect. If you're using the EC2 launch type (and not Fargate) for your tasks and want to turn on the awslogs log driver, your Amazon ECS container instances require at least version 1.9.0 of the container agent."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon ECS metric data is automatically sent to CloudWatch in 1-minute periods. Amazon ECS service using the Fargate launch type has CloudWatch CPU and memory utilization metrics that can be enabled from the ECS console</strong> - Indeed, Amazon ECS metric data is automatically sent to CloudWatch in 1-minute periods. Also, any Amazon ECS service using the Fargate launch type has CloudWatch CPU and memory utilization metrics automatically, so you don't need to take any manual steps. But, these are system logs and not the application logs as is needed in the current use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Download and install the unified CloudWatch agent on the ECS instances to collect internal system-level metrics and application logs from the instances. The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch logs and can be queried for report generation</strong> - This statement is incorrect and given only as a distractor. ECS Fargate is serverless and hence the scope of downloading and installing software on the instance does not arise."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html#specify-log-config",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/userguide/cloudwatch-metrics.html#available_cloudwatch_metrics"
    ]
  },
  {
    "id": 42,
    "question": "<p>An investment firm wants to continuously generate time-series analytics of the stocks being purchased by its customers. The firm wants to build a live leaderboard with near-real-time analytics for these in-demand stocks.</p>\n\n<p>Which of the following represents a fully managed solution with the least cost to address this use-case?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Kinesis Data Streams to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Kinesis Data Streams to ingest data and Amazon Kinesis Client Library to the application logic to generate leaderboard scores and time-series analytics</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Kinesis Firehose to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Kinesis Firehose to ingest data and Amazon Athena to generate leaderboard scores and time-series analytics</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Kinesis Firehose to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</strong></p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Datadog, New Relic, MongoDB, and Splunk. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt your data streams before loading, minimizing the amount of storage used and increasing security.</p>\n\n<p>Amazon Kinesis Data Analytics is the easiest way to transform and analyze streaming data in real-time with Apache Flink. Apache Flink is an open source framework and engine for processing data streams. Amazon Kinesis Data Analytics reduces the complexity of building, managing, and integrating Apache Flink applications with other AWS services.</p>\n\n<p>Amazon Kinesis Data Analytics provides built-in functions to filter, aggregate, and transform streaming data for advanced analytics. It processes streaming data with sub-second latencies, enabling you to analyze and respond to incoming data and events in real-time.</p>\n\n<p>Amazon Kinesis Data Analytics is serverless; there are no servers to manage. It runs your streaming applications without requiring you to provision or manage any infrastructure. Amazon Kinesis Data Analytics automatically scales the infrastructure up and down as required to process incoming data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Kinesis Data Streams to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Although Kinesis Data Streams supports on-demand provisioning of shards, however, the data ingestion cost along with the per hour shards cost would be more than the corresponding cost incurred while using Firehose. The use-case clearly states that the company wants a fully managed solution with the least cost, so Kinesis Firehose is a better solution.</p>\n\n<p><strong>Use Kinesis Data Streams to ingest data and Amazon Kinesis Client Library to the application logic to generate leaderboard scores and time-series analytics</strong> - The Amazon Kinesis Client Library (KCL) is a pre-built library that helps you build consumer applications for reading and processing data from an Amazon Kinesis data stream. The KCL handles complex issues such as adapting to changes in data stream volume, load balancing streaming data, coordinating distributed services, and processing data with fault-tolerance. The KCL enables you to focus on business logic while building applications.</p>\n\n<p>If you want a fully managed solution and you want to use SQL to process the data from your data stream, you should use Kinesis Data Analytics. Use KCL if you need to build a custom processing solution whose requirements are not met by Kinesis Data Analytics, and you can manage the resulting consumer application.</p>\n\n<p><strong>Use Kinesis Firehose to ingest data and Amazon Athena to generate leaderboard scores and time-series analytics</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena is used for running analytics on S3 based data. For running analytics on real-time streaming data, Kinesis Data Analytics is the right fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/data-analysis.html\">https://docs.aws.amazon.com/firehose/latest/dev/data-analysis.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/pricing/\">https://aws.amazon.com/kinesis/data-firehose/pricing/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/pricing/\">https://aws.amazon.com/kinesis/data-streams/pricing/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use Kinesis Firehose to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Datadog, New Relic, MongoDB, and Splunk. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt your data streams before loading, minimizing the amount of storage used and increasing security."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Analytics is the easiest way to transform and analyze streaming data in real-time with Apache Flink. Apache Flink is an open source framework and engine for processing data streams. Amazon Kinesis Data Analytics reduces the complexity of building, managing, and integrating Apache Flink applications with other AWS services."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Analytics provides built-in functions to filter, aggregate, and transform streaming data for advanced analytics. It processes streaming data with sub-second latencies, enabling you to analyze and respond to incoming data and events in real-time."
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Analytics is serverless; there are no servers to manage. It runs your streaming applications without requiring you to provision or manage any infrastructure. Amazon Kinesis Data Analytics automatically scales the infrastructure up and down as required to process incoming data."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Kinesis Data Streams to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more."
      },
      {
        "answer": "",
        "explanation": "Although Kinesis Data Streams supports on-demand provisioning of shards, however, the data ingestion cost along with the per hour shards cost would be more than the corresponding cost incurred while using Firehose. The use-case clearly states that the company wants a fully managed solution with the least cost, so Kinesis Firehose is a better solution."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Kinesis Data Streams to ingest data and Amazon Kinesis Client Library to the application logic to generate leaderboard scores and time-series analytics</strong> - The Amazon Kinesis Client Library (KCL) is a pre-built library that helps you build consumer applications for reading and processing data from an Amazon Kinesis data stream. The KCL handles complex issues such as adapting to changes in data stream volume, load balancing streaming data, coordinating distributed services, and processing data with fault-tolerance. The KCL enables you to focus on business logic while building applications."
      },
      {
        "answer": "",
        "explanation": "If you want a fully managed solution and you want to use SQL to process the data from your data stream, you should use Kinesis Data Analytics. Use KCL if you need to build a custom processing solution whose requirements are not met by Kinesis Data Analytics, and you can manage the resulting consumer application."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Kinesis Firehose to ingest data and Amazon Athena to generate leaderboard scores and time-series analytics</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena is used for running analytics on S3 based data. For running analytics on real-time streaming data, Kinesis Data Analytics is the right fit."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/firehose/latest/dev/data-analysis.html",
      "https://aws.amazon.com/kinesis/data-analytics/faqs/",
      "https://aws.amazon.com/kinesis/data-firehose/pricing/",
      "https://aws.amazon.com/kinesis/data-streams/pricing/"
    ]
  },
  {
    "id": 43,
    "question": "<p>An organization with high data volume workloads have successfully moved to DynamoDB after having many issues with traditional database systems. However, a few months into production, DynamoDB tables are consistently recording high latency.</p>\n\n<p>As a Developer Associate, which of the following would you suggest to reduce the latency? (Select two)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Reduce connection pooling, which keeps the connections alive even when user requests are not present, thereby, blocking the services</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Consider using Global tables if your application is accessed by globally distributed users</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use DynamoDB Accelerator (DAX) for businesses with heavy write-only workloads</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use eventually consistent reads in place of strongly consistent reads whenever possible</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Increase the request timeout settings, so the client gets enough time to complete the requests, thereby reducing retries on the system</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup, and restore and in-memory caching for internet-scale applications.</p>\n\n<p><strong>Consider using Global tables if your application is accessed by globally distributed users</strong> - If you have globally dispersed users, consider using global tables. With global tables, you can specify the AWS Regions where you want the table to be available. This can significantly reduce latency for your users. So, reducing the distance between the client and the DynamoDB endpoint is an important performance fix to be considered.</p>\n\n<p><strong>Use eventually consistent reads in place of strongly consistent reads whenever possible</strong> - If your application doesn't require strongly consistent reads, consider using eventually consistent reads. Eventually consistent reads are cheaper and are less likely to experience high latency.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the request timeout settings, so the client gets enough time to complete the requests, thereby reducing retries on the system</strong> - This statement is incorrect. The right way is to reduce the request timeout settings. This causes the client to abandon high latency requests after the specified time period and then send a second request that usually completes much faster than the first.</p>\n\n<p><strong>Reduce connection pooling, which keeps the connections alive even when user requests are not present, thereby, blocking the services</strong> - This is not correct. When you're not making requests, consider having the client send dummy traffic to a DynamoDB table. Alternatively, you can reuse client connections or use connection pooling. All of these techniques keep internal caches warm, which helps keep latency low.</p>\n\n<p><strong>Use DynamoDB Accelerator (DAX) for businesses with heavy write-only workloads</strong> - This is not correct. If your traffic is read-heavy, consider using a caching service such as DynamoDB Accelerator (DAX). DAX is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement—from milliseconds to microseconds—even at millions of requests per second.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-high-latency/\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-high-latency/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup, and restore and in-memory caching for internet-scale applications."
      },
      {
        "answer": "",
        "explanation": "<strong>Consider using Global tables if your application is accessed by globally distributed users</strong> - If you have globally dispersed users, consider using global tables. With global tables, you can specify the AWS Regions where you want the table to be available. This can significantly reduce latency for your users. So, reducing the distance between the client and the DynamoDB endpoint is an important performance fix to be considered."
      },
      {
        "answer": "",
        "explanation": "<strong>Use eventually consistent reads in place of strongly consistent reads whenever possible</strong> - If your application doesn't require strongly consistent reads, consider using eventually consistent reads. Eventually consistent reads are cheaper and are less likely to experience high latency."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Increase the request timeout settings, so the client gets enough time to complete the requests, thereby reducing retries on the system</strong> - This statement is incorrect. The right way is to reduce the request timeout settings. This causes the client to abandon high latency requests after the specified time period and then send a second request that usually completes much faster than the first."
      },
      {
        "answer": "",
        "explanation": "<strong>Reduce connection pooling, which keeps the connections alive even when user requests are not present, thereby, blocking the services</strong> - This is not correct. When you're not making requests, consider having the client send dummy traffic to a DynamoDB table. Alternatively, you can reuse client connections or use connection pooling. All of these techniques keep internal caches warm, which helps keep latency low."
      },
      {
        "answer": "",
        "explanation": "<strong>Use DynamoDB Accelerator (DAX) for businesses with heavy write-only workloads</strong> - This is not correct. If your traffic is read-heavy, consider using a caching service such as DynamoDB Accelerator (DAX). DAX is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement—from milliseconds to microseconds—even at millions of requests per second."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-high-latency/",
      "https://aws.amazon.com/dynamodb/"
    ]
  },
  {
    "id": 44,
    "question": "<p>You are working for a technology startup building web and mobile applications. You would like to pull Docker images from the ECR repository called <code>demo</code> so you can start running local tests against the latest application version.</p>\n\n<p>Which of the following commands must you run to pull existing Docker images from ECR? (Select two)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p><code>$(aws ecr get-login --no-include-email)</code></p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p><code>aws docker push 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p><code>docker pull 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p><code>docker build -t 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p><code>docker login -u $AWS_ACCESS_KEY_ID -p $AWS_SECRET_ACCESS_KEY</code></p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment",
    "explanation": "<p>Correct options:</p>\n\n<p><strong><code>$(aws ecr get-login --no-include-email)</code></strong></p>\n\n<p><strong><code>docker pull 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></strong></p>\n\n<p>The get-login command retrieves a token that is valid for a specified registry for 12 hours, and then it prints a docker login command with that authorization token. You can execute the printed command to log in to your registry with Docker, or just run it automatically using the $() command wrapper. After you have logged in to an Amazon ECR registry with this command, you can use the Docker CLI to push and pull images from that registry until the token expires. The docker pull command is used to pull an image from the ECR registry.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>docker login -u $AWS_ACCESS_KEY_ID -p $AWS_SECRET_ACCESS_KEY</code></strong> - You cannot login to AWS ECR this way. AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are only used by the CLI and not by docker.</p>\n\n<p><strong><code>aws docker push 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></strong> - docker push here is the wrong answer, you need to use docker pull.</p>\n\n<p><strong><code>docker build -t 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></strong> - This is a docker command that is used to build Docker images from a Dockerfile.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login.html\">https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong><code>$(aws ecr get-login --no-include-email)</code></strong>",
        "explanation": ""
      },
      {
        "answer": "<strong><code>docker pull 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The get-login command retrieves a token that is valid for a specified registry for 12 hours, and then it prints a docker login command with that authorization token. You can execute the printed command to log in to your registry with Docker, or just run it automatically using the $() command wrapper. After you have logged in to an Amazon ECR registry with this command, you can use the Docker CLI to push and pull images from that registry until the token expires. The docker pull command is used to pull an image from the ECR registry."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong><code>docker login -u $AWS_ACCESS_KEY_ID -p $AWS_SECRET_ACCESS_KEY</code></strong> - You cannot login to AWS ECR this way. AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are only used by the CLI and not by docker."
      },
      {
        "answer": "",
        "explanation": "<strong><code>aws docker push 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></strong> - docker push here is the wrong answer, you need to use docker pull."
      },
      {
        "answer": "",
        "explanation": "<strong><code>docker build -t 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</code></strong> - This is a docker command that is used to build Docker images from a Dockerfile."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login.html"
    ]
  },
  {
    "id": 45,
    "question": "<p>A photo-sharing application manages its EC2 server fleet running behind an Application Load Balancer and the traffic is fronted by a CloudFront distribution. The development team wants to decouple the user authentication process for the application so that the application servers can just focus on the business logic.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to address this use-case with minimal development effort?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Cognito Authentication via Cognito User Pools for your CloudFront distribution</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Cognito Authentication via Cognito Identity Pools for your CloudFront distribution</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Cognito Authentication via Cognito Identity Pools for your Application Load Balancer</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Cognito Authentication via Cognito User Pools for your Application Load Balancer</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito Authentication via Cognito User Pools for your Application Load Balancer</strong></p>\n\n<p>Application Load Balancer can be used to securely authenticate users for accessing your applications. This enables you to offload the work of authenticating users to your load balancer so that your applications can focus on their business logic. You can use Cognito User Pools to authenticate users through well-known social IdPs, such as Amazon, Facebook, or Google, through the user pools supported by Amazon Cognito or through corporate identities, using SAML, LDAP, or Microsoft AD, through the user pools supported by Amazon Cognito. You configure user authentication by creating an authenticate action for one or more listener rules. The authenticate-cognito and authenticate-oidc action types are supported only with HTTPS listeners.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a><p></p>\n\n<p>Please make sure that you adhere to the following configurations while using CloudFront distribution in front of your Application Load Balancer:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a><p></p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Cognito Authentication via Cognito Identity Pools for your Application Load Balancer</strong> - There is no such thing as using Cognito Authentication via Cognito Identity Pools for managing user authentication for the application. Application-specific user authentication can be provided via Cognito User Pools. Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token.</p>\n\n<p><strong>Use Cognito Authentication via Cognito User Pools for your CloudFront distribution</strong> - You cannot directly integrate Cognito User Pools with CloudFront distribution as you have to create a separate Lambda@Edge function to accomplish the authentication via Cognito User Pools. This involves additional development effort, so this option is not the best fit for the given use-case.</p>\n\n<p><strong>Use Cognito Authentication via Cognito Identity Pools for your CloudFront distribution</strong> - You cannot use Cognito Identity Pools for managing user authentication, so this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/\">https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use Cognito Authentication via Cognito User Pools for your Application Load Balancer</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Application Load Balancer can be used to securely authenticate users for accessing your applications. This enables you to offload the work of authenticating users to your load balancer so that your applications can focus on their business logic. You can use Cognito User Pools to authenticate users through well-known social IdPs, such as Amazon, Facebook, or Google, through the user pools supported by Amazon Cognito or through corporate identities, using SAML, LDAP, or Microsoft AD, through the user pools supported by Amazon Cognito. You configure user authentication by creating an authenticate action for one or more listener rules. The authenticate-cognito and authenticate-oidc action types are supported only with HTTPS listeners."
      },
      {
        "link": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i2.jpg",
        "answer": "",
        "explanation": "Please make sure that you adhere to the following configurations while using CloudFront distribution in front of your Application Load Balancer:"
      },
      {
        "link": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html"
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i3.jpg",
        "answer": "",
        "explanation": "Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:"
      },
      {
        "link": "https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Cognito Authentication via Cognito Identity Pools for your Application Load Balancer</strong> - There is no such thing as using Cognito Authentication via Cognito Identity Pools for managing user authentication for the application. Application-specific user authentication can be provided via Cognito User Pools. Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Cognito Authentication via Cognito User Pools for your CloudFront distribution</strong> - You cannot directly integrate Cognito User Pools with CloudFront distribution as you have to create a separate Lambda@Edge function to accomplish the authentication via Cognito User Pools. This involves additional development effort, so this option is not the best fit for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Cognito Authentication via Cognito Identity Pools for your CloudFront distribution</strong> - You cannot use Cognito Identity Pools for managing user authentication, so this option is not correct."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html",
      "https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/"
    ]
  },
  {
    "id": 46,
    "question": "<p>A developer in your company has configured a build using AWS CodeBuild. The build fails and the developer needs to quickly troubleshoot the issue to see which commands or settings located in the BuildSpec file are causing an issue.</p>\n\n<p>Which approach will help them accomplish this?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Run AWS CodeBuild locally using CodeBuild Agent</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>SSH into the CodeBuild Docker container</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Freeze the CodeBuild during its next execution</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable detailed monitoring</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Run AWS CodeBuild locally using CodeBuild Agent</strong></p>\n\n<p>AWS CodeBuild is a fully managed build service. There are no servers to provision and scale, or software to install, configure, and operate.</p>\n\n<p>With the Local Build support for AWS CodeBuild, you just specify the location of your source code, choose your build settings, and CodeBuild runs build scripts for compiling, testing, and packaging your code. You can use the AWS CodeBuild agent to test and debug builds on a local machine.</p>\n\n<p>By building an application on a local machine you can:</p>\n\n<p>Test the integrity and contents of a buildspec file locally.</p>\n\n<p>Test and build an application locally before committing.</p>\n\n<p>Identify and fix errors quickly from your local development environment.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSH into the CodeBuild Docker container</strong> - It is not possible to SSH into the CodeBuild Docker container, that's why you should test and fix errors locally.</p>\n\n<p><strong>Freeze the CodeBuild during its next execution</strong> - You cannot freeze the CodeBuild process but you can stop it. Please see more details on - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/stop-build.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/stop-build.html</a></p>\n\n<p><strong>Enable detailed monitoring</strong> - Detailed monitoring is available for EC2 instances. You do not enable detailed monitoring but you can specify output logs to be captured via CloudTrail.</p>\n\n<p>AWS CodeBuild is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in CodeBuild. CloudTrail captures all API calls for CodeBuild as events, including calls from the CodeBuild console and from code calls to the CodeBuild APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an S3 bucket, including events for CodeBuild.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/announcing-local-build-support-for-aws-codebuild/\">https://aws.amazon.com/blogs/devops/announcing-local-build-support-for-aws-codebuild/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/use-codebuild-agent.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/use-codebuild-agent.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Run AWS CodeBuild locally using CodeBuild Agent</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS CodeBuild is a fully managed build service. There are no servers to provision and scale, or software to install, configure, and operate."
      },
      {
        "answer": "",
        "explanation": "With the Local Build support for AWS CodeBuild, you just specify the location of your source code, choose your build settings, and CodeBuild runs build scripts for compiling, testing, and packaging your code. You can use the AWS CodeBuild agent to test and debug builds on a local machine."
      },
      {
        "answer": "",
        "explanation": "By building an application on a local machine you can:"
      },
      {
        "answer": "",
        "explanation": "Test the integrity and contents of a buildspec file locally."
      },
      {
        "answer": "",
        "explanation": "Test and build an application locally before committing."
      },
      {
        "answer": "",
        "explanation": "Identify and fix errors quickly from your local development environment."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>SSH into the CodeBuild Docker container</strong> - It is not possible to SSH into the CodeBuild Docker container, that's why you should test and fix errors locally."
      },
      {
        "link": "https://docs.aws.amazon.com/codebuild/latest/userguide/stop-build.html",
        "answer": "",
        "explanation": "<strong>Freeze the CodeBuild during its next execution</strong> - You cannot freeze the CodeBuild process but you can stop it. Please see more details on - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/stop-build.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/stop-build.html</a>"
      },
      {
        "answer": "",
        "explanation": "<strong>Enable detailed monitoring</strong> - Detailed monitoring is available for EC2 instances. You do not enable detailed monitoring but you can specify output logs to be captured via CloudTrail."
      },
      {
        "answer": "",
        "explanation": "AWS CodeBuild is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in CodeBuild. CloudTrail captures all API calls for CodeBuild as events, including calls from the CodeBuild console and from code calls to the CodeBuild APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an S3 bucket, including events for CodeBuild."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codebuild/latest/userguide/stop-build.html",
      "https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html",
      "https://aws.amazon.com/blogs/devops/announcing-local-build-support-for-aws-codebuild/",
      "https://docs.aws.amazon.com/codebuild/latest/userguide/use-codebuild-agent.html"
    ]
  },
  {
    "id": 47,
    "question": "<p>A startup manages its Cloud resources with Elastic Beanstalk. The environment consists of few Amazon EC2 instances, an Auto Scaling Group (ASG), and an Elastic Load Balancer. Even after the Load Balancer marked an EC2 instance as unhealthy, the ASG has not replaced it with a healthy instance.</p>\n\n<p>As a Developer, suggest the necessary configurations to automate the replacement of unhealthy instance.</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The health check type of your instance's Auto Scaling group, must be changed from EC2 to ELB by using a configuration file</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Auto Scaling group doesn't automatically replace the unhealthy instances marked by the load balancer. They have to be manually replaced from AWS Console</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The ping path field of the Load Balancer is configured incorrectly</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Health check parameters were configured for checking the instance health alone. The instance failed because of application failure which was not configured as a parameter for health check status</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The health check type of your instance's Auto Scaling group, must be changed from EC2 to ELB by using a configuration file</strong> - By default, the health check configuration of your Auto Scaling group is set as an EC2 type that performs a status check of EC2 instances. To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from EC2 to ELB by using a configuration file.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Health check parameters were configured for checking the instance health alone. The instance failed because of application failure which was not configured as a parameter for health check status</strong> - This is an incorrect statement. Status checks, by definition, cover only an EC2 instance's health, and not the health of your application, server, or any Docker containers running on the instance.</p>\n\n<p><strong>Auto Scaling group doesn't automatically replace the unhealthy instances marked by the load balancer. They have to be manually replaced from AWS Console</strong> - Incorrect statement. As discussed above, if the health check type of ASG is changed from EC2 to ELB, Auto Scaling will be able to replace the unhealthy instance.</p>\n\n<p><strong>The ping path field of the Load Balancer is configured incorrectly</strong> - Ping path is a health check configuration field of Elastic Load Balancer. If the ping path is configured wrong, ELB will not be able to reach the instance and hence will consider the instance unhealthy. However, this would then apply to all instances, not just once instance. So it does not address the issue given in the use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-instance-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-instance-automation/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The health check type of your instance's Auto Scaling group, must be changed from EC2 to ELB by using a configuration file</strong> - By default, the health check configuration of your Auto Scaling group is set as an EC2 type that performs a status check of EC2 instances. To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from EC2 to ELB by using a configuration file."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Health check parameters were configured for checking the instance health alone. The instance failed because of application failure which was not configured as a parameter for health check status</strong> - This is an incorrect statement. Status checks, by definition, cover only an EC2 instance's health, and not the health of your application, server, or any Docker containers running on the instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Auto Scaling group doesn't automatically replace the unhealthy instances marked by the load balancer. They have to be manually replaced from AWS Console</strong> - Incorrect statement. As discussed above, if the health check type of ASG is changed from EC2 to ELB, Auto Scaling will be able to replace the unhealthy instance."
      },
      {
        "answer": "",
        "explanation": "<strong>The ping path field of the Load Balancer is configured incorrectly</strong> - Ping path is a health check configuration field of Elastic Load Balancer. If the ping path is configured wrong, ELB will not be able to reach the instance and hence will consider the instance unhealthy. However, this would then apply to all instances, not just once instance. So it does not address the issue given in the use-case."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-instance-automation/",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html"
    ]
  },
  {
    "id": 48,
    "question": "<p>You are a software engineer working for an IT company and are asked to contribute to a growing internal application that includes dashboards for data visualization. You are provisioning your AWS DynamoDB table and need to perform 10 strongly consistent reads per second of 4 KB in size each.</p>\n\n<p>How many Read Capacity Units (RCUs) are needed?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>20</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>40</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>10</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>5</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>Before proceeding with the calculations, please review the following:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q38-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q38-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q38-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q38-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p><strong>10</strong></p>\n\n<p>One Read Capacity Unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. The total number of Read Capacity Units required depends on the item size, and whether you want an eventually consistent or strongly consistent read.</p>\n\n<p>1) Item Size / 4KB, rounding to the nearest whole number.</p>\n\n<p>So, in the above case, 4KB / 4 KB = 1 read capacity unit.</p>\n\n<p>2) 1 read capacity unit per item (since strongly consistent read) × No of reads per second</p>\n\n<p>So, in the above case, 1 x 10 = 10 read capacity units.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>40</strong></p>\n\n<p><strong>20</strong></p>\n\n<p><strong>5</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Before proceeding with the calculations, please review the following:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html",
        "answer": "",
        "explanation": "via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a>"
      },
      {
        "answer": "<strong>10</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "One Read Capacity Unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. The total number of Read Capacity Units required depends on the item size, and whether you want an eventually consistent or strongly consistent read."
      },
      {
        "answer": "",
        "explanation": "1) Item Size / 4KB, rounding to the nearest whole number."
      },
      {
        "answer": "",
        "explanation": "So, in the above case, 4KB / 4 KB = 1 read capacity unit."
      },
      {
        "answer": "",
        "explanation": "2) 1 read capacity unit per item (since strongly consistent read) × No of reads per second"
      },
      {
        "answer": "",
        "explanation": "So, in the above case, 1 x 10 = 10 read capacity units."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>40</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>20</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>5</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the details provided in the explanation above, so these are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html"
    ]
  },
  {
    "id": 49,
    "question": "<p>A developer has just integrated an AWS Lambda function to an Amazon API Gateway API. The integration has led to errors that the developer is unable to troubleshoot. The developer has decided to enable CloudWatch logging at the method level for the API Gateway API.</p>\n\n<p>What are the key points of consideration while configuring configuring method-level logging for the API Gateway? (Select two)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>API Gateway API log groups or streams can only be deleted and recreated by redeploying the API</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>In access logging, only $context and $input variables are supported</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Security Token Service(STS) is used by API Gateway for logging data to CloudWatch logs. Hence, AWS STS has to be enabled for the Region that you're using</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>To enable CloudWatch Logs for all or only some of the methods, you must also specify the ARN of an IAM role that enables API Gateway to write information to CloudWatch Logs on behalf of your user. The IAM role must also contain the following trust relationship statement</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>You are charged for accessing method-level and stage-level CloudWatch metrics, but not for API-level metrics</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>AWS Security Token Service(STS) is used by API Gateway for logging data to CloudWatch logs. Hence, AWS STS has to be enabled for the Region that you're using</strong></p>\n\n<p>API Gateway calls AWS Security Token Service to assume the IAM role, so make sure that AWS STS is enabled for the Region. If you receive an error when setting the IAM role ARN, check your AWS Security Token Service account settings to make sure that AWS STS is enabled in the Region that you're using.</p>\n\n<p><strong>To enable CloudWatch Logs for all or only some of the methods, you must also specify the ARN of an IAM role that enables API Gateway to write information to CloudWatch Logs on behalf of your user. The IAM role must also contain the following trust relationship statement</strong></p>\n\n<p>To enable CloudWatch Logs for all or only some of the methods, you must also specify the ARN of an IAM role that enables API Gateway to write information to CloudWatch Logs on behalf of your user. To do so, choose <code>Settings</code> from the APIs main navigation pane. Then enter the ARN of an IAM role in the <code>CloudWatch log role ARN</code> text field. The IAM role must also contain the trust relationship statement.</p>\n\n<p>Policy of AmazonAPIGatewayPushToCloudWatchLogs for IAM role:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q9-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q9-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html#how-to-stage-settings-console\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html#how-to-stage-settings-console</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In access logging, only $context and $input variables are supported</strong> - This statement is incorrect. In access logging, you, as an API developer, want to log who has accessed your API and how the caller accessed the API. You can create your own log group or choose an existing log group that could be managed by API Gateway. To specify the access details, you select $context variables and choose a log group as the destination. Only $context variables are supported (not $input, and so on).</p>\n\n<p><strong>You are charged for accessing method-level and stage-level CloudWatch metrics, but not for API-level metrics</strong> - This statement is incorrect. Your account is charged for accessing method-level CloudWatch metrics, but not the API-level or stage-level metrics.</p>\n\n<p><strong>API Gateway API log groups or streams can only be deleted and recreated by redeploying the API</strong> - API Gateway API log groups or streams can be deleted from the CloudWatch console. But, it is not recommended.</p>\n\n<p>Do not manually delete API Gateway API log groups or streams; let API Gateway manage these resources. Manually deleting log groups or streams may cause API requests and responses not to be logged. If that happens, you can delete the entire log group for the API and redeploy the API. This is because API Gateway creates log groups or log streams for an API stage at the time when it is deployed.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html#how-to-stage-settings-console\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html#how-to-stage-settings-console</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-logging.html#apigateway-cloudwatch-log-formats\">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-logging.html#apigateway-cloudwatch-log-formats</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/view-cloudwatch-log-events-in-cloudwatch-console.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/view-cloudwatch-log-events-in-cloudwatch-console.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>AWS Security Token Service(STS) is used by API Gateway for logging data to CloudWatch logs. Hence, AWS STS has to be enabled for the Region that you're using</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "API Gateway calls AWS Security Token Service to assume the IAM role, so make sure that AWS STS is enabled for the Region. If you receive an error when setting the IAM role ARN, check your AWS Security Token Service account settings to make sure that AWS STS is enabled in the Region that you're using."
      },
      {
        "answer": "<strong>To enable CloudWatch Logs for all or only some of the methods, you must also specify the ARN of an IAM role that enables API Gateway to write information to CloudWatch Logs on behalf of your user. The IAM role must also contain the following trust relationship statement</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "To enable CloudWatch Logs for all or only some of the methods, you must also specify the ARN of an IAM role that enables API Gateway to write information to CloudWatch Logs on behalf of your user. To do so, choose <code>Settings</code> from the APIs main navigation pane. Then enter the ARN of an IAM role in the <code>CloudWatch log role ARN</code> text field. The IAM role must also contain the trust relationship statement."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q9-i1.jpg",
        "answer": "",
        "explanation": "Policy of AmazonAPIGatewayPushToCloudWatchLogs for IAM role:"
      },
      {
        "link": "https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html#how-to-stage-settings-console"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>In access logging, only $context and $input variables are supported</strong> - This statement is incorrect. In access logging, you, as an API developer, want to log who has accessed your API and how the caller accessed the API. You can create your own log group or choose an existing log group that could be managed by API Gateway. To specify the access details, you select $context variables and choose a log group as the destination. Only $context variables are supported (not $input, and so on)."
      },
      {
        "answer": "",
        "explanation": "<strong>You are charged for accessing method-level and stage-level CloudWatch metrics, but not for API-level metrics</strong> - This statement is incorrect. Your account is charged for accessing method-level CloudWatch metrics, but not the API-level or stage-level metrics."
      },
      {
        "answer": "",
        "explanation": "<strong>API Gateway API log groups or streams can only be deleted and recreated by redeploying the API</strong> - API Gateway API log groups or streams can be deleted from the CloudWatch console. But, it is not recommended."
      },
      {
        "answer": "",
        "explanation": "Do not manually delete API Gateway API log groups or streams; let API Gateway manage these resources. Manually deleting log groups or streams may cause API requests and responses not to be logged. If that happens, you can delete the entire log group for the API and redeploy the API. This is because API Gateway creates log groups or log streams for an API stage at the time when it is deployed."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html#how-to-stage-settings-console",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-logging.html#apigateway-cloudwatch-log-formats",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/view-cloudwatch-log-events-in-cloudwatch-console.html"
    ]
  },
  {
    "id": 50,
    "question": "<p>You are a developer working at a cloud company that embraces serverless. You have performed your initial deployment and would like to work towards adding API Gateway stages and associate them with existing deployments. Your stages will include prod, test, and dev and will need to match a Lambda function variant that can be updated over time.</p>\n\n<p>Which of the following features must you add to achieve this? (select two)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Lambda Aliases</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Lambda Versions</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Stage Variables</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Mapping Templates</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Lambda X-Ray integration</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Stage Variables</strong></p>\n\n<p>Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of an API. They act like environment variables and can be used in your API setup and mapping templates. With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.</p>\n\n<p>For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage and calls a different web host (for example, beta.example.com).</p>\n\n<p><strong>Lambda Aliases</strong></p>\n\n<p>A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.</p>\n\n<p>Lambda Aliases allow you to create a \"mutable\" Lambda version that points to whatever version you want in the backend. This allows you to have a \"dev\", \"test\", prod\" Lambda alias that can remain stable over time.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Lambda Versions</strong> - Versions are immutable and cannot be updated over time. So this option is not correct.</p>\n\n<p><strong>Lambda X-Ray integration</strong> - This is good for tracing and debugging requests so it can be looked at as a good option for troubleshooting issues in the future. This is not the right fit for the given use-case.</p>\n\n<p><strong>Mapping Templates</strong> - Mapping template overrides provides you with the flexibility to perform many-to-one parameter mappings; override parameters after standard API Gateway mappings have been applied; conditionally map parameters based on body content or other parameter values; programmatically create new parameters on the fly, and override status codes returned by your integration endpoint. This is not the right fit for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Stage Variables</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of an API. They act like environment variables and can be used in your API setup and mapping templates. With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints."
      },
      {
        "answer": "",
        "explanation": "For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage and calls a different web host (for example, beta.example.com)."
      },
      {
        "answer": "<strong>Lambda Aliases</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN."
      },
      {
        "answer": "",
        "explanation": "Lambda Aliases allow you to create a \"mutable\" Lambda version that points to whatever version you want in the backend. This allows you to have a \"dev\", \"test\", prod\" Lambda alias that can remain stable over time."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Lambda Versions</strong> - Versions are immutable and cannot be updated over time. So this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Lambda X-Ray integration</strong> - This is good for tracing and debugging requests so it can be looked at as a good option for troubleshooting issues in the future. This is not the right fit for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Mapping Templates</strong> - Mapping template overrides provides you with the flexibility to perform many-to-one parameter mappings; override parameters after standard API Gateway mappings have been applied; conditionally map parameters based on body content or other parameter values; programmatically create new parameters on the fly, and override status codes returned by your integration endpoint. This is not the right fit for the given use-case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html"
    ]
  },
  {
    "id": 51,
    "question": "<p>A developer while working on Amazon EC2 instances, realized that an instance was not needed and had shut it down. But another instance of the same type automatically got launched in the account.</p>\n\n<p>Which of the following options can attribute the given sequence of actions?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The instance could have been a part of Application Load Balancer and hence was automatically started</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The user did not have the right permissions to shutdown the instance. User needs root permissions to terminate an instance</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The instance could have been a part of Network Load Balancer and hence was automatically started</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Instance might be part of Auto Scaling Group and hence re-launched similar instance</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Instance might be part of Auto Scaling Group and hence re-launched similar instance</strong> - Auto Scaling groups can be configured to launch an instance to replace an instance that is undergoing maintenance. This could have been the reason why an instance of the same type got launched automatically. The size of an Auto Scaling group depends on the number of instances that you set as the desired capacity. If you wish to terminate an instance that is part of Auto Scaling Group, the configuration of the group should be changed to a reduced number of instances, so the automatic launch of instances does not happen when an unwanted instance is terminated.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The user did not have the right permissions to shutdown the instance. User needs root permissions to terminate an instance</strong> - This is an incorrect statement. If the user does not have enough permissions, then the action itself is unavailable for him. A user does not need root permissions to terminate an EC2 instance.</p>\n\n<p><strong>The instance could have been a part of the Application Load Balancer and hence was automatically started</strong> - Application Load Balancer is used to balance the incoming traffic requests equally among the available EC2 instances so keep the performance and availability at its best. ALBs are configured with Auto Scaling Groups, but this is not specified in the use-case. In the absence of Auto Scaling Group, ALB cannot launch instances by itself.</p>\n\n<p><strong>The instance could have been a part of Network Load Balancer and hence was automatically started</strong> - As explained above for ALB, a Network Load Balancer is not capable of launching instances by itself if it's not configured with an Auto Scaling Group.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/detach-instance-asg.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/detach-instance-asg.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Instance might be part of Auto Scaling Group and hence re-launched similar instance</strong> - Auto Scaling groups can be configured to launch an instance to replace an instance that is undergoing maintenance. This could have been the reason why an instance of the same type got launched automatically. The size of an Auto Scaling group depends on the number of instances that you set as the desired capacity. If you wish to terminate an instance that is part of Auto Scaling Group, the configuration of the group should be changed to a reduced number of instances, so the automatic launch of instances does not happen when an unwanted instance is terminated."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The user did not have the right permissions to shutdown the instance. User needs root permissions to terminate an instance</strong> - This is an incorrect statement. If the user does not have enough permissions, then the action itself is unavailable for him. A user does not need root permissions to terminate an EC2 instance."
      },
      {
        "answer": "",
        "explanation": "<strong>The instance could have been a part of the Application Load Balancer and hence was automatically started</strong> - Application Load Balancer is used to balance the incoming traffic requests equally among the available EC2 instances so keep the performance and availability at its best. ALBs are configured with Auto Scaling Groups, but this is not specified in the use-case. In the absence of Auto Scaling Group, ALB cannot launch instances by itself."
      },
      {
        "answer": "",
        "explanation": "<strong>The instance could have been a part of Network Load Balancer and hence was automatically started</strong> - As explained above for ALB, a Network Load Balancer is not capable of launching instances by itself if it's not configured with an Auto Scaling Group."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/detach-instance-asg.html"
    ]
  },
  {
    "id": 52,
    "question": "<p>A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed.</p>\n\n<p>Which of the following options represents the best solution for the given requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Elastic File System (EFS) Standard–IA storage class</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon Elastic Block Store (EBS)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Elastic File System (EFS) Standard storage class</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Elastic File System (EFS) Standard–IA storage class</strong> - Amazon EFS is a file storage service for use with Amazon compute (EC2, containers, serverless) and on-premises servers. Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently accessible storage for up to thousands of Amazon EC2 instances.</p>\n\n<p>The Standard–IA storage class reduces storage costs for files that are not accessed every day. It does this without sacrificing the high availability, high durability, elasticity, and POSIX file system access that Amazon EFS provides. AWS recommends Standard-IA storage if you need your full dataset to be readily accessible and want to automatically save on storage costs for files that are less frequently accessed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class</strong> - Amazon S3 is an object storage service. Amazon S3 makes data available through an Internet API that can be accessed anywhere. It is not a file storage service, as is needed in the use case.</p>\n\n<p><strong>Amazon Elastic File System (EFS) Standard storage class</strong> - Amazon EFS Standard storage classes are ideal for workloads that require the highest levels of durability and availability. The EFS Standard storage class is used for frequently accessed files. It is the storage class to which customer data is initially written for Standard storage classes. The company is also looking at cutting costs by optimally storing the infrequently accessed data. Hence, EFS standard storage class is not the right solution for the given use case.</p>\n\n<p><strong>Amazon Elastic Block Store (EBS)</strong> - Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest latency access to data from a single EC2 instance. EBS volume cannot be accessed by hundreds of EC2 instances concurrently. It is not a file storage service, as is needed in the use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html\">https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Elastic File System (EFS) Standard–IA storage class</strong> - Amazon EFS is a file storage service for use with Amazon compute (EC2, containers, serverless) and on-premises servers. Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently accessible storage for up to thousands of Amazon EC2 instances."
      },
      {
        "answer": "",
        "explanation": "The Standard–IA storage class reduces storage costs for files that are not accessed every day. It does this without sacrificing the high availability, high durability, elasticity, and POSIX file system access that Amazon EFS provides. AWS recommends Standard-IA storage if you need your full dataset to be readily accessible and want to automatically save on storage costs for files that are less frequently accessed."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class</strong> - Amazon S3 is an object storage service. Amazon S3 makes data available through an Internet API that can be accessed anywhere. It is not a file storage service, as is needed in the use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Elastic File System (EFS) Standard storage class</strong> - Amazon EFS Standard storage classes are ideal for workloads that require the highest levels of durability and availability. The EFS Standard storage class is used for frequently accessed files. It is the storage class to which customer data is initially written for Standard storage classes. The company is also looking at cutting costs by optimally storing the infrequently accessed data. Hence, EFS standard storage class is not the right solution for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Elastic Block Store (EBS)</strong> - Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest latency access to data from a single EC2 instance. EBS volume cannot be accessed by hundreds of EC2 instances concurrently. It is not a file storage service, as is needed in the use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html"
    ]
  },
  {
    "id": 53,
    "question": "<p>A developer is configuring an Amazon EC2 Auto Scaling Group that has to launch both Spot and On-Demand instances based on the requirement. Also, the CodeDeploy agent has to be automatically installed on these EC2 instances. All the EC2 instances are running on the Amazon Linux operating system.</p>\n\n<p>What is the most operationally efficient way to configure this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use launch configurations to configure the EC2 Auto Scaling Group for On-Demand and spot instances. Add the shell script to the <code>Launch configuration</code> tab on the AWS console. This shell script will install the CodeDeploy agent</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Systems Manager for installing and updating the CodeDeploy agent automatically for Spot and On-Demand instances</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure AWS Resource Access Manager(RAM) to schedule the automatic install of CodeDeploy agent on the EC2 instances. RAM automatic schedules work on only Linux machines and not on Windows operating systems</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use launch templates to configure the EC2 Auto Scaling Group for On-Demand and spot instances. When you create a launch template use the User data field to add a configuration script that runs when the instance starts. This shell script can, in turn, install the CodeDeploy agent</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use launch templates to configure the EC2 Auto Scaling Group for On-Demand and spot instances. When you create a launch template use the User data field to add a configuration script that runs when the instance starts. This shell script can, in turn, install the CodeDeploy agent</strong></p>\n\n<p>A launch template specifies instance configuration information that includes the ID of the Amazon Machine Image (AMI), the instance type, a key pair, security groups, and other parameters used to launch EC2 instances. Defining a launch template instead of a launch configuration allows you to have multiple versions of a launch template.</p>\n\n<p>When you create a launch template, you can use the User data field to add a configuration script that runs when the instance starts. This shell script installs the CodeDeploy agent for all AWS Regions and supported Amazon Linux and Ubuntu distributions. You can configure CodeDeploy to auto-update on boot by setting the AUTOUPDATE variable to true.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use launch configurations to configure the EC2 Auto Scaling Group for On-Demand and spot instances. Add the shell script to the <code>Launch configuration</code> tab on the AWS console. This shell script will install the CodeDeploy agent</strong> - This statement is incorrect. Not all Amazon EC2 Auto Scaling features are available when you use launch configurations. For example, you cannot create an Auto Scaling group that launches both Spot and On-Demand Instances or that specifies multiple instance types. You must use a launch template to configure these features. AWS strongly recommends not using launch configurations anymore.</p>\n\n<p><strong>Use AWS Systems Manager for installing and updating the CodeDeploy agent automatically for Spot and On-Demand instances</strong> - AWS Systems Manager also uses launch templates to automatically deploy CodeDeploy agent. However, for this use case, SSM is not operationally efficient since SSM agent needs to be installed first on all EC2 instances and then you can install the CodeDeploy agent.</p>\n\n<p><strong>Configure AWS Resource Access Manager(RAM) to schedule the automatic installation of CodeDeploy agent on the EC2 instances. RAM automatic schedules work on only Linux machines and not on Windows operating systems</strong> - AWS RAM helps you securely share your resources across AWS accounts, within your organization or organizational units (OUs), and with IAM roles and users for supported resource types. You can use AWS RAM to share resources with other AWS accounts. This eliminates the need to provision and manage resources in every account. RAM cannot be used to install the CodeDeploy agent, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/codedeploy-agent-launch-template/\">https://aws.amazon.com/premiumsupport/knowledge-center/codedeploy-agent-launch-template/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-configurations.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-configurations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent-operations-install-ssm.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent-operations-install-ssm.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use launch templates to configure the EC2 Auto Scaling Group for On-Demand and spot instances. When you create a launch template use the User data field to add a configuration script that runs when the instance starts. This shell script can, in turn, install the CodeDeploy agent</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A launch template specifies instance configuration information that includes the ID of the Amazon Machine Image (AMI), the instance type, a key pair, security groups, and other parameters used to launch EC2 instances. Defining a launch template instead of a launch configuration allows you to have multiple versions of a launch template."
      },
      {
        "answer": "",
        "explanation": "When you create a launch template, you can use the User data field to add a configuration script that runs when the instance starts. This shell script installs the CodeDeploy agent for all AWS Regions and supported Amazon Linux and Ubuntu distributions. You can configure CodeDeploy to auto-update on boot by setting the AUTOUPDATE variable to true."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use launch configurations to configure the EC2 Auto Scaling Group for On-Demand and spot instances. Add the shell script to the <code>Launch configuration</code> tab on the AWS console. This shell script will install the CodeDeploy agent</strong> - This statement is incorrect. Not all Amazon EC2 Auto Scaling features are available when you use launch configurations. For example, you cannot create an Auto Scaling group that launches both Spot and On-Demand Instances or that specifies multiple instance types. You must use a launch template to configure these features. AWS strongly recommends not using launch configurations anymore."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Systems Manager for installing and updating the CodeDeploy agent automatically for Spot and On-Demand instances</strong> - AWS Systems Manager also uses launch templates to automatically deploy CodeDeploy agent. However, for this use case, SSM is not operationally efficient since SSM agent needs to be installed first on all EC2 instances and then you can install the CodeDeploy agent."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure AWS Resource Access Manager(RAM) to schedule the automatic installation of CodeDeploy agent on the EC2 instances. RAM automatic schedules work on only Linux machines and not on Windows operating systems</strong> - AWS RAM helps you securely share your resources across AWS accounts, within your organization or organizational units (OUs), and with IAM roles and users for supported resource types. You can use AWS RAM to share resources with other AWS accounts. This eliminates the need to provision and manage resources in every account. RAM cannot be used to install the CodeDeploy agent, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/codedeploy-agent-launch-template/",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-configurations.html",
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent-operations-install-ssm.html"
    ]
  },
  {
    "id": 54,
    "question": "<p>A developer has just completed configuring the Application Load Balancer for the EC2 instances. Just as he started testing his configuration, he realized that he has missed assigning target groups to his ALB.</p>\n\n<p>Which error code should he expect in his debug logs?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>HTTP 503</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>HTTP 500</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>HTTP 403</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>HTTP 504</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>HTTP 503</strong> - HTTP 503 indicates 'Service unavailable' error. This error in ALB is an indicator of the target groups for the load balancer having no registered targets.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>HTTP 500</strong> - HTTP 500 indicates 'Internal server' error. There are several reasons for their error: A client submitted a request without an HTTP protocol, and the load balancer was unable to generate a redirect URL, there was an error executing the web ACL rules.</p>\n\n<p><strong>HTTP 504</strong> - HTTP 504 is 'Gateway timeout' error. Several reasons for this error, to quote a few: The load balancer failed to establish a connection to the target before the connection timeout expired, The load balancer established a connection to the target but the target did not respond before the idle timeout period elapsed.</p>\n\n<p><strong>HTTP 403</strong> - HTTP 403 is 'Forbidden' error. You configured an AWS WAF web access control list (web ACL) to monitor requests to your Application Load Balancer and it blocked a request.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>HTTP 503</strong> - HTTP 503 indicates 'Service unavailable' error. This error in ALB is an indicator of the target groups for the load balancer having no registered targets."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>HTTP 500</strong> - HTTP 500 indicates 'Internal server' error. There are several reasons for their error: A client submitted a request without an HTTP protocol, and the load balancer was unable to generate a redirect URL, there was an error executing the web ACL rules."
      },
      {
        "answer": "",
        "explanation": "<strong>HTTP 504</strong> - HTTP 504 is 'Gateway timeout' error. Several reasons for this error, to quote a few: The load balancer failed to establish a connection to the target before the connection timeout expired, The load balancer established a connection to the target but the target did not respond before the idle timeout period elapsed."
      },
      {
        "answer": "",
        "explanation": "<strong>HTTP 403</strong> - HTTP 403 is 'Forbidden' error. You configured an AWS WAF web access control list (web ACL) to monitor requests to your Application Load Balancer and it blocked a request."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html"
    ]
  },
  {
    "id": 55,
    "question": "<p>A development team has configured an Elastic Load Balancer for host-based routing. The idea is to support multiple subdomains and different top-level domains.</p>\n\n<p>The rule *.sample.com matches which of the following?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SAMPLE.COM</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>sample.com</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>sample.test.com</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>test.sample.com</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>test.sample.com</strong> - You can use host conditions to define rules that route requests based on the hostname in the host header (also known as host-based routing). This enables you to support multiple subdomains and different top-level domains using a single load balancer.</p>\n\n<p>A hostname is not case-sensitive, can be up to 128 characters in length, and can contain any of the following characters:\n1. A–Z, a–z, 0–9\n2. - .\n3. * (matches 0 or more characters)\n4. ? (matches exactly 1 character)</p>\n\n<p>You must include at least one \".\" character. You can include only alphabetical characters after the final \".\" character.</p>\n\n<p>The rule *.sample.com matches test.sample.com but doesn't match sample.com.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>sample.com</strong></p>\n\n<p><strong>sample.test.com</strong></p>\n\n<p><strong>SAMPLE.COM</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>test.sample.com</strong> - You can use host conditions to define rules that route requests based on the hostname in the host header (also known as host-based routing). This enables you to support multiple subdomains and different top-level domains using a single load balancer."
      },
      {
        "answer": "",
        "explanation": "A hostname is not case-sensitive, can be up to 128 characters in length, and can contain any of the following characters:\n1. A–Z, a–z, 0–9\n2. - .\n3. * (matches 0 or more characters)\n4. ? (matches exactly 1 character)"
      },
      {
        "answer": "",
        "explanation": "You must include at least one \".\" character. You can include only alphabetical characters after the final \".\" character."
      },
      {
        "answer": "",
        "explanation": "The rule *.sample.com matches test.sample.com but doesn't match sample.com."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>sample.com</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>sample.test.com</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>SAMPLE.COM</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html"
    ]
  },
  {
    "id": 56,
    "question": "<p>You're a developer maintaining a web application written in .NET. The application makes references to public objects in a public S3 accessible bucket using a public URL. While doing a code review your colleague advises that the approach is not a best practice because some of the objects contain private data. After the administrator makes the S3 bucket private you can no longer access the S3 objects but you would like to create an application that will enable people to access some objects as needed with a time policy constraint.</p>\n\n<p>Which of the following options will give access to the objects?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Using IAM policy</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Using pre-signed URL</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Using Routing Policy</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Using bucket policy</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p>\"Using pre-signed URL\"</p>\n\n<p>All objects by default are private, with object owner having permission to access the objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The pre-signed URLs are valid only for the specified duration.</p>\n\n<p>Please see this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q36-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q36-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a><p></p>\n\n<p>Incorrect:</p>\n\n<p>\"Using bucket policy\" - You can use this policy to limit users from a source IP address however for time-based constraints you are better off using a pre-signed URL.</p>\n\n<p>\"Using Routing Policy\" - This concept applies to DNS in Route 53, so this option is ruled out.</p>\n\n<p>\"Using IAM policy\" - You can use IAM policy to grant access toa specific bucket however for time-based constraints you are better off using a pre-signed URL.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Using pre-signed URL</strong>"
      },
      {
        "answer": "",
        "explanation": "All objects by default are private, with object owner having permission to access the objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The pre-signed URLs are valid only for the specified duration."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q36-i1.jpg",
        "answer": "",
        "explanation": "Please see this note for more details:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html"
      },
      {
        "answer": "",
        "explanation": "Incorrect:"
      },
      {
        "answer": "",
        "explanation": "<strong>Using bucket policy</strong> - You can use this policy to limit users from a source IP address however for time-based constraints you are better off using a pre-signed URL."
      },
      {
        "answer": "",
        "explanation": "<strong>Using Routing Policy</strong> - This concept applies to DNS in Route 53, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Using IAM policy</strong> - You can use IAM policy to grant access toa specific bucket however for time-based constraints you are better off using a pre-signed URL."
      }
    ],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html"
    ]
  },
  {
    "id": 57,
    "question": "<p>A company stores confidential data on an Amazon Simple Storage Service (S3) bucket. New regulatory guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage S3 encryption keys.</p>\n\n<p>Which of the following options should you use?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SSE-C</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Client Side Encryption</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SSE-S3</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>SSE-KMS</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>SSE-S3</strong></p>\n\n<p>Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-C</strong> - You manage the encryption keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects.</p>\n\n<p><strong>Client-Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p><strong>SSE-KMS</strong> - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>SSE-S3</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>SSE-C</strong> - You manage the encryption keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects."
      },
      {
        "answer": "",
        "explanation": "<strong>Client-Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools."
      },
      {
        "answer": "",
        "explanation": "<strong>SSE-KMS</strong> - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html"
    ]
  },
  {
    "id": 58,
    "question": "<p>A development team has inherited a web application running in the us-east-1 region with three availability zones (us-east-1a, us-east1-b, and us-east-1c) whose incoming web traffic is routed by a load balancer. When one of the EC2 instances hosting the web application crashes, the team realizes that the load balancer continues to route traffic to that instance causing intermittent issues.</p>\n\n<p>Which of the following should the development team do to minimize this problem?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable Health Checks</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable SSL</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable Stickiness</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable Multi AZ deployments</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable Health Checks</strong></p>\n\n<p>To discover the availability of your EC2 instances, a load balancer periodically sends pings, attempts connections, or sends requests to test the EC2 instances. These tests are called health checks. The status of the instances that are healthy at the time of the health check is InService. The status of any instances that are unhealthy at the time of the health check is OutOfService.</p>\n\n<p>Load Balancer Health Checks:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q39-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q39-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Stickiness</strong> - Stickiness enables the load balancer to bind a user's session to a specific instance, it cannot be used for gauging the health of an instance.</p>\n\n<p><strong>Enable Multi-AZ deployments</strong> - It's a good practice to provision instances in more than one availability zone however you still need a way to check the health status of the instances, so this option is incorrect.</p>\n\n<p><strong>Enable SSL</strong> - This option has been added as a distractor. SSL encrypts the transmission of data between a web server and a browser.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Enable Health Checks</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "To discover the availability of your EC2 instances, a load balancer periodically sends pings, attempts connections, or sends requests to test the EC2 instances. These tests are called health checks. The status of the instances that are healthy at the time of the health check is InService. The status of any instances that are unhealthy at the time of the health check is OutOfService."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q39-i1.jpg",
        "answer": "",
        "explanation": "Load Balancer Health Checks:"
      },
      {
        "link": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable Stickiness</strong> - Stickiness enables the load balancer to bind a user's session to a specific instance, it cannot be used for gauging the health of an instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable Multi-AZ deployments</strong> - It's a good practice to provision instances in more than one availability zone however you still need a way to check the health status of the instances, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable SSL</strong> - This option has been added as a distractor. SSL encrypts the transmission of data between a web server and a browser."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html"
    ]
  },
  {
    "id": 59,
    "question": "<p>As a Developer Associate, you are responsible for the data management of the AWS Kinesis streams at your company. The security team has mandated stricter security requirements by leveraging mechanisms available with the Kinesis Data Streams service that won't require code changes on your end.</p>\n\n<p>Which of the following features meet the given requirements? (Select two)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Envelope Encryption</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Client-Side Encryption</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>KMS encryption for data at rest</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Encryption in flight with HTTPS endpoint</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>SSE-C encryption</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Security",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>KMS encryption for data at rest</strong></p>\n\n<p><strong>Encryption in flight with HTTPS endpoint</strong></p>\n\n<p>Server-side encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it's written to the Kinesis stream storage layer and decrypted after it's retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. Also, the HTTPS protocol ensures that data inflight is encrypted as well.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-C encryption</strong> - SSE-C is functionality in Amazon S3 where S3 encrypts your data, on your behalf, using keys that you provide. This does not apply for the given use-case.</p>\n\n<p><strong>Client-Side Encryption</strong> - This involves code changes, so the option is incorrect.</p>\n\n<p><strong>Envelope Encryption</strong> - This involves code changes, so the option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html\">https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>KMS encryption for data at rest</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Encryption in flight with HTTPS endpoint</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Server-side encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it's written to the Kinesis stream storage layer and decrypted after it's retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. Also, the HTTPS protocol ensures that data inflight is encrypted as well."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>SSE-C encryption</strong> - SSE-C is functionality in Amazon S3 where S3 encrypts your data, on your behalf, using keys that you provide. This does not apply for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Client-Side Encryption</strong> - This involves code changes, so the option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Envelope Encryption</strong> - This involves code changes, so the option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html"
    ]
  },
  {
    "id": 60,
    "question": "<p>Your application sends messages to an Amazon Simple Queue Service (SQS) queue frequently, which are then polled by another application that specifies which message to retrieve.</p>\n\n<p>Which of the following options describe the maximum number of messages that can be retrieved at one time?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>20</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>5</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>10</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>100</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>10</strong></p>\n\n<p>After you send messages to a queue, you can receive and delete them. When you request messages from a queue, you can't specify which messages to retrieve. Instead, you specify the maximum number of messages (up to 10) that you want to retrieve.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>5</strong></p>\n\n<p><strong>20</strong></p>\n\n<p><strong>100</strong></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-receive-delete-message.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-receive-delete-message.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>10</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "After you send messages to a queue, you can receive and delete them. When you request messages from a queue, you can't specify which messages to retrieve. Instead, you specify the maximum number of messages (up to 10) that you want to retrieve."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>5</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>20</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>100</strong>",
        "explanation": ""
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-receive-delete-message.html"
    ]
  },
  {
    "id": 61,
    "question": "<p>A video encoding application running on an EC2 instance takes about 20 seconds on average to process each raw footage file. The application picks the new job messages from an SQS queue. The development team needs to account for the use-case when the video encoding process takes longer than usual so that the same raw footage is not processed by multiple consumers.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to address this use-case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use ChangeMessageVisibility action to extend a message's visibility timeout</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use WaitTimeSeconds action to short poll and extend a message's visibility timeout</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use DelaySeconds action to delay a message's visibility timeout</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use WaitTimeSeconds action to long poll and extend a message's visibility timeout</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use ChangeMessageVisibility action to extend a message's visibility timeout</strong></p>\n\n<p>Amazon SQS uses a visibility timeout to prevent other consumers from receiving and processing the same message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q5-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q5-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a><p></p>\n\n<p>For example, you have a message with a visibility timeout of 5 minutes. After 3 minutes, you call ChangeMessageVisibility with a timeout of 10 minutes. You can continue to call ChangeMessageVisibility to extend the visibility timeout to the maximum allowed time. If you try to extend the visibility timeout beyond the maximum, your request is rejected. So, for the given use-case, the application can set the initial visibility timeout to 1 minute and then continue to update the ChangeMessageVisibility value if required.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q5-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q5-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use DelaySeconds action to delay a message's visibility timeout</strong> - Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. To set delay seconds on individual messages, rather than on an entire queue, use message timers to allow Amazon SQS to use the message timer's DelaySeconds value instead of the delay queue's DelaySeconds value. You cannot use DelaySeconds to alter the visibility of a message which has been picked for processing.</p>\n\n<p><strong>Use WaitTimeSeconds action to short poll and extend a message's visibility timeout</strong></p>\n\n<p><strong>Use WaitTimeSeconds action to long poll and extend a message's visibility timeout</strong></p>\n\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. Both these options have been added as distractors as WaitTimeSeconds (via short polling or long polling) cannot be used to influence the message's visibility.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use ChangeMessageVisibility action to extend a message's visibility timeout</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon SQS uses a visibility timeout to prevent other consumers from receiving and processing the same message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html"
      },
      {
        "answer": "",
        "explanation": "For example, you have a message with a visibility timeout of 5 minutes. After 3 minutes, you call ChangeMessageVisibility with a timeout of 10 minutes. You can continue to call ChangeMessageVisibility to extend the visibility timeout to the maximum allowed time. If you try to extend the visibility timeout beyond the maximum, your request is rejected. So, for the given use-case, the application can set the initial visibility timeout to 1 minute and then continue to update the ChangeMessageVisibility value if required."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use DelaySeconds action to delay a message's visibility timeout</strong> - Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. To set delay seconds on individual messages, rather than on an entire queue, use message timers to allow Amazon SQS to use the message timer's DelaySeconds value instead of the delay queue's DelaySeconds value. You cannot use DelaySeconds to alter the visibility of a message which has been picked for processing."
      },
      {
        "answer": "<strong>Use WaitTimeSeconds action to short poll and extend a message's visibility timeout</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Use WaitTimeSeconds action to long poll and extend a message's visibility timeout</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon SQS provides short polling and long polling to receive messages from a queue. Both these options have been added as distractors as WaitTimeSeconds (via short polling or long polling) cannot be used to influence the message's visibility."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html"
    ]
  },
  {
    "id": 62,
    "question": "<p>A company uses microservices-based infrastructure to process the API calls from clients, perform request filtering and cache requests using the AWS API Gateway. Users report receiving 501 error code and you have been contacted to find out what is failing.</p>\n\n<p>Which service will you choose to help you troubleshoot?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use API Gateway service</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use X-Ray service</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use CloudWatch service</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use CloudTrail service</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use X-Ray service</strong> - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components. You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.</p>\n\n<p>X-Ray Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a><p></p>\n\n<p>AWS X-Ray creates a map of services used by your application with trace data that you can use to drill into specific services or issues. This provides a view of connections between services in your application and aggregated data for each service, including average latency and failure rates. You can create dependency trees, perform cross-availability zone or region call detections, and more.</p>\n\n<p>X-Ray Service maps:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/xray/features/\">https://aws.amazon.com/xray/features/</a><p></p>\n\n<p>X-Ray Traces:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/xray/features/\">https://aws.amazon.com/xray/features/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudTrail service</strong> - With CloudTrail, you can get a history of AWS API calls for your account - including API calls made via the AWS Management Console, AWS SDKs, command-line tools, and higher-level AWS services (such as AWS CloudFormation). This is a very useful service for general monitoring and tracking. But, it will not give a detailed analysis of the outcome of microservices or drill into specific issues. For the current use case, X-Ray offers a better solution.</p>\n\n<p><strong>Use API Gateway service</strong> - Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API Gateway will not be able to drill into the flow between different microservices or their issues.</p>\n\n<p><strong>Use CloudWatch service</strong> - Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. CloudWatch can collect numbers and respond to AWS service-related events, but it can't help you debug microservices specific issues on AWS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/features/\">https://aws.amazon.com/cloudwatch/features/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use X-Ray service</strong> - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components. You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i1.jpg",
        "answer": "",
        "explanation": "X-Ray Overview:"
      },
      {
        "link": "https://aws.amazon.com/xray/"
      },
      {
        "answer": "",
        "explanation": "AWS X-Ray creates a map of services used by your application with trace data that you can use to drill into specific services or issues. This provides a view of connections between services in your application and aggregated data for each service, including average latency and failure rates. You can create dependency trees, perform cross-availability zone or region call detections, and more."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i2.jpg",
        "answer": "",
        "explanation": "X-Ray Service maps:"
      },
      {
        "link": "https://aws.amazon.com/xray/features/"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i3.jpg",
        "answer": "",
        "explanation": "X-Ray Traces:"
      },
      {
        "link": "https://aws.amazon.com/xray/features/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use CloudTrail service</strong> - With CloudTrail, you can get a history of AWS API calls for your account - including API calls made via the AWS Management Console, AWS SDKs, command-line tools, and higher-level AWS services (such as AWS CloudFormation). This is a very useful service for general monitoring and tracking. But, it will not give a detailed analysis of the outcome of microservices or drill into specific issues. For the current use case, X-Ray offers a better solution."
      },
      {
        "answer": "",
        "explanation": "<strong>Use API Gateway service</strong> - Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API Gateway will not be able to drill into the flow between different microservices or their issues."
      },
      {
        "answer": "",
        "explanation": "<strong>Use CloudWatch service</strong> - Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. CloudWatch can collect numbers and respond to AWS service-related events, but it can't help you debug microservices specific issues on AWS."
      }
    ],
    "references": [
      "https://aws.amazon.com/xray/",
      "https://aws.amazon.com/xray/features/",
      "https://aws.amazon.com/cloudtrail/",
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/cloudwatch/features/"
    ]
  },
  {
    "id": 63,
    "question": "<p>A company has sensitive data stored in an Amazon S3 bucket that is encrypted using AWS Key Management Service (AWS KMS). A developer wants to enforce encryption in transit for all users who have been granted permission to use the S3 GetObject operation across multiple AWS accounts.</p>\n\n<p>Which of the following represents the best solution for this use case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure a resource-based policy on the S3 bucket to deny access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure a resource-based policy on the KMS key to allow access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure a resource-based policy on the KMS key to deny access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a resource-based policy on the S3 bucket to allow access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a resource-based policy on the S3 bucket to deny access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></strong></p>\n\n<p>If you want to prevent potential attackers from manipulating network traffic, you can use HTTPS (TLS) to only allow encrypted connections while restricting HTTP requests from accessing your bucket. To determine whether the request is HTTP or HTTPS, use the aws:SecureTransport global condition key in your S3 bucket policy. The aws:SecureTransport condition key checks whether a request was sent by using HTTP.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q16-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q16-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a resource-based policy on the S3 bucket to allow access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></strong> - This option contradicts the explanation provided above.</p>\n\n<p><strong>Configure a resource-based policy on the KMS key to allow access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></strong></p>\n\n<p><strong>Configure a resource-based policy on the KMS key to deny access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></strong></p>\n\n<p>Since the use case is about granting permission to use the S3 GetObject operations with encryption in transit, you cannot use a Resource-based policy for KMS. So, both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/\">https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Configure a resource-based policy on the S3 bucket to deny access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "If you want to prevent potential attackers from manipulating network traffic, you can use HTTPS (TLS) to only allow encrypted connections while restricting HTTP requests from accessing your bucket. To determine whether the request is HTTP or HTTPS, use the aws:SecureTransport global condition key in your S3 bucket policy. The aws:SecureTransport condition key checks whether a request was sent by using HTTP."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure a resource-based policy on the S3 bucket to allow access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></strong> - This option contradicts the explanation provided above."
      },
      {
        "answer": "<strong>Configure a resource-based policy on the KMS key to allow access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Configure a resource-based policy on the KMS key to deny access when a request has the condition <code>\"aws:SecureTransport\": \"false\"</code></strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Since the use case is about granting permission to use the S3 GetObject operations with encryption in transit, you cannot use a Resource-based policy for KMS. So, both these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html",
      "https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/"
    ]
  },
  {
    "id": 64,
    "question": "<p>A company that specializes in cloud communications platform as a service allows software developers to programmatically use their services to send and receive text messages. The initial platform did not have a scalable architecture as all components were hosted on one server and should be redesigned for high availability and scalability.</p>\n\n<p>Which of the following options can be used to implement the new architecture? (select two)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SES + S3</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>API Gateway + Lambda</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>CloudWatch + CloudFront</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>ALB + ECS</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>EBS + RDS</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>ALB + ECS</strong></p>\n\n<p>Amazon Elastic Container Service (ECS) is a highly scalable, high-performance container management service that supports Docker containers and allows you to easily run applications on a managed cluster of Amazon EC2 instances.</p>\n\n<p>How ECS Works:\n<img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/ecs/\">https://aws.amazon.com/ecs/</a><p></p>\n\n<p>Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.</p>\n\n<p>When you use ECS with a load balancer such as ALB deployed across multiple Availability Zones, it helps provide a scalable and highly available REST API.</p>\n\n<p><strong>API Gateway + Lambda</strong></p>\n\n<p>Amazon API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale. Using API Gateway, you can create an API that acts as a “front door” for applications to access data, business logic, or functionality from your back-end services, such as EC2 or Lambda functions.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a><p></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a><p></p>\n\n<p>API Gateway and Lambda help achieve the same purpose integrating some capabilities such as authentication in a serverless fashion, with fully scalable and highly available architectures.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SES + S3</strong> - The combination of these services only provide email and object storage services.</p>\n\n<p><strong>CloudWatch + CloudFront</strong> - The combination of these services only provide monitoring and fast content delivery network (CDN) services.</p>\n\n<p><strong>EBS + RDS</strong> - The combination of these services only provide elastic block storage and database services.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/\">https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/\">https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>ALB + ECS</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic Container Service (ECS) is a highly scalable, high-performance container management service that supports Docker containers and allows you to easily run applications on a managed cluster of Amazon EC2 instances."
      },
      {
        "image": "https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png",
        "answer": "",
        "explanation": "How ECS Works:"
      },
      {
        "link": "https://aws.amazon.com/ecs/"
      },
      {
        "answer": "",
        "explanation": "Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones."
      },
      {
        "answer": "",
        "explanation": "When you use ECS with a load balancer such as ALB deployed across multiple Availability Zones, it helps provide a scalable and highly available REST API."
      },
      {
        "answer": "<strong>API Gateway + Lambda</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale. Using API Gateway, you can create an API that acts as a “front door” for applications to access data, business logic, or functionality from your back-end services, such as EC2 or Lambda functions."
      },
      {
        "image": "https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png",
        "answer": "",
        "explanation": "How API Gateway Works:"
      },
      {
        "link": "https://aws.amazon.com/api-gateway/"
      },
      {
        "answer": "",
        "explanation": "AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume."
      },
      {
        "image": "https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png",
        "answer": "",
        "explanation": "How Lambda function works:"
      },
      {
        "link": "https://aws.amazon.com/lambda/"
      },
      {
        "answer": "",
        "explanation": "API Gateway and Lambda help achieve the same purpose integrating some capabilities such as authentication in a serverless fashion, with fully scalable and highly available architectures."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>SES + S3</strong> - The combination of these services only provide email and object storage services."
      },
      {
        "answer": "",
        "explanation": "<strong>CloudWatch + CloudFront</strong> - The combination of these services only provide monitoring and fast content delivery network (CDN) services."
      },
      {
        "answer": "",
        "explanation": "<strong>EBS + RDS</strong> - The combination of these services only provide elastic block storage and database services."
      }
    ],
    "references": [
      "https://aws.amazon.com/ecs/",
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/",
      "https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/"
    ]
  },
  {
    "id": 65,
    "question": "<p>A developer is configuring Amazon ECS container instances to send log information to CloudWatch Logs. For the container instances to be able to send log data to CloudWatch Logs, an IAM policy needs to be created that will allow the container instances to use the CloudWatch Logs APIs.</p>\n\n<p>Which policy is the right fit for the given requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents,\n                \"ecs:DescribeServices\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:&lt;ARN of the Log Group&gt;\"\n            ]\n        }\n    ]\n}\n</code></pre>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:*\"\n            ]\n        }\n    ]\n}\n</code></pre>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:*\"\n            ]\n        }\n    ]\n}\n</code></pre>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:DescribeLogGroups\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:*\"\n            ]\n        }\n    ]\n}\n</code></pre>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Before your container instances can send log data to CloudWatch Logs, you must create an IAM policy to allow your container instances to use the CloudWatch Logs APIs, and then you must attach that policy to <code>ecsInstanceRole</code>.</p>\n\n<p>This policy has one statement that grants permissions to create log groups and log streams, to upload log events to log streams, and to list details about log streams.</p>\n\n<p>The wildcard character (<em>) at the end of the Resource value means that the statement allows permission for the logs:CreateLogGroup, logs:CreateLogStream, logs:PutLogEvents, and logs:DescribeLogStreams actions on any log group. To limit this permission to a specific log group, replace the wildcard character (</em>) in the resource ARN with the specific log group ARN</p>\n\n<p>Incorrect options:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<ul>\n<li>Permission to list details of the log stream needs to be attached to this policy.</li>\n</ul>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents,\n                \"ecs:DescribeServices\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:&lt;ARN of the Log Group&gt;\"\n            ]\n        }\n    ]\n}```\n\n- ecs:DescribeServices permission is not needed, but logs:DescribeLogStreams permissions are needed for the policy to perform as expected.\n\n</code></pre>\n\n<p>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:DescribeLogGroups\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:<em>:</em>:*\"\n            ]\n        }\n    ]\n}\n```</p>\n\n<ul>\n<li>logs:DescribeLogGroups is an erroneous permission here.</li>\n</ul>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/iam-identity-based-access-control-cwl.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/iam-identity-based-access-control-cwl.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:*\"\n            ]\n        }\n    ]\n}\n</code></pre>"
      },
      {
        "answer": "",
        "explanation": "Before your container instances can send log data to CloudWatch Logs, you must create an IAM policy to allow your container instances to use the CloudWatch Logs APIs, and then you must attach that policy to <code>ecsInstanceRole</code>."
      },
      {
        "answer": "",
        "explanation": "This policy has one statement that grants permissions to create log groups and log streams, to upload log events to log streams, and to list details about log streams."
      },
      {
        "answer": "",
        "explanation": "The wildcard character (<em>) at the end of the Resource value means that the statement allows permission for the logs:CreateLogGroup, logs:CreateLogStream, logs:PutLogEvents, and logs:DescribeLogStreams actions on any log group. To limit this permission to a specific log group, replace the wildcard character (</em>) in the resource ARN with the specific log group ARN"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:*\"\n            ]\n        }\n    ]\n}\n</code></pre>"
      },
      {
        "answer": "",
        "explanation": "<ul>\n<li>Permission to list details of the log stream needs to be attached to this policy.</li>\n</ul>"
      },
      {
        "answer": "",
        "explanation": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents,\n                \"ecs:DescribeServices\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:&lt;ARN of the Log Group&gt;\"\n            ]\n        }\n    ]\n}```\n\n- ecs:DescribeServices permission is not needed, but logs:DescribeLogStreams permissions are needed for the policy to perform as expected.\n\n</code></pre>"
      },
      {
        "answer": "",
        "explanation": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:DescribeLogGroups\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:<em>:</em>:*\"\n            ]\n        }\n    ]\n}\n```"
      },
      {
        "answer": "",
        "explanation": "<ul>\n<li>logs:DescribeLogGroups is an erroneous permission here.</li>\n</ul>"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/iam-identity-based-access-control-cwl.html"
    ]
  }
]