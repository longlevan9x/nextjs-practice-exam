[
  {
    "id": 1,
    "question": "<p>A company needs a version control system for their fast development lifecycle with incremental changes, version control, and support to existing Git tools.</p>\n\n<p>Which AWS service will meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS CodeCommit</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>AWS CodeBuild</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS CodePipeline</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Versioned S3 Bucket</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS CodeCommit</strong> - AWS CodeCommit is a fully-managed Source Control service that hosts secure Git-based repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem. AWS CodeCommit helps you collaborate on code with teammates via pull requests, branching and merging. AWS CodeCommit keeps your repositories close to your build, staging, and production environments in the AWS cloud. You can transfer incremental changes instead of the entire application.\nAWS CodeCommit supports all Git commands and works with your existing Git tools. You can keep using your preferred development environment plugins, continuous integration/continuous delivery systems, and graphical clients with CodeCommit.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Versioned S3 Bucket</strong> - AWS CodeCommit is designed for collaborative software development. It manages batches of changes across multiple files, offers parallel branching, and includes version differencing (\"diffing\"). In comparison, Amazon S3 versioning supports recovering past versions of individual files but doesn’t support tracking batched changes that span multiple files or other features needed for collaborative software development.</p>\n\n<p><strong>AWS CodePipeline</strong> - AWS CodePipeline is a fully managed \"continuous delivery\" service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define.</p>\n\n<p><strong>AWS CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don’t need to provision, manage, and scale your own build servers. CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codecommit/\">https://aws.amazon.com/codecommit/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/\">https://aws.amazon.com/codepipeline/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codebuild/\">https://aws.amazon.com/codebuild/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS CodeCommit</strong> - AWS CodeCommit is a fully-managed Source Control service that hosts secure Git-based repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem. AWS CodeCommit helps you collaborate on code with teammates via pull requests, branching and merging. AWS CodeCommit keeps your repositories close to your build, staging, and production environments in the AWS cloud. You can transfer incremental changes instead of the entire application.\nAWS CodeCommit supports all Git commands and works with your existing Git tools. You can keep using your preferred development environment plugins, continuous integration/continuous delivery systems, and graphical clients with CodeCommit."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Versioned S3 Bucket</strong> - AWS CodeCommit is designed for collaborative software development. It manages batches of changes across multiple files, offers parallel branching, and includes version differencing (\"diffing\"). In comparison, Amazon S3 versioning supports recovering past versions of individual files but doesn’t support tracking batched changes that span multiple files or other features needed for collaborative software development."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS CodePipeline</strong> - AWS CodePipeline is a fully managed \"continuous delivery\" service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don’t need to provision, manage, and scale your own build servers. CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue."
      }
    ],
    "references": [
      "https://aws.amazon.com/codecommit/",
      "https://aws.amazon.com/codepipeline/",
      "https://aws.amazon.com/codebuild/",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html"
    ]
  },
  {
    "id": 2,
    "question": "<p>A developer is looking at establishing access control for an API that connects to a Lambda function downstream.</p>\n\n<p>Which of the following represents a mechanism that CANNOT be used for authenticating with the API Gateway?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Security Token Service (STS)</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Standard AWS IAM roles and policies</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Cognito User Pools</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Lambda Authorizer</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a><p></p>\n\n<p><strong>AWS Security Token Service (STS)</strong> - AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). However, it is not supported by API Gateway.</p>\n\n<p>API Gateway supports the following mechanisms for authentication and authorization:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q55-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q55-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Standard AWS IAM roles and policies</strong> - Standard AWS IAM roles and policies offer flexible and robust access controls that can be applied to an entire API or individual methods. IAM roles and policies can be used for controlling who can create and manage your APIs, as well as who can invoke them.</p>\n\n<p><strong>Lambda Authorizer</strong> - Lambda authorizers are Lambda functions that control access to REST API methods using bearer token authentication—as well as information described by headers, paths, query strings, stage variables, or context variables request parameters. Lambda authorizers are used to control who can invoke REST API methods.</p>\n\n<p><strong>Cognito User Pools</strong> - Amazon Cognito user pools let you create customizable authentication and authorization solutions for your REST APIs. Amazon Cognito user pools are used to control who can invoke REST API methods.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html\">https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud."
      },
      {
        "image": "https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png",
        "answer": "",
        "explanation": "How API Gateway Works:"
      },
      {
        "link": "https://aws.amazon.com/api-gateway/"
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Security Token Service (STS)</strong> - AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). However, it is not supported by API Gateway."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q55-i1.jpg",
        "answer": "",
        "explanation": "API Gateway supports the following mechanisms for authentication and authorization:"
      },
      {
        "link": "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Standard AWS IAM roles and policies</strong> - Standard AWS IAM roles and policies offer flexible and robust access controls that can be applied to an entire API or individual methods. IAM roles and policies can be used for controlling who can create and manage your APIs, as well as who can invoke them."
      },
      {
        "answer": "",
        "explanation": "<strong>Lambda Authorizer</strong> - Lambda authorizers are Lambda functions that control access to REST API methods using bearer token authentication—as well as information described by headers, paths, query strings, stage variables, or context variables request parameters. Lambda authorizers are used to control who can invoke REST API methods."
      },
      {
        "answer": "",
        "explanation": "<strong>Cognito User Pools</strong> - Amazon Cognito user pools let you create customizable authentication and authorization solutions for your REST APIs. Amazon Cognito user pools are used to control who can invoke REST API methods."
      }
    ],
    "references": [
      "https://aws.amazon.com/api-gateway/",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html",
      "https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html"
    ]
  },
  {
    "id": 3,
    "question": "<p>A business has purchased one m4.xlarge Reserved Instance but it has used three m4.xlarge instances concurrently for an hour.</p>\n\n<p>As a Developer, explain how the instances are charged?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>All instances are charged at one hour of Reserved Instance usage</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>One instance is charged at one hour of On-Demand usage and the other two instances are charged at two hours of Reserved Instance usage</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>All instances are charged at one hour of On-Demand Instance usage</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>One instance is charged at one hour of Reserved Instance usage and the other two instances are charged at two hours of On-Demand usage</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>All Reserved Instances provide you with a discount compared to On-Demand pricing.</p>\n\n<p><strong>One instance is charged at one hour of Reserved Instance usage and the other two instances are charged at two hours of On-Demand usage</strong></p>\n\n<p>A Reserved Instance billing benefit can apply to a maximum of 3600 seconds (one hour) of instance usage per clock-hour. You can run multiple instances concurrently, but can only receive the benefit of the Reserved Instance discount for a total of 3600 seconds per clock-hour; instance usage that exceeds 3600 seconds in a clock-hour is billed at the On-Demand rate.</p>\n\n<p>Please review this note on the EC2 Reserved Instance types:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q34-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q34-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html</a><p></p>\n\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q34-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q34-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>All instances are charged at one hour of Reserved Instance usage</strong> - This is incorrect.</p>\n\n<p><strong>All instances are charged at one hour of On-Demand Instance usage</strong> - This is incorrect.</p>\n\n<p><strong>One instance is charged at one hour of On-Demand usage and the other two instances are charged at two hours of Reserved Instance usage</strong> - This is incorrect. If multiple eligible instances are running concurrently, the Reserved Instance billing benefit is applied to all the instances at the same time up to a maximum of 3600 seconds in a clock-hour; thereafter, On-Demand rates apply.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts-reserved-instances-application.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts-reserved-instances-application.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "All Reserved Instances provide you with a discount compared to On-Demand pricing."
      },
      {
        "answer": "<strong>One instance is charged at one hour of Reserved Instance usage and the other two instances are charged at two hours of On-Demand usage</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A Reserved Instance billing benefit can apply to a maximum of 3600 seconds (one hour) of instance usage per clock-hour. You can run multiple instances concurrently, but can only receive the benefit of the Reserved Instance discount for a total of 3600 seconds per clock-hour; instance usage that exceeds 3600 seconds in a clock-hour is billed at the On-Demand rate."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q34-i1.jpg",
        "answer": "",
        "explanation": "Please review this note on the EC2 Reserved Instance types:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q34-i2.jpg",
        "answer": "",
        "explanation": "High Level Overview of EC2 Instance Purchase Options:"
      },
      {
        "link": "https://aws.amazon.com/ec2/pricing/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>All instances are charged at one hour of Reserved Instance usage</strong> - This is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>All instances are charged at one hour of On-Demand Instance usage</strong> - This is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>One instance is charged at one hour of On-Demand usage and the other two instances are charged at two hours of Reserved Instance usage</strong> - This is incorrect. If multiple eligible instances are running concurrently, the Reserved Instance billing benefit is applied to all the instances at the same time up to a maximum of 3600 seconds in a clock-hour; thereafter, On-Demand rates apply."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html",
      "https://aws.amazon.com/ec2/pricing/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts-reserved-instances-application.html"
    ]
  },
  {
    "id": 4,
    "question": "<p>A business hosts its website on Amazon EC2 instances and employs Auto Scaling to adjust its resources according to traffic spikes. However, users globally report slow loading times because static content hosted on the EC2 instances takes too long to load, even outside of busy periods.</p>\n\n<p>What pair of actions should be taken to improve the latency of the website? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Transfer the application’s static content hosted on EC2 instances to Amazon S3</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Upgrade the CPU and RAM available to the EC2 instances</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Double the Auto Scaling group’s desired capacity</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Migrate the application to AWS Lambda</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Set up an Amazon CloudFront distribution to cache the static content with Amazon S3 configured as the origin</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an Amazon CloudFront distribution to cache the static content with Amazon S3 configured as the origin</strong></p>\n\n<p><strong>Transfer the application’s static content hosted on EC2 instances to Amazon S3</strong></p>\n\n<p>Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.</p>\n\n<p>For the given use case, you can transfer the static content from EC2 instances to Amazon S3. Then, you can specify the origin as the Amazon S3 bucket from which CloudFront gets your files which will then be distributed from CloudFront edge locations all over the world. An origin stores the original, definitive version of your objects. If you're serving content over HTTP, your origin is either an Amazon S3 bucket or an HTTP server, such as a web server.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/images/AmazonCloudFront/latest/DeveloperGuide/images/how-you-configure-cf.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/images/AmazonCloudFront/latest/DeveloperGuide/images/how-you-configure-cf.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upgrade the CPU and RAM available to the EC2 instances</strong> - Since the static content takes too long to load even outside of busy periods, this implies that the underlying root cause is the high end-to-end network latency rather than the hardware of the EC2 instance.</p>\n\n<p><strong>Double the Auto Scaling group’s desired capacity</strong> - The desired capacity represents the initial capacity of the Auto Scaling group at the time of creation. An Auto Scaling group attempts to maintain the desired capacity. It starts by launching the number of instances that are specified for the desired capacity, and maintains this number of instances as long as there are no scaling policies or scheduled actions attached to the Auto Scaling group. Since the static content takes too long to load even outside of busy periods, so doubling the desired capacity would still not address the underlying root cause of high end-to-end network latency.</p>\n\n<p><strong>Migrate the application to AWS Lambda</strong> - You cannot store static content on AWS Lambda, so this option just serves as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GettingStarted.SimpleDistribution.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GettingStarted.SimpleDistribution.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Set up an Amazon CloudFront distribution to cache the static content with Amazon S3 configured as the origin</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Transfer the application’s static content hosted on EC2 instances to Amazon S3</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance."
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can transfer the static content from EC2 instances to Amazon S3. Then, you can specify the origin as the Amazon S3 bucket from which CloudFront gets your files which will then be distributed from CloudFront edge locations all over the world. An origin stores the original, definitive version of your objects. If you're serving content over HTTP, your origin is either an Amazon S3 bucket or an HTTP server, such as a web server."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Upgrade the CPU and RAM available to the EC2 instances</strong> - Since the static content takes too long to load even outside of busy periods, this implies that the underlying root cause is the high end-to-end network latency rather than the hardware of the EC2 instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Double the Auto Scaling group’s desired capacity</strong> - The desired capacity represents the initial capacity of the Auto Scaling group at the time of creation. An Auto Scaling group attempts to maintain the desired capacity. It starts by launching the number of instances that are specified for the desired capacity, and maintains this number of instances as long as there are no scaling policies or scheduled actions attached to the Auto Scaling group. Since the static content takes too long to load even outside of busy periods, so doubling the desired capacity would still not address the underlying root cause of high end-to-end network latency."
      },
      {
        "answer": "",
        "explanation": "<strong>Migrate the application to AWS Lambda</strong> - You cannot store static content on AWS Lambda, so this option just serves as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GettingStarted.SimpleDistribution.html"
    ]
  },
  {
    "id": 5,
    "question": "<p>A data analytics company is processing real-time Internet-of-Things (IoT) data via Kinesis Producer Library (KPL) and sending the data to a Kinesis Data Streams driven application. The application has halted data processing because of a ProvisionedThroughputExceeded exception.</p>\n\n<p>Which of the following actions would help in addressing this issue? (Select two)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Increase the number of shards within your data streams to provide enough capacity</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure the data producer to retry with an exponential backoff</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Kinesis Agent instead of Kinesis Producer Library (KPL) for sending data to Kinesis Data Streams</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon SQS instead of Kinesis Data Streams</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use Kinesis enhanced fan-out for Kinesis Data Streams</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the data producer to retry with an exponential backoff</strong></p>\n\n<p><strong>Increase the number of shards within your data streams to provide enough capacity</strong></p>\n\n<p>Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources.</p>\n\n<p>How Kinesis Data Streams Work\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a><p></p>\n\n<p>The capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception.</p>\n\n<p>If this is due to a temporary rise of the data stream’s input data rate, retry (with exponential backoff) by the data producer will eventually lead to the completion of the requests.</p>\n\n<p>If this is due to a sustained rise of the data stream’s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Kinesis Agent instead of Kinesis Producer Library (KPL) for sending data to Kinesis Data Streams</strong> - Kinesis Agent works with data producers. Using Kinesis Agent instead of KPL will not help as the constraint is the capacity limit of the Kinesis Data Stream.</p>\n\n<p><strong>Use Amazon SQS instead of Kinesis Data Streams</strong> - This is a distractor as using SQS will not help address the ProvisionedThroughputExceeded exception for the Kinesis Data Stream. This option does not address the issues in the use-case.</p>\n\n<p><strong>Use Kinesis enhanced fan-out for Kinesis Data Streams</strong> - You should use enhanced fan-out if you have, or expect to have, multiple consumers retrieving data from a stream in parallel. Therefore, using enhanced fan-out will not help address the ProvisionedThroughputExceeded exception as the constraint is the capacity limit of the Kinesis Data Stream.</p>\n\n<p>Please review this note for more details on enhanced fan-out for Kinesis Data Streams:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q6-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q6-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a><p></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Configure the data producer to retry with an exponential backoff</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Increase the number of shards within your data streams to provide enough capacity</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png",
        "answer": "",
        "explanation": "How Kinesis Data Streams Work"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/"
      },
      {
        "answer": "",
        "explanation": "The capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception."
      },
      {
        "answer": "",
        "explanation": "If this is due to a temporary rise of the data stream’s input data rate, retry (with exponential backoff) by the data producer will eventually lead to the completion of the requests."
      },
      {
        "answer": "",
        "explanation": "If this is due to a sustained rise of the data stream’s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Kinesis Agent instead of Kinesis Producer Library (KPL) for sending data to Kinesis Data Streams</strong> - Kinesis Agent works with data producers. Using Kinesis Agent instead of KPL will not help as the constraint is the capacity limit of the Kinesis Data Stream."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SQS instead of Kinesis Data Streams</strong> - This is a distractor as using SQS will not help address the ProvisionedThroughputExceeded exception for the Kinesis Data Stream. This option does not address the issues in the use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Kinesis enhanced fan-out for Kinesis Data Streams</strong> - You should use enhanced fan-out if you have, or expect to have, multiple consumers retrieving data from a stream in parallel. Therefore, using enhanced fan-out will not help address the ProvisionedThroughputExceeded exception as the constraint is the capacity limit of the Kinesis Data Stream."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q6-i1.jpg",
        "answer": "",
        "explanation": "Please review this note for more details on enhanced fan-out for Kinesis Data Streams:"
      },
      {
        "link": "https://aws.amazon.com/kinesis/data-streams/faqs/"
      }
    ],
    "references": [
      "https://aws.amazon.com/kinesis/data-streams/",
      "https://aws.amazon.com/kinesis/data-streams/faqs/"
    ]
  },
  {
    "id": 6,
    "question": "<p>A Developer is configuring Amazon EC2 Auto Scaling group to scale dynamically.</p>\n\n<p>Which metric below is NOT part of Target Tracking Scaling Policy?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>ALBRequestCountPerTarget</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>ASGAverageNetworkOut</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>ASGAverageCPUUtilization</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>ApproximateNumberOfMessagesVisible</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>ApproximateNumberOfMessagesVisible</strong> - This is a CloudWatch Amazon SQS queue metric. The number of messages in a queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue. Hence, this metric does not work for target tracking.</p>\n\n<p>Incorrect options:</p>\n\n<p>With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value.</p>\n\n<p>It is important to note that a target tracking scaling policy assumes that it should scale out your Auto Scaling group when the specified metric is above the target value. You cannot use a target tracking scaling policy to scale out your Auto Scaling group when the specified metric is below the target value.</p>\n\n<p><strong>ASGAverageCPUUtilization</strong> - This is a predefined metric for target tracking scaling policy. This represents the Average CPU utilization of the Auto Scaling group.</p>\n\n<p><strong>ASGAverageNetworkOut</strong> - This is a predefined metric for target tracking scaling policy. This represents the Average number of bytes sent out on all network interfaces by the Auto Scaling group.</p>\n\n<p><strong>ALBRequestCountPerTarget</strong> - This is a predefined metric for target tracking scaling policy. This represents the Number of requests completed per target in an Application Load Balancer target group.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>ApproximateNumberOfMessagesVisible</strong> - This is a CloudWatch Amazon SQS queue metric. The number of messages in a queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue. Hence, this metric does not work for target tracking."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value."
      },
      {
        "answer": "",
        "explanation": "It is important to note that a target tracking scaling policy assumes that it should scale out your Auto Scaling group when the specified metric is above the target value. You cannot use a target tracking scaling policy to scale out your Auto Scaling group when the specified metric is below the target value."
      },
      {
        "answer": "",
        "explanation": "<strong>ASGAverageCPUUtilization</strong> - This is a predefined metric for target tracking scaling policy. This represents the Average CPU utilization of the Auto Scaling group."
      },
      {
        "answer": "",
        "explanation": "<strong>ASGAverageNetworkOut</strong> - This is a predefined metric for target tracking scaling policy. This represents the Average number of bytes sent out on all network interfaces by the Auto Scaling group."
      },
      {
        "answer": "",
        "explanation": "<strong>ALBRequestCountPerTarget</strong> - This is a predefined metric for target tracking scaling policy. This represents the Number of requests completed per target in an Application Load Balancer target group."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html"
    ]
  },
  {
    "id": 7,
    "question": "<p>The app development team at a social gaming mobile app wants to simplify the user sign up process for the app. The team is looking for a fully managed scalable solution for user management in anticipation of the rapid growth that the app foresees.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest so that it requires the LEAST amount of development effort?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Cognito User pools to facilitate sign up and user management for the mobile app</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Cognito Identity pools to facilitate sign up and user management for the mobile app</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a custom solution using Lambda and DynamoDB to facilitate sign up and user management for the mobile app</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a custom solution using EC2 and DynamoDB to facilitate sign up and user management for the mobile app</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito User pools to facilitate sign up and user management for the mobile app</strong></p>\n\n<p>Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps. Your users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, Google or Apple.</p>\n\n<p>A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.</p>\n\n<p>Cognito is fully managed by AWS and works out of the box so it meets the requirements for the given use-case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q2-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q2-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Cognito Identity pools to facilitate sign up and user management for the mobile app</strong> - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q2-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q2-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a><p></p>\n\n<p><strong>Create a custom solution with EC2 and DynamoDB to facilitate sign up and user management for the mobile app</strong></p>\n\n<p><strong>Create a custom solution with Lambda and DynamoDB to facilitate sign up and user management for the mobile app</strong></p>\n\n<p>As the problem statement mentions that the solution needs to be fully managed and should require the least amount of development effort, so you cannot use EC2 or Lambda functions with DynamoDB to create a custom solution.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use Cognito User pools to facilitate sign up and user management for the mobile app</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps. Your users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, Google or Apple."
      },
      {
        "answer": "",
        "explanation": "A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK."
      },
      {
        "answer": "",
        "explanation": "Cognito is fully managed by AWS and works out of the box so it meets the requirements for the given use-case."
      },
      {
        "link": "https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Cognito Identity pools to facilitate sign up and user management for the mobile app</strong> - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools."
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q2-i2.jpg",
        "answer": "",
        "explanation": "Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:"
      },
      {
        "link": "https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Create a custom solution with EC2 and DynamoDB to facilitate sign up and user management for the mobile app</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Create a custom solution with Lambda and DynamoDB to facilitate sign up and user management for the mobile app</strong>"
      },
      {
        "answer": "",
        "explanation": "As the problem statement mentions that the solution needs to be fully managed and should require the least amount of development effort, so you cannot use EC2 or Lambda functions with DynamoDB to create a custom solution."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html"
    ]
  },
  {
    "id": 8,
    "question": "<p>A development team is working on an AWS Lambda function that accesses DynamoDB. The Lambda function must do an upsert, that is, it must retrieve an item and update some of its attributes or create the item if it does not exist.</p>\n\n<p>Which of the following represents the solution with MINIMUM IAM permissions that can be used for the Lambda function to achieve this functionality?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>dynamodb:GetRecords, dynamodb:PutItem, dynamodb:UpdateTable</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>dynamodb:UpdateItem, dynamodb:GetItem, dynamodb:PutItem</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>dynamodb:AddItem, dynamodb:GetItem</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>dynamodb:UpdateItem, dynamodb:GetItem</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>dynamodb:UpdateItem, dynamodb:GetItem</strong> - With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing TransactWriteItems or TransactGetItems operation.</p>\n\n<p>You can use AWS Identity and Access Management (IAM) to restrict the actions that transactional operations can perform in Amazon DynamoDB. Permissions for Put, Update, Delete, and Get actions are governed by the permissions used for the underlying PutItem, UpdateItem, DeleteItem, and GetItem operations. For the ConditionCheck action, you can use the <code>dynamodb:ConditionCheck</code> permission in IAM policies.</p>\n\n<p><code>UpdateItem</code> action of DynamoDB APIs, edits an existing item's attributes or adds a new item to the table if it does not already exist. You can put, delete, or add attribute values. You can also perform a conditional update on an existing item (insert a new attribute name-value pair if it doesn't exist, or replace an existing name-value pair if it has certain expected attribute values).</p>\n\n<p>There is no need to inlcude the <code>dynamodb:PutItem</code> action for the given use-case.</p>\n\n<p>So, the IAM policy must include permissions to get and update the item in the DynamoDB table.</p>\n\n<p>Actions defined by DynamoDB:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q48-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q48-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html\">https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>dynamodb:AddItem, dynamodb:GetItem</strong></p>\n\n<p><strong>dynamodb:GetRecords, dynamodb:PutItem, dynamodb:UpdateTable</strong></p>\n\n<p><strong>dynamodb:UpdateItem, dynamodb:GetItem, dynamodb:PutItem</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html\">https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html</a></p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/transaction-apis-iam.html\">https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/transaction-apis-iam.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html\">https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>dynamodb:UpdateItem, dynamodb:GetItem</strong> - With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing TransactWriteItems or TransactGetItems operation."
      },
      {
        "answer": "",
        "explanation": "You can use AWS Identity and Access Management (IAM) to restrict the actions that transactional operations can perform in Amazon DynamoDB. Permissions for Put, Update, Delete, and Get actions are governed by the permissions used for the underlying PutItem, UpdateItem, DeleteItem, and GetItem operations. For the ConditionCheck action, you can use the <code>dynamodb:ConditionCheck</code> permission in IAM policies."
      },
      {
        "answer": "",
        "explanation": "<code>UpdateItem</code> action of DynamoDB APIs, edits an existing item's attributes or adds a new item to the table if it does not already exist. You can put, delete, or add attribute values. You can also perform a conditional update on an existing item (insert a new attribute name-value pair if it doesn't exist, or replace an existing name-value pair if it has certain expected attribute values)."
      },
      {
        "answer": "",
        "explanation": "There is no need to inlcude the <code>dynamodb:PutItem</code> action for the given use-case."
      },
      {
        "answer": "",
        "explanation": "So, the IAM policy must include permissions to get and update the item in the DynamoDB table."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q48-i1.jpg",
        "answer": "",
        "explanation": "Actions defined by DynamoDB:"
      },
      {
        "link": "https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>dynamodb:AddItem, dynamodb:GetItem</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>dynamodb:GetRecords, dynamodb:PutItem, dynamodb:UpdateTable</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>dynamodb:UpdateItem, dynamodb:GetItem, dynamodb:PutItem</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazondynamodb.html",
      "https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/transaction-apis-iam.html"
    ]
  },
  {
    "id": 9,
    "question": "<p>As an AWS certified developer associate, you are working on an AWS CloudFormation template that will create resources for a company's cloud infrastructure. Your template is composed of three stacks which are Stack-A, Stack-B, and Stack-C. Stack-A will provision a VPC, a security group, and subnets for public web applications that will be referenced in Stack-B and Stack-C.</p>\n\n<p>After running the stacks you decide to delete them, in which order should you do it?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Stack A, then Stack B, then Stack C</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Stack A, Stack C then Stack B</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Stack C then Stack A then Stack B</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Stack B, then Stack C, then Stack A</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a><p></p>\n\n<p><strong>Stack B, then Stack C, then Stack A</strong></p>\n\n<p>All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you must delete Stack B as well as Stack C, before you delete Stack A.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q23-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q23-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Stack A, then Stack B, then Stack C</strong> - All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you cannot delete Stack A first because that's being referenced in the other Stacks.</p>\n\n<p><strong>Stack A, Stack C then Stack B</strong> - All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you cannot delete Stack A first because that's being referenced in the other Stacks.</p>\n\n<p><strong>Stack C then Stack A then Stack B</strong> - Stack C is fine but you should delete Stack B before Stack A because all of the imports must be removed before you can delete the exporting stack or modify the output value.</p>\n\n<p>Reference:\n<a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png",
        "answer": "",
        "explanation": "How CloudFormation Works:"
      },
      {
        "link": "https://aws.amazon.com/cloudformation/"
      },
      {
        "answer": "<strong>Stack B, then Stack C, then Stack A</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you must delete Stack B as well as Stack C, before you delete Stack A."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Stack A, then Stack B, then Stack C</strong> - All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you cannot delete Stack A first because that's being referenced in the other Stacks."
      },
      {
        "answer": "",
        "explanation": "<strong>Stack A, Stack C then Stack B</strong> - All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you cannot delete Stack A first because that's being referenced in the other Stacks."
      },
      {
        "answer": "",
        "explanation": "<strong>Stack C then Stack A then Stack B</strong> - Stack C is fine but you should delete Stack B before Stack A because all of the imports must be removed before you can delete the exporting stack or modify the output value."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html",
        "answer": "",
        "explanation": "Reference:\n<a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html</a>"
      }
    ],
    "references": [
      "https://aws.amazon.com/cloudformation/",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html"
    ]
  },
  {
    "id": 10,
    "question": "<p>Recently in your organization, the AWS X-Ray SDK was bundled into each Lambda function to record outgoing calls for tracing purposes. When your team leader goes to the X-Ray service in the AWS Management Console to get an overview of the information collected, they discover that no data is available.</p>\n\n<p>What is the most likely reason for this issue?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>X-Ray only works with AWS Lambda aliases</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable X-Ray sampling</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Change the security group rules</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Fix the IAM Role</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a><p></p>\n\n<p><strong>Fix the IAM Role</strong></p>\n\n<p>Create an IAM role with write permissions and assign it to the resources running your application. You can use AWS Identity and Access Management (IAM) to grant X-Ray permissions to users and compute resources in your account. This should be one of the first places you start by checking that your permissions are properly configured before exploring other troubleshooting options.</p>\n\n<p>Here is an example of X-Ray Read-Only permissions via an IAM policy:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"xray:GetSamplingRules\",\n                \"xray:GetSamplingTargets\",\n                \"xray:GetSamplingStatisticSummaries\",\n                \"xray:BatchGetTraces\",\n                \"xray:GetServiceGraph\",\n                \"xray:GetTraceGraph\",\n                \"xray:GetTraceSummaries\",\n                \"xray:GetGroups\",\n                \"xray:GetGroup\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Another example of write permissions for using X-Ray via an IAM policy:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"xray:PutTraceSegments\",\n                \"xray:PutTelemetryRecords\",\n                \"xray:GetSamplingRules\",\n                \"xray:GetSamplingTargets\",\n                \"xray:GetSamplingStatisticSummaries\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable X-Ray sampling</strong> - If permissions are not configured correctly sampling will not work, so this option is not correct.</p>\n\n<p><strong>X-Ray only works with AWS Lambda aliases</strong> - This is not true, aliases are pointers to specific Lambda function versions. To use the X-Ray SDK on Lambda, bundle it with your function code each time you create a new version.</p>\n\n<p><strong>Change the security group rules</strong> - You grant permissions to your Lambda function to access other resources using an IAM role and not via security groups.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/security_iam_troubleshoot.html\">https://docs.aws.amazon.com/xray/latest/devguide/security_iam_troubleshoot.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png",
        "answer": "",
        "explanation": "How X-Ray Works:"
      },
      {
        "link": "https://aws.amazon.com/xray/"
      },
      {
        "answer": "<strong>Fix the IAM Role</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Create an IAM role with write permissions and assign it to the resources running your application. You can use AWS Identity and Access Management (IAM) to grant X-Ray permissions to users and compute resources in your account. This should be one of the first places you start by checking that your permissions are properly configured before exploring other troubleshooting options."
      },
      {
        "answer": "",
        "explanation": "Here is an example of X-Ray Read-Only permissions via an IAM policy:"
      },
      {
        "answer": "",
        "explanation": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"xray:GetSamplingRules\",\n                \"xray:GetSamplingTargets\",\n                \"xray:GetSamplingStatisticSummaries\",\n                \"xray:BatchGetTraces\",\n                \"xray:GetServiceGraph\",\n                \"xray:GetTraceGraph\",\n                \"xray:GetTraceSummaries\",\n                \"xray:GetGroups\",\n                \"xray:GetGroup\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n</code></pre>"
      },
      {
        "answer": "",
        "explanation": "Another example of write permissions for using X-Ray via an IAM policy:"
      },
      {
        "answer": "",
        "explanation": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"xray:PutTraceSegments\",\n                \"xray:PutTelemetryRecords\",\n                \"xray:GetSamplingRules\",\n                \"xray:GetSamplingTargets\",\n                \"xray:GetSamplingStatisticSummaries\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n</code></pre>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable X-Ray sampling</strong> - If permissions are not configured correctly sampling will not work, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>X-Ray only works with AWS Lambda aliases</strong> - This is not true, aliases are pointers to specific Lambda function versions. To use the X-Ray SDK on Lambda, bundle it with your function code each time you create a new version."
      },
      {
        "answer": "",
        "explanation": "<strong>Change the security group rules</strong> - You grant permissions to your Lambda function to access other resources using an IAM role and not via security groups."
      }
    ],
    "references": [
      "https://aws.amazon.com/xray/",
      "https://docs.aws.amazon.com/xray/latest/devguide/security_iam_troubleshoot.html"
    ]
  },
  {
    "id": 11,
    "question": "<p>An Accounting firm extensively uses Amazon EBS volumes for persistent storage of application data of Amazon EC2 instances. The volumes are encrypted to protect the critical data of the clients. As part of managing the security credentials, the project manager has come across a policy snippet that looks like the following:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Allow for use of this Key\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111122223333:role/UserRole\"\n            },\n            \"Action\": [\n                \"kms:GenerateDataKeyWithoutPlaintext\",\n                \"kms:Decrypt\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"Allow for EC2 Use\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111122223333:role/UserRole\"\n            },\n            \"Action\": [\n                \"kms:CreateGrant\",\n                \"kms:ListGrants\",\n                \"kms:RevokeGrant\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                \"kms:ViaService\": \"ec2.us-west-2.amazonaws.com\"\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>Which of the following options are correct regarding the policy?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The second statement in this policy provides the security group (mentioned in first statement of the policy), the ability to create, list, and revoke grants for Amazon EC2</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The first statement provides the security group the ability to generate a data key and decrypt that data key from the CMK when necessary</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The first statement provides a specified IAM principal the ability to generate a data key and decrypt that data key from the CMK when necessary</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>The second statement in the policy mentions that all the resources stated in the first statement can take the specified role which will provide the ability to create, list, and revoke grants for Amazon EC2</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The first statement provides a specified IAM principal the ability to generate a data key and decrypt that data key from the CMK when necessary</strong> - To create and use an encrypted Amazon Elastic Block Store (EBS) volume, you need permissions to use Amazon EBS. The key policy associated with the CMK would need to include these. The above policy is an example of one such policy.</p>\n\n<p>In this CMK policy, the first statement provides a specified IAM principal the ability to generate a data key and decrypt that data key from the CMK when necessary. These two APIs are necessary to encrypt the EBS volume while it’s attached to an Amazon Elastic Compute Cloud (EC2) instance.</p>\n\n<p>The second statement in this policy provides the specified IAM principal the ability to create, list, and revoke grants for Amazon EC2. Grants are used to delegate a subset of permissions to AWS services, or other principals, so that they can use your keys on your behalf. In this case, the condition policy explicitly ensures that only Amazon EC2 can use the grants. Amazon EC2 will use them to re-attach an encrypted EBS volume back to an instance if the volume gets detached due to a planned or unplanned outage. These events will be recorded within AWS CloudTrail when, and if, they do occur for your\nauditing.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The first statement provides the security group the ability to generate a data key and decrypt that data key from the CMK when necessary</strong></p>\n\n<p><strong>The second statement in this policy provides the security group (mentioned in the first statement of the policy), the ability to create, list, and revoke grants for Amazon EC2</strong></p>\n\n<p><strong>The second statement in the policy mentions that all the resources stated in the first statement can take the specified role which will provide the ability to create, list, and revoke grants for Amazon EC2</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://d0.awsstatic.com/whitepapers/aws-kms-best-practices.pdf\">https://d0.awsstatic.com/whitepapers/aws-kms-best-practices.pdf</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The first statement provides a specified IAM principal the ability to generate a data key and decrypt that data key from the CMK when necessary</strong> - To create and use an encrypted Amazon Elastic Block Store (EBS) volume, you need permissions to use Amazon EBS. The key policy associated with the CMK would need to include these. The above policy is an example of one such policy."
      },
      {
        "answer": "",
        "explanation": "In this CMK policy, the first statement provides a specified IAM principal the ability to generate a data key and decrypt that data key from the CMK when necessary. These two APIs are necessary to encrypt the EBS volume while it’s attached to an Amazon Elastic Compute Cloud (EC2) instance."
      },
      {
        "answer": "",
        "explanation": "The second statement in this policy provides the specified IAM principal the ability to create, list, and revoke grants for Amazon EC2. Grants are used to delegate a subset of permissions to AWS services, or other principals, so that they can use your keys on your behalf. In this case, the condition policy explicitly ensures that only Amazon EC2 can use the grants. Amazon EC2 will use them to re-attach an encrypted EBS volume back to an instance if the volume gets detached due to a planned or unplanned outage. These events will be recorded within AWS CloudTrail when, and if, they do occur for your\nauditing."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>The first statement provides the security group the ability to generate a data key and decrypt that data key from the CMK when necessary</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "<strong>The second statement in this policy provides the security group (mentioned in the first statement of the policy), the ability to create, list, and revoke grants for Amazon EC2</strong>"
      },
      {
        "answer": "<strong>The second statement in the policy mentions that all the resources stated in the first statement can take the specified role which will provide the ability to create, list, and revoke grants for Amazon EC2</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided above, so these options are incorrect."
      }
    ],
    "references": [
      "https://d0.awsstatic.com/whitepapers/aws-kms-best-practices.pdf"
    ]
  },
  {
    "id": 12,
    "question": "<p>While troubleshooting, a developer realized that the Amazon EC2 instance is unable to connect to the Internet using the Internet Gateway.</p>\n\n<p>Which conditions should be met for Internet connectivity to be established? (Select two)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The instance's subnet is not associated with any route table</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The instance's subnet is associated with multiple route tables with conflicting configurations</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The route table in the instance’s subnet should have a route to an Internet Gateway</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>The subnet has been configured to be Public and has no access to the internet</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic</strong> - The network access control lists (ACLs) that are associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivity</p>\n\n<p><strong>The route table in the instance’s subnet should have a route to an Internet Gateway</strong> - A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instance’s subnet should have a route defined to the Internet Gateway.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The instance's subnet is not associated with any route table</strong> - This is an incorrect statement. A subnet is implicitly associated with the main route table if it is not explicitly associated with a particular route table. So, a subnet is always associated with some route table.</p>\n\n<p><strong>The instance's subnet is associated with multiple route tables with conflicting configurations</strong> - This is an incorrect statement. A subnet can only be associated with one route table at a time.</p>\n\n<p><strong>The subnet has been configured to be Public and has no access to internet</strong> - This is an incorrect statement. Public subnets have access to the internet via Internet Gateway.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic</strong> - The network access control lists (ACLs) that are associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivity"
      },
      {
        "answer": "",
        "explanation": "<strong>The route table in the instance’s subnet should have a route to an Internet Gateway</strong> - A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instance’s subnet should have a route defined to the Internet Gateway."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The instance's subnet is not associated with any route table</strong> - This is an incorrect statement. A subnet is implicitly associated with the main route table if it is not explicitly associated with a particular route table. So, a subnet is always associated with some route table."
      },
      {
        "answer": "",
        "explanation": "<strong>The instance's subnet is associated with multiple route tables with conflicting configurations</strong> - This is an incorrect statement. A subnet can only be associated with one route table at a time."
      },
      {
        "answer": "",
        "explanation": "<strong>The subnet has been configured to be Public and has no access to internet</strong> - This is an incorrect statement. Public subnets have access to the internet via Internet Gateway."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html"
    ]
  },
  {
    "id": 13,
    "question": "<p>A university has created a student portal that is accessible through a smartphone app and web application. The smartphone app is available in both Android and IOS and the web application works on most major browsers. Students will be able to do group study online and create forum questions. All changes made via smartphone devices should be available even when offline and should synchronize with other devices.</p>\n\n<p>Which of the following AWS services will meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Cognito Identity Pools</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Cognito User Pools</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Cognito Sync</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>BeanStalk</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Cognito Sync</strong></p>\n\n<p>Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Cognito Identity Pools</strong> - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.</p>\n\n<p><strong>Cognito User Pools</strong> - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q40-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q40-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a><p></p>\n\n<p><strong>Beanstalk</strong> - With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>How Elastic BeanStalk Works:\n<img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/clearbox-flow-00.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/clearbox-flow-00.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a><p></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Cognito Sync</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Cognito Identity Pools</strong> - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools."
      },
      {
        "answer": "",
        "explanation": "<strong>Cognito User Pools</strong> - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK."
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q40-i1.jpg",
        "answer": "",
        "explanation": "Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:"
      },
      {
        "link": "https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Beanstalk</strong> - With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring."
      },
      {
        "image": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/clearbox-flow-00.png",
        "answer": "",
        "explanation": "How Elastic BeanStalk Works:"
      },
      {
        "link": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html"
    ]
  },
  {
    "id": 14,
    "question": "<p>As an AWS Certified Developer Associate, you have been hired to work with the development team at a company to create a REST API using the serverless architecture.</p>\n\n<p>Which of the following solutions will you choose to move the company to the serverless architecture paradigm?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Public-facing Application Load Balancer with ECS on Amazon EC2</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Route 53 with EC2 as backend</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Fargate with Lambda at the front</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>API Gateway exposing Lambda Functionality</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>API Gateway exposing Lambda Functionality</strong></p>\n\n<p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a><p></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a><p></p>\n\n<p>API Gateway can expose Lambda functionality through RESTful APIs. Both are serverless options offered by AWS and hence the right choice for this scenario, considering all the functionality they offer.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Fargate with Lambda at the front</strong> - Lambda cannot directly handle RESTful API requests. You can invoke a Lambda function over HTTPS by defining a custom RESTful API using Amazon API Gateway. So, Fargate with Lambda as the front-facing service is a wrong combination, though both Fargate and Lambda are serverless.</p>\n\n<p><strong>Public-facing Application Load Balancer with ECS on Amazon EC2</strong> - ECS on Amazon EC2 does not come under serverless and hence cannot be considered for this use case.</p>\n\n<p><strong>Route 53 with EC2 as backend</strong> - Amazon EC2 is not a serverless service and hence cannot be considered for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/serverless/\">https://aws.amazon.com/serverless/</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>API Gateway exposing Lambda Functionality</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services."
      },
      {
        "image": "https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png",
        "answer": "",
        "explanation": "How API Gateway Works:"
      },
      {
        "link": "https://aws.amazon.com/api-gateway/"
      },
      {
        "answer": "",
        "explanation": "AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume."
      },
      {
        "image": "https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png",
        "answer": "",
        "explanation": "How Lambda function works:"
      },
      {
        "link": "https://aws.amazon.com/lambda/"
      },
      {
        "answer": "",
        "explanation": "API Gateway can expose Lambda functionality through RESTful APIs. Both are serverless options offered by AWS and hence the right choice for this scenario, considering all the functionality they offer."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Fargate with Lambda at the front</strong> - Lambda cannot directly handle RESTful API requests. You can invoke a Lambda function over HTTPS by defining a custom RESTful API using Amazon API Gateway. So, Fargate with Lambda as the front-facing service is a wrong combination, though both Fargate and Lambda are serverless."
      },
      {
        "answer": "",
        "explanation": "<strong>Public-facing Application Load Balancer with ECS on Amazon EC2</strong> - ECS on Amazon EC2 does not come under serverless and hence cannot be considered for this use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Route 53 with EC2 as backend</strong> - Amazon EC2 is not a serverless service and hence cannot be considered for this use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/api-gateway/",
      "https://aws.amazon.com/lambda/",
      "https://aws.amazon.com/serverless/"
    ]
  },
  {
    "id": 15,
    "question": "<p>A pharmaceutical company uses Amazon EC2 instances for application hosting and Amazon CloudFront for content delivery. A new research paper with critical findings has to be shared with a research team that is spread across the world.</p>\n\n<p>Which of the following represents the most optimal solution to address this requirement without compromising the security of the content?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use CloudFront signed cookies feature to control access to the file</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Using CloudFront's Field-Level Encryption to help protect sensitive data</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use CloudFront signed URL feature to control access to the file</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use CloudFront signed URL feature to control access to the file</strong></p>\n\n<p>A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content.</p>\n\n<p>Here's an overview of how you configure CloudFront for signed URLs and how CloudFront responds when a user uses a signed URL to request a file:</p>\n\n<ol>\n<li><p>In your CloudFront distribution, specify one or more trusted key groups, which contain the public keys that CloudFront can use to verify the URL signature. You use the corresponding private keys to sign the URLs.</p></li>\n<li><p>Develop your application to determine whether a user should have access to your content and to create signed URLs for the files or parts of your application that you want to restrict access to.</p></li>\n<li><p>A user requests a file for which you want to require signed URLs. Your application verifies that the user is entitled to access the file: they've signed in, they've paid for access to the content, or they've met some other requirement for access.</p></li>\n<li><p>Your application creates and returns a signed URL to the user. The signed URL allows the user to download or stream the content.</p></li>\n</ol>\n\n<p>This step is automatic; the user usually doesn't have to do anything additional to access the content. For example, if a user is accessing your content in a web browser, your application returns the signed URL to the browser. The browser immediately uses the signed URL to access the file in the CloudFront edge cache without any intervention from the user.</p>\n\n<ol>\n<li>CloudFront uses the public key to validate the signature and confirm that the URL hasn't been tampered with. If the signature is invalid, the request is rejected. If the request meets the requirements in the policy statement, CloudFront does the standard operations: determines whether the file is already in the edge cache, forwards the request to the origin if necessary, and returns the file to the user.</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudFront signed cookies feature to control access to the file</strong> - CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website. Our requirement has only one file that needs to be shared and hence signed URL is the optimal solution.</p>\n\n<p>Signed URLs take precedence over signed cookies. If you use both signed URLs and signed cookies to control access to the same files and a viewer uses a signed URL to request a file, CloudFront determines whether to return the file to the viewer based only on the signed URL.</p>\n\n<p><strong>Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront</strong> - AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to CloudFront, and lets you control access to your content. Based on conditions that you specify, such as the values of query strings or the IP addresses that requests originate from, CloudFront responds to requests either with the requested content or with an HTTP status code 403 (Forbidden). A firewall is optimal for broader use cases than restricted access to a single file.</p>\n\n<p><strong>Using CloudFront's Field-Level Encryption to help protect sensitive data</strong> - CloudFront's field-level encryption further encrypts sensitive data in an HTTPS form using field-specific encryption keys (which you supply) before a POST request is forwarded to your origin. This ensures that sensitive data can only be decrypted and viewed by certain components or services in your application stack. This feature is not useful for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/12/introducing-field-level-encryption-on-amazon-cloudfront/\">https://aws.amazon.com/about-aws/whats-new/2017/12/introducing-field-level-encryption-on-amazon-cloudfront/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use CloudFront signed URL feature to control access to the file</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content."
      },
      {
        "answer": "",
        "explanation": "Here's an overview of how you configure CloudFront for signed URLs and how CloudFront responds when a user uses a signed URL to request a file:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li><p>In your CloudFront distribution, specify one or more trusted key groups, which contain the public keys that CloudFront can use to verify the URL signature. You use the corresponding private keys to sign the URLs.</p></li>\n<li><p>Develop your application to determine whether a user should have access to your content and to create signed URLs for the files or parts of your application that you want to restrict access to.</p></li>\n<li><p>A user requests a file for which you want to require signed URLs. Your application verifies that the user is entitled to access the file: they've signed in, they've paid for access to the content, or they've met some other requirement for access.</p></li>\n<li><p>Your application creates and returns a signed URL to the user. The signed URL allows the user to download or stream the content.</p></li>\n</ol>"
      },
      {
        "answer": "",
        "explanation": "This step is automatic; the user usually doesn't have to do anything additional to access the content. For example, if a user is accessing your content in a web browser, your application returns the signed URL to the browser. The browser immediately uses the signed URL to access the file in the CloudFront edge cache without any intervention from the user."
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li>CloudFront uses the public key to validate the signature and confirm that the URL hasn't been tampered with. If the signature is invalid, the request is rejected. If the request meets the requirements in the policy statement, CloudFront does the standard operations: determines whether the file is already in the edge cache, forwards the request to the origin if necessary, and returns the file to the user.</li>\n</ol>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use CloudFront signed cookies feature to control access to the file</strong> - CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website. Our requirement has only one file that needs to be shared and hence signed URL is the optimal solution."
      },
      {
        "answer": "",
        "explanation": "Signed URLs take precedence over signed cookies. If you use both signed URLs and signed cookies to control access to the same files and a viewer uses a signed URL to request a file, CloudFront determines whether to return the file to the viewer based only on the signed URL."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront</strong> - AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to CloudFront, and lets you control access to your content. Based on conditions that you specify, such as the values of query strings or the IP addresses that requests originate from, CloudFront responds to requests either with the requested content or with an HTTP status code 403 (Forbidden). A firewall is optimal for broader use cases than restricted access to a single file."
      },
      {
        "answer": "",
        "explanation": "<strong>Using CloudFront's Field-Level Encryption to help protect sensitive data</strong> - CloudFront's field-level encryption further encrypts sensitive data in an HTTPS form using field-specific encryption keys (which you supply) before a POST request is forwarded to your origin. This ensures that sensitive data can only be decrypted and viewed by certain components or services in your application stack. This feature is not useful for the given use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html",
      "https://aws.amazon.com/about-aws/whats-new/2017/12/introducing-field-level-encryption-on-amazon-cloudfront/"
    ]
  },
  {
    "id": 16,
    "question": "<p>A startup has been experimenting with DynamoDB in its new test environment. The development team has discovered that some of the write operations have been overwriting existing items that have the specified primary key. This has messed up their data, leading to data discrepancies.</p>\n\n<p>Which DynamoDB write option should be selected to prevent this kind of overwriting?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Batch writes</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Atomic Counters</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Conditional writes</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Scan operation</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Conditional writes</strong> - DynamoDB optionally supports conditional writes for write operations (PutItem, UpdateItem, DeleteItem). A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error.</p>\n\n<p>For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item. This is the right choice for the current scenario.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Batch writes</strong> - Bath operations (read and write) help reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Applications benefit from this parallelism without having to manage concurrency or threading. But, this is of no use in the current scenario of overwriting changes.</p>\n\n<p><strong>Atomic Counters</strong> - Atomic Counters is a numeric attribute that is incremented, unconditionally, without interfering with other write requests. You might use an atomic counter to track the number of visitors to a website. This functionality is not useful for the current scenario.</p>\n\n<p><strong>Use Scan operation</strong> - A Scan operation in Amazon DynamoDB reads every item in a table or a secondary index. By default, a Scan operation returns all of the data attributes for every item in the table or index. This is given as a distractor and not related to DynamoDB item updates.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Conditional writes</strong> - DynamoDB optionally supports conditional writes for write operations (PutItem, UpdateItem, DeleteItem). A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error."
      },
      {
        "answer": "",
        "explanation": "For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item. This is the right choice for the current scenario."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Batch writes</strong> - Bath operations (read and write) help reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Applications benefit from this parallelism without having to manage concurrency or threading. But, this is of no use in the current scenario of overwriting changes."
      },
      {
        "answer": "",
        "explanation": "<strong>Atomic Counters</strong> - Atomic Counters is a numeric attribute that is incremented, unconditionally, without interfering with other write requests. You might use an atomic counter to track the number of visitors to a website. This functionality is not useful for the current scenario."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Scan operation</strong> - A Scan operation in Amazon DynamoDB reads every item in a table or a secondary index. By default, a Scan operation returns all of the data attributes for every item in the table or index. This is given as a distractor and not related to DynamoDB item updates."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate"
    ]
  },
  {
    "id": 17,
    "question": "<p>A company has created an Amazon S3 bucket that holds customer data. The team lead has just enabled access logging to this bucket. The bucket size has grown substantially after starting access logging. Since no new files have been added to the bucket, the perplexed team lead is looking for an answer.</p>\n\n<p>Which of the following reasons explains this behavior?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A DDoS attack on your S3 bucket can potentially blow up the size of data in the bucket if the bucket security is compromised during the attack</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Object Encryption has been enabled and each object is stored twice as part of this configuration</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Erroneous Bucket policies for batch uploads can sometimes be responsible for the exponential growth of S3 Bucket size</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>S3 access logging is pointing to the same bucket and is responsible for the substantial growth of bucket size</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>S3 access logging is pointing to the same bucket and is responsible for the substantial growth of bucket size</strong> - When your source bucket and target bucket are the same bucket, additional logs are created for the logs that are written to the bucket. The extra logs about logs might make it harder to find the log that you are looking for. This configuration would drastically increase the size of the S3 bucket.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q57-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q57-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-server-access-logs-same-bucket/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-server-access-logs-same-bucket/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Erroneous Bucket policies for batch uploads can sometimes be responsible for the exponential growth of S3 Bucket size</strong> - This is an incorrect statement. A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. A bucket policy, for batch processes or normal processes, will not increase the size of the bucket or the objects in it.</p>\n\n<p><strong>A DDOS attack on your S3 bucket can potentially blow up the size of data in the bucket if the bucket security is compromised during the attack</strong> - This is an incorrect statement. AWS handles DDoS attacks on all of its managed services. However, a DDoS attack will not increase the size of the bucket.</p>\n\n<p><strong>Object Encryption has been enabled and each object is stored twice as part of this configuration</strong> - Encryption does not increase a bucket's size, that too, on daily basis, as if the case in the current scenario</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/set-permissions.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/set-permissions.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>S3 access logging is pointing to the same bucket and is responsible for the substantial growth of bucket size</strong> - When your source bucket and target bucket are the same bucket, additional logs are created for the logs that are written to the bucket. The extra logs about logs might make it harder to find the log that you are looking for. This configuration would drastically increase the size of the S3 bucket."
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/s3-server-access-logs-same-bucket/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Erroneous Bucket policies for batch uploads can sometimes be responsible for the exponential growth of S3 Bucket size</strong> - This is an incorrect statement. A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. A bucket policy, for batch processes or normal processes, will not increase the size of the bucket or the objects in it."
      },
      {
        "answer": "",
        "explanation": "<strong>A DDOS attack on your S3 bucket can potentially blow up the size of data in the bucket if the bucket security is compromised during the attack</strong> - This is an incorrect statement. AWS handles DDoS attacks on all of its managed services. However, a DDoS attack will not increase the size of the bucket."
      },
      {
        "answer": "",
        "explanation": "<strong>Object Encryption has been enabled and each object is stored twice as part of this configuration</strong> - Encryption does not increase a bucket's size, that too, on daily basis, as if the case in the current scenario"
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/s3-server-access-logs-same-bucket/",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/user-guide/set-permissions.html"
    ]
  },
  {
    "id": 18,
    "question": "<p>A company is using a Border Gateway Protocol (BGP) based AWS VPN connection to connect from its on-premises data center to Amazon EC2 instances in the company’s account. The development team can access an EC2 instance in subnet A but is unable to access an EC2 instance in subnet B in the same VPC.</p>\n\n<p>Which logs can be used to verify whether the traffic is reaching subnet B?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>VPC Flow Logs</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Subnet logs</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>BGP logs</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>VPN logs</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>VPC Flow Logs</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. After you've created a flow log, you can retrieve and view its data in the chosen destination.</p>\n\n<p>You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored.</p>\n\n<p>Flow log data for a monitored network interface is recorded as flow log records, which are log events consisting of fields that describe the traffic flow.</p>\n\n<p>To create a flow log, you specify:</p>\n\n<ol>\n<li><p>The resource for which to create the flow log</p></li>\n<li><p>The type of traffic to capture (accepted traffic, rejected traffic, or all traffic)</p></li>\n<li><p>The destinations to which you want to publish the flow log data</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>VPN logs</strong></p>\n\n<p><strong>Subnet logs</strong></p>\n\n<p><strong>BGP logs</strong></p>\n\n<p>These three options are incorrect and have been added as distractors.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>VPC Flow Logs</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. After you've created a flow log, you can retrieve and view its data in the chosen destination."
      },
      {
        "answer": "",
        "explanation": "You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored."
      },
      {
        "answer": "",
        "explanation": "Flow log data for a monitored network interface is recorded as flow log records, which are log events consisting of fields that describe the traffic flow."
      },
      {
        "answer": "",
        "explanation": "To create a flow log, you specify:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li><p>The resource for which to create the flow log</p></li>\n<li><p>The type of traffic to capture (accepted traffic, rejected traffic, or all traffic)</p></li>\n<li><p>The destinations to which you want to publish the flow log data</p></li>\n</ol>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>VPN logs</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Subnet logs</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>BGP logs</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options are incorrect and have been added as distractors."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html"
    ]
  },
  {
    "id": 19,
    "question": "<p>A developer with access to the AWS Management Console terminated an instance in the us-east-1a availability zone. The attached EBS volume remained and is now available for attachment to other instances. Your colleague launches a new Linux EC2 instance in the us-east-1e availability zone and is attempting to attach the EBS volume. Your colleague informs you that it is not possible and need your help.</p>\n\n<p>Which of the following explanations would you provide to them?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The EBS volume is encrypted</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>EBS volumes are AZ locked</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>EBS volumes are region locked</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The required IAM permissions are missing</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>EBS volumes are AZ locked</strong></p>\n\n<p>An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. For current-generation volumes attached to current-generation instance types, you can dynamically increase size, modify the provisioned IOPS capacity, and change volume type on live production volumes.</p>\n\n<p>When you create an EBS volume, it is automatically replicated within its Availability Zone to prevent data loss due to the failure of any single hardware component. You can attach an EBS volume to an EC2 instance in the same Availability Zone.</p>\n\n<p>![EBS Volume Overview]https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q62-i1.jpg)\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EBS volumes are region locked</strong> - It's confined to an Availability Zone and not by region.</p>\n\n<p><strong>The required IAM permissions are missing</strong> - This is a possibility as well but if permissions are not an issue then you are still confined to an availability zone.</p>\n\n<p><strong>The EBS volume is encrypted</strong> - This doesn't affect the ability to attach an EBS volume.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>EBS volumes are AZ locked</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. For current-generation volumes attached to current-generation instance types, you can dynamically increase size, modify the provisioned IOPS capacity, and change volume type on live production volumes."
      },
      {
        "answer": "",
        "explanation": "When you create an EBS volume, it is automatically replicated within its Availability Zone to prevent data loss due to the failure of any single hardware component. You can attach an EBS volume to an EC2 instance in the same Availability Zone."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html",
        "answer": "",
        "explanation": "![EBS Volume Overview]https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q62-i1.jpg)\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html</a>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>EBS volumes are region locked</strong> - It's confined to an Availability Zone and not by region."
      },
      {
        "answer": "",
        "explanation": "<strong>The required IAM permissions are missing</strong> - This is a possibility as well but if permissions are not an issue then you are still confined to an availability zone."
      },
      {
        "answer": "",
        "explanation": "<strong>The EBS volume is encrypted</strong> - This doesn't affect the ability to attach an EBS volume."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html"
    ]
  },
  {
    "id": 20,
    "question": "<p>An Auto Scaling group has a maximum capacity of 3, a current capacity of 2, and a scaling policy that adds 3 instances.</p>\n\n<p>When executing this scaling policy, what is the expected outcome?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon EC2 Auto Scaling adds 3 instances to the group</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon EC2 Auto Scaling adds only 1 instance to the group</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon EC2 Auto Scaling does not add any instances to the group, but suggests changing the scaling policy to add one instance</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon EC2 Auto Scaling adds 3 instances to the group and scales down 2 of those instances eventually</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>A scaling policy instructs Amazon EC2 Auto Scaling to track a specific CloudWatch metric, and it defines what action to take when the associated CloudWatch alarm is in ALARM.</p>\n\n<p>When a scaling policy is executed, if the capacity calculation produces a number outside of the minimum and maximum size range of the group, Amazon EC2 Auto Scaling ensures that the new capacity never goes outside of the minimum and maximum size limits.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q21-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q21-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a><p></p>\n\n<p><strong>Amazon EC2 Auto Scaling adds only 1 instance to the group</strong></p>\n\n<p>For the given use-case, Amazon EC2 Auto Scaling adds only 1 instance to the group to prevent the group from exceeding its maximum size.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon EC2 Auto Scaling adds 3 instances to the group</strong> - This is an incorrect statement. Auto Scaling ensures that the new capacity never goes outside of the minimum and maximum size limits.</p>\n\n<p><strong>Amazon EC2 Auto Scaling adds 3 instances to the group and scales down 2 of those instances eventually</strong> - This is an incorrect statement. Adding the instances initially and immediately downsizing them is impractical.</p>\n\n<p><strong>Amazon EC2 Auto Scaling does not add any instances to the group, but suggests changing the scaling policy to add one instance</strong> - This option has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "A scaling policy instructs Amazon EC2 Auto Scaling to track a specific CloudWatch metric, and it defines what action to take when the associated CloudWatch alarm is in ALARM."
      },
      {
        "answer": "",
        "explanation": "When a scaling policy is executed, if the capacity calculation produces a number outside of the minimum and maximum size range of the group, Amazon EC2 Auto Scaling ensures that the new capacity never goes outside of the minimum and maximum size limits."
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html"
      },
      {
        "answer": "<strong>Amazon EC2 Auto Scaling adds only 1 instance to the group</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "For the given use-case, Amazon EC2 Auto Scaling adds only 1 instance to the group to prevent the group from exceeding its maximum size."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 Auto Scaling adds 3 instances to the group</strong> - This is an incorrect statement. Auto Scaling ensures that the new capacity never goes outside of the minimum and maximum size limits."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 Auto Scaling adds 3 instances to the group and scales down 2 of those instances eventually</strong> - This is an incorrect statement. Adding the instances initially and immediately downsizing them is impractical."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon EC2 Auto Scaling does not add any instances to the group, but suggests changing the scaling policy to add one instance</strong> - This option has been added as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html"
    ]
  },
  {
    "id": 21,
    "question": "<p>A developer in your company was just promoted to Team Lead and will be in charge of code deployment on EC2 instances via AWS CodeCommit and AWS CodeDeploy. Per the new requirements, the deployment process should be able to change permissions for deployed files as well as verify the deployment success.</p>\n\n<p>Which of the following actions should the new Developer take?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Define a <code>buildspec.yml</code> file in the codebuild/ directory</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Define an <code>appspec.yml</code> file in the root directory</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Define a <code>buildspec.yml</code> file in the root directory</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Define an <code>appspec.yml</code> file in the codebuild/ directory</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Define an <code>appspec.yml</code> file in the root directory</strong>: An AppSpec file must be a YAML-formatted file named appspec.yml and it must be placed in the root of the directory structure of an application's source code.</p>\n\n<p>The AppSpec file is used to:</p>\n\n<p>Map the source files in your application revision to their destinations on the instance.</p>\n\n<p>Specify custom permissions for deployed files.</p>\n\n<p>Specify scripts to be run on each instance at various stages of the deployment process.</p>\n\n<p>During deployment, the CodeDeploy agent looks up the name of the current event in the hooks section of the AppSpec file. If the event is not found, the CodeDeploy agent moves on to the next step. If the event is found, the CodeDeploy agent retrieves the list of scripts to execute. The scripts are run sequentially, in the order in which they appear in the file. The status of each script is logged in the CodeDeploy agent log file on the instance.</p>\n\n<p>If a script runs successfully, it returns an exit code of 0 (zero). If the CodeDeploy agent installed on the operating system doesn't match what's listed in the AppSpec file, the deployment fails.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Define a <code>buildspec.yml</code> file in the root directory</strong> - This is a file used by AWS CodeBuild to run a build. This is not relevant to the given use case.</p>\n\n<p><strong>Define a <code>buildspec.yml</code> file in the codebuild/ directory</strong> - This is a file used by AWS CodeBuild to run a build. This is not relevant to the given use case.</p>\n\n<p><strong>Define an <code>appspec.yml</code> file in the codebuild/ directory</strong> - This file is for AWS CodeDeploy and must be placed in the root of the directory structure of an application's source code.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/application-specification-files.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/application-specification-files.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Define an <code>appspec.yml</code> file in the root directory</strong>: An AppSpec file must be a YAML-formatted file named appspec.yml and it must be placed in the root of the directory structure of an application's source code."
      },
      {
        "answer": "",
        "explanation": "The AppSpec file is used to:"
      },
      {
        "answer": "",
        "explanation": "Map the source files in your application revision to their destinations on the instance."
      },
      {
        "answer": "",
        "explanation": "Specify custom permissions for deployed files."
      },
      {
        "answer": "",
        "explanation": "Specify scripts to be run on each instance at various stages of the deployment process."
      },
      {
        "answer": "",
        "explanation": "During deployment, the CodeDeploy agent looks up the name of the current event in the hooks section of the AppSpec file. If the event is not found, the CodeDeploy agent moves on to the next step. If the event is found, the CodeDeploy agent retrieves the list of scripts to execute. The scripts are run sequentially, in the order in which they appear in the file. The status of each script is logged in the CodeDeploy agent log file on the instance."
      },
      {
        "answer": "",
        "explanation": "If a script runs successfully, it returns an exit code of 0 (zero). If the CodeDeploy agent installed on the operating system doesn't match what's listed in the AppSpec file, the deployment fails."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Define a <code>buildspec.yml</code> file in the root directory</strong> - This is a file used by AWS CodeBuild to run a build. This is not relevant to the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Define a <code>buildspec.yml</code> file in the codebuild/ directory</strong> - This is a file used by AWS CodeBuild to run a build. This is not relevant to the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Define an <code>appspec.yml</code> file in the codebuild/ directory</strong> - This file is for AWS CodeDeploy and must be placed in the root of the directory structure of an application's source code."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/application-specification-files.html"
    ]
  },
  {
    "id": 22,
    "question": "<p>What steps can a developer take to optimize the performance of a CPU-bound AWS Lambda function and ensure fast response time?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Increase the function's provisioned concurrency</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Increase the function's timeout</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Increase the function's memory</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Increase the function's CPU</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Increase the function's memory</strong></p>\n\n<p>Memory is the principal lever available to Lambda developers for controlling the performance of a function. You can configure the amount of memory allocated to a Lambda function, between 128 MB and 10,240 MB. The Lambda console defaults new functions to the smallest setting and many developers also choose 128 MB for their functions.</p>\n\n<p>The amount of memory also determines the amount of virtual CPU available to a function. Adding more memory proportionally increases the amount of CPU, increasing the overall computational power available. If a function is CPU-, network- or memory-bound, then changing the memory setting can dramatically improve its performance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the function's provisioned concurrency</strong></p>\n\n<p><strong>Increase the function's reserved concurrency</strong></p>\n\n<p>In Lambda, concurrency is the number of requests your function can handle at the same time. There are two types of concurrency controls available:</p>\n\n<p>Reserved concurrency – Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can use that concurrency. There is no charge for configuring reserved concurrency for a function.</p>\n\n<p>Provisioned concurrency – Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations. Note that configuring provisioned concurrency incurs charges to your AWS account.</p>\n\n<p>Neither reserved concurrency nor provisioned concurrency has any impact on the CPU available to a function, so both these options are incorrect</p>\n\n<p><strong>Increase the function's CPU</strong> - This is a distractor as you cannot directly increase the CPU available to a function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/operatorguide/computing-power.html\">https://docs.aws.amazon.com/lambda/latest/operatorguide/computing-power.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Increase the function's memory</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Memory is the principal lever available to Lambda developers for controlling the performance of a function. You can configure the amount of memory allocated to a Lambda function, between 128 MB and 10,240 MB. The Lambda console defaults new functions to the smallest setting and many developers also choose 128 MB for their functions."
      },
      {
        "answer": "",
        "explanation": "The amount of memory also determines the amount of virtual CPU available to a function. Adding more memory proportionally increases the amount of CPU, increasing the overall computational power available. If a function is CPU-, network- or memory-bound, then changing the memory setting can dramatically improve its performance."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Increase the function's provisioned concurrency</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "<strong>Increase the function's reserved concurrency</strong>"
      },
      {
        "answer": "",
        "explanation": "In Lambda, concurrency is the number of requests your function can handle at the same time. There are two types of concurrency controls available:"
      },
      {
        "answer": "",
        "explanation": "Reserved concurrency – Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can use that concurrency. There is no charge for configuring reserved concurrency for a function."
      },
      {
        "answer": "",
        "explanation": "Provisioned concurrency – Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations. Note that configuring provisioned concurrency incurs charges to your AWS account."
      },
      {
        "answer": "",
        "explanation": "Neither reserved concurrency nor provisioned concurrency has any impact on the CPU available to a function, so both these options are incorrect"
      },
      {
        "answer": "",
        "explanation": "<strong>Increase the function's CPU</strong> - This is a distractor as you cannot directly increase the CPU available to a function."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/operatorguide/computing-power.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html"
    ]
  },
  {
    "id": 23,
    "question": "<p>As a Team Lead, you are expected to generate a report of the code builds for every week to report internally and to the client. This report consists of the number of code builds performed for a week, the percentage success and failure, and overall time spent on these builds by the team members. You also need to retrieve the CodeBuild logs for failed builds and analyze them in Athena.</p>\n\n<p>Which of the following options will help achieve this?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable S3 and CloudWatch Logs integration</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Lambda integration</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS CloudTrail and deliver logs to S3</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use CloudWatch Events</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable S3 and CloudWatch Logs integration</strong> - AWS CodeBuild monitors functions on your behalf and reports metrics through Amazon CloudWatch. These metrics include the number of total builds, failed builds, successful builds, and the duration of builds. You can monitor your builds at two levels: Project level, AWS account level. You can export log data from your log groups to an Amazon S3 bucket and use this data in custom processing and analysis, or to load onto other systems.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use CloudWatch Events</strong> - You can integrate CloudWatch Events with CodeBuild. However, we are looking at storing and running queries on logs, so Cloudwatch logs with S3 integration makes sense for this context.o</p>\n\n<p><strong>Use AWS Lambda integration</strong> - Lambda is a good choice to use boto3 library to read logs programmatically. But, CloudWatch and S3 integration is already built-in and is an optimized way of managing the given use-case.</p>\n\n<p><strong>Use AWS CloudTrail and deliver logs to S3</strong> - AWS CodeBuild is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in CodeBuild. CloudTrail captures all API calls for CodeBuild as events, including calls from the CodeBuild console and from code calls to the CodeBuild APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an S3 bucket, including events for CodeBuild. This is an important feature for monitoring a service but isn't a good fit for the current scenario.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-metrics.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-metrics.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/getting-started-input-bucket-console.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/getting-started-input-bucket-console.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Enable S3 and CloudWatch Logs integration</strong> - AWS CodeBuild monitors functions on your behalf and reports metrics through Amazon CloudWatch. These metrics include the number of total builds, failed builds, successful builds, and the duration of builds. You can monitor your builds at two levels: Project level, AWS account level. You can export log data from your log groups to an Amazon S3 bucket and use this data in custom processing and analysis, or to load onto other systems."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use CloudWatch Events</strong> - You can integrate CloudWatch Events with CodeBuild. However, we are looking at storing and running queries on logs, so Cloudwatch logs with S3 integration makes sense for this context.o"
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Lambda integration</strong> - Lambda is a good choice to use boto3 library to read logs programmatically. But, CloudWatch and S3 integration is already built-in and is an optimized way of managing the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS CloudTrail and deliver logs to S3</strong> - AWS CodeBuild is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in CodeBuild. CloudTrail captures all API calls for CodeBuild as events, including calls from the CodeBuild console and from code calls to the CodeBuild APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an S3 bucket, including events for CodeBuild. This is an important feature for monitoring a service but isn't a good fit for the current scenario."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-metrics.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html",
      "https://docs.aws.amazon.com/codebuild/latest/userguide/getting-started-input-bucket-console.html"
    ]
  },
  {
    "id": 24,
    "question": "<p>As a Developer, you are given a document written in YAML that represents the architecture of a serverless application. The first line of the document contains <code>Transform: 'AWS::Serverless-2016-10-31'</code>.</p>\n\n<p>What does the <code>Transform</code> section in the document represent?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Presence of <code>Transform</code> section indicates it is a Serverless Application Model (SAM) template</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Presence of <code>Transform</code> section indicates it is a CloudFormation Parameter</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>It represents an intrinsic function</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>It represents a Lambda function definition</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. Templates include several major sections. The \"Resources\" section is the only required section. The optional \"Transform\" section specifies one or more macros that AWS CloudFormation uses to process your template.</p>\n\n<p><strong>Presence of <code>Transform</code> section indicates it is a Serverless Application Model (SAM) template</strong> - The AWS::Serverless transform, which is a macro hosted by AWS CloudFormation, takes an entire template written in the AWS Serverless Application Model (AWS SAM) syntax and transforms and expands it into a compliant AWS CloudFormation template. So, the presence of the <code>Transform</code> section indicates, the document is a SAM template.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>It represents a Lambda function definition</strong> - Lambda function is created using \"AWS::Lambda::Function\" resource and has no connection to <code>Transform</code> section.</p>\n\n<p><strong>It represents an intrinsic function</strong> - Intrinsic Functions in templates are used to assign values to properties that are not available until runtime. They usually start with <code>Fn::</code> or <code>!</code>. Example: <code>!Sub</code> or <code>Fn::Sub</code>.</p>\n\n<p><strong>Presence of 'Transform' section indicates it is a CloudFormation Parameter</strong> - CloudFormation parameters are part of <code>Parameters</code> block of the template, similar to below code:</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS CloudFormation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. Templates include several major sections. The \"Resources\" section is the only required section. The optional \"Transform\" section specifies one or more macros that AWS CloudFormation uses to process your template."
      },
      {
        "answer": "",
        "explanation": "<strong>Presence of <code>Transform</code> section indicates it is a Serverless Application Model (SAM) template</strong> - The AWS::Serverless transform, which is a macro hosted by AWS CloudFormation, takes an entire template written in the AWS Serverless Application Model (AWS SAM) syntax and transforms and expands it into a compliant AWS CloudFormation template. So, the presence of the <code>Transform</code> section indicates, the document is a SAM template."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>It represents a Lambda function definition</strong> - Lambda function is created using \"AWS::Lambda::Function\" resource and has no connection to <code>Transform</code> section."
      },
      {
        "answer": "",
        "explanation": "<strong>It represents an intrinsic function</strong> - Intrinsic Functions in templates are used to assign values to properties that are not available until runtime. They usually start with <code>Fn::</code> or <code>!</code>. Example: <code>!Sub</code> or <code>Fn::Sub</code>."
      },
      {
        "answer": "",
        "explanation": "<strong>Presence of 'Transform' section indicates it is a CloudFormation Parameter</strong> - CloudFormation parameters are part of <code>Parameters</code> block of the template, similar to below code:"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html"
    ]
  },
  {
    "id": 25,
    "question": "<p>The technology team at an investment bank uses DynamoDB to facilitate high-frequency trading where multiple trades can try and update an item at the same time.</p>\n\n<p>Which of the following actions would make sure that only the last updated value of any item is used in the application?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use ConsistentRead = true while doing GetItem operation for any item</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use ConsistentRead = false while doing PutItem operation for any item</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use ConsistentRead = true while doing UpdateItem operation for any item</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use ConsistentRead = true while doing PutItem operation for any item</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use ConsistentRead = true while doing GetItem operation for any item</strong></p>\n\n<p>DynamoDB supports eventually consistent and strongly consistent reads.</p>\n\n<p>Eventually Consistent Reads</p>\n\n<p>When you read data from a DynamoDB table, the response might not reflect the results of a recently completed write operation. The response might include some stale data. If you repeat your read request after a short time, the response should return the latest data.</p>\n\n<p>Strongly Consistent Reads</p>\n\n<p>When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful.</p>\n\n<p>DynamoDB uses eventually consistent reads by default. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. As per the given use-case, to make sure that only the last updated value of any item is used in the application, you should use strongly consistent reads by setting ConsistentRead = true for GetItem operation.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q9-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q9-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use ConsistentRead = true while doing UpdateItem operation for any item</strong></p>\n\n<p><strong>Use ConsistentRead = true while doing PutItem operation for any item</strong></p>\n\n<p><strong>Use ConsistentRead = false while doing PutItem operation for any item</strong></p>\n\n<p>As mentioned in the explanation above, strongly consistent reads apply only while using the read operations (such as GetItem, Query, and Scan). So these three options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use ConsistentRead = true while doing GetItem operation for any item</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "DynamoDB supports eventually consistent and strongly consistent reads."
      },
      {
        "answer": "",
        "explanation": "Eventually Consistent Reads"
      },
      {
        "answer": "",
        "explanation": "When you read data from a DynamoDB table, the response might not reflect the results of a recently completed write operation. The response might include some stale data. If you repeat your read request after a short time, the response should return the latest data."
      },
      {
        "answer": "",
        "explanation": "Strongly Consistent Reads"
      },
      {
        "answer": "",
        "explanation": "When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful."
      },
      {
        "answer": "",
        "explanation": "DynamoDB uses eventually consistent reads by default. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. As per the given use-case, to make sure that only the last updated value of any item is used in the application, you should use strongly consistent reads by setting ConsistentRead = true for GetItem operation."
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Use ConsistentRead = true while doing UpdateItem operation for any item</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Use ConsistentRead = true while doing PutItem operation for any item</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Use ConsistentRead = false while doing PutItem operation for any item</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As mentioned in the explanation above, strongly consistent reads apply only while using the read operations (such as GetItem, Query, and Scan). So these three options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html"
    ]
  },
  {
    "id": 26,
    "question": "<p>The development team at a HealthCare company has deployed EC2 instances in AWS Account A. These instances need to access patient data with Personally Identifiable Information (PII) on multiple S3 buckets in another AWS Account B.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an IAM role with S3 access in Account B and set Account A as a trusted entity. Create another role (instance profile) in Account A and attach it to the EC2 instances in Account A and add an inline policy to this role to assume the role from Account B</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Copy the underlying AMI for the EC2 instances from Account A into Account B. Launch EC2 instances in Account B using this AMI and then access the PII data on Amazon S3 in Account B</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Add a bucket policy to all the Amazon S3 buckets in Account B to allow access from EC2 instances in Account A</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an IAM role (instance profile) in Account A and set Account B as a trusted entity. Attach this role to the EC2 instances in Account A and add an inline policy to this role to access S3 data from Account B</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM role with S3 access in Account B and set Account A as a trusted entity. Create another role (instance profile) in Account A and attach it to the EC2 instances in Account A and add an inline policy to this role to assume the role from Account B</strong></p>\n\n<p>You can give EC2 instances in one account (\"account A\") permissions to assume a role from another account (\"account B\") to access resources such as S3 buckets. You need to create an IAM role in Account B and set Account A as a trusted entity. Then attach a policy to this IAM role such that it delegates access to Amazon S3 like so -</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::awsexamplebucket1\",\n                \"arn:aws:s3:::awsexamplebucket1/*\",\n                \"arn:aws:s3:::awsexamplebucket2\",\n                \"arn:aws:s3:::awsexamplebucket2/*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Then you can create another role (instance profile) in Account A and attach it to the EC2 instances in Account A and add an inline policy to this role to assume the role from Account B like so -</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": \"arn:aws:iam::AccountB_ID:role/ROLENAME\"\n        }\n    ]\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM role (instance profile) in Account A and set Account B as a trusted entity. Attach this role to the EC2 instances in Account A and add an inline policy to this role to access S3 data from Account B</strong> - This option contradicts the explanation provided earlier in the explanation, hence this option is incorrect.</p>\n\n<p><strong>Copy the underlying AMI for the EC2 instances from Account A into Account B. Launch EC2 instances in Account B using this AMI and then access the PII data on Amazon S3 in Account B</strong> - Copying the AMI is a distractor as this does not solve the use-case outlined in the problem statement.</p>\n\n<p><strong>Add a bucket policy to all the Amazon S3 buckets in Account B to allow access from EC2 instances in Account A</strong> - Just adding a bucket policy in Account B is not enough, as you also need to create an IAM policy in Account A to access S3 objects in Account B.</p>\n\n<p>Please review this reference material for a deep-dive on cross-account access to objects that are in Amazon S3 buckets -\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/\">https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-instance-access-bucket/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-instance-access-bucket/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Create an IAM role with S3 access in Account B and set Account A as a trusted entity. Create another role (instance profile) in Account A and attach it to the EC2 instances in Account A and add an inline policy to this role to assume the role from Account B</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can give EC2 instances in one account (\"account A\") permissions to assume a role from another account (\"account B\") to access resources such as S3 buckets. You need to create an IAM role in Account B and set Account A as a trusted entity. Then attach a policy to this IAM role such that it delegates access to Amazon S3 like so -"
      },
      {
        "answer": "",
        "explanation": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::awsexamplebucket1\",\n                \"arn:aws:s3:::awsexamplebucket1/*\",\n                \"arn:aws:s3:::awsexamplebucket2\",\n                \"arn:aws:s3:::awsexamplebucket2/*\"\n            ]\n        }\n    ]\n}\n</code></pre>"
      },
      {
        "answer": "",
        "explanation": "Then you can create another role (instance profile) in Account A and attach it to the EC2 instances in Account A and add an inline policy to this role to assume the role from Account B like so -"
      },
      {
        "answer": "",
        "explanation": "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": \"arn:aws:iam::AccountB_ID:role/ROLENAME\"\n        }\n    ]\n}\n</code></pre>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an IAM role (instance profile) in Account A and set Account B as a trusted entity. Attach this role to the EC2 instances in Account A and add an inline policy to this role to access S3 data from Account B</strong> - This option contradicts the explanation provided earlier in the explanation, hence this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Copy the underlying AMI for the EC2 instances from Account A into Account B. Launch EC2 instances in Account B using this AMI and then access the PII data on Amazon S3 in Account B</strong> - Copying the AMI is a distractor as this does not solve the use-case outlined in the problem statement."
      },
      {
        "answer": "",
        "explanation": "<strong>Add a bucket policy to all the Amazon S3 buckets in Account B to allow access from EC2 instances in Account A</strong> - Just adding a bucket policy in Account B is not enough, as you also need to create an IAM policy in Account A to access S3 objects in Account B."
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/",
        "answer": "",
        "explanation": "Please review this reference material for a deep-dive on cross-account access to objects that are in Amazon S3 buckets -\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/\">https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/</a>"
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/",
      "https://aws.amazon.com/premiumsupport/knowledge-center/s3-instance-access-bucket/"
    ]
  },
  {
    "id": 27,
    "question": "<p>A developer wants to securely store and retrieve various types of variables, such as remote API authentication information, API URL, and related credentials across different environments of an application deployed on Amazon Elastic Container Service (Amazon ECS).</p>\n\n<p>What would be the best approach that needs minimal modifications in the application code?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure the application to fetch the variables and credentials from AWS Systems Manager Parameter Store by leveraging hierarchical unique paths in Parameter Store for each variable in each environment</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure the application to fetch the variables from each of the deployed environments by defining the authentication information and API URL in the ECS task definition as unique names during the deployment process</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure the application to fetch the variables from AWS KMS by storing the API URL and credentials as unique keys in KMS for each environment</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure the application to fetch the variables from an encrypted file that is stored with the application by storing the API URL and credentials in unique files for each environment</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the application to fetch the variables and credentials from AWS Systems Manager Parameter Store by leveraging hierarchical unique paths in Parameter Store for each variable in each environment</strong></p>\n\n<p>Parameter Stores is a capability of AWS Systems Manager that provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter.</p>\n\n<p>Managing dozens or hundreds of parameters as a flat list is time-consuming and prone to errors. It can also be difficult to identify the correct parameter for a task. This means you might accidentally use the wrong parameter, or you might create multiple parameters that use the same configuration data.</p>\n\n<p>You can use parameter hierarchies to help you organize and manage parameters. A hierarchy is a parameter name that includes a path that you define by using forward slashes (/).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q30-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q30-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-hierarchies.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-hierarchies.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the application to fetch the variables from AWS KMS by storing the API URL and credentials as unique keys in KMS for each environment</strong> - AWS KMS lets you create, manage, and control cryptographic keys across your applications and AWS services. KMS is not a key-value service that can be used for the given use case.</p>\n\n<p><strong>Configure the application to fetch the variables from an encrypted file that is stored with the application by storing the API URL and credentials in unique files for each environment</strong> - It is not considered a security best practice to store sensitive data and credentials in an encrypted file with the application. So this option is incorrect.</p>\n\n<p><strong>Configure the application to fetch the variables from each of the deployed environments by defining the authentication information and API URL in the ECS task definition as unique names during the deployment process</strong> - ECS task definition can be thought of as a blueprint for your application. Task definitions specify various parameters for your application. Examples of task definition parameters are which containers to use, which launch type to use, which ports should be opened for your application, and what data volumes should be used with the containers in the task. The specific parameters available for the task definition depend on which launch type you are using. The task definition is a text file, in JSON format, that describes one or more containers, up to a maximum of ten, that form your application. A task is the instantiation of a task definition within a cluster. After you create a task definition for your application within Amazon ECS, you can specify the number of tasks to run on your cluster.</p>\n\n<p>AWS recommends storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters. Environment variables specified in the task definition are readable by all users and roles that are allowed the DescribeTaskDefinition action for the task definition. So this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q30-i3.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q30-i3.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html</a><p></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-hierarchies.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-hierarchies.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kms/\">https://aws.amazon.com/kms/</a></p>\n\n<p><a href=\"https://ecsworkshop.com/introduction/ecs_basics/task_definition/\">https://ecsworkshop.com/introduction/ecs_basics/task_definition/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Configure the application to fetch the variables and credentials from AWS Systems Manager Parameter Store by leveraging hierarchical unique paths in Parameter Store for each variable in each environment</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Parameter Stores is a capability of AWS Systems Manager that provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter."
      },
      {
        "answer": "",
        "explanation": "Managing dozens or hundreds of parameters as a flat list is time-consuming and prone to errors. It can also be difficult to identify the correct parameter for a task. This means you might accidentally use the wrong parameter, or you might create multiple parameters that use the same configuration data."
      },
      {
        "answer": "",
        "explanation": "You can use parameter hierarchies to help you organize and manage parameters. A hierarchy is a parameter name that includes a path that you define by using forward slashes (/)."
      },
      {
        "link": "https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-hierarchies.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure the application to fetch the variables from AWS KMS by storing the API URL and credentials as unique keys in KMS for each environment</strong> - AWS KMS lets you create, manage, and control cryptographic keys across your applications and AWS services. KMS is not a key-value service that can be used for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure the application to fetch the variables from an encrypted file that is stored with the application by storing the API URL and credentials in unique files for each environment</strong> - It is not considered a security best practice to store sensitive data and credentials in an encrypted file with the application. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure the application to fetch the variables from each of the deployed environments by defining the authentication information and API URL in the ECS task definition as unique names during the deployment process</strong> - ECS task definition can be thought of as a blueprint for your application. Task definitions specify various parameters for your application. Examples of task definition parameters are which containers to use, which launch type to use, which ports should be opened for your application, and what data volumes should be used with the containers in the task. The specific parameters available for the task definition depend on which launch type you are using. The task definition is a text file, in JSON format, that describes one or more containers, up to a maximum of ten, that form your application. A task is the instantiation of a task definition within a cluster. After you create a task definition for your application within Amazon ECS, you can specify the number of tasks to run on your cluster."
      },
      {
        "answer": "",
        "explanation": "AWS recommends storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters. Environment variables specified in the task definition are readable by all users and roles that are allowed the DescribeTaskDefinition action for the task definition. So this option is incorrect."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-hierarchies.html",
      "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html",
      "https://aws.amazon.com/kms/",
      "https://ecsworkshop.com/introduction/ecs_basics/task_definition/"
    ]
  },
  {
    "id": 28,
    "question": "<p>A developer needs to automate software package deployment to both Amazon EC2 instances and virtual servers running on-premises, as part of continuous integration and delivery that the business has adopted.</p>\n\n<p>Which AWS service should he use to accomplish this task?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Elastic Beanstalk</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS CodeBuild</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS CodeDeploy</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>AWS CodePipeline</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p>Continuous integration is a DevOps software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run.</p>\n\n<p>Continuous delivery is a software development practice where code changes are automatically prepared for a release to production. A pillar of modern application development, continuous delivery expands upon continuous integration by deploying all code changes to a testing environment and/or a production environment after the build stage.</p>\n\n<p><strong>AWS CodeDeploy</strong> - AWS CodeDeploy is a fully managed \"deployment\" service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. This is the right choice for the current use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS CodePipeline</strong> - AWS CodePipeline is a fully managed \"continuous delivery\" service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define. This enables you to rapidly and reliably deliver features and updates. Whereas CodeDeploy is a deployment service, CodePipeline is a continuous delivery service. For our current scenario, CodeDeploy is the correct choice.</p>\n\n<p><strong>AWS CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don’t need to provision, manage, and scale your own build servers. CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.</p>\n\n<p><strong>AWS Elastic Beanstalk</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codedeploy/\">https://aws.amazon.com/codedeploy/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/\">https://aws.amazon.com/codepipeline/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codebuild/\">https://aws.amazon.com/codebuild/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticbeanstalk/\">https://aws.amazon.com/elasticbeanstalk/</a></p>\n\n<p><a href=\"https://aws.amazon.com/devops/continuous-delivery/\">https://aws.amazon.com/devops/continuous-delivery/</a></p>\n\n<p><a href=\"https://aws.amazon.com/devops/continuous-integration/\">https://aws.amazon.com/devops/continuous-integration/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Continuous integration is a DevOps software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run."
      },
      {
        "answer": "",
        "explanation": "Continuous delivery is a software development practice where code changes are automatically prepared for a release to production. A pillar of modern application development, continuous delivery expands upon continuous integration by deploying all code changes to a testing environment and/or a production environment after the build stage."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS CodeDeploy</strong> - AWS CodeDeploy is a fully managed \"deployment\" service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. This is the right choice for the current use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS CodePipeline</strong> - AWS CodePipeline is a fully managed \"continuous delivery\" service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define. This enables you to rapidly and reliably deliver features and updates. Whereas CodeDeploy is a deployment service, CodePipeline is a continuous delivery service. For our current scenario, CodeDeploy is the correct choice."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don’t need to provision, manage, and scale your own build servers. CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Elastic Beanstalk</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time."
      }
    ],
    "references": [
      "https://aws.amazon.com/codedeploy/",
      "https://aws.amazon.com/codepipeline/",
      "https://aws.amazon.com/codebuild/",
      "https://aws.amazon.com/elasticbeanstalk/",
      "https://aws.amazon.com/devops/continuous-delivery/",
      "https://aws.amazon.com/devops/continuous-integration/"
    ]
  },
  {
    "id": 29,
    "question": "<p>While defining a business workflow as state machine on AWS Step Functions, a developer has configured several states.</p>\n\n<p>Which of the following would you identify as the state that represents a single unit of work performed by a state machine?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<pre><code>\"wait_until\" : {\n  \"Type\": \"Wait\",\n  \"Timestamp\": \"2016-03-14T01:59:00Z\",\n  \"Next\": \"NextState\"\n}\n</code></pre>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<pre><code>\"FailState\": {\n  \"Type\": \"Fail\",\n  \"Cause\": \"Invalid response.\",\n  \"Error\": \"ErrorA\"\n}\n</code></pre>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<pre><code>\"No-op\": {\n  \"Type\": \"Task\",\n  \"Result\": {\n    \"x-datum\": 0.381018,\n    \"y-datum\": 622.2269926397355\n  },\n  \"ResultPath\": \"$.coords\",\n  \"Next\": \"End\"\n}\n</code></pre>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<pre><code>\"HelloWorld\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:HelloFunction\",\n  \"Next\": \"AfterHelloWorldState\",\n  \"Comment\": \"Run the HelloWorld Lambda function\"\n}\n</code></pre>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<pre><code>\"HelloWorld\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:HelloFunction\",\n  \"Next\": \"AfterHelloWorldState\",\n  \"Comment\": \"Run the HelloWorld Lambda function\"\n}\n</code></pre>\n\n<p>A Task state (\"Type\": \"Task\") represents a single unit of work performed by a state machine.</p>\n\n<p>All work in your state machine is done by tasks. A task performs work by using an activity or an AWS Lambda function, or by passing parameters to the API actions of other services.</p>\n\n<p>AWS Step Functions can invoke Lambda functions directly from a task state. A Lambda function is a cloud-native task that runs on AWS Lambda. You can write Lambda functions in a variety of programming languages, using the AWS Management Console or by uploading code to Lambda.</p>\n\n<p>Incorrect options:</p>\n\n<pre><code>\"wait_until\" : {\n  \"Type\": \"Wait\",\n  \"Timestamp\": \"2016-03-14T01:59:00Z\",\n  \"Next\": \"NextState\"\n}\n</code></pre>\n\n<ul>\n<li>A Wait state (\"Type\": \"Wait\") delays the state machine from continuing for a specified time.</li>\n</ul>\n\n<pre><code>\"No-op\": {\n  \"Type\": \"Task\",\n  \"Result\": {\n    \"x-datum\": 0.381018,\n    \"y-datum\": 622.2269926397355\n  },\n  \"ResultPath\": \"$.coords\",\n  \"Next\": \"End\"\n}\n</code></pre>\n\n<ul>\n<li><code>Resource</code> field is a required parameter for <code>Task</code> state. This definition is not of a <code>Task</code> but of type <code>Pass</code>.</li>\n</ul>\n\n<pre><code>\"FailState\": {\n  \"Type\": \"Fail\",\n  \"Cause\": \"Invalid response.\",\n  \"Error\": \"ErrorA\"\n}\n</code></pre>\n\n<ul>\n<li>A Fail state (\"Type\": \"Fail\") stops the execution of the state machine and marks it as a failure unless it is caught by a Catch block. Because Fail states always exit the state machine, they have no Next field and don't require an End field.</li>\n</ul>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html\">https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<pre><code>\"HelloWorld\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:HelloFunction\",\n  \"Next\": \"AfterHelloWorldState\",\n  \"Comment\": \"Run the HelloWorld Lambda function\"\n}\n</code></pre>"
      },
      {
        "answer": "",
        "explanation": "A Task state (\"Type\": \"Task\") represents a single unit of work performed by a state machine."
      },
      {
        "answer": "",
        "explanation": "All work in your state machine is done by tasks. A task performs work by using an activity or an AWS Lambda function, or by passing parameters to the API actions of other services."
      },
      {
        "answer": "",
        "explanation": "AWS Step Functions can invoke Lambda functions directly from a task state. A Lambda function is a cloud-native task that runs on AWS Lambda. You can write Lambda functions in a variety of programming languages, using the AWS Management Console or by uploading code to Lambda."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<pre><code>\"wait_until\" : {\n  \"Type\": \"Wait\",\n  \"Timestamp\": \"2016-03-14T01:59:00Z\",\n  \"Next\": \"NextState\"\n}\n</code></pre>"
      },
      {
        "answer": "",
        "explanation": "<ul>\n<li>A Wait state (\"Type\": \"Wait\") delays the state machine from continuing for a specified time.</li>\n</ul>"
      },
      {
        "answer": "",
        "explanation": "<pre><code>\"No-op\": {\n  \"Type\": \"Task\",\n  \"Result\": {\n    \"x-datum\": 0.381018,\n    \"y-datum\": 622.2269926397355\n  },\n  \"ResultPath\": \"$.coords\",\n  \"Next\": \"End\"\n}\n</code></pre>"
      },
      {
        "answer": "",
        "explanation": "<ul>\n<li><code>Resource</code> field is a required parameter for <code>Task</code> state. This definition is not of a <code>Task</code> but of type <code>Pass</code>.</li>\n</ul>"
      },
      {
        "answer": "",
        "explanation": "<pre><code>\"FailState\": {\n  \"Type\": \"Fail\",\n  \"Cause\": \"Invalid response.\",\n  \"Error\": \"ErrorA\"\n}\n</code></pre>"
      },
      {
        "answer": "",
        "explanation": "<ul>\n<li>A Fail state (\"Type\": \"Fail\") stops the execution of the state machine and marks it as a failure unless it is caught by a Catch block. Because Fail states always exit the state machine, they have no Next field and don't require an End field.</li>\n</ul>"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html"
    ]
  },
  {
    "id": 30,
    "question": "<p>A development team is building a game where players can buy items with virtual coins. For every virtual coin bought by a user, both the players table as well as the items table in DynamodDB need to be updated simultaneously using an all-or-nothing operation.</p>\n\n<p>As a developer associate, how will you implement this functionality?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Capture the transactions in the players table using DynamoDB streams and then sync with the items table</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use <code>TransactWriteItems</code> API of DynamoDB Transactions</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Capture the transactions in the items table using DynamoDB streams and then sync with the players table</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use <code>BatchWriteItem</code> API to update multiple tables simultaneously</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:\n<strong>Use <code>TransactWriteItems</code> API of DynamoDB Transactions</strong></p>\n\n<p>With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing <code>TransactWriteItems</code> or <code>TransactGetItems</code> operation.</p>\n\n<p><code>TransactWriteItems</code> is a synchronous and idempotent write operation that groups up to 25 write actions in a single all-or-nothing operation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same Region. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions are completed atomically so that either all of them succeed or none of them succeeds.</p>\n\n<p>You can optionally include a client token when you make a TransactWriteItems call to ensure that the request is idempotent. Making your transactions idempotent helps prevent application errors if the same operation is submitted multiple times due to a connection time-out or other connectivity issue.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use <code>BatchWriteItem</code> API to update multiple tables simultaneously</strong> - A <code>TransactWriteItems</code> operation differs from a <code>BatchWriteItem</code> operation in that all the actions it contains must be completed successfully, or no changes are made at all. With a <code>BatchWriteItem</code> operation, it is possible that only some of the actions in the batch succeed while the others do not.</p>\n\n<p><strong>Capture the transactions in the players table using DynamoDB streams and then sync with the items table</strong></p>\n\n<p><strong>Capture the transactions in the items table using DynamoDB streams and then sync with the players table</strong></p>\n\n<p>Many applications benefit from capturing changes to items stored in a DynamoDB table, at the point in time when such changes occur. DynamoDB supports streaming of item-level change data capture records in near-real-time. You can build applications that consume these streams and take action based on the contents. DynamoDB streams cannot be used to capture transactions in DynamoDB, therefore both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html#transaction-apis-txwriteitems\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html#transaction-apis-txwriteitems</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "Correct option:\n<strong>Use <code>TransactWriteItems</code> API of DynamoDB Transactions</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing <code>TransactWriteItems</code> or <code>TransactGetItems</code> operation."
      },
      {
        "answer": "",
        "explanation": "<code>TransactWriteItems</code> is a synchronous and idempotent write operation that groups up to 25 write actions in a single all-or-nothing operation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same Region. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions are completed atomically so that either all of them succeed or none of them succeeds."
      },
      {
        "answer": "",
        "explanation": "You can optionally include a client token when you make a TransactWriteItems call to ensure that the request is idempotent. Making your transactions idempotent helps prevent application errors if the same operation is submitted multiple times due to a connection time-out or other connectivity issue."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use <code>BatchWriteItem</code> API to update multiple tables simultaneously</strong> - A <code>TransactWriteItems</code> operation differs from a <code>BatchWriteItem</code> operation in that all the actions it contains must be completed successfully, or no changes are made at all. With a <code>BatchWriteItem</code> operation, it is possible that only some of the actions in the batch succeed while the others do not."
      },
      {
        "answer": "<strong>Capture the transactions in the players table using DynamoDB streams and then sync with the items table</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Capture the transactions in the items table using DynamoDB streams and then sync with the players table</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Many applications benefit from capturing changes to items stored in a DynamoDB table, at the point in time when such changes occur. DynamoDB supports streaming of item-level change data capture records in near-real-time. You can build applications that consume these streams and take action based on the contents. DynamoDB streams cannot be used to capture transactions in DynamoDB, therefore both these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html#transaction-apis-txwriteitems"
    ]
  },
  {
    "id": 31,
    "question": "<p>A CRM application is hosted on Amazon EC2 instances with the database tier using DynamoDB. The customers have raised privacy and security concerns regarding sending and receiving data across the public internet.</p>\n\n<p>As a developer associate, which of the following would you suggest as an optimal solution for providing communication between EC2 instances and DynamoDB without using the public internet?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Internet Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a NAT Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The firm can use a virtual private network (VPN) to route all DynamoDB network traffic through their own corporate network infrastructure</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure VPC endpoints for DynamoDB that will provide required internal access without using public internet</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure VPC endpoints for DynamoDB that will provide required internal access without using public internet</strong></p>\n\n<p>When you create a VPC endpoint for DynamoDB, any requests to a DynamoDB endpoint within the Region (for example, dynamodb.us-west-2.amazonaws.com) are routed to a private DynamoDB endpoint within the Amazon network. You don't need to modify your applications running on EC2 instances in your VPC. The endpoint name remains the same, but the route to DynamoDB stays entirely within the Amazon network, and does not access the public internet. You use endpoint policies to control access to DynamoDB. Traffic between your VPC and the AWS service does not leave the Amazon network.</p>\n\n<p>Using Amazon VPC Endpoints to Access DynamoDB:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q13-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q13-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The firm can use a virtual private network (VPN) to route all DynamoDB network traffic through their own corporate network infrastructure</strong> - You can address the requested security concerns by using a virtual private network (VPN) to route all DynamoDB network traffic through your own corporate network infrastructure. However, this approach can introduce bandwidth and availability challenges and hence is not an optimal solution here.</p>\n\n<p><strong>Create a NAT Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</strong> - You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. NAT Gateway is not useful here since the instance and DynamoDB are present in AWS network and do not need NAT Gateway for communicating with each other.</p>\n\n<p><strong>Create an Internet Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</strong> - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. Using an Internet Gateway would imply that the EC2 instances are connecting to DynamoDB using the public internet. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/Carrier_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/Carrier_Gateway.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Configure VPC endpoints for DynamoDB that will provide required internal access without using public internet</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "When you create a VPC endpoint for DynamoDB, any requests to a DynamoDB endpoint within the Region (for example, dynamodb.us-west-2.amazonaws.com) are routed to a private DynamoDB endpoint within the Amazon network. You don't need to modify your applications running on EC2 instances in your VPC. The endpoint name remains the same, but the route to DynamoDB stays entirely within the Amazon network, and does not access the public internet. You use endpoint policies to control access to DynamoDB. Traffic between your VPC and the AWS service does not leave the Amazon network."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q13-i1.jpg",
        "answer": "",
        "explanation": "Using Amazon VPC Endpoints to Access DynamoDB:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The firm can use a virtual private network (VPN) to route all DynamoDB network traffic through their own corporate network infrastructure</strong> - You can address the requested security concerns by using a virtual private network (VPN) to route all DynamoDB network traffic through your own corporate network infrastructure. However, this approach can introduce bandwidth and availability challenges and hence is not an optimal solution here."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a NAT Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</strong> - You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. NAT Gateway is not useful here since the instance and DynamoDB are present in AWS network and do not need NAT Gateway for communicating with each other."
      },
      {
        "answer": "",
        "explanation": "<strong>Create an Internet Gateway to provide the necessary communication channel between EC2 instances and DynamoDB</strong> - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. Using an Internet Gateway would imply that the EC2 instances are connecting to DynamoDB using the public internet. Therefore, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
      "https://docs.aws.amazon.com/vpc/latest/userguide/Carrier_Gateway.html"
    ]
  },
  {
    "id": 32,
    "question": "<p>You are a development team lead setting permissions for other IAM users with limited permissions. On the AWS Management Console, you created a dev group where new developers will be added, and on your workstation, you configured a developer profile. You would like to test that this user cannot terminate instances.</p>\n\n<p>Which of the following options would you execute?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Using the CLI, create a dummy EC2 and delete it using another CLI call</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Retrieve the policy using the EC2 metadata service and use the IAM policy simulator</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the AWS CLI --test option</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the AWS CLI --dry-run option</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the AWS CLI --dry-run option</strong>: The --dry-run option checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation, otherwise, it is UnauthorizedOperation.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the AWS CLI --test option</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p><strong>Retrieve the policy using the EC2 metadata service and use the IAM policy simulator</strong> - EC2 metadata service is used to retrieve dynamic information such as instance-id, local-hostname, public-hostname. This cannot be used to check whether you have the required permissions for the action.</p>\n\n<p><strong>Using the CLI, create a dummy EC2 and delete it using another CLI call</strong> - That would not work as the current EC2 may have permissions that the dummy instance does not have. If permissions were the same it can work but it's not as elegant as using the dry-run option.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html\">https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/ec2/terminate-instances.html\">https://docs.aws.amazon.com/cli/latest/reference/ec2/terminate-instances.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the AWS CLI --dry-run option</strong>: The --dry-run option checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation, otherwise, it is UnauthorizedOperation."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the AWS CLI --test option</strong> - This is a made-up option and has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Retrieve the policy using the EC2 metadata service and use the IAM policy simulator</strong> - EC2 metadata service is used to retrieve dynamic information such as instance-id, local-hostname, public-hostname. This cannot be used to check whether you have the required permissions for the action."
      },
      {
        "answer": "",
        "explanation": "<strong>Using the CLI, create a dummy EC2 and delete it using another CLI call</strong> - That would not work as the current EC2 may have permissions that the dummy instance does not have. If permissions were the same it can work but it's not as elegant as using the dry-run option."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html",
      "https://docs.aws.amazon.com/cli/latest/reference/ec2/terminate-instances.html"
    ]
  },
  {
    "id": 33,
    "question": "<p>A junior developer has been asked to configure access to an Amazon EC2 instance hosting a web application. The developer has configured a new security group to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules. A custom Network Access Control List (NACL) connected with the instance's subnet is configured to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules.</p>\n\n<p>Which of the following solutions would you suggest if the EC2 instance needs to accept and respond to requests from the internet?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The configuration is complete on the EC2 instance for accepting and responding to requests</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port range</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port range</strong></p>\n\n<p>Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic. By default, each custom Network ACL denies all inbound and outbound traffic until you add rules.</p>\n\n<p>To enable the connection to a service running on an instance, the associated network ACL must allow both:\n1. Inbound traffic on the port that the service is listening on\n2. Outbound traffic to ephemeral ports</p>\n\n<p>When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port.</p>\n\n<p>The designated ephemeral port becomes the destination port for return traffic from the service. Outbound traffic to the ephemeral port must be allowed in the network ACL.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The configuration is complete on the EC2 instance for accepting and responding to requests</strong> - As explained above, this is an incorrect statement.</p>\n\n<p><strong>An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port</strong> - Security groups are stateful. Therefore you don't need a rule that allows responses to inbound traffic.</p>\n\n<p><em>Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway</em>* - Security Groups are stateful. Hence, return traffic is automatically allowed, so there is no need to configure an outbound rule on the security group.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/\">https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port range</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic. By default, each custom Network ACL denies all inbound and outbound traffic until you add rules."
      },
      {
        "answer": "",
        "explanation": "To enable the connection to a service running on an instance, the associated network ACL must allow both:\n1. Inbound traffic on the port that the service is listening on\n2. Outbound traffic to ephemeral ports"
      },
      {
        "answer": "",
        "explanation": "When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port."
      },
      {
        "answer": "",
        "explanation": "The designated ephemeral port becomes the destination port for return traffic from the service. Outbound traffic to the ephemeral port must be allowed in the network ACL."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The configuration is complete on the EC2 instance for accepting and responding to requests</strong> - As explained above, this is an incorrect statement."
      },
      {
        "answer": "",
        "explanation": "<strong>An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port</strong> - Security groups are stateful. Therefore you don't need a rule that allows responses to inbound traffic."
      },
      {
        "answer": "",
        "explanation": "<em>Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway</em>* - Security Groups are stateful. Hence, return traffic is automatically allowed, so there is no need to configure an outbound rule on the security group."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/",
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports"
    ]
  },
  {
    "id": 34,
    "question": "<p>You are a developer working on a web application written in Java and would like to use AWS Elastic Beanstalk for deployment because it would handle deployment, capacity provisioning, load balancing, auto-scaling, and application health monitoring. In the past, you connected to your provisioned instances through SSH to issue configuration commands. Now, you would like a configuration mechanism that automatically applies settings for you.</p>\n\n<p>Which of the following options would help do this?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Include config files in .ebextensions/ at the root of your source code</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use an AWS Lambda hook</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy a CloudFormation wrapper</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SSM parameter store as an input to your Elastic Beanstalk Configurations</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Include config files in .ebextensions/ at the root of your source code</strong></p>\n\n<p>The option_settings section of a configuration file defines values for configuration options. Configuration options let you configure your Elastic Beanstalk environment, the AWS resources in it, and the software that runs your application. Configuration files are only one of several ways to set configuration options.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q27-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q27-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy a CloudFormation wrapper</strong> - This is a made-up option. This has been added as a distractor.</p>\n\n<p><strong>Use SSM parameter store as an input to your Elastic Beanstalk Configurations</strong> - SSM parameter is still not supported for Elastic Beanstalk. So this option is incorrect.</p>\n\n<p><strong>Use an AWS Lambda hook</strong> - Lambda functions are not the best-fit to trigger these configuration changes as it would involve significant development effort.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions-optionsettings.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions-optionsettings.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Include config files in .ebextensions/ at the root of your source code</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The option_settings section of a configuration file defines values for configuration options. Configuration options let you configure your Elastic Beanstalk environment, the AWS resources in it, and the software that runs your application. Configuration files are only one of several ways to set configuration options."
      },
      {
        "link": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy a CloudFormation wrapper</strong> - This is a made-up option. This has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Use SSM parameter store as an input to your Elastic Beanstalk Configurations</strong> - SSM parameter is still not supported for Elastic Beanstalk. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use an AWS Lambda hook</strong> - Lambda functions are not the best-fit to trigger these configuration changes as it would involve significant development effort."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions-optionsettings.html"
    ]
  },
  {
    "id": 35,
    "question": "<p>A diagnostic lab stores its data on DynamoDB. The lab wants to backup a particular DynamoDB table data on Amazon S3, so it can download the S3 backup locally for some operational use.</p>\n\n<p>Which of the following options is NOT feasible?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Hive with Amazon EMR to export your data to an S3 bucket and download locally</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the DynamoDB on-demand backup capability to write to Amazon S3 and download locally</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Glue to copy your table to Amazon S3 and download locally</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Data Pipeline to export your table to an S3 bucket in the account of your choice and download locally</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the DynamoDB on-demand backup capability to write to Amazon S3 and download locally</strong> - This option is not feasible for the given use-case. DynamoDB has two built-in backup methods (On-demand, Point-in-time recovery) that write to Amazon S3, but you will not have access to the S3 buckets that are used for these backups.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q58-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q58-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/\">https://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Data Pipeline to export your table to an S3 bucket in the account of your choice and download locally</strong> - This is the easiest method. This method is used when you want to make a one-time backup using the lowest amount of AWS resources possible. Data Pipeline uses Amazon EMR to create the backup, and the scripting is done for you. You don't have to learn Apache Hive or Apache Spark to accomplish this task.</p>\n\n<p><strong>Use Hive with Amazon EMR to export your data to an S3 bucket and download locally</strong> - Use Hive to export data to an S3 bucket. Or, use the open-source emr-dynamodb-connector to manage your own custom backup method in Spark or Hive. These methods are the best practice to use if you're an active Amazon EMR user and are comfortable with Hive or Spark. These methods offer more control than the Data Pipeline method.</p>\n\n<p><strong>Use AWS Glue to copy your table to Amazon S3 and download locally</strong> - Use AWS Glue to copy your table to Amazon S3. This is the best practice to use if you want automated, continuous backups that you can also use in another service, such as Amazon Athena.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/EMR_Hive_Commands.html#EMR_Hive_Commands_exporting\">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/EMR_Hive_Commands.html#EMR_Hive_Commands_exporting</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/how-to-export-an-amazon-dynamodb-table-to-amazon-s3-using-aws-step-functions-and-aws-glue/\">https://aws.amazon.com/blogs/big-data/how-to-export-an-amazon-dynamodb-table-to-amazon-s3-using-aws-step-functions-and-aws-glue/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the DynamoDB on-demand backup capability to write to Amazon S3 and download locally</strong> - This option is not feasible for the given use-case. DynamoDB has two built-in backup methods (On-demand, Point-in-time recovery) that write to Amazon S3, but you will not have access to the S3 buckets that are used for these backups."
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use AWS Data Pipeline to export your table to an S3 bucket in the account of your choice and download locally</strong> - This is the easiest method. This method is used when you want to make a one-time backup using the lowest amount of AWS resources possible. Data Pipeline uses Amazon EMR to create the backup, and the scripting is done for you. You don't have to learn Apache Hive or Apache Spark to accomplish this task."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Hive with Amazon EMR to export your data to an S3 bucket and download locally</strong> - Use Hive to export data to an S3 bucket. Or, use the open-source emr-dynamodb-connector to manage your own custom backup method in Spark or Hive. These methods are the best practice to use if you're an active Amazon EMR user and are comfortable with Hive or Spark. These methods offer more control than the Data Pipeline method."
      },
      {
        "answer": "",
        "explanation": "<strong>Use AWS Glue to copy your table to Amazon S3 and download locally</strong> - Use AWS Glue to copy your table to Amazon S3. This is the best practice to use if you want automated, continuous backups that you can also use in another service, such as Amazon Athena."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html",
      "https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html",
      "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/EMR_Hive_Commands.html#EMR_Hive_Commands_exporting",
      "https://aws.amazon.com/blogs/big-data/how-to-export-an-amazon-dynamodb-table-to-amazon-s3-using-aws-step-functions-and-aws-glue/"
    ]
  },
  {
    "id": 36,
    "question": "<p>A pharmaceutical company runs their database workloads on Provisioned IOPS SSD (io1) volumes.</p>\n\n<p>As a Developer Associate, which of the following options would you identify as an <strong>INVALID</strong> configuration for io1 EBS volume types?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>200 GiB size volume with 5000 IOPS</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>200 GiB size volume with 10000 IOPS</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>200 GiB size volume with 15000 IOPS</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>200 GiB size volume with 2000 IOPS</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>200 GiB size volume with 15000 IOPS</strong> - This is an invalid configuration. The maximum ratio of provisioned IOPS to requested volume size (in GiB) is 50:1. So, for a 200 GiB volume size, max IOPS possible is 200*50 = 10000 IOPS.</p>\n\n<p>Overview of Provisioned IOPS SSD (io1) volumes:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q36-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q36-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p>Provisioned IOPS SSD (io1) volumes allow you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time. An io1 volume can range in size from 4 GiB to 16 TiB. The maximum ratio of provisioned IOPS to the requested volume size (in GiB) is 50:1. For example, a 100 GiB volume can be provisioned with up to 5,000 IOPS.</p>\n\n<p><strong>200 GiB size volume with 2000 IOPS</strong> - As explained above, up to 10000 IOPS is a valid configuration for the given use-case.</p>\n\n<p><strong>200 GiB size volume with 10000 IOPS</strong> - As explained above, up to 10000 IOPS is a valid configuration for the given use-case.</p>\n\n<p><strong>200 GiB size volume with 5000 IOPS</strong> - As explained above, up to 10000 IOPS is a valid configuration for the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>200 GiB size volume with 15000 IOPS</strong> - This is an invalid configuration. The maximum ratio of provisioned IOPS to requested volume size (in GiB) is 50:1. So, for a 200 GiB volume size, max IOPS possible is 200*50 = 10000 IOPS."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q36-i1.jpg",
        "answer": "",
        "explanation": "Overview of Provisioned IOPS SSD (io1) volumes:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Provisioned IOPS SSD (io1) volumes allow you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time. An io1 volume can range in size from 4 GiB to 16 TiB. The maximum ratio of provisioned IOPS to the requested volume size (in GiB) is 50:1. For example, a 100 GiB volume can be provisioned with up to 5,000 IOPS."
      },
      {
        "answer": "",
        "explanation": "<strong>200 GiB size volume with 2000 IOPS</strong> - As explained above, up to 10000 IOPS is a valid configuration for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>200 GiB size volume with 10000 IOPS</strong> - As explained above, up to 10000 IOPS is a valid configuration for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>200 GiB size volume with 5000 IOPS</strong> - As explained above, up to 10000 IOPS is a valid configuration for the given use-case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"
    ]
  },
  {
    "id": 37,
    "question": "<p>The development team at a retail company is gearing up for the upcoming Thanksgiving sale and wants to make sure that the application's serverless backend running via Lambda functions does not hit latency bottlenecks as a result of the traffic spike.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to address this use-case?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>No need to make any special provisions as Lambda is automatically scalable because of its serverless nature</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Add an Application Load Balancer in front of the Lambda functions</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure Application Auto Scaling to manage Lambda provisioned concurrency on a schedule</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Application Auto Scaling to manage Lambda provisioned concurrency on a schedule</strong></p>\n\n<p>Concurrency is the number of requests that a Lambda function is serving at any given time. If a Lambda function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.</p>\n\n<p>Due to a spike in traffic, when Lambda functions scale, this causes the portion of requests that are served by new instances to have higher latency than the rest. To enable your function to scale without fluctuations in latency, use provisioned concurrency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency.</p>\n\n<p>You can configure Application Auto Scaling to manage provisioned concurrency on a schedule or based on utilization. Use scheduled scaling to increase provisioned concurrency in anticipation of peak traffic. To increase provisioned concurrency automatically as needed, use the Application Auto Scaling API to register a target and create a scaling policy.</p>\n\n<p>Please see this note for more details on provisioned concurrency:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q3-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q3-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule</strong> - To ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency. When a function has reserved concurrency, no other function can use that concurrency. More importantly, reserved concurrency also limits the maximum concurrency for the function, and applies to the function as a whole, including versions and aliases.</p>\n\n<p>You cannot configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule.</p>\n\n<p><strong>Add an Application Load Balancer in front of the Lambda functions</strong> - This is a distractor as just adding the Application Load Balancer will not help in scaling the Lambda functions to address the surge in traffic.</p>\n\n<p><strong>No need to make any special provisions as Lambda is automatically scalable because of its serverless nature</strong> - It's true that Lambda is serverless, however, due to the surge in traffic the Lambda functions can still hit the concurrency limits. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Configure Application Auto Scaling to manage Lambda provisioned concurrency on a schedule</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Concurrency is the number of requests that a Lambda function is serving at any given time. If a Lambda function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency."
      },
      {
        "answer": "",
        "explanation": "Due to a spike in traffic, when Lambda functions scale, this causes the portion of requests that are served by new instances to have higher latency than the rest. To enable your function to scale without fluctuations in latency, use provisioned concurrency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency."
      },
      {
        "answer": "",
        "explanation": "You can configure Application Auto Scaling to manage provisioned concurrency on a schedule or based on utilization. Use scheduled scaling to increase provisioned concurrency in anticipation of peak traffic. To increase provisioned concurrency automatically as needed, use the Application Auto Scaling API to register a target and create a scaling policy."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q3-i1.jpg",
        "answer": "",
        "explanation": "Please see this note for more details on provisioned concurrency:"
      },
      {
        "link": "https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule</strong> - To ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency. When a function has reserved concurrency, no other function can use that concurrency. More importantly, reserved concurrency also limits the maximum concurrency for the function, and applies to the function as a whole, including versions and aliases."
      },
      {
        "answer": "",
        "explanation": "You cannot configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule."
      },
      {
        "answer": "",
        "explanation": "<strong>Add an Application Load Balancer in front of the Lambda functions</strong> - This is a distractor as just adding the Application Load Balancer will not help in scaling the Lambda functions to address the surge in traffic."
      },
      {
        "answer": "",
        "explanation": "<strong>No need to make any special provisions as Lambda is automatically scalable because of its serverless nature</strong> - It's true that Lambda is serverless, however, due to the surge in traffic the Lambda functions can still hit the concurrency limits. So this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html"
    ]
  },
  {
    "id": 38,
    "question": "<p>A developer is building a serverless application on AWS and wants to establish an accelerated development workflow. The workflow must allow the developer to deploy incremental changes for testing without deploying the entire application for every code commit. The developer wants to streamline the process while minimizing deployment time.</p>\n\n<p>What should the developer do to meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the <code>sam sync</code> command from the AWS Serverless Application Model (AWS SAM) to deploy incremental changes</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use the <code>cdk diff</code> command from the AWS Cloud Development Kit (AWS CDK) to deploy incremental changes to AWS for testing</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the <code>sam deploy</code> command from the AWS Serverless Application Model (AWS SAM) to deploy incremental changes</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the <code>cdk deploy</code> command from the AWS Cloud Development Kit (AWS CDK) to deploy incremental changes to AWS for testing</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the <code>sam sync</code> command from the AWS Serverless Application Model (AWS SAM) to deploy incremental changes</strong></p>\n\n<p>The <code>sam sync</code> command is specifically designed to enable rapid iteration by synchronizing local changes with the deployed serverless application on AWS. This allows developers to quickly test incremental changes without the overhead of redeploying the entire stack, aligning perfectly with the developer's requirements.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the <code>cdk deploy</code> command from the AWS Cloud Development Kit (AWS CDK) to deploy incremental changes to AWS for testing</strong> - The <code>cdk deploy</code> command is a general-purpose deployment tool for AWS CDK stacks. While it can deploy changes, it does not offer the same level of optimization for incremental changes as sam sync for serverless applications. This command is more suitable for CDK projects rather than SAM-based serverless applications.</p>\n\n<p><strong>Use the <code>sam deploy</code> command from the AWS Serverless Application Model (AWS SAM) to deploy incremental changes</strong> - The <code>sam deploy</code> command performs a full deployment of the application or updated resources, even for small changes. It does not specifically optimize for incremental deployments, which results in longer deployment times.</p>\n\n<p><strong>Use the <code>cdk diff</code> command from the AWS Cloud Development Kit (AWS CDK) to deploy incremental changes to AWS for testing</strong> - The <code>cdk diff</code> command is not a deployment command. It is used to generate a difference report between the deployed and local stacks. It does not synchronize or deploy incremental changes, making it irrelevant for this requirement.use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/using-sam-cli-sync.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/using-sam-cli-sync.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use the <code>sam sync</code> command from the AWS Serverless Application Model (AWS SAM) to deploy incremental changes</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The <code>sam sync</code> command is specifically designed to enable rapid iteration by synchronizing local changes with the deployed serverless application on AWS. This allows developers to quickly test incremental changes without the overhead of redeploying the entire stack, aligning perfectly with the developer's requirements."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use the <code>cdk deploy</code> command from the AWS Cloud Development Kit (AWS CDK) to deploy incremental changes to AWS for testing</strong> - The <code>cdk deploy</code> command is a general-purpose deployment tool for AWS CDK stacks. While it can deploy changes, it does not offer the same level of optimization for incremental changes as sam sync for serverless applications. This command is more suitable for CDK projects rather than SAM-based serverless applications."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the <code>sam deploy</code> command from the AWS Serverless Application Model (AWS SAM) to deploy incremental changes</strong> - The <code>sam deploy</code> command performs a full deployment of the application or updated resources, even for small changes. It does not specifically optimize for incremental deployments, which results in longer deployment times."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the <code>cdk diff</code> command from the AWS Cloud Development Kit (AWS CDK) to deploy incremental changes to AWS for testing</strong> - The <code>cdk diff</code> command is not a deployment command. It is used to generate a difference report between the deployed and local stacks. It does not synchronize or deploy incremental changes, making it irrelevant for this requirement.use-case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/using-sam-cli-sync.html"
    ]
  },
  {
    "id": 39,
    "question": "<p>After a code review, a developer has been asked to make his publicly accessible S3 buckets private, and enable access to objects with a time-bound constraint.</p>\n\n<p>Which of the following options will address the given use-case?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>It is not possible to implement time constraints on Amazon S3 Bucket access</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Routing policies to re-route unintended access</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Bucket policy to block the unintended access</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Share pre-signed URLs with resources that need access</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Share pre-signed URLs with resources that need access</strong> - All objects by default are private, with the object owner having permission to access the objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The pre-signed URLs are valid only for the specified duration.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Bucket policy to block the unintended access</strong> - A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. Bucket policy can be used to block off unintended access, but it's not possible to provide time-based access, as is the case in the current use case.</p>\n\n<p><strong>Use Routing policies to re-route unintended access</strong> - There is no such facility directly available with Amazon S3.</p>\n\n<p><strong>It is not possible to implement time constraints on Amazon S3 Bucket access</strong> - This is an incorrect statement. As explained above, it is possible to give time-bound access permissions on S3 buckets and objects.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Share pre-signed URLs with resources that need access</strong> - All objects by default are private, with the object owner having permission to access the objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The pre-signed URLs are valid only for the specified duration."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Bucket policy to block the unintended access</strong> - A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. Bucket policy can be used to block off unintended access, but it's not possible to provide time-based access, as is the case in the current use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Routing policies to re-route unintended access</strong> - There is no such facility directly available with Amazon S3."
      },
      {
        "answer": "",
        "explanation": "<strong>It is not possible to implement time constraints on Amazon S3 Bucket access</strong> - This is an incorrect statement. As explained above, it is possible to give time-bound access permissions on S3 buckets and objects."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html"
    ]
  },
  {
    "id": 40,
    "question": "<p>A business has their test environment built on Amazon EC2 configured on General purpose SSD volume.</p>\n\n<p>At which gp2 volume size will their test environment hit the max IOPS?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>16 TiB</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>2.7 TiB</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>5.3 TiB</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>10.6 TiB</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>The performance of gp2 volumes is tied to volume size, which determines the baseline performance level of the volume and how quickly it accumulates I/O credits; larger volumes have higher baseline performance levels and accumulate I/O credits faster.</p>\n\n<p><strong>5.3 TiB</strong> - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods of time. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size.</p>\n\n<p>Maximum IOPS vs Volume Size for General Purpose SSD (gp2) volumes:\n<img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/gp2_iops_1.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/gp2_iops_1.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>10.6 TiB</strong> - As explained above, this is an incorrect option.</p>\n\n<p><strong>16 TiB</strong> - As explained above, this is an incorrect option.</p>\n\n<p><strong>2.7 TiB</strong> - As explained above, this is an incorrect option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "The performance of gp2 volumes is tied to volume size, which determines the baseline performance level of the volume and how quickly it accumulates I/O credits; larger volumes have higher baseline performance levels and accumulate I/O credits faster."
      },
      {
        "answer": "",
        "explanation": "<strong>5.3 TiB</strong> - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods of time. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size."
      },
      {
        "image": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/gp2_iops_1.png",
        "answer": "",
        "explanation": "Maximum IOPS vs Volume Size for General Purpose SSD (gp2) volumes:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>10.6 TiB</strong> - As explained above, this is an incorrect option."
      },
      {
        "answer": "",
        "explanation": "<strong>16 TiB</strong> - As explained above, this is an incorrect option."
      },
      {
        "answer": "",
        "explanation": "<strong>2.7 TiB</strong> - As explained above, this is an incorrect option."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"
    ]
  },
  {
    "id": 41,
    "question": "<p>As a Senior Developer, you are tasked with creating several API Gateway powered APIs along with your team of developers. The developers are working on the API in the development environment, but they find the changes made to the APIs are not reflected when the API is called.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for this use-case?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Developers need IAM permissions on API execution component of API Gateway</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Stage Variables for development state of API</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable Lambda authorizer to access API</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Redeploy the API to an existing stage or to a new stage</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Redeploy the API to an existing stage or to a new stage</strong></p>\n\n<p>After creating your API, you must deploy it to make it callable by your users. To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. Every time you update an API, you must redeploy the API to an existing stage or to a new stage. Updating an API includes modifying routes, methods, integrations, authorizers, and anything else other than stage settings.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Developers need IAM permissions on API execution component of API Gateway</strong> - Access control access to Amazon API Gateway APIs is done with IAM permissions. To call a deployed API or to refresh the API caching, you must grant the API caller permissions to perform required IAM actions supported by the API execution component of API Gateway. In the current scenario, developers do not need permissions on \"execution components\" but on \"management components\" of API Gateway that help them to create, deploy, and manage an API. Hence, this statement is an incorrect option.</p>\n\n<p><strong>Enable Lambda authorizer to access API</strong> - A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. So, this feature too helps in access control, but in the current scenario its the developers and not the users who are facing the issue. So, this statement is an incorrect option.</p>\n\n<p><strong>Use Stage Variables for development state of API</strong> - Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates. Stage variables are not connected to the scenario described in the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Redeploy the API to an existing stage or to a new stage</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "After creating your API, you must deploy it to make it callable by your users. To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. Every time you update an API, you must redeploy the API to an existing stage or to a new stage. Updating an API includes modifying routes, methods, integrations, authorizers, and anything else other than stage settings."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Developers need IAM permissions on API execution component of API Gateway</strong> - Access control access to Amazon API Gateway APIs is done with IAM permissions. To call a deployed API or to refresh the API caching, you must grant the API caller permissions to perform required IAM actions supported by the API execution component of API Gateway. In the current scenario, developers do not need permissions on \"execution components\" but on \"management components\" of API Gateway that help them to create, deploy, and manage an API. Hence, this statement is an incorrect option."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable Lambda authorizer to access API</strong> - A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. So, this feature too helps in access control, but in the current scenario its the developers and not the users who are facing the issue. So, this statement is an incorrect option."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Stage Variables for development state of API</strong> - Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates. Stage variables are not connected to the scenario described in the current use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html"
    ]
  },
  {
    "id": 42,
    "question": "<p>The development team at a multi-national retail company wants to support trusted third-party authenticated users from the supplier organizations to create and update records in specific DynamoDB tables in the company's AWS account.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest for the given use-case?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Cognito User pools to enable trusted third-party authenticated users to access DynamoDB</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a new IAM group in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM group credentials to access DynamoDB</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a new IAM user in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM user credentials to access DynamoDB</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Cognito Identity pools to enable trusted third-party authenticated users to access DynamoDB</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito Identity pools to enable trusted third-party authenticated users to access DynamoDB</strong></p>\n\n<p>Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. Amazon Cognito identity pools support the following identity providers:</p>\n\n<p>Public providers: Login with Amazon (Identity Pools), Facebook (Identity Pools), Google (Identity Pools), Sign in with Apple (Identity Pools).</p>\n\n<p>Amazon Cognito User Pools</p>\n\n<p>Open ID Connect Providers (Identity Pools)</p>\n\n<p>SAML Identity Providers (Identity Pools)</p>\n\n<p>Developer Authenticated Identities (Identity Pools)</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q10-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q10-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Cognito User pools to enable trusted third-party authenticated users to access DynamoDB</strong> - A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Cognito User Pools cannot be used to obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB.</p>\n\n<p><strong>Create a new IAM user in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM user credentials to access DynamoDB</strong></p>\n\n<p><strong>Create a new IAM group in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM group credentials to access DynamoDB</strong></p>\n\n<p>Both these options involve setting up IAM resources such as IAM users or IAM groups just to provide access to DynamoDB tables. As the users are already trusted third-party authenticated users, Cognito Identity Pool can address this use-case in an elegant way.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use Cognito Identity pools to enable trusted third-party authenticated users to access DynamoDB</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. Amazon Cognito identity pools support the following identity providers:"
      },
      {
        "answer": "",
        "explanation": "Public providers: Login with Amazon (Identity Pools), Facebook (Identity Pools), Google (Identity Pools), Sign in with Apple (Identity Pools)."
      },
      {
        "answer": "",
        "explanation": "Amazon Cognito User Pools"
      },
      {
        "answer": "",
        "explanation": "Open ID Connect Providers (Identity Pools)"
      },
      {
        "answer": "",
        "explanation": "SAML Identity Providers (Identity Pools)"
      },
      {
        "answer": "",
        "explanation": "Developer Authenticated Identities (Identity Pools)"
      },
      {
        "answer": "",
        "explanation": "Exam Alert:"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q10-i1.jpg",
        "answer": "",
        "explanation": "Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:"
      },
      {
        "link": "https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Cognito User pools to enable trusted third-party authenticated users to access DynamoDB</strong> - A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Cognito User Pools cannot be used to obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB."
      },
      {
        "answer": "<strong>Create a new IAM user in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM user credentials to access DynamoDB</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Create a new IAM group in the company's AWS account for each of the third-party authenticated users from the supplier organizations. The users can then use the IAM group credentials to access DynamoDB</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Both these options involve setting up IAM resources such as IAM users or IAM groups just to provide access to DynamoDB tables. As the users are already trusted third-party authenticated users, Cognito Identity Pool can address this use-case in an elegant way."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html"
    ]
  },
  {
    "id": 43,
    "question": "<p>A company uses Amazon Simple Email Service (SES) to cost-effectively send susbscription emails to the customers. Intermittently, the SES service throws the error: <code>Throttling – Maximum sending rate exceeded</code>.</p>\n\n<p>As a developer associate, which of the following would you recommend to fix this issue?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure Timeout mechanism for each request made to the SES service</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement retry mechanism for all 4xx errors to avoid throttling error</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Raise a service request with Amazon to increase the throttling limit for the SES API</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again</strong> - A “Throttling – Maximum sending rate exceeded” error is retriable. This error is different than other errors returned by Amazon SES. A request rejected with a “Throttling” error can be retried at a later time and is likely to succeed.</p>\n\n<p>Retries are “selfish.” In other words, when a client retries, it spends more of the server's time to get a higher chance of success. Where failures are rare or transient, that's not a problem. This is because the overall number of retried requests is small, and the tradeoff of increasing apparent availability works well. When failures are caused by overload, retries that increase load can make matters significantly worse. They can even delay recovery by keeping the load high long after the original issue is resolved.</p>\n\n<p>The preferred solution is to use a backoff. Instead of retrying immediately and aggressively, the client waits some amount of time between tries. The most common pattern is an exponential backoff, where the wait time is increased exponentially after every attempt.</p>\n\n<p>A variety of factors can affect your send rate, e.g. message size, network performance or Amazon SES availability. The advantage of the exponential backoff approach is that your application will self-tune and it will call Amazon SES at close to the maximum allowed rate.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Timeout mechanism for each request made to the SES service</strong> - Requests are configured to timeout if they do not complete successfully in a given time. This helps free up the database, application and any other resource that could potentially keep on waiting to eventually succeed. But, if errors are caused by load, retries can be ineffective if all clients retry at the same time. Throttling error signifies that load is high on SES and it does not make sense to keep retrying.</p>\n\n<p><strong>Raise a service request with Amazon to increase the throttling limit for the SES API</strong> - If throttling error is persistent, then it indicates a high load on the system consistently and increasing the throttling limit will be the right solution for the problem. But, the error is only intermittent here, signifying that decreasing the rate of requests will handle the error.</p>\n\n<p><strong>Implement retry mechanism for all 4xx errors to avoid throttling error</strong> - 4xx status codes indicate that there was a problem with the client request. Common client request errors include providing invalid credentials and omitting required parameters. When you get a 4xx error, you need to correct the problem and resubmit a properly formed client request. Throttling is a server error and not a client error, hence retry on 4xx errors does not make sense here.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/\">https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/messaging-and-targeting/how-to-handle-a-throttling-maximum-sending-rate-exceeded-error/\">https://aws.amazon.com/blogs/messaging-and-targeting/how-to-handle-a-throttling-maximum-sending-rate-exceeded-error/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again</strong> - A “Throttling – Maximum sending rate exceeded” error is retriable. This error is different than other errors returned by Amazon SES. A request rejected with a “Throttling” error can be retried at a later time and is likely to succeed."
      },
      {
        "answer": "",
        "explanation": "Retries are “selfish.” In other words, when a client retries, it spends more of the server's time to get a higher chance of success. Where failures are rare or transient, that's not a problem. This is because the overall number of retried requests is small, and the tradeoff of increasing apparent availability works well. When failures are caused by overload, retries that increase load can make matters significantly worse. They can even delay recovery by keeping the load high long after the original issue is resolved."
      },
      {
        "answer": "",
        "explanation": "The preferred solution is to use a backoff. Instead of retrying immediately and aggressively, the client waits some amount of time between tries. The most common pattern is an exponential backoff, where the wait time is increased exponentially after every attempt."
      },
      {
        "answer": "",
        "explanation": "A variety of factors can affect your send rate, e.g. message size, network performance or Amazon SES availability. The advantage of the exponential backoff approach is that your application will self-tune and it will call Amazon SES at close to the maximum allowed rate."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure Timeout mechanism for each request made to the SES service</strong> - Requests are configured to timeout if they do not complete successfully in a given time. This helps free up the database, application and any other resource that could potentially keep on waiting to eventually succeed. But, if errors are caused by load, retries can be ineffective if all clients retry at the same time. Throttling error signifies that load is high on SES and it does not make sense to keep retrying."
      },
      {
        "answer": "",
        "explanation": "<strong>Raise a service request with Amazon to increase the throttling limit for the SES API</strong> - If throttling error is persistent, then it indicates a high load on the system consistently and increasing the throttling limit will be the right solution for the problem. But, the error is only intermittent here, signifying that decreasing the rate of requests will handle the error."
      },
      {
        "answer": "",
        "explanation": "<strong>Implement retry mechanism for all 4xx errors to avoid throttling error</strong> - 4xx status codes indicate that there was a problem with the client request. Common client request errors include providing invalid credentials and omitting required parameters. When you get a 4xx error, you need to correct the problem and resubmit a properly formed client request. Throttling is a server error and not a client error, hence retry on 4xx errors does not make sense here."
      }
    ],
    "references": [
      "https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/",
      "https://aws.amazon.com/blogs/messaging-and-targeting/how-to-handle-a-throttling-maximum-sending-rate-exceeded-error/"
    ]
  },
  {
    "id": 44,
    "question": "<p>You create an Auto Scaling group to work with an Application Load Balancer. The scaling group is configured with a minimum size value of 5, a maximum value of 20, and the desired capacity value of 10. One of the 10 EC2 instances has been reported as unhealthy.</p>\n\n<p>Which of the following actions will take place?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The ASG will detach the EC2 instance from the group, and leave it running</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The ASG will keep the instance running and re-start the application</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The ASG will format the root EBS drive on the EC2 instance and run the User Data again</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The ASG will terminate the EC2 Instance</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The ASG will terminate the EC2 Instance</strong></p>\n\n<p>To maintain the same number of instances, Amazon EC2 Auto Scaling performs a periodic health check on running instances within an Auto Scaling group. When it finds that an instance is unhealthy, it terminates that instance and launches a new one. Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The ASG will detach the EC2 instance from the group, and leave it running</strong> - The goal of the auto-scaling group is to get rid of the bad instance and replace it</p>\n\n<p><strong>The ASG will keep the instance running and re-start the application</strong> - The ASG does not have control of your application</p>\n\n<p><strong>The ASG will format the root EBS drive on the EC2 instance and run the User Data again</strong> - This will not happen, the ASG cannot assume the format of your EBS drive, and User Data only runs once at instance first boot.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html#replace-unhealthy-instance\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html#replace-unhealthy-instance</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>The ASG will terminate the EC2 Instance</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "To maintain the same number of instances, Amazon EC2 Auto Scaling performs a periodic health check on running instances within an Auto Scaling group. When it finds that an instance is unhealthy, it terminates that instance and launches a new one. Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The ASG will detach the EC2 instance from the group, and leave it running</strong> - The goal of the auto-scaling group is to get rid of the bad instance and replace it"
      },
      {
        "answer": "",
        "explanation": "<strong>The ASG will keep the instance running and re-start the application</strong> - The ASG does not have control of your application"
      },
      {
        "answer": "",
        "explanation": "<strong>The ASG will format the root EBS drive on the EC2 instance and run the User Data again</strong> - This will not happen, the ASG cannot assume the format of your EBS drive, and User Data only runs once at instance first boot."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance",
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html#replace-unhealthy-instance"
    ]
  },
  {
    "id": 45,
    "question": "<p>Other than the <code>Resources</code> section, which of the following sections in a Serverless Application Model (SAM) Template is mandatory?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Globals</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Transform</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Parameters</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Mappings</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Transform</strong></p>\n\n<p>The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications on AWS.</p>\n\n<p>A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda function—it can include additional resources such as APIs, databases, and event source mappings.</p>\n\n<p>Serverless Application Model (SAM) Templates include several major sections. Transform and Resources are the only required sections.</p>\n\n<p>Please review this note for more details:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q5-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q5-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Parameters</strong></p>\n\n<p><strong>Mappings</strong></p>\n\n<p><strong>Globals</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these options are not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Transform</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications on AWS."
      },
      {
        "answer": "",
        "explanation": "A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda function—it can include additional resources such as APIs, databases, and event source mappings."
      },
      {
        "answer": "",
        "explanation": "Serverless Application Model (SAM) Templates include several major sections. Transform and Resources are the only required sections."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q5-i1.jpg",
        "answer": "",
        "explanation": "Please review this note for more details:"
      },
      {
        "link": "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Parameters</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Mappings</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Globals</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the details provided in the explanation above, so these options are not correct."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html"
    ]
  },
  {
    "id": 46,
    "question": "<p>Consider an application that enables users to store their mobile phone images in the cloud and supports tens of thousands of users. The application should utilize an Amazon API Gateway REST API that leverages AWS Lambda functions for photo processing while storing photo details in Amazon DynamoDB. The application should allow users to create an account, upload images, and retrieve previously uploaded images, with images ranging in size from 500 KB to 5 MB.</p>\n\n<p>How will you design the application with the least operational overhead?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Cognito identity pools to manage user accounts and set up an Amazon Cognito identity pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images as well as the image metadata in a DynamoDB table. Have the Lambda function retrieve previously uploaded images from DynamoDB</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Cognito identity pools to create an IAM user for each user of the application during the sign-up process. Leverage IAM authentication in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</strong></p>\n\n<p>A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and SAML identity providers. Whether your users sign in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK).</p>\n\n<p>User pools provide:</p>\n\n<p>Sign-up and sign-in services.</p>\n\n<p>A built-in, customizable web UI to sign in users.</p>\n\n<p>Social sign-in with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as sign-in with SAML identity providers from your user pool.</p>\n\n<p>User directory management and user profiles.</p>\n\n<p>Security features such as multi-factor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.</p>\n\n<p>Customized workflows and user migration through AWS Lambda triggers.</p>\n\n<p>To use an Amazon Cognito user pool with your Amazon API Gateway API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header.</p>\n\n<p>For the given use case, you can use a Cognito user pool to manage user accounts and configure an Amazon Cognito user pool authorizer in API Gateway to control access to the API. You should use a Lambda function to store the actual images on S3 and the image metadata on DynamoDB. Finally, you can get the images using the Lambda function that leverages the metadata stored in DynamoDB.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Cognito identity pools to manage user accounts and set up an Amazon Cognito identity pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</strong></p>\n\n<p><strong>Use Cognito identity pools to create an IAM user for each user of the application during the sign-up process. Leverage IAM authentication in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</strong></p>\n\n<p>Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. You cannot use identity pools to manage users or to create IAM users. So both of these options are incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q29-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q29-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/\">https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/</a><p></p>\n\n<p><strong>Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images as well as the image metadata in a DynamoDB table. Have the Lambda function retrieve previously uploaded images from DynamoDB</strong> - You cannot use DynamoDB to store images as the maximum allowed item size is 400KB and the images range in size from 500KB to 5MB. You should also note that storing images on DynamoDB is an anti-pattern. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/\">https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and SAML identity providers. Whether your users sign in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK)."
      },
      {
        "answer": "",
        "explanation": "User pools provide:"
      },
      {
        "answer": "",
        "explanation": "Sign-up and sign-in services."
      },
      {
        "answer": "",
        "explanation": "A built-in, customizable web UI to sign in users."
      },
      {
        "answer": "",
        "explanation": "Social sign-in with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as sign-in with SAML identity providers from your user pool."
      },
      {
        "answer": "",
        "explanation": "User directory management and user profiles."
      },
      {
        "answer": "",
        "explanation": "Security features such as multi-factor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification."
      },
      {
        "answer": "",
        "explanation": "Customized workflows and user migration through AWS Lambda triggers."
      },
      {
        "answer": "",
        "explanation": "To use an Amazon Cognito user pool with your Amazon API Gateway API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header."
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can use a Cognito user pool to manage user accounts and configure an Amazon Cognito user pool authorizer in API Gateway to control access to the API. You should use a Lambda function to store the actual images on S3 and the image metadata on DynamoDB. Finally, you can get the images using the Lambda function that leverages the metadata stored in DynamoDB."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Use Cognito identity pools to manage user accounts and set up an Amazon Cognito identity pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Use Cognito identity pools to create an IAM user for each user of the application during the sign-up process. Leverage IAM authentication in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. You cannot use identity pools to manage users or to create IAM users. So both of these options are incorrect."
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/"
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images as well as the image metadata in a DynamoDB table. Have the Lambda function retrieve previously uploaded images from DynamoDB</strong> - You cannot use DynamoDB to store images as the maximum allowed item size is 400KB and the images range in size from 500KB to 5MB. You should also note that storing images on DynamoDB is an anti-pattern. So this option is incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html"
    ]
  },
  {
    "id": 47,
    "question": "<p>The development team at an e-commerce company completed the last deployment for their application at a reduced capacity because of the deployment policy. The application took a performance hit because of the traffic spike due to an on-going sale.</p>\n\n<p>Which of the following represents the BEST deployment option for the upcoming application version such that it maintains at least the FULL capacity of the application and MINIMAL impact of failed deployment?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy the new application version using 'All at once' deployment policy</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy the new application version using 'Rolling with additional batch' deployment policy</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy the new application version using 'Rolling' deployment policy</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy the new application version using 'Immutable' deployment policy</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy the new application version using 'Immutable' deployment policy</strong></p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>How Elastic BeanStalk Works:\n<img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/clearbox-flow-00.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/clearbox-flow-00.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a><p></p>\n\n<p>The 'Immutable' deployment policy ensures that your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks. In case of deployment failure, the new instances are terminated, so the impact is minimal.</p>\n\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q7-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q7-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the new application version using 'All at once' deployment policy</strong> - Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time. Also in case of deployment failure, the application sees a downtime, so this option is not correct.</p>\n\n<p><strong>Deploy the new application version using 'Rolling' deployment policy</strong> - This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. However in case of deployment failure, the rollback process is via manual redeploy, so it's not as quick as the Immutable deployment.</p>\n\n<p><strong>Deploy the new application version using 'Rolling with additional batch' deployment policy</strong> - This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. However in case of deployment failure, the rollback process is via manual redeploy, so it's not as quick as the Immutable deployment.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Deploy the new application version using 'Immutable' deployment policy</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring."
      },
      {
        "image": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/clearbox-flow-00.png",
        "answer": "",
        "explanation": "How Elastic BeanStalk Works:"
      },
      {
        "link": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html"
      },
      {
        "answer": "",
        "explanation": "The 'Immutable' deployment policy ensures that your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks. In case of deployment failure, the new instances are terminated, so the impact is minimal."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q7-i1.jpg",
        "answer": "",
        "explanation": "Overview of Elastic Beanstalk Deployment Policies:"
      },
      {
        "link": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy the new application version using 'All at once' deployment policy</strong> - Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time. Also in case of deployment failure, the application sees a downtime, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy the new application version using 'Rolling' deployment policy</strong> - This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. However in case of deployment failure, the rollback process is via manual redeploy, so it's not as quick as the Immutable deployment."
      },
      {
        "answer": "",
        "explanation": "<strong>Deploy the new application version using 'Rolling with additional batch' deployment policy</strong> - This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. However in case of deployment failure, the rollback process is via manual redeploy, so it's not as quick as the Immutable deployment."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html"
    ]
  },
  {
    "id": 48,
    "question": "<p>You have launched several AWS Lambda functions written in Java. A new requirement was given that over 1MB of data should be passed to the functions and should be encrypted and decrypted at runtime.</p>\n\n<p>Which of the following methods is suitable to address the given use-case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Envelope Encryption and store as environment variable</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Envelope Encryption and reference the data as file within the code</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use KMS direct encryption and store as file</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use KMS Encryption and store as environment variable</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Envelope Encryption and reference the data as file within the code</strong></p>\n\n<p>While AWS KMS does support sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency.</p>\n\n<p>AWS Lambda environment variables can have a maximum size of 4 KB. Additionally, the direct 'Encrypt' API of KMS also has an upper limit of 4 KB for the data payload. To encrypt 1 MB, you need to use the Encryption SDK and pack the encrypted file with the lambda function.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use KMS direct encryption and store as file</strong> - You can only encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information, so this option is not correct for the given use-case.</p>\n\n<p><strong>Use Envelope Encryption and store as an environment variable</strong> - Environment variables must not exceed 4 KB, so this option is not correct for the given use-case.</p>\n\n<p><strong>Use KMS Encryption and store as an environment variable</strong> - You can encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information. Lambda Environment variables must not exceed 4 KB. So this option is not correct for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html\">https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kms/faqs/\">https://aws.amazon.com/kms/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use Envelope Encryption and reference the data as file within the code</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "While AWS KMS does support sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency."
      },
      {
        "answer": "",
        "explanation": "AWS Lambda environment variables can have a maximum size of 4 KB. Additionally, the direct 'Encrypt' API of KMS also has an upper limit of 4 KB for the data payload. To encrypt 1 MB, you need to use the Encryption SDK and pack the encrypted file with the lambda function."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use KMS direct encryption and store as file</strong> - You can only encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information, so this option is not correct for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Envelope Encryption and store as an environment variable</strong> - Environment variables must not exceed 4 KB, so this option is not correct for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use KMS Encryption and store as an environment variable</strong> - You can encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information. Lambda Environment variables must not exceed 4 KB. So this option is not correct for the given use-case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html",
      "https://aws.amazon.com/kms/faqs/"
    ]
  },
  {
    "id": 49,
    "question": "<p>A developer is defining the signers that can create signed URLs for their Amazon CloudFront distributions.</p>\n\n<p>Which of the following statements should the developer consider while defining the signers? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>When you create a signer, the public key is with CloudFront and private key is used to sign a portion of URL</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Both the signers (trusted key groups and CloudFront key pairs) can be managed using the CloudFront APIs</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>CloudFront key pairs can be created with any account that has administrative permissions and full access to CloudFront resources</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>You can also use AWS Identity and Access Management (IAM) permissions policies to restrict what the root user can do with CloudFront key pairs</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Security",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>When you create a signer, the public key is with CloudFront and private key is used to sign a portion of URL</strong> - Each signer that you use to create CloudFront signed URLs or signed cookies must have a public–private key pair. The signer uses its private key to sign the URL or cookies, and CloudFront uses the public key to verify the signature.</p>\n\n<p>When you create signed URLs or signed cookies, you use the private key from the signer’s key pair to sign a portion of the URL or the cookie. When someone requests a restricted file, CloudFront compares the signature in the URL or cookie with the unsigned URL or cookie, to verify that it hasn’t been tampered with. CloudFront also verifies that the URL or cookie is valid, meaning, for example, that the expiration date and time haven’t passed.</p>\n\n<p><strong>When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account</strong> - When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account.</p>\n\n<p>Whereas, with CloudFront key groups, you can associate a higher number of public keys with your CloudFront distribution, giving you more flexibility in how you use and manage the public keys. By default, you can associate up to four key groups with a single distribution, and you can have up to five public keys in a key group.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can also use AWS Identity and Access Management (IAM) permissions policies to restrict what the root user can do with CloudFront key pairs</strong> - When you use the AWS account root user to manage CloudFront key pairs, you can’t restrict what the root user can do or the conditions in which it can do them. You can’t apply IAM permissions policies to the root user, which is one reason why AWS best practices recommend against using the root user.</p>\n\n<p><strong>CloudFront key pairs can be created with any account that has administrative permissions and full access to CloudFront resources</strong> - CloudFront key pairs can only be created using the root user account and hence is not a best practice to create CloudFront key pairs as signers.</p>\n\n<p><strong>Both the signers (trusted key groups and CloudFront key pairs) can be managed using the CloudFront APIs</strong> - With CloudFront key groups, you can manage public keys, key groups, and trusted signers using the CloudFront API. You can use the API to automate key creation and key rotation. When you use the AWS root user, you have to use the AWS Management Console to manage CloudFront key pairs, so you can’t automate the process.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>When you create a signer, the public key is with CloudFront and private key is used to sign a portion of URL</strong> - Each signer that you use to create CloudFront signed URLs or signed cookies must have a public–private key pair. The signer uses its private key to sign the URL or cookies, and CloudFront uses the public key to verify the signature."
      },
      {
        "answer": "",
        "explanation": "When you create signed URLs or signed cookies, you use the private key from the signer’s key pair to sign a portion of the URL or the cookie. When someone requests a restricted file, CloudFront compares the signature in the URL or cookie with the unsigned URL or cookie, to verify that it hasn’t been tampered with. CloudFront also verifies that the URL or cookie is valid, meaning, for example, that the expiration date and time haven’t passed."
      },
      {
        "answer": "",
        "explanation": "<strong>When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account</strong> - When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account."
      },
      {
        "answer": "",
        "explanation": "Whereas, with CloudFront key groups, you can associate a higher number of public keys with your CloudFront distribution, giving you more flexibility in how you use and manage the public keys. By default, you can associate up to four key groups with a single distribution, and you can have up to five public keys in a key group."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>You can also use AWS Identity and Access Management (IAM) permissions policies to restrict what the root user can do with CloudFront key pairs</strong> - When you use the AWS account root user to manage CloudFront key pairs, you can’t restrict what the root user can do or the conditions in which it can do them. You can’t apply IAM permissions policies to the root user, which is one reason why AWS best practices recommend against using the root user."
      },
      {
        "answer": "",
        "explanation": "<strong>CloudFront key pairs can be created with any account that has administrative permissions and full access to CloudFront resources</strong> - CloudFront key pairs can only be created using the root user account and hence is not a best practice to create CloudFront key pairs as signers."
      },
      {
        "answer": "",
        "explanation": "<strong>Both the signers (trusted key groups and CloudFront key pairs) can be managed using the CloudFront APIs</strong> - With CloudFront key groups, you can manage public keys, key groups, and trusted signers using the CloudFront API. You can use the API to automate key creation and key rotation. When you use the AWS root user, you have to use the AWS Management Console to manage CloudFront key pairs, so you can’t automate the process."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html"
    ]
  },
  {
    "id": 50,
    "question": "<p>Your company has embraced cloud-native microservices architectures. New applications must be dockerized and stored in a registry service offered by AWS. The architecture should support dynamic port mapping and support multiple tasks from a single service on the same container instance. All services should run on the same EC2 instance.</p>\n\n<p>Which of the following options offers the best-fit solution for the given use-case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Classic Load Balancer + ECS</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Application Load Balancer + ECS</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Application Load Balancer + Beanstalk</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Classic Load Balancer + Beanstalk</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Application Load Balancer + ECS</strong></p>\n\n<p>Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control over your infrastructure, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.</p>\n\n<p><img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/ecs/\">https://aws.amazon.com/ecs/</a><p></p>\n\n<p>An Application load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/images/component_architecture.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/images/component_architecture.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a><p></p>\n\n<p>When you deploy your services using Amazon Elastic Container Service (Amazon ECS), you can use dynamic port mapping to support multiple tasks from a single service on the same container instance. Amazon ECS manages updates to your services by automatically registering and deregistering containers with your target group using the instance ID and port for each container.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q24-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q24-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Classic Load Balancer + Beanstalk</strong> - The Classic Load Balancer doesn't allow you to run multiple copies of a task on the same instance. Instead, with the Classic Load Balancer, you must statically map port numbers on a container instance. So this option is ruled out.</p>\n\n<p><strong>Application Load Balancer + Beanstalk</strong> - You can create docker environments that support multiple containers per Amazon EC2 instance with a multi-container Docker platform for Elastic Beanstalk. However, ECS gives you finer control.</p>\n\n<p><strong>Classic Load Balancer + ECS</strong> - The Classic Load Balancer doesn't allow you to run multiple copies of a task in the same instance. Instead, with the Classic Load Balancer, you must statically map port numbers on a container instance. So this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/tutorial-target-ecs-containers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/tutorial-target-ecs-containers.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Application Load Balancer + ECS</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control over your infrastructure, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type."
      },
      {
        "link": "https://aws.amazon.com/ecs/"
      },
      {
        "answer": "",
        "explanation": "An Application load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions."
      },
      {
        "link": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html"
      },
      {
        "answer": "",
        "explanation": "When you deploy your services using Amazon Elastic Container Service (Amazon ECS), you can use dynamic port mapping to support multiple tasks from a single service on the same container instance. Amazon ECS manages updates to your services by automatically registering and deregistering containers with your target group using the instance ID and port for each container."
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Classic Load Balancer + Beanstalk</strong> - The Classic Load Balancer doesn't allow you to run multiple copies of a task on the same instance. Instead, with the Classic Load Balancer, you must statically map port numbers on a container instance. So this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Application Load Balancer + Beanstalk</strong> - You can create docker environments that support multiple containers per Amazon EC2 instance with a multi-container Docker platform for Elastic Beanstalk. However, ECS gives you finer control."
      },
      {
        "answer": "",
        "explanation": "<strong>Classic Load Balancer + ECS</strong> - The Classic Load Balancer doesn't allow you to run multiple copies of a task in the same instance. Instead, with the Classic Load Balancer, you must statically map port numbers on a container instance. So this option is ruled out."
      }
    ],
    "references": [
      "https://aws.amazon.com/ecs/",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs",
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/tutorial-target-ecs-containers.html",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html"
    ]
  },
  {
    "id": 51,
    "question": "<p>An e-commerce company manages a microservices application that receives orders from various partners through a customized API for each partner exposed via Amazon API Gateway. The orders are processed by a shared Lambda function.</p>\n\n<p>How can the company notify each partner regarding the status of their respective orders in the most efficient manner, without affecting other partners' orders? Also, the solution should be scalable to accommodate new partners with minimal code changes required.</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a separate Lambda function for each partner. Set up an SNS topic and subscribe each partner to the SNS topic. Modify each partner's Lambda function to publish messages with specific attributes to the SNS topic and apply the appropriate filter policy to the topic subscriptions</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an SNS topic and subscribe each partner to the SNS topic. Modify the Lambda function to publish messages with specific attributes to the SNS topic and apply the appropriate filter policy to the topic subscriptions</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up a separate SNS topic for each partner. Modify the Lambda function to publish messages for each partner to the partner's SNS topic</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up a separate SNS topic for each partner and subscribe each partner to the respective SNS topic. Modify the Lambda function to publish messages with specific attributes to the partner's SNS topic and apply the appropriate filter policy to the topic subscriptions</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an SNS topic and subscribe each partner to the SNS topic. Modify the Lambda function to publish messages with specific attributes to the SNS topic and apply the appropriate filter policy to the topic subscriptions</strong></p>\n\n<p>An Amazon SNS topic is a logical access point that acts as a communication channel. A topic lets you group multiple endpoints (such as AWS Lambda, Amazon SQS, HTTP/S, or an email address). For example, to broadcast the messages of a message-producer system (such as, an e-commerce website) working with multiple other services that require its messages (for example, checkout and fulfillment systems), you can create a topic for your producer system.</p>\n\n<p>By default, an Amazon SNS topic subscriber receives every message that's published to the topic. To receive only a subset of the messages, a subscriber must assign a filter policy to the topic subscription. A filter policy is a JSON object containing properties that define which messages the subscriber receives. Amazon SNS supports policies that act on the message attributes or the message body, according to the filter policy scope that you set for the subscription. Filter policies for the message body assume that the message payload is a well-formed JSON object.</p>\n\n<p>For the given use case, you can change the Lambda function to publish messages with specific attributes to the single SNS topic and apply the appropriate filter policy to the topic subscriptions for each of the partners. This is also easily scalable for new partners since only the filter policy needs to be set up for the new partner.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a separate SNS topic for each partner. Modify the Lambda function to publish messages for each partner to the partner's SNS topic</strong></p>\n\n<p><strong>Set up a separate SNS topic for each partner and subscribe each partner to the respective SNS topic. Modify the Lambda function to publish messages with specific attributes to the partner's SNS topic and apply the appropriate filter policy to the topic subscriptions</strong></p>\n\n<p>Both of these options represent an inefficient solution as there is no need to segregate each partner's updates into a separate SNS topic. A single SNS topic with distinct filter policies is sufficient.</p>\n\n<p><strong>Set up a separate Lambda function for each partner. Set up an SNS topic and subscribe each partner to the SNS topic. Modify each partner's Lambda function to publish messages with specific attributes to the SNS topic and apply the appropriate filter policy to the topic subscriptions</strong> - This is again an inefficient solution as there is no need to create a separate Lambda function for each partner as just a shared Lambda function is sufficient to process the orders and send an update to the single SNS topic with distinct filter policies.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-create-topic.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-create-topic.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Set up an SNS topic and subscribe each partner to the SNS topic. Modify the Lambda function to publish messages with specific attributes to the SNS topic and apply the appropriate filter policy to the topic subscriptions</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An Amazon SNS topic is a logical access point that acts as a communication channel. A topic lets you group multiple endpoints (such as AWS Lambda, Amazon SQS, HTTP/S, or an email address). For example, to broadcast the messages of a message-producer system (such as, an e-commerce website) working with multiple other services that require its messages (for example, checkout and fulfillment systems), you can create a topic for your producer system."
      },
      {
        "answer": "",
        "explanation": "By default, an Amazon SNS topic subscriber receives every message that's published to the topic. To receive only a subset of the messages, a subscriber must assign a filter policy to the topic subscription. A filter policy is a JSON object containing properties that define which messages the subscriber receives. Amazon SNS supports policies that act on the message attributes or the message body, according to the filter policy scope that you set for the subscription. Filter policies for the message body assume that the message payload is a well-formed JSON object."
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can change the Lambda function to publish messages with specific attributes to the single SNS topic and apply the appropriate filter policy to the topic subscriptions for each of the partners. This is also easily scalable for new partners since only the filter policy needs to be set up for the new partner."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Set up a separate SNS topic for each partner. Modify the Lambda function to publish messages for each partner to the partner's SNS topic</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Set up a separate SNS topic for each partner and subscribe each partner to the respective SNS topic. Modify the Lambda function to publish messages with specific attributes to the partner's SNS topic and apply the appropriate filter policy to the topic subscriptions</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Both of these options represent an inefficient solution as there is no need to segregate each partner's updates into a separate SNS topic. A single SNS topic with distinct filter policies is sufficient."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up a separate Lambda function for each partner. Set up an SNS topic and subscribe each partner to the SNS topic. Modify each partner's Lambda function to publish messages with specific attributes to the SNS topic and apply the appropriate filter policy to the topic subscriptions</strong> - This is again an inefficient solution as there is no need to create a separate Lambda function for each partner as just a shared Lambda function is sufficient to process the orders and send an update to the single SNS topic with distinct filter policies."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/sns/latest/dg/sns-create-topic.html",
      "https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html"
    ]
  },
  {
    "id": 52,
    "question": "<p>A social gaming application supports the transfer of gift vouchers between users. When a user hits a certain milestone on the leaderboard, they earn a gift voucher that can be redeemed or transferred to another user. The development team wants to ensure that this transfer is captured in the database such that the records for both users are either written successfully with the new gift vouchers or the status quo is maintained.</p>\n\n<p>Which of the following solutions represent the best-fit options to meet the requirements for the given use-case? (Select two)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the Amazon Athena transactional read and write APIs on the table items as a single, all-or-nothing operation</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Complete both operations on Amazon RedShift in a single transaction block</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Complete both operations on RDS MySQL in a single transaction block</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use the DynamoDB transactional read and write APIs on the table items as a single, all-or-nothing operation</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Perform DynamoDB read and write operations with ConsistentRead parameter set to true</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the DynamoDB transactional read and write APIs on the table items as a single, all-or-nothing operation</strong></p>\n\n<p>You can use DynamoDB transactions to make coordinated all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, helping you to maintain data correctness in your applications.</p>\n\n<p>DynamoDB Transactions Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q8-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q8-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a><p></p>\n\n<p><strong>Complete both operations on RDS MySQL in a single transaction block</strong></p>\n\n<p>Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database with support for transactions in the cloud. A relational database is a collection of data items with pre-defined relationships between them. RDS supports the most demanding database applications. You can choose between two SSD-backed storage options: one optimized for high-performance Online Transaction Processing (OLTP) applications, and the other for cost-effective general-purpose use.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q8-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q8-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/relational-database/\">https://aws.amazon.com/relational-database/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Perform DynamoDB read and write operations with ConsistentRead parameter set to true</strong> - DynamoDB uses eventually consistent reads unless you specify otherwise. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. Read consistency does not facilitate DynamoDB transactions and this option has been added as a distractor.</p>\n\n<p><strong>Complete both operations on Amazon RedShift in a single transaction block</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. It cannot be used to manage database transactions.</p>\n\n<p><strong>Use the Amazon Athena transactional read and write APIs on the table items as a single, all-or-nothing operation</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. It cannot be used to manage database transactions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/relational-database/\">https://aws.amazon.com/relational-database/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use the DynamoDB transactional read and write APIs on the table items as a single, all-or-nothing operation</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can use DynamoDB transactions to make coordinated all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, helping you to maintain data correctness in your applications."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q8-i1.jpg",
        "answer": "",
        "explanation": "DynamoDB Transactions Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html"
      },
      {
        "answer": "<strong>Complete both operations on RDS MySQL in a single transaction block</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database with support for transactions in the cloud. A relational database is a collection of data items with pre-defined relationships between them. RDS supports the most demanding database applications. You can choose between two SSD-backed storage options: one optimized for high-performance Online Transaction Processing (OLTP) applications, and the other for cost-effective general-purpose use."
      },
      {
        "link": "https://aws.amazon.com/relational-database/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Perform DynamoDB read and write operations with ConsistentRead parameter set to true</strong> - DynamoDB uses eventually consistent reads unless you specify otherwise. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. Read consistency does not facilitate DynamoDB transactions and this option has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Complete both operations on Amazon RedShift in a single transaction block</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. It cannot be used to manage database transactions."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the Amazon Athena transactional read and write APIs on the table items as a single, all-or-nothing operation</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. It cannot be used to manage database transactions."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html",
      "https://aws.amazon.com/relational-database/"
    ]
  },
  {
    "id": 53,
    "question": "<p>A company wants to share information with a third party via an HTTP API endpoint managed by the third party. The company has the necessary API key to access the endpoint and the integration of the API key with the company's application code must not impact the application's performance.</p>\n\n<p>What is the most secure approach?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Keep the API credentials in a local code variable and use the local code variable at runtime to make the API call</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Keep the API credentials in an encrypted file in S3 and use the credentials to make the API call by fetching the API credentials from S3 at runtime by using the AWS SDK</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Keep the API credentials in an encrypted table in MySQL RDS and use the credentials to make the API call by fetching the API credentials from RDS at runtime by using the AWS SDK</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Keep the API credentials in AWS Secrets Manager and use the credentials to make the API call by fetching the API credentials at runtime by using the AWS SDK</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Keep the API credentials in AWS Secrets Manager and use the credentials to make the API call by fetching the API credentials at runtime by using the AWS SDK</strong></p>\n\n<p>Secrets Manager enables you to replace hardcoded credentials in your code, including passwords, with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can't be compromised by someone examining your code, because the secret no longer exists in the code. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a specified schedule. This enables you to replace long-term secrets with short-term ones, significantly reducing the risk of compromise.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q38-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q38-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a><p></p>\n\n<p>In the past, when you created a custom application to retrieve information from a database, you typically embedded the credentials, the secret, for accessing the database directly in the application. When the time came to rotate the credentials, you had to do more than just create new credentials. You had to invest time to update the application to use the new credentials. Then you distributed the updated application. If you had multiple applications with shared credentials and you missed updating one of them, the application failed. Because of this risk, many customers choose not to regularly rotate credentials, which effectively substitutes one risk for another. You can also use caching with Secrets Manager to significantly improve the availability and latency of applications.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Keep the API credentials in an encrypted table in MySQL RDS and use the credentials to make the API call by fetching the API credentials from RDS at runtime by using the AWS SDK</strong></p>\n\n<p><strong>Keep the API credentials in an encrypted file in S3 and use the credentials to make the API call by fetching the API credentials from S3 at runtime by using the AWS SDK</strong></p>\n\n<p><strong>Keep the API credentials in a local code variable and use the local code variable at runtime to make the API call</strong></p>\n\n<p>It is considered a security bad practice to keep sensitive access credentials in code, database, or a flat file on a file system or object storage. Therefore, all three options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/improve-availability-and-latency-of-applications-by-using-aws-secret-managers-python-client-side-caching-library/\">https://aws.amazon.com/blogs/security/improve-availability-and-latency-of-applications-by-using-aws-secret-managers-python-client-side-caching-library/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Keep the API credentials in AWS Secrets Manager and use the credentials to make the API call by fetching the API credentials at runtime by using the AWS SDK</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Secrets Manager enables you to replace hardcoded credentials in your code, including passwords, with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can't be compromised by someone examining your code, because the secret no longer exists in the code. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a specified schedule. This enables you to replace long-term secrets with short-term ones, significantly reducing the risk of compromise."
      },
      {
        "link": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html"
      },
      {
        "answer": "",
        "explanation": "In the past, when you created a custom application to retrieve information from a database, you typically embedded the credentials, the secret, for accessing the database directly in the application. When the time came to rotate the credentials, you had to do more than just create new credentials. You had to invest time to update the application to use the new credentials. Then you distributed the updated application. If you had multiple applications with shared credentials and you missed updating one of them, the application failed. Because of this risk, many customers choose not to regularly rotate credentials, which effectively substitutes one risk for another. You can also use caching with Secrets Manager to significantly improve the availability and latency of applications."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Keep the API credentials in an encrypted table in MySQL RDS and use the credentials to make the API call by fetching the API credentials from RDS at runtime by using the AWS SDK</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Keep the API credentials in an encrypted file in S3 and use the credentials to make the API call by fetching the API credentials from S3 at runtime by using the AWS SDK</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Keep the API credentials in a local code variable and use the local code variable at runtime to make the API call</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "It is considered a security bad practice to keep sensitive access credentials in code, database, or a flat file on a file system or object storage. Therefore, all three options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html",
      "https://aws.amazon.com/blogs/security/improve-availability-and-latency-of-applications-by-using-aws-secret-managers-python-client-side-caching-library/"
    ]
  },
  {
    "id": 54,
    "question": "<p>A company has a cloud system in AWS with components that send and receive messages using SQS queues. While reviewing the system you see that it processes a lot of information and would like to be aware of any limits of the system.</p>\n\n<p>Which of the following represents the maximum number of messages that can be stored in an SQS queue?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>10000</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>10000000</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>no limit</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>100000</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p>\"no limit\": There are no message limits for storing in SQS, but 'in-flight messages' do have limits. Make sure to delete messages after you have processed them. There can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue).</p>\n\n<p>Incorrect options:</p>\n\n<p>\"10000\"</p>\n\n<p>\"100000\"</p>\n\n<p>\"10000000\"</p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-limits.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-limits.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "\"no limit\": There are no message limits for storing in SQS, but 'in-flight messages' do have limits. Make sure to delete messages after you have processed them. There can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue)."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>10000</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>100000</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>10000000</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the details provided in the explanation above, so these are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-limits.html"
    ]
  },
  {
    "id": 55,
    "question": "<p>The development team at an analytics company is using SQS queues for decoupling the various components of application architecture. As the consumers need additional time to process SQS messages, the development team wants to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend to the development team?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use delay queues to postpone the delivery of new messages to the queue for a few seconds</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use FIFO queues to postpone the delivery of new messages to the queue for a few seconds</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use delay queues to postpone the delivery of new messages to the queue for a few seconds</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Delay queues let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q4-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q4-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use FIFO queues to postpone the delivery of new messages to the queue for a few seconds</strong> - SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. You cannot use FIFO queues to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p><strong>Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds</strong> - Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. You cannot use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p><strong>Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds</strong> - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use delay queues to postpone the delivery of new messages to the queue for a few seconds</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent."
      },
      {
        "answer": "",
        "explanation": "Delay queues let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use FIFO queues to postpone the delivery of new messages to the queue for a few seconds</strong> - SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. You cannot use FIFO queues to postpone the delivery of new messages to the queue for a few seconds."
      },
      {
        "answer": "",
        "explanation": "<strong>Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds</strong> - Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. You cannot use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds."
      },
      {
        "answer": "",
        "explanation": "<strong>Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds</strong> - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to postpone the delivery of new messages to the queue for a few seconds."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html"
    ]
  },
  {
    "id": 56,
    "question": "<p>A company uses AWS CodeDeploy to deploy applications from GitHub to EC2 instances running Amazon Linux. The deployment process uses a file called appspec.yml for specifying deployment hooks. A final lifecycle event should be specified to verify the deployment success.</p>\n\n<p>Which of the following hook events should be used to verify the success of the deployment?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>ApplicationStart</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>ValidateService</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AfterInstall</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AllowTraffic</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.</p>\n\n<p>An EC2/On-Premises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q56-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q56-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order</a><p></p>\n\n<p><strong>ValidateService</strong>: ValidateService is the last deployment lifecycle event. It is used to verify the deployment was completed successfully.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AfterInstall</strong> - You can use this deployment lifecycle event for tasks such as configuring your application or changing file permissions</p>\n\n<p><strong>ApplicationStart</strong> - You typically use this deployment lifecycle event to restart services that were stopped during ApplicationStop</p>\n\n<p><strong>AllowTraffic</strong> - During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. This event is reserved for the AWS CodeDeploy agent and cannot be used to run scripts</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications."
      },
      {
        "answer": "",
        "explanation": "An EC2/On-Premises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook."
      },
      {
        "link": "https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order"
      },
      {
        "answer": "",
        "explanation": "<strong>ValidateService</strong>: ValidateService is the last deployment lifecycle event. It is used to verify the deployment was completed successfully."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AfterInstall</strong> - You can use this deployment lifecycle event for tasks such as configuring your application or changing file permissions"
      },
      {
        "answer": "",
        "explanation": "<strong>ApplicationStart</strong> - You typically use this deployment lifecycle event to restart services that were stopped during ApplicationStop"
      },
      {
        "answer": "",
        "explanation": "<strong>AllowTraffic</strong> - During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. This event is reserved for the AWS CodeDeploy agent and cannot be used to run scripts"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order"
    ]
  },
  {
    "id": 57,
    "question": "<p>A company runs its flagship application on a fleet of Amazon EC2 instances. After misplacing a couple of private keys from the SSH key pairs, they have decided to re-use their SSH key pairs for the different instances across AWS Regions.</p>\n\n<p>As a Developer Associate, which of the following would you recommend to address this use-case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Encrypt the private SSH key and store it in the S3 bucket to be accessed from any AWS Region</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Generate a public SSH key from a private SSH key. Then, import the key into each of your AWS Regions</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>It is not possible to reuse SSH key pairs across AWS Regions</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the public and private SSH key pair in AWS Trusted Advisor and access it across AWS Regions</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Generate a public SSH key from a private SSH key. Then, import the key into each of your AWS Regions</strong></p>\n\n<p>Here is the correct way of reusing SSH keys in your AWS Regions:</p>\n\n<ol>\n<li><p>Generate a public SSH key (.pub) file from the private SSH key (.pem) file.</p></li>\n<li><p>Set the AWS Region you wish to import to.</p></li>\n<li><p>Import the public SSH key into the new Region.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>It is not possible to reuse SSH key pairs across AWS Regions</strong> - As explained above, it is possible to reuse with manual import.</p>\n\n<p><strong>Store the public and private SSH key pair in AWS Trusted Advisor and access it across AWS Regions</strong> - AWS Trusted Advisor is an application that draws upon best practices learned from AWS' aggregated operational history of serving hundreds of thousands of AWS customers. Trusted Advisor inspects your AWS environment and makes recommendations for saving money, improving system performance, or closing security gaps. It does not store key pair credentials.</p>\n\n<p><strong>Encrypt the private SSH key and store it in the S3 bucket to be accessed from any AWS Region</strong> - Storing private key to Amazon S3 is possible. But, this will not make the key accessible for all AWS Regions, as is the need in the current use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Generate a public SSH key from a private SSH key. Then, import the key into each of your AWS Regions</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Here is the correct way of reusing SSH keys in your AWS Regions:"
      },
      {
        "answer": "",
        "explanation": "<ol>\n<li><p>Generate a public SSH key (.pub) file from the private SSH key (.pem) file.</p></li>\n<li><p>Set the AWS Region you wish to import to.</p></li>\n<li><p>Import the public SSH key into the new Region.</p></li>\n</ol>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>It is not possible to reuse SSH key pairs across AWS Regions</strong> - As explained above, it is possible to reuse with manual import."
      },
      {
        "answer": "",
        "explanation": "<strong>Store the public and private SSH key pair in AWS Trusted Advisor and access it across AWS Regions</strong> - AWS Trusted Advisor is an application that draws upon best practices learned from AWS' aggregated operational history of serving hundreds of thousands of AWS customers. Trusted Advisor inspects your AWS environment and makes recommendations for saving money, improving system performance, or closing security gaps. It does not store key pair credentials."
      },
      {
        "answer": "",
        "explanation": "<strong>Encrypt the private SSH key and store it in the S3 bucket to be accessed from any AWS Region</strong> - Storing private key to Amazon S3 is possible. But, this will not make the key accessible for all AWS Regions, as is the need in the current use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html"
    ]
  },
  {
    "id": 58,
    "question": "<p>Your team lead has asked you to learn AWS CloudFormation to create a collection of related AWS resources and provision them in an orderly fashion. You decide to provide AWS-specific parameter types to catch invalid values.</p>\n\n<p>When specifying parameters which of the following is not a valid Parameter type?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>String</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>DependentParameter</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS::EC2::KeyPair::KeyName</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>CommaDelimitedList</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a><p></p>\n\n<p>Parameter types enable CloudFormation to validate inputs earlier in the stack creation process.</p>\n\n<p>CloudFormation currently supports the following parameter types:</p>\n\n<pre><code>String – A literal string\nNumber – An integer or float\nList&lt;Number&gt; – An array of integers or floats\nCommaDelimitedList – An array of literal strings that are separated by commas\nAWS::EC2::KeyPair::KeyName – An Amazon EC2 key pair name\nAWS::EC2::SecurityGroup::Id – A security group ID\nAWS::EC2::Subnet::Id – A subnet ID\nAWS::EC2::VPC::Id – A VPC ID\nList&lt;AWS::EC2::VPC::Id&gt; – An array of VPC IDs\nList&lt;AWS::EC2::SecurityGroup::Id&gt; – An array of security group IDs\nList&lt;AWS::EC2::Subnet::Id&gt; – An array of subnet IDs\n</code></pre>\n\n<p><strong>DependentParameter</strong></p>\n\n<p>In CloudFormation, parameters are all independent and cannot depend on each other. Therefore, this is an invalid parameter type.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>String</strong></p>\n\n<p><strong>CommaDelimitedList</strong></p>\n\n<p><strong>AWS::EC2::KeyPair::KeyName</strong></p>\n\n<p>As mentioned in the explanation above, these are valid parameter types.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/using-the-new-cloudformation-parameter-types/\">https://aws.amazon.com/blogs/devops/using-the-new-cloudformation-parameter-types/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png",
        "answer": "",
        "explanation": "How CloudFormation Works:"
      },
      {
        "link": "https://aws.amazon.com/cloudformation/"
      },
      {
        "answer": "",
        "explanation": "Parameter types enable CloudFormation to validate inputs earlier in the stack creation process."
      },
      {
        "answer": "",
        "explanation": "CloudFormation currently supports the following parameter types:"
      },
      {
        "answer": "",
        "explanation": "<pre><code>String – A literal string\nNumber – An integer or float\nList&lt;Number&gt; – An array of integers or floats\nCommaDelimitedList – An array of literal strings that are separated by commas\nAWS::EC2::KeyPair::KeyName – An Amazon EC2 key pair name\nAWS::EC2::SecurityGroup::Id – A security group ID\nAWS::EC2::Subnet::Id – A subnet ID\nAWS::EC2::VPC::Id – A VPC ID\nList&lt;AWS::EC2::VPC::Id&gt; – An array of VPC IDs\nList&lt;AWS::EC2::SecurityGroup::Id&gt; – An array of security group IDs\nList&lt;AWS::EC2::Subnet::Id&gt; – An array of subnet IDs\n</code></pre>"
      },
      {
        "answer": "<strong>DependentParameter</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "In CloudFormation, parameters are all independent and cannot depend on each other. Therefore, this is an invalid parameter type."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>String</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>CommaDelimitedList</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>AWS::EC2::KeyPair::KeyName</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As mentioned in the explanation above, these are valid parameter types."
      }
    ],
    "references": [
      "https://aws.amazon.com/cloudformation/",
      "https://aws.amazon.com/blogs/devops/using-the-new-cloudformation-parameter-types/"
    ]
  },
  {
    "id": 59,
    "question": "<p>A developer wants to package the code and dependencies for the application-specific Lambda functions as container images to be hosted on Amazon Elastic Container Registry (ECR).</p>\n\n<p>Which of the following options are correct for the given requirement? (Select two)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>You can test the containers locally using the Lambda Runtime API</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Lambda service does not support Lambda functions that use multi-architecture container images</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>You can deploy Lambda function as a container image, with a maximum size of 15 GB</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>To deploy a container image to Lambda, the container image must implement the Lambda Runtime API</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Lambda supports both Windows and Linux-based container images</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>To deploy a container image to Lambda, the container image must implement the Lambda Runtime API</strong> - To deploy a container image to Lambda, the container image must implement the Lambda Runtime API. The AWS open-source runtime interface clients implement the API. You can add a runtime interface client to your preferred base image to make it compatible with Lambda.</p>\n\n<p><strong>AWS Lambda service does not support Lambda functions that use multi-architecture container images</strong> - Lambda provides multi-architecture base images. However, the image you build for your function must target only one of the architectures. Lambda does not support functions that use multi-architecture container images.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Lambda supports both Windows and Linux-based container images</strong> - Lambda currently supports only Linux-based container images.</p>\n\n<p><strong>You can test the containers locally using the Lambda Runtime API</strong> - You can test the containers locally using the Lambda Runtime Interface Emulator.</p>\n\n<p><strong>You can deploy Lambda function as a container image, with a maximum size of 15 GB</strong> - You can deploy Lambda function as container image with the maximum size of 10GB.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/images-create.html\">https://docs.aws.amazon.com/lambda/latest/dg/images-create.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>To deploy a container image to Lambda, the container image must implement the Lambda Runtime API</strong> - To deploy a container image to Lambda, the container image must implement the Lambda Runtime API. The AWS open-source runtime interface clients implement the API. You can add a runtime interface client to your preferred base image to make it compatible with Lambda."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS Lambda service does not support Lambda functions that use multi-architecture container images</strong> - Lambda provides multi-architecture base images. However, the image you build for your function must target only one of the architectures. Lambda does not support functions that use multi-architecture container images."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Lambda supports both Windows and Linux-based container images</strong> - Lambda currently supports only Linux-based container images."
      },
      {
        "answer": "",
        "explanation": "<strong>You can test the containers locally using the Lambda Runtime API</strong> - You can test the containers locally using the Lambda Runtime Interface Emulator."
      },
      {
        "answer": "",
        "explanation": "<strong>You can deploy Lambda function as a container image, with a maximum size of 15 GB</strong> - You can deploy Lambda function as container image with the maximum size of 10GB."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/images-create.html"
    ]
  },
  {
    "id": 60,
    "question": "<p>A company wants to automate its order fulfillment and inventory tracking workflow. Starting from order creation to updating inventory to shipment, the entire process has to be tracked, managed and updated automatically.</p>\n\n<p>Which of the following would you recommend as the most optimal solution for this requirement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure Amazon EventBridge to track the flow of work from order management to inventory tracking systems</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Step Functions to coordinate and manage the components of order management and inventory tracking workflow</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Simple Queue Service (Amazon SQS) queue to pass information from order management to inventory tracking workflow</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon SNS to develop event-driven applications that can share information</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Step Functions to coordinate and manage the components of order management and inventory tracking workflow</strong></p>\n\n<p>AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that maintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by your business logic.</p>\n\n<p>AWS Step Functions enables you to implement a business process as a series of steps that make up a workflow. The individual steps in the workflow can invoke a Lambda function or a container that has some business logic, update a database such as DynamoDB or publish a message to a queue once that step or the entire workflow completes execution.</p>\n\n<p>Benefits of Step Functions:</p>\n\n<p>Build and update apps quickly: AWS Step Functions lets you build visual workflows that enable the fast translation of business requirements into technical requirements. You can build applications in a matter of minutes, and when needs change, you can swap or reorganize components without customizing any code.</p>\n\n<p>Improve resiliency: AWS Step Functions manages state, checkpoints and restarts for you to make sure that your application executes in order and as expected. Built-in try/catch, retry and rollback capabilities deal with errors and exceptions automatically.</p>\n\n<p>Write less code: AWS Step Functions manages the logic of your application for you and implements basic primitives such as branching, parallel execution, and timeouts. This removes extra code that may be repeated in your microservices and functions.</p>\n\n<p>How Step Functions work:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q19-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q19-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Simple Queue Service (Amazon SQS) queue to pass information from order management to inventory tracking workflow</strong> - You should consider AWS Step Functions when you need to coordinate service components in the development of highly scalable and auditable applications. You should consider using Amazon Simple Queue Service (Amazon SQS), when you need a reliable, highly scalable, hosted queue for sending, storing, and receiving messages between services. Step Functions keeps track of all tasks and events in an application. Amazon SQS requires you to implement your own application-level tracking, especially if your application uses multiple queues.</p>\n\n<p><strong>Configure Amazon EventBridge to track the flow of work from order management to inventory tracking systems</strong> - Both Amazon EventBridge and Amazon SNS can be used to develop event-driven applications, and your choice will depend on your specific needs. Amazon EventBridge is recommended when you want to build an application that reacts to events from SaaS applications and/or AWS services. Amazon EventBridge is the only event-based service that integrates directly with third-party SaaS partners.</p>\n\n<p><strong>Use Amazon SNS to develop event-driven applications that can share information</strong> - Amazon SNS is recommended when you want to build an application that reacts to high throughput or low latency messages published by other applications or microservices (as Amazon SNS provides nearly unlimited throughput), or for applications that need very high fan-out (thousands or millions of endpoints).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/faqs/\">https://aws.amazon.com/step-functions/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/eventbridge/faqs/\">https://aws.amazon.com/eventbridge/faqs/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use AWS Step Functions to coordinate and manage the components of order management and inventory tracking workflow</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that maintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by your business logic."
      },
      {
        "answer": "",
        "explanation": "AWS Step Functions enables you to implement a business process as a series of steps that make up a workflow. The individual steps in the workflow can invoke a Lambda function or a container that has some business logic, update a database such as DynamoDB or publish a message to a queue once that step or the entire workflow completes execution."
      },
      {
        "answer": "",
        "explanation": "Benefits of Step Functions:"
      },
      {
        "answer": "",
        "explanation": "Build and update apps quickly: AWS Step Functions lets you build visual workflows that enable the fast translation of business requirements into technical requirements. You can build applications in a matter of minutes, and when needs change, you can swap or reorganize components without customizing any code."
      },
      {
        "answer": "",
        "explanation": "Improve resiliency: AWS Step Functions manages state, checkpoints and restarts for you to make sure that your application executes in order and as expected. Built-in try/catch, retry and rollback capabilities deal with errors and exceptions automatically."
      },
      {
        "answer": "",
        "explanation": "Write less code: AWS Step Functions manages the logic of your application for you and implements basic primitives such as branching, parallel execution, and timeouts. This removes extra code that may be repeated in your microservices and functions."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q19-i1.jpg",
        "answer": "",
        "explanation": "How Step Functions work:"
      },
      {
        "link": "https://aws.amazon.com/step-functions/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use Amazon Simple Queue Service (Amazon SQS) queue to pass information from order management to inventory tracking workflow</strong> - You should consider AWS Step Functions when you need to coordinate service components in the development of highly scalable and auditable applications. You should consider using Amazon Simple Queue Service (Amazon SQS), when you need a reliable, highly scalable, hosted queue for sending, storing, and receiving messages between services. Step Functions keeps track of all tasks and events in an application. Amazon SQS requires you to implement your own application-level tracking, especially if your application uses multiple queues."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon EventBridge to track the flow of work from order management to inventory tracking systems</strong> - Both Amazon EventBridge and Amazon SNS can be used to develop event-driven applications, and your choice will depend on your specific needs. Amazon EventBridge is recommended when you want to build an application that reacts to events from SaaS applications and/or AWS services. Amazon EventBridge is the only event-based service that integrates directly with third-party SaaS partners."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Amazon SNS to develop event-driven applications that can share information</strong> - Amazon SNS is recommended when you want to build an application that reacts to high throughput or low latency messages published by other applications or microservices (as Amazon SNS provides nearly unlimited throughput), or for applications that need very high fan-out (thousands or millions of endpoints)."
      }
    ],
    "references": [
      "https://aws.amazon.com/step-functions/",
      "https://aws.amazon.com/step-functions/faqs/",
      "https://aws.amazon.com/eventbridge/faqs/"
    ]
  },
  {
    "id": 61,
    "question": "<p>As a senior architect, you are responsible for the development, support, maintenance, and implementation of all database applications written using NoSQL technology. A new project demands a throughput requirement of 10 strongly consistent reads per second of 6KB in size each.</p>\n\n<p>How many read capacity units will you need when configuring your DynamoDB table?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>20</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>60</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>10</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>30</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>Before proceeding with the calculations, please review the following:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q44-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q44-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q44-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q44-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div><p></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p><strong>20</strong></p>\n\n<p>One read capacity unit represents one strongly consistent read per second for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units.</p>\n\n<p>1) Item Size / 4KB, rounding to the nearest whole number.</p>\n\n<p>So, in the above case, 6KB / 4 KB = 1.5 or 2 read capacity units.</p>\n\n<p>2) 1 read capacity unit per item (since strongly consistent read) × No of reads per second</p>\n\n<p>So, in the above case, 2 x 10 = 20 read capacity units.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>60</strong></p>\n\n<p><strong>30</strong></p>\n\n<p><strong>10</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Before proceeding with the calculations, please review the following:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html",
        "answer": "",
        "explanation": "via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a>"
      },
      {
        "answer": "<strong>20</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "One read capacity unit represents one strongly consistent read per second for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units."
      },
      {
        "answer": "",
        "explanation": "1) Item Size / 4KB, rounding to the nearest whole number."
      },
      {
        "answer": "",
        "explanation": "So, in the above case, 6KB / 4 KB = 1.5 or 2 read capacity units."
      },
      {
        "answer": "",
        "explanation": "2) 1 read capacity unit per item (since strongly consistent read) × No of reads per second"
      },
      {
        "answer": "",
        "explanation": "So, in the above case, 2 x 10 = 20 read capacity units."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>60</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>30</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>10</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the details provided in the explanation above, so these are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html"
    ]
  },
  {
    "id": 62,
    "question": "<p>A developer working with EC2 Windows instance has installed Kinesis Agent for Windows to stream JSON-formatted log files to Amazon Simple Storage Service (S3) via Amazon Kinesis Data Firehose. The developer wants to understand the sink type capabilities of Kinesis Firehose.</p>\n\n<p>Which of the following sink types is NOT supported by Kinesis Firehose.</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon ElastiCache with Amazon S3 as backup</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon Simple Storage Service (Amazon S3) as a direct Firehose destination</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Redshift with Amazon S3</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Elasticsearch Service (Amazon ES) with optionally backing up data to Amazon S3</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon Elasticsearch Service (Amazon ES), and Splunk. With Kinesis Data Firehose, you don't need to write applications or manage resources. You configure your data producers to send data to Kinesis Data Firehose, and it automatically delivers the data to the destination that you specified.</p>\n\n<p><strong>Amazon ElastiCache with Amazon S3 as backup</strong> - Amazon ElastiCache is a fully managed in-memory data store, compatible with Redis or Memcached. ElastiCache is NOT a supported destination for Amazon Kinesis Data Firehose.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Elasticsearch Service (Amazon ES) with optionally backing up data to Amazon S3</strong> - Amazon ES is a supported destination type for Kinesis Firehose. Streaming data is delivered to your Amazon ES cluster, and can optionally be backed up to your S3 bucket concurrently.</p>\n\n<p>Data Flow for ES:\n<img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-es.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-es.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a><p></p>\n\n<p><strong>Amazon Simple Storage Service (Amazon S3) as a direct Firehose destination</strong> - For Amazon S3 destinations, streaming data is delivered to your S3 bucket. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket.</p>\n\n<p>Data Flow for S3:\n<img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-s3.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-s3.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a><p></p>\n\n<p><strong>Amazon Redshift with Amazon S3</strong> - For Amazon Redshift destinations, streaming data is delivered to your S3 bucket first. Kinesis Data Firehose then issues an Amazon Redshift COPY command to load data from your S3 bucket to your Amazon Redshift cluster. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket.</p>\n\n<p>Data Flow for Redshift:\n<img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-rs.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-rs.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a><p></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon Elasticsearch Service (Amazon ES), and Splunk. With Kinesis Data Firehose, you don't need to write applications or manage resources. You configure your data producers to send data to Kinesis Data Firehose, and it automatically delivers the data to the destination that you specified."
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon ElastiCache with Amazon S3 as backup</strong> - Amazon ElastiCache is a fully managed in-memory data store, compatible with Redis or Memcached. ElastiCache is NOT a supported destination for Amazon Kinesis Data Firehose."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Amazon Elasticsearch Service (Amazon ES) with optionally backing up data to Amazon S3</strong> - Amazon ES is a supported destination type for Kinesis Firehose. Streaming data is delivered to your Amazon ES cluster, and can optionally be backed up to your S3 bucket concurrently."
      },
      {
        "image": "https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-es.png",
        "answer": "",
        "explanation": "Data Flow for ES:"
      },
      {
        "link": "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Simple Storage Service (Amazon S3) as a direct Firehose destination</strong> - For Amazon S3 destinations, streaming data is delivered to your S3 bucket. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket."
      },
      {
        "image": "https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-s3.png",
        "answer": "",
        "explanation": "Data Flow for S3:"
      },
      {
        "link": "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"
      },
      {
        "answer": "",
        "explanation": "<strong>Amazon Redshift with Amazon S3</strong> - For Amazon Redshift destinations, streaming data is delivered to your S3 bucket first. Kinesis Data Firehose then issues an Amazon Redshift COPY command to load data from your S3 bucket to your Amazon Redshift cluster. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket."
      },
      {
        "image": "https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-rs.png",
        "answer": "",
        "explanation": "Data Flow for Redshift:"
      },
      {
        "link": "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"
    ]
  },
  {
    "id": 63,
    "question": "<p>A serverless application built on AWS processes customer orders 24/7 using an AWS Lambda function and communicates with an external vendor's HTTP API for payment processing. The development team wants to notify the support team in near real-time using an existing Amazon Simple Notification Service (Amazon SNS) topic, but only when the external API error rate exceeds 5% of the total transactions processed in an hour.</p>\n\n<p>As an AWS Certified Developer Associate, which option will you suggest as the most efficient solution?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Logs Insights to query the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Logs Insights on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure CloudWatch metrics with detailed monitoring for the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Metric Filter to look at the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Metric Filter on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure and push high-resolution custom metrics to CloudWatch that record the failures of the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure and push high-resolution custom metrics to CloudWatch that record the failures of the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate</strong></p>\n\n<p>You can publish your own metrics, known as custom metrics, to CloudWatch using the AWS CLI or an API.</p>\n\n<p>Each metric is one of the following:</p>\n\n<p>Standard resolution, with data having a one-minute granularity</p>\n\n<p>High resolution, with data at a granularity of one second</p>\n\n<p>Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p>\n\n<p>High-resolution metrics can give you more immediate insight into your application's sub-minute activity. Keep in mind that every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a high-resolution metric can lead to higher charges.</p>\n\n<p>You can create metric and composite alarms in Amazon CloudWatch. For the given use case, you can set up a CloudWatch metric alarm that watches the custom metric that captures the API errors and then triggers the alarm when the API error rate exceeds the 5% threshold. The alarm then sends a notification via the existing SNS topic.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure CloudWatch metrics with detailed monitoring for the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate</strong> - CloudWatch provides two categories of monitoring: basic monitoring and detailed monitoring. Detailed monitoring options differ based on the services that offer it. For example, Amazon EC2 detailed monitoring provides more frequent metrics, published at one-minute intervals, instead of the five-minute intervals used in Amazon EC2 basic monitoring. Detailed monitoring is offered by only some services. As explained above, you need to use custom metrics to capture data for the external payment processing API calls since detailed monitoring for the standard CloudWatch metrics cannot be used for this scenario.</p>\n\n<p><strong>Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Logs Insights to query the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Logs Insights on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate</strong> - CloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you more efficiently and effectively respond to operational issues. This option is not the right fit for the given use case since Lambda cannot monitor the output of the CloudWatch Logs Insights on a real-time basis since it is being invoked on a schedule. Also, it is not an efficient solution since Lambda will need significant custom code to parse and compute the external API error rate from the CloudWatch Logs Insights data.</p>\n\n<p><strong>Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Metric Filter to look at the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Metric Filter on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate</strong> - You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. This option is not the best fit for the given use case since Lambda cannot monitor the output of the CloudWatch Metric Filter on a real-time basis since it is being invoked on a schedule. Also, it is not an efficient solution since Lambda will need significant custom code to parse and compute the external API error rate from the CloudWatch Metric Filter data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-push-custom-metrics/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-push-custom-metrics/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Configure and push high-resolution custom metrics to CloudWatch that record the failures of the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can publish your own metrics, known as custom metrics, to CloudWatch using the AWS CLI or an API."
      },
      {
        "answer": "",
        "explanation": "Each metric is one of the following:"
      },
      {
        "answer": "",
        "explanation": "Standard resolution, with data having a one-minute granularity"
      },
      {
        "answer": "",
        "explanation": "High resolution, with data at a granularity of one second"
      },
      {
        "answer": "",
        "explanation": "Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds."
      },
      {
        "answer": "",
        "explanation": "High-resolution metrics can give you more immediate insight into your application's sub-minute activity. Keep in mind that every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a high-resolution metric can lead to higher charges."
      },
      {
        "answer": "",
        "explanation": "You can create metric and composite alarms in Amazon CloudWatch. For the given use case, you can set up a CloudWatch metric alarm that watches the custom metric that captures the API errors and then triggers the alarm when the API error rate exceeds the 5% threshold. The alarm then sends a notification via the existing SNS topic."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure CloudWatch metrics with detailed monitoring for the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate</strong> - CloudWatch provides two categories of monitoring: basic monitoring and detailed monitoring. Detailed monitoring options differ based on the services that offer it. For example, Amazon EC2 detailed monitoring provides more frequent metrics, published at one-minute intervals, instead of the five-minute intervals used in Amazon EC2 basic monitoring. Detailed monitoring is offered by only some services. As explained above, you need to use custom metrics to capture data for the external payment processing API calls since detailed monitoring for the standard CloudWatch metrics cannot be used for this scenario."
      },
      {
        "answer": "",
        "explanation": "<strong>Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Logs Insights to query the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Logs Insights on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate</strong> - CloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you more efficiently and effectively respond to operational issues. This option is not the right fit for the given use case since Lambda cannot monitor the output of the CloudWatch Logs Insights on a real-time basis since it is being invoked on a schedule. Also, it is not an efficient solution since Lambda will need significant custom code to parse and compute the external API error rate from the CloudWatch Logs Insights data."
      },
      {
        "answer": "",
        "explanation": "<strong>Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Metric Filter to look at the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Metric Filter on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate</strong> - You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. This option is not the best fit for the given use case since Lambda cannot monitor the output of the CloudWatch Metric Filter on a real-time basis since it is being invoked on a schedule. Also, it is not an efficient solution since Lambda will need significant custom code to parse and compute the external API error rate from the CloudWatch Metric Filter data."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html",
      "https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-push-custom-metrics/",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html"
    ]
  },
  {
    "id": 64,
    "question": "<p>An application running on EC2 instances processes messages from an SQS queue. However, sometimes the messages are not processed and they end up in errors.  These messages need to be isolated for further processing and troubleshooting.</p>\n\n<p>Which of the following options will help achieve this?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Increase the VisibilityTimeout</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use DeleteMessage</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Implement a Dead-Letter Queue</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Reduce the VisibilityTimeout</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement a Dead-Letter Queue</strong> - Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.\nAmazon SQS does not create the dead-letter queue automatically. You must first create the queue before using it as a dead-letter queue.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q59-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q59-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the VisibilityTimeout</strong> - When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Immediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. Increasing visibility timeout will not help in troubleshooting the messages running into error or isolating them from the rest. Hence this is an incorrect option for the current use case.</p>\n\n<p><strong>Use DeleteMessage</strong> - Deletes the specified message from the specified queue. This will not help understand the reason for error or isolate messages ending with the error.</p>\n\n<p><strong>Reduce the VisibilityTimeout</strong> - As explained above, VisibilityTimeout makes sure that the message is not read by any other consumer while it is being processed by one consumer. By reducing the VisibilityTimeout, more consumers will receive the same failed message. Hence, this is an incorrect option for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Implement a Dead-Letter Queue</strong> - Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.\nAmazon SQS does not create the dead-letter queue automatically. You must first create the queue before using it as a dead-letter queue."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Increase the VisibilityTimeout</strong> - When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Immediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. Increasing visibility timeout will not help in troubleshooting the messages running into error or isolating them from the rest. Hence this is an incorrect option for the current use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use DeleteMessage</strong> - Deletes the specified message from the specified queue. This will not help understand the reason for error or isolate messages ending with the error."
      },
      {
        "answer": "",
        "explanation": "<strong>Reduce the VisibilityTimeout</strong> - As explained above, VisibilityTimeout makes sure that the message is not read by any other consumer while it is being processed by one consumer. By reducing the VisibilityTimeout, more consumers will receive the same failed message. Hence, this is an incorrect option for this use case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html"
    ]
  },
  {
    "id": 65,
    "question": "<p>A media publishing company is using Amazon EC2 instances for running their business-critical applications. Their IT team is looking at reserving capacity apart from savings plans for the critical instances.</p>\n\n<p>As a Developer Associate, which of the following reserved instance types you would select to provide capacity reservations?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Regional Reserved Instances</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Neither Regional Reserved Instances nor Zonal Reserved Instances</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Zonal Reserved Instances</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Both Regional Reserved Instances and Zonal Reserved Instances</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>When you purchase a Reserved Instance for a specific Availability Zone, it's referred to as a Zonal Reserved Instance. Zonal Reserved Instances provide capacity reservations as well as discounts.</p>\n\n<p><strong>Zonal Reserved Instances</strong> - A zonal Reserved Instance provides a capacity reservation in the specified Availability Zone. Capacity Reservations enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration. This gives you the ability to create and manage Capacity Reservations independently from the billing discounts offered by Savings Plans or regional Reserved Instances.</p>\n\n<p>Regional and Zonal Reserved Instances:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q31-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q31-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html</a><p></p>\n\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q31-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q31-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Regional Reserved Instances</strong> - When you purchase a Reserved Instance for a Region, it's referred to as a regional Reserved Instance. A regional Reserved Instance does not provide a capacity reservation.</p>\n\n<p><strong>Both Regional Reserved Instances and Zonal Reserved Instances</strong> - As discussed above, only Zonal Reserved Instances provide capacity reservation.</p>\n\n<p><strong>Neither Regional Reserved Instances nor Zonal Reserved Instances</strong> - As discussed above, Zonal Reserved Instances provide capacity reservation.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "When you purchase a Reserved Instance for a specific Availability Zone, it's referred to as a Zonal Reserved Instance. Zonal Reserved Instances provide capacity reservations as well as discounts."
      },
      {
        "answer": "",
        "explanation": "<strong>Zonal Reserved Instances</strong> - A zonal Reserved Instance provides a capacity reservation in the specified Availability Zone. Capacity Reservations enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration. This gives you the ability to create and manage Capacity Reservations independently from the billing discounts offered by Savings Plans or regional Reserved Instances."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q31-i1.jpg",
        "answer": "",
        "explanation": "Regional and Zonal Reserved Instances:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q31-i2.jpg",
        "answer": "",
        "explanation": "High Level Overview of EC2 Instance Purchase Options:"
      },
      {
        "link": "https://aws.amazon.com/ec2/pricing/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Regional Reserved Instances</strong> - When you purchase a Reserved Instance for a Region, it's referred to as a regional Reserved Instance. A regional Reserved Instance does not provide a capacity reservation."
      },
      {
        "answer": "",
        "explanation": "<strong>Both Regional Reserved Instances and Zonal Reserved Instances</strong> - As discussed above, only Zonal Reserved Instances provide capacity reservation."
      },
      {
        "answer": "",
        "explanation": "<strong>Neither Regional Reserved Instances nor Zonal Reserved Instances</strong> - As discussed above, Zonal Reserved Instances provide capacity reservation."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html",
      "https://aws.amazon.com/ec2/pricing/",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html"
    ]
  }
]