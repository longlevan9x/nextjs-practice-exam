[
  {
    "id": 1,
    "question": "<p>A company has hosted its restaurant review website on an Amazon EC2 instance. The website supports multiple languages and the preferred language is added as a query string parameter to the request. The directory structure and file names for all versions of the website are identical. The website responds with the chosen language's webpage when accessed directly. However, when the same webpage is accessed through the configured CloudFront distribution, it defaults to a single language.</p>\n\n<p>How will you fix this issue?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a new cache policy for the CloudFront distribution and set the cache behavior to <code>Query string forwarding and caching</code>. In the Query string whitelist field include the language string. Update the CloudFront distribution to use the new cache policy</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a new cache policy for the CloudFront distribution and set the cache behavior to <code>None</code> to improve caching performance. Update the CloudFront distribution to use the new cache policy</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a new cache policy for the CloudFront distribution and set the cache behavior to <code>Cache based on selected request headers</code>. Use <code>Whitelist Headers</code> as the caching criteria</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Choose the <code>Customize</code> option for the <code>Object Caching</code> setting and reduce the <code>Default TTL</code> value so that CloudFront forwards requests to your origin more frequently</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a new cache policy for the CloudFront distribution and set the cache behavior to Query string forwarding and caching. In the Query string whitelist field include the language string. Update the CloudFront distribution to use the new cache policy</strong></p>\n\n<p>CloudFront can cache different versions of your content based on the values of query string parameters. Forward all, cache based on whitelist option should be chosen if your origin server returns different versions of your objects based on one or more query string parameters. Then specify the parameters that you want CloudFront to use as a basis for caching in the Query string whitelist field.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new cache policy for the CloudFront distribution and set the cache behavior to None to improve caching performance. Update the CloudFront distribution to use the new cache policy</strong> - None improves Caching. Choose this option if your origin returns the same version of an object regardless of the values of query string parameters. This increases the likelihood that CloudFront can serve a request from the cache, which improves performance and reduces the load on your origin.</p>\n\n<p><strong>Create a new cache policy for the CloudFront distribution and set the cache behavior to <code>Cache based on selected request headers</code>. Use <code>Whitelist Headers</code> as the caching criteria</strong> - <code>Cache based on selected request headers</code> is not a valid option since the use case mentions using query string parameters.</p>\n\n<p><strong>Choose the <code>Customize</code> option for the <code>Object Caching</code> setting and reduce the <code>Default TTL</code> value so that CloudFront forwards requests to your origin more frequently</strong> - Default TTL specifies the default amount of time, in seconds, that you want objects to stay in CloudFront caches before CloudFront forwards another request to your origin to determine whether the object has been updated. This option is irrelevant for the current use case since the response returned defaulting to the same language is not a TTL issue.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesQueryString\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesQueryString</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesQueryStringWhiteList\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesQueryStringWhiteList</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Create a new cache policy for the CloudFront distribution and set the cache behavior to Query string forwarding and caching. In the Query string whitelist field include the language string. Update the CloudFront distribution to use the new cache policy</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "CloudFront can cache different versions of your content based on the values of query string parameters. Forward all, cache based on whitelist option should be chosen if your origin server returns different versions of your objects based on one or more query string parameters. Then specify the parameters that you want CloudFront to use as a basis for caching in the Query string whitelist field."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a new cache policy for the CloudFront distribution and set the cache behavior to None to improve caching performance. Update the CloudFront distribution to use the new cache policy</strong> - None improves Caching. Choose this option if your origin returns the same version of an object regardless of the values of query string parameters. This increases the likelihood that CloudFront can serve a request from the cache, which improves performance and reduces the load on your origin."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a new cache policy for the CloudFront distribution and set the cache behavior to <code>Cache based on selected request headers</code>. Use <code>Whitelist Headers</code> as the caching criteria</strong> - <code>Cache based on selected request headers</code> is not a valid option since the use case mentions using query string parameters."
      },
      {
        "answer": "",
        "explanation": "<strong>Choose the <code>Customize</code> option for the <code>Object Caching</code> setting and reduce the <code>Default TTL</code> value so that CloudFront forwards requests to your origin more frequently</strong> - Default TTL specifies the default amount of time, in seconds, that you want objects to stay in CloudFront caches before CloudFront forwards another request to your origin to determine whether the object has been updated. This option is irrelevant for the current use case since the response returned defaulting to the same language is not a TTL issue."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesQueryString",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesQueryStringWhiteList"
    ]
  },
  {
    "id": 2,
    "question": "<p>A development team has a mix of applications hosted on-premises as well as on EC2 instances. The on-premises application controls all applications deployed on the EC2 instances. In case of any errors, the team wants to leverage Amazon CloudWatch to monitor and troubleshoot the on-premises application.</p>\n\n<p>As a Developer Associate, which of the following solutions would you suggest to address this use-case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Upload log files from the on-premises server to S3 and let CloudWatch process the files from S3</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure the CloudWatch agent on the on-premises server by using IAM user credentials with permissions for CloudWatch</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure CloudWatch Logs to directly read the logs from the on-premises server</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Upload log files from the on-premises server to an EC2 instance which further forwards the logs to CloudWatch</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the CloudWatch agent on the on-premises server by using IAM user credentials with permissions for CloudWatch</strong></p>\n\n<p>The CloudWatch agent enables you to do the following:</p>\n\n<p>Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.</p>\n\n<p>Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p>\n\n<p>To enable the CloudWatch agent to send data from an on-premises server, you must specify the access key and secret key of the IAM user that you created earlier.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q5-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q5-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure CloudWatch Logs to directly read the logs from the on-premises server</strong> - This is a made-up option as you cannot have CloudWatch Logs directly communicate with the on-premises server. You have to go via the CloudWatch Agent.</p>\n\n<p><strong>Upload log files from the on-premises server to an EC2 instance which further forwards the logs to CloudWatch</strong></p>\n\n<p><strong>Upload log files from the on-premises server to S3 and let CloudWatch process the files from S3</strong></p>\n\n<p>Both these options require significant customizations and still will not be as neatly integrated with CloudWatch as compared to just using the CloudWatch Agent which is available off-the-shelf.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Configure the CloudWatch agent on the on-premises server by using IAM user credentials with permissions for CloudWatch</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The CloudWatch agent enables you to do the following:"
      },
      {
        "answer": "",
        "explanation": "Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS."
      },
      {
        "answer": "",
        "explanation": "Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server."
      },
      {
        "answer": "",
        "explanation": "To enable the CloudWatch agent to send data from an on-premises server, you must specify the access key and secret key of the IAM user that you created earlier."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure CloudWatch Logs to directly read the logs from the on-premises server</strong> - This is a made-up option as you cannot have CloudWatch Logs directly communicate with the on-premises server. You have to go via the CloudWatch Agent."
      },
      {
        "answer": "<strong>Upload log files from the on-premises server to an EC2 instance which further forwards the logs to CloudWatch</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Upload log files from the on-premises server to S3 and let CloudWatch process the files from S3</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Both these options require significant customizations and still will not be as neatly integrated with CloudWatch as compared to just using the CloudWatch Agent which is available off-the-shelf."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html"
    ]
  },
  {
    "id": 3,
    "question": "<p>An IT company leverages CodePipeline to automate its release pipelines. The development team wants to write a Lambda function that will send notifications for state changes within the pipeline.</p>\n\n<p>As a Developer Associate, which steps would you suggest to associate the Lambda function with the event source?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an Amazon CloudWatch alarm that monitors status changes in Code Pipeline and triggers the Lambda function</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an Amazon CloudWatch Events rule that uses CodePipeline as an event source with the target as the Lambda function</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use the Lambda console to configure a trigger that invokes the Lambda function with CodePipeline as the event source</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the CodePipeline console to set up a trigger for the Lambda function</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams.</p>\n\n<p>CloudWatch Events Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q3-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q3-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a><p></p>\n\n<p><strong>Set up an Amazon CloudWatch Events rule that uses CodePipeline as an event source with the target as the Lambda function</strong></p>\n\n<p>You can use Amazon CloudWatch Events to detect and react to changes in the state of a pipeline, stage, or action. Then, based on rules you create, CloudWatch Events invokes one or more target actions when a pipeline, stage, or action enters the state you specify in a rule. For the given use-case, you can set up a rule that detects pipeline changes and invokes an AWS Lambda function.</p>\n\n<p>Amazon CloudWatch Events With CodePipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q3-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q3-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\n<a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that monitors status changes in Code Pipeline and triggers the Lambda function</strong> - As mentioned in the explanation above, you need to use a CloudWatch event and not CloudWatch alarm for this use-case.</p>\n\n<p><strong>Use the Lambda console to configure a trigger that invokes the Lambda function with CodePipeline as the event source</strong> - You cannot create a trigger with CodePipeline as the event source via the Lambda Console.</p>\n\n<p><strong>Use the CodePipeline console to set up a trigger for the Lambda function</strong> - CodePipeline console cannot be used to configure a trigger for a Lambda function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q3-i1.jpg",
        "answer": "",
        "explanation": "CloudWatch Events Key Concepts:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html"
      },
      {
        "answer": "<strong>Set up an Amazon CloudWatch Events rule that uses CodePipeline as an event source with the target as the Lambda function</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can use Amazon CloudWatch Events to detect and react to changes in the state of a pipeline, stage, or action. Then, based on rules you create, CloudWatch Events invokes one or more target actions when a pipeline, stage, or action enters the state you specify in a rule. For the given use-case, you can set up a rule that detects pipeline changes and invokes an AWS Lambda function."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q3-i2.jpg",
        "answer": "",
        "explanation": "Amazon CloudWatch Events With CodePipeline:"
      },
      {
        "link": "https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Set up an Amazon CloudWatch alarm that monitors status changes in Code Pipeline and triggers the Lambda function</strong> - As mentioned in the explanation above, you need to use a CloudWatch event and not CloudWatch alarm for this use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the Lambda console to configure a trigger that invokes the Lambda function with CodePipeline as the event source</strong> - You cannot create a trigger with CodePipeline as the event source via the Lambda Console."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the CodePipeline console to set up a trigger for the Lambda function</strong> - CodePipeline console cannot be used to configure a trigger for a Lambda function."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html",
      "https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html"
    ]
  },
  {
    "id": 4,
    "question": "<p>An IT company uses AWS CloudFormation templates to provision their AWS infrastructure for Amazon EC2, Amazon VPC, and Amazon S3 resources. Using cross-stack referencing, a developer creates a stack called <code>NetworkStack</code> which will export the <code>subnetId</code> that can be used when creating EC2 instances in another stack.</p>\n\n<p>To use the exported value in another stack, which of the following functions must be used?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p><code>!Ref</code></p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p><code>!GetAtt</code></p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p><code>!Sub</code></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p><code>!ImportValue</code></p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong><code>!ImportValue</code></strong></p>\n\n<p>The intrinsic function <code>Fn::ImportValue</code> returns the value of an output exported by another stack. You typically use this function to create cross-stack references.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>!Ref</code></strong> - Returns the value of the specified parameter or resource.</p>\n\n<p><strong><code>!GetAtt</code></strong> - Returns the value of an attribute from a resource in the template.</p>\n\n<p><strong><code>!Sub</code></strong> - Substitutes variables in an input string with values that you specify.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong><code>!ImportValue</code></strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The intrinsic function <code>Fn::ImportValue</code> returns the value of an output exported by another stack. You typically use this function to create cross-stack references."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong><code>!Ref</code></strong> - Returns the value of the specified parameter or resource."
      },
      {
        "answer": "",
        "explanation": "<strong><code>!GetAtt</code></strong> - Returns the value of an attribute from a resource in the template."
      },
      {
        "answer": "",
        "explanation": "<strong><code>!Sub</code></strong> - Substitutes variables in an input string with values that you specify."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html"
    ]
  },
  {
    "id": 5,
    "question": "<p>Your Lambda function must use the Node.js drivers to connect to your RDS PostgreSQL database in your VPC.</p>\n\n<p>How do you bundle your Lambda function to add the dependencies?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Zip the function and the dependencies separately and upload them in AWS Lambda as two parts</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Zip the function as-is with a package.json file so that AWS Lambda can resolve the dependencies for you</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Put the function and the dependencies in one folder and zip them together</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Upload the code through the AWS console and upload the dependencies as a zip</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a><p></p>\n\n<p><strong>Put the function and the dependencies in one folder and zip them together</strong></p>\n\n<p>A deployment package is a ZIP archive that contains your function code and dependencies. You need to create a deployment package if you use the Lambda API to manage functions, or if you need to include libraries and dependencies other than the AWS SDK. You can upload the package directly to Lambda, or you can use an Amazon S3 bucket, and then upload it to Lambda. If the deployment package is larger than 50 MB, you must use Amazon S3. This is the standard way of packaging Lambda functions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Zip the function as-is with a package.json file so that AWS Lambda can resolve the dependencies for you</strong></p>\n\n<p><strong>Upload the code through the AWS console and upload the dependencies as a zip</strong></p>\n\n<p><strong>Zip the function and the dependencies separately and upload them in AWS Lambda as two parts</strong></p>\n\n<p>These three options are incorrect as there's only one way of deploying a Lambda function, which is to provide the zip file with all dependencies that it'll need.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html\">https://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume."
      },
      {
        "image": "https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png",
        "answer": "",
        "explanation": "How Lambda function works:"
      },
      {
        "link": "https://aws.amazon.com/lambda/"
      },
      {
        "answer": "<strong>Put the function and the dependencies in one folder and zip them together</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A deployment package is a ZIP archive that contains your function code and dependencies. You need to create a deployment package if you use the Lambda API to manage functions, or if you need to include libraries and dependencies other than the AWS SDK. You can upload the package directly to Lambda, or you can use an Amazon S3 bucket, and then upload it to Lambda. If the deployment package is larger than 50 MB, you must use Amazon S3. This is the standard way of packaging Lambda functions."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Zip the function as-is with a package.json file so that AWS Lambda can resolve the dependencies for you</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Upload the code through the AWS console and upload the dependencies as a zip</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Zip the function and the dependencies separately and upload them in AWS Lambda as two parts</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options are incorrect as there's only one way of deploying a Lambda function, which is to provide the zip file with all dependencies that it'll need."
      }
    ],
    "references": [
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html"
    ]
  },
  {
    "id": 6,
    "question": "<p>Your company is shifting towards Elastic Container Service (ECS) to deploy applications. The process should be automated using the AWS CLI to create a service where at least ten instances of a task definition are kept running under the default cluster.</p>\n\n<p>Which of the following commands should be executed?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p><code>aws ecs run-task --cluster default --task-definition ecs-demo</code></p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p><code>aws ecs create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p><code>docker-compose create ecs-simple-service</code></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p><code>aws ecr create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong><code>aws ecs create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></strong></p>\n\n<p>To create a new service you would use this command which creates a service in your default region called ecs-simple-service. The service uses the ecs-demo task definition and it maintains 10 instantiations of that task.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>aws ecr create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></strong> - This command is referencing a different service called Amazon Elastic Container Registry (ECR) which's is a fully-managed Docker container registry</p>\n\n<p><strong><code>docker-compose create ecs-simple-service</code></strong> - This is a docker command to create containers for a service.</p>\n\n<p><strong><code>aws ecs run-task --cluster default --task-definition ecs-demo</code></strong> - This is a valid command but used for starting a new task using a specified task definition.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/ecs/create-service.html\">https://docs.aws.amazon.com/cli/latest/reference/ecs/create-service.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong><code>aws ecs create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "To create a new service you would use this command which creates a service in your default region called ecs-simple-service. The service uses the ecs-demo task definition and it maintains 10 instantiations of that task."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong><code>aws ecr create-service --service-name ecs-simple-service --task-definition ecs-demo --desired-count 10</code></strong> - This command is referencing a different service called Amazon Elastic Container Registry (ECR) which's is a fully-managed Docker container registry"
      },
      {
        "answer": "",
        "explanation": "<strong><code>docker-compose create ecs-simple-service</code></strong> - This is a docker command to create containers for a service."
      },
      {
        "answer": "",
        "explanation": "<strong><code>aws ecs run-task --cluster default --task-definition ecs-demo</code></strong> - This is a valid command but used for starting a new task using a specified task definition."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/cli/latest/reference/ecs/create-service.html"
    ]
  },
  {
    "id": 7,
    "question": "<p>An e-commerce company has deployed its application on AWS Elastic Beanstalk. The Auto Scaling group associated with the Beanstalk environment has three Amazon EC2 instances. When the number of instances falls below two, it severely impacts the performance of the web application. The company currently uses the default all-at-once deployment policy and is looking for an effective strategy for future deployments.</p>\n\n<p>Which of the following represents the most cost-effective deployment strategy for the company?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Opt for rolling with additional batch deployment strategy. Set the batch size parameter to 1</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure an Elastic Load Balancer to front the Auto Scaling Group and choose two different Availability Zones (AZs) for deployment</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Opt for rolling deployment strategy. Set the batch size to 2</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Opt for traffic-splitting deployment strategy with traffic split parameter set to 50% of the total traffic</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Opt for rolling with additional batch deployment strategy. Set the batch size parameter to 1</strong></p>\n\n<p>With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. It leaves the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version.</p>\n\n<p>To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.</p>\n\n<p>For the rolling and rolling with additional batch deployment policies, you can configure Batch size – The size of the set of instances to deploy in each batch. Choose Percentage to configure a percentage of the total number of EC2 instances in the Auto Scaling group (up to 100 percent), or choose Fixed to configure a fixed number of instances (up to the maximum instance count in your environment's Auto Scaling configuration).</p>\n\n<p>Example configuration for rolling With additional batch:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q39-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q39-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Opt for a traffic-splitting deployment strategy with a traffic split parameter set to 50% of the total traffic</strong> - Traffic-splitting deployments let you perform canary testing as part of your application deployment. In a traffic-splitting deployment, Elastic Beanstalk launches a full set of new instances just like during an immutable deployment. It then forwards a specified percentage of incoming client traffic to the new application version for a specified evaluation period. This turns out to be a costly solution for the given use case.</p>\n\n<p><strong>Configure an Elastic Load Balancer to front the Auto Scaling Group and choose two different Availability Zones (AZs) for deployment</strong> - Adding AZs makes the entire configuration resilient for failures post-deployment. However, it does not represent a valid deployment strategy for the given use case. This option has been added as a distractor.</p>\n\n<p><strong>Opt for a rolling deployment strategy. Set the batch size to 2</strong> - As already discussed, with rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. With a batch size set to 2, only 1 instance will be left for traffic during deployment, impacting the performance of the e-commerce application. Hence, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Opt for rolling with additional batch deployment strategy. Set the batch size parameter to 1</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. It leaves the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version."
      },
      {
        "answer": "",
        "explanation": "To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances."
      },
      {
        "answer": "",
        "explanation": "For the rolling and rolling with additional batch deployment policies, you can configure Batch size – The size of the set of instances to deploy in each batch. Choose Percentage to configure a percentage of the total number of EC2 instances in the Auto Scaling group (up to 100 percent), or choose Fixed to configure a fixed number of instances (up to the maximum instance count in your environment's Auto Scaling configuration)."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q39-i1.jpg",
        "answer": "",
        "explanation": "Example configuration for rolling With additional batch:"
      },
      {
        "link": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Opt for a traffic-splitting deployment strategy with a traffic split parameter set to 50% of the total traffic</strong> - Traffic-splitting deployments let you perform canary testing as part of your application deployment. In a traffic-splitting deployment, Elastic Beanstalk launches a full set of new instances just like during an immutable deployment. It then forwards a specified percentage of incoming client traffic to the new application version for a specified evaluation period. This turns out to be a costly solution for the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure an Elastic Load Balancer to front the Auto Scaling Group and choose two different Availability Zones (AZs) for deployment</strong> - Adding AZs makes the entire configuration resilient for failures post-deployment. However, it does not represent a valid deployment strategy for the given use case. This option has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>Opt for a rolling deployment strategy. Set the batch size to 2</strong> - As already discussed, with rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. With a batch size set to 2, only 1 instance will be left for traffic during deployment, impacting the performance of the e-commerce application. Hence, this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html"
    ]
  },
  {
    "id": 8,
    "question": "<p>A popular mobile app retrieves data from an AWS DynamoDB table that was provisioned with read-capacity units (RCU’s) that are evenly shared across four partitions. One of those partitions is receiving more traffic than the other partitions, causing hot partition issues.</p>\n\n<p>What technology will allow you to reduce the read traffic on your AWS DynamoDB table with minimal effort?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>DynamoDB DAX</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>More partitions</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>DynamoDB Streams</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>ElastiCache</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>DynamoDB DAX</strong></p>\n\n<p>Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement: from milliseconds to microseconds: even at millions of requests per second.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DynamoDB Streams</strong> - A stream record contains information about a data modification to a single item in a DynamoDB table. This is not the correct option for the given use-case.</p>\n\n<p><strong>ElastiCache</strong> - ElastiCache can cache the results from anything but you will need to modify your code to check the cache before querying the main query store. As the given use-case mandates minimal effort, so this option is not correct.</p>\n\n<p><strong>More partitions</strong> - This option has been added as a distractor as DynamoDB handles that for you automatically.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>DynamoDB DAX</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement: from milliseconds to microseconds: even at millions of requests per second."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>DynamoDB Streams</strong> - A stream record contains information about a data modification to a single item in a DynamoDB table. This is not the correct option for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>ElastiCache</strong> - ElastiCache can cache the results from anything but you will need to modify your code to check the cache before querying the main query store. As the given use-case mandates minimal effort, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>More partitions</strong> - This option has been added as a distractor as DynamoDB handles that for you automatically."
      }
    ],
    "references": [
      "https://aws.amazon.com/dynamodb/dax/"
    ]
  },
  {
    "id": 9,
    "question": "<p>A data analytics company ingests a large number of messages and stores them in an RDS database using Lambda. Because of the increased payload size, it is taking more than 15 minutes to process each message.</p>\n\n<p>As a Developer Associate, which of the following options would you recommend to process each message in the MOST scalable way?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Provision an EC2 instance to poll the messages from an SQS queue</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Provision EC2 instances in an Auto Scaling group to poll the messages from an SQS queue</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use DynamoDB instead of RDS as database</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Contact AWS Support to increase the Lambda timeout to 60 minutes</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Provision EC2 instances in an Auto Scaling group to poll the messages from an SQS queue</strong></p>\n\n<p>As each message takes more than 15 minutes to process, therefore the development team cannot use Lambda for message processing. To build a scalable solution, the EC2 instances must be provisioning via an Auto Scaling group to handle variations in the message processing workload.</p>\n\n<p>Amazon EC2 Auto Scaling Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q4-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q4-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provision an EC2 instance to poll the messages from an SQS queue</strong> - Just using a single EC2 instance may not be sufficient to handle a sudden spike in the number of incoming messages.</p>\n\n<p><strong>Contact AWS Support to increase the Lambda timeout to 60 minutes</strong> - AWS Support cannot increase the Lambda timeout upper limit.</p>\n\n<p><strong>Use DynamoDB instead of RDS as database</strong> - This option has been added as a distractor, as changing the database would have no impact on the Lambda timeout while processing the message.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Provision EC2 instances in an Auto Scaling group to poll the messages from an SQS queue</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As each message takes more than 15 minutes to process, therefore the development team cannot use Lambda for message processing. To build a scalable solution, the EC2 instances must be provisioning via an Auto Scaling group to handle variations in the message processing workload."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q4-i1.jpg",
        "answer": "",
        "explanation": "Amazon EC2 Auto Scaling Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Provision an EC2 instance to poll the messages from an SQS queue</strong> - Just using a single EC2 instance may not be sufficient to handle a sudden spike in the number of incoming messages."
      },
      {
        "answer": "",
        "explanation": "<strong>Contact AWS Support to increase the Lambda timeout to 60 minutes</strong> - AWS Support cannot increase the Lambda timeout upper limit."
      },
      {
        "answer": "",
        "explanation": "<strong>Use DynamoDB instead of RDS as database</strong> - This option has been added as a distractor, as changing the database would have no impact on the Lambda timeout while processing the message."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html"
    ]
  },
  {
    "id": 10,
    "question": "<p>A company has recently launched a new gaming application that the users are adopting rapidly. The company uses RDS MySQL as the database. The development team wants an urgent solution to this issue where the rapidly increasing workload might exceed the available database storage.</p>\n\n<p>As a developer associate, which of the following solutions would you recommend so that it requires minimum development effort to address this requirement?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Migrate RDS MySQL database to DynamoDB which automatically allocates storage space when required</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create read replica for RDS MySQL</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Migrate RDS MySQL database to Aurora which offers storage auto-scaling</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable storage auto-scaling for RDS MySQL</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable storage auto-scaling for RDS MySQL</strong></p>\n\n<p>If your workload is unpredictable, you can enable storage autoscaling for an Amazon RDS DB instance. With storage autoscaling enabled, when Amazon RDS detects that you are running out of free database space it automatically scales up your storage. Amazon RDS starts a storage modification for an autoscaling-enabled DB instance when these factors apply:</p>\n\n<p>Free available space is less than 10 percent of the allocated storage.</p>\n\n<p>The low-storage condition lasts at least five minutes.</p>\n\n<p>At least six hours have passed since the last storage modification.</p>\n\n<p>The maximum storage threshold is the limit that you set for autoscaling the DB instance. You can't set the maximum storage threshold for autoscaling-enabled instances to a value greater than the maximum allocated storage.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate RDS MySQL to Aurora which offers storage auto-scaling</strong> - Although Aurora offers automatic storage scaling, this option is ruled out since it involves significant systems administration effort to migrate from RDS MySQL to Aurora. It is much easier to just enable storage auto-scaling for RDS MySQL.</p>\n\n<p><strong>Migrate RDS MySQL database to DynamoDB which automatically allocates storage space when required</strong> - This option is ruled out since DynamoDB is a NoSQL database which implies significant development effort to change the application logic to connect and query data from the underlying database. It is much easier to just enable storage auto-scaling for RDS MySQL.</p>\n\n<p><strong>Create read replica for RDS MySQL</strong> - Read replicas make it easy to take advantage of supported engines' built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create multiple read replicas for a given source DB Instance and distribute your application’s read traffic amongst them. This option acts as a distractor as read replicas cannot help to automatically scale storage for the primary database.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Enable storage auto-scaling for RDS MySQL</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "If your workload is unpredictable, you can enable storage autoscaling for an Amazon RDS DB instance. With storage autoscaling enabled, when Amazon RDS detects that you are running out of free database space it automatically scales up your storage. Amazon RDS starts a storage modification for an autoscaling-enabled DB instance when these factors apply:"
      },
      {
        "answer": "",
        "explanation": "Free available space is less than 10 percent of the allocated storage."
      },
      {
        "answer": "",
        "explanation": "The low-storage condition lasts at least five minutes."
      },
      {
        "answer": "",
        "explanation": "At least six hours have passed since the last storage modification."
      },
      {
        "answer": "",
        "explanation": "The maximum storage threshold is the limit that you set for autoscaling the DB instance. You can't set the maximum storage threshold for autoscaling-enabled instances to a value greater than the maximum allocated storage."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Migrate RDS MySQL to Aurora which offers storage auto-scaling</strong> - Although Aurora offers automatic storage scaling, this option is ruled out since it involves significant systems administration effort to migrate from RDS MySQL to Aurora. It is much easier to just enable storage auto-scaling for RDS MySQL."
      },
      {
        "answer": "",
        "explanation": "<strong>Migrate RDS MySQL database to DynamoDB which automatically allocates storage space when required</strong> - This option is ruled out since DynamoDB is a NoSQL database which implies significant development effort to change the application logic to connect and query data from the underlying database. It is much easier to just enable storage auto-scaling for RDS MySQL."
      },
      {
        "answer": "",
        "explanation": "<strong>Create read replica for RDS MySQL</strong> - Read replicas make it easy to take advantage of supported engines' built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create multiple read replicas for a given source DB Instance and distribute your application’s read traffic amongst them. This option acts as a distractor as read replicas cannot help to automatically scale storage for the primary database."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html"
    ]
  },
  {
    "id": 11,
    "question": "<p>An EC2 instance has an IAM instance role attached to it, providing it read and write access to the S3 bucket 'my_bucket'. You have tested the IAM instance role and both reads and writes are working. You then remove the IAM role from the EC2 instance and test both read and write again. Writes stopped working but reads are still working.</p>\n\n<p>What is the likely cause of this behavior?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The EC2 instance is using cached temporary IAM credentials</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Removing an instance role from an EC2 instance can take a few minutes before being active</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The S3 bucket policy authorizes reads</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>When a read is done on a bucket, there's a grace period of 5 minutes to do the same read again</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The S3 bucket policy authorizes reads</strong></p>\n\n<p>When evaluating an IAM policy of an EC2 instance doing actions on S3, the least-privilege union of both the IAM policy of the EC2 instance and the bucket policy of the S3 bucket are taken into account.</p>\n\n<p>For the given use-case, as IAM role has been removed, therefore only the S3 bucket policy comes into effect which authorizes reads.</p>\n\n<p>Here is a great reference blog for understanding the various scenarios for using IAM policy vs S3 bucket policy -\n<a href=\"https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/\">https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EC2 instance is using cached temporary IAM credentials</strong> - As the IAM instance role has been removed that wouldn't be the case</p>\n\n<p><strong>Removing an instance role from an EC2 instance can take a few minutes before being active</strong> - It is immediately active and even if it wasn't, it wouldn't make sense as we can still do reads but not writes.</p>\n\n<p><strong>When a read is done on a bucket, there's a grace period of 5 minutes to do the same read again</strong> - This is not true. Every single request is evaluated against IAM in the AWS model.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/\">https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>The S3 bucket policy authorizes reads</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "When evaluating an IAM policy of an EC2 instance doing actions on S3, the least-privilege union of both the IAM policy of the EC2 instance and the bucket policy of the S3 bucket are taken into account."
      },
      {
        "answer": "",
        "explanation": "For the given use-case, as IAM role has been removed, therefore only the S3 bucket policy comes into effect which authorizes reads."
      },
      {
        "link": "https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/",
        "answer": "",
        "explanation": "Here is a great reference blog for understanding the various scenarios for using IAM policy vs S3 bucket policy -\n<a href=\"https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/\">https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/</a>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The EC2 instance is using cached temporary IAM credentials</strong> - As the IAM instance role has been removed that wouldn't be the case"
      },
      {
        "answer": "",
        "explanation": "<strong>Removing an instance role from an EC2 instance can take a few minutes before being active</strong> - It is immediately active and even if it wasn't, it wouldn't make sense as we can still do reads but not writes."
      },
      {
        "answer": "",
        "explanation": "<strong>When a read is done on a bucket, there's a grace period of 5 minutes to do the same read again</strong> - This is not true. Every single request is evaluated against IAM in the AWS model."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/"
    ]
  },
  {
    "id": 12,
    "question": "<p>When your company first created an AWS account, you began with a single sign-in principal called a root user account that had complete access to all AWS services and resources.</p>\n\n<p>What should you do to adhere to best practices for using the root user account?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>It should be accessible using the access key id and secret access key</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>It should be accessible by no one, throw away the passwords after creating the account</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>It should be accessible by 3 to 6 members of the IT team</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>It should be accessible by one admin only after enabling Multi-factor authentication</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>It should be accessible by one admin only after enabling Multi-factor authentication</strong></p>\n\n<p>AWS Root Account Security Best Practices:\n<img src=\"\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials</a><p></p>\n\n<p>If you continue to use the root user credentials, we recommend that you follow the security best practice to enable multi-factor authentication (MFA) for your account. Because your root user can perform sensitive operations in your account, adding a layer of authentication helps you to better secure your account. Multiple types of MFA are available.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>It should be accessible by 3 to 6 members of the IT team</strong> - Only the owner of the AWS account should have access to the root account credentials. You should create an IT group with admin permissions via IAM and then assign a few users to this group.</p>\n\n<p><strong>It should be accessible using the access key id and secret access key</strong> - AWS recommends that you should not use the access key id and secret access key for the AWS account root user.</p>\n\n<p><strong>It should be accessible by no one, throw away the passwords after creating the account</strong> - You will still need to store the password somewhere for your root account.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>It should be accessible by one admin only after enabling Multi-factor authentication</strong>",
        "explanation": ""
      },
      {
        "image": "",
        "answer": "",
        "explanation": "AWS Root Account Security Best Practices:"
      },
      {
        "link": "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials"
      },
      {
        "answer": "",
        "explanation": "If you continue to use the root user credentials, we recommend that you follow the security best practice to enable multi-factor authentication (MFA) for your account. Because your root user can perform sensitive operations in your account, adding a layer of authentication helps you to better secure your account. Multiple types of MFA are available."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>It should be accessible by 3 to 6 members of the IT team</strong> - Only the owner of the AWS account should have access to the root account credentials. You should create an IT group with admin permissions via IAM and then assign a few users to this group."
      },
      {
        "answer": "",
        "explanation": "<strong>It should be accessible using the access key id and secret access key</strong> - AWS recommends that you should not use the access key id and secret access key for the AWS account root user."
      },
      {
        "answer": "",
        "explanation": "<strong>It should be accessible by no one, throw away the passwords after creating the account</strong> - You will still need to store the password somewhere for your root account."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#lock-away-credentials"
    ]
  },
  {
    "id": 13,
    "question": "<p>Which of the following CLI options will allow you to retrieve a subset of the attributes coming from a DynamoDB scan?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>--projection-expression</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>--max-items</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>--filter-expression</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>--page-size</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>--projection-expression</strong></p>\n\n<p>A projection expression is a string that identifies the attributes you want. To retrieve a single attribute, specify its name. For multiple attributes, the names must be comma-separated.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q23-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q23-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html</a><p></p>\n\n<p>To read data from a table, you use operations such as GetItem, Query, or Scan. DynamoDB returns all of the item attributes by default. To get just some, rather than all of the attributes, use a projection expression.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>--filter-expression</strong> - If you need to further refine the Query results, you can optionally provide a filter expression. A filter expression determines which items within the Query results should be returned to you. All of the other results are discarded. A filter expression is applied after Query finishes, but before the results are returned. Therefore, a Query will consume the same amount of read capacity, regardless of whether a filter expression is present.</p>\n\n<p><strong>--page-size</strong> - You can use the --page-size option to specify that the AWS CLI requests a smaller number of items from each call to the AWS service. The CLI still retrieves the full list but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call.</p>\n\n<p><strong>--max-items</strong> - To include fewer items at a time in the AWS CLI output, use the --max-items option. The AWS CLI still handles pagination with the service as described above, but prints out only the number of items at a time that you specify.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>--projection-expression</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A projection expression is a string that identifies the attributes you want. To retrieve a single attribute, specify its name. For multiple attributes, the names must be comma-separated."
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html"
      },
      {
        "answer": "",
        "explanation": "To read data from a table, you use operations such as GetItem, Query, or Scan. DynamoDB returns all of the item attributes by default. To get just some, rather than all of the attributes, use a projection expression."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>--filter-expression</strong> - If you need to further refine the Query results, you can optionally provide a filter expression. A filter expression determines which items within the Query results should be returned to you. All of the other results are discarded. A filter expression is applied after Query finishes, but before the results are returned. Therefore, a Query will consume the same amount of read capacity, regardless of whether a filter expression is present."
      },
      {
        "answer": "",
        "explanation": "<strong>--page-size</strong> - You can use the --page-size option to specify that the AWS CLI requests a smaller number of items from each call to the AWS service. The CLI still retrieves the full list but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call."
      },
      {
        "answer": "",
        "explanation": "<strong>--max-items</strong> - To include fewer items at a time in the AWS CLI output, use the --max-items option. The AWS CLI still handles pagination with the service as described above, but prints out only the number of items at a time that you specify."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html"
    ]
  },
  {
    "id": 14,
    "question": "<p>A developer created an online shopping application that runs on EC2 instances behind load balancers. The same web application version is hosted on several EC2 instances and the instances run in an Auto Scaling group. The application uses STS to request credentials but after an hour your application stops working.</p>\n\n<p>What is the most likely cause of this issue?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The IAM service is experiencing downtime once an hour</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>A lambda function revokes your access every hour</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Your IAM policy is wrong</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Your application needs to renew the credentials after 1 hour when they expire</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Your application needs to renew the credentials after 1 hour when they expire</strong></p>\n\n<p>AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). By default, AWS Security Token Service (STS) is available as a global service, and all AWS STS requests go to a single endpoint at https://sts.amazonaws.com.</p>\n\n<p>Credentials that are created by using account credentials can range from 900 seconds (15 minutes) up to a maximum of 3,600 seconds (1 hour), with a default of 1 hour. Hence you need to renew the credentials post expiry.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q45-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q45-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Your IAM policy is wrong</strong> - If your policy was wrong, a reboot would not solve the issue.</p>\n\n<p><strong>A lambda function revokes your access every hour</strong> - Revoking can be done by an IAM policy. Lambda function cannot revoke access.</p>\n\n<p><strong>The IAM service is experiencing downtime once an hour</strong> - The IAM service is reliable as it's managed by AWS.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Your application needs to renew the credentials after 1 hour when they expire</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). By default, AWS Security Token Service (STS) is available as a global service, and all AWS STS requests go to a single endpoint at https://sts.amazonaws.com."
      },
      {
        "answer": "",
        "explanation": "Credentials that are created by using account credentials can range from 900 seconds (15 minutes) up to a maximum of 3,600 seconds (1 hour), with a default of 1 hour. Hence you need to renew the credentials post expiry."
      },
      {
        "link": "https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Your IAM policy is wrong</strong> - If your policy was wrong, a reboot would not solve the issue."
      },
      {
        "answer": "",
        "explanation": "<strong>A lambda function revokes your access every hour</strong> - Revoking can be done by an IAM policy. Lambda function cannot revoke access."
      },
      {
        "answer": "",
        "explanation": "<strong>The IAM service is experiencing downtime once an hour</strong> - The IAM service is reliable as it's managed by AWS."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html"
    ]
  },
  {
    "id": 15,
    "question": "<p>An e-commerce company has multiple EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a DynamoDB table.</p>\n\n<p>How would you go about providing private access to these AWS resources which are not part of this custom VPC?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an API endpoint for S3 and then connect to the S3 service using the private IP address</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB service using the private IP address</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</strong></p>\n\n<p>Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.</p>\n\n<p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p>\n\n<p>There are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.</p>\n\n<p>A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported:</p>\n\n<p>Amazon S3</p>\n\n<p>DynamoDB</p>\n\n<p>You should note that S3 now supports both gateway endpoints as well as the interface endpoints.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB service using the private IP address</strong></p>\n\n<p><strong>Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address</strong></p>\n\n<p>DynamoDB does not support interface endpoints, so these two options are incorrect.</p>\n\n<p><strong>Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an API endpoint for S3 and then connect to the S3 service using the private IP address</strong> - There is no such thing as an API endpoint for S3. API endpoints are used with AWS API Gateway. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic."
      },
      {
        "answer": "",
        "explanation": "A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network."
      },
      {
        "answer": "",
        "explanation": "There are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service."
      },
      {
        "answer": "",
        "explanation": "A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported:"
      },
      {
        "answer": "",
        "explanation": "Amazon S3"
      },
      {
        "answer": "",
        "explanation": "DynamoDB"
      },
      {
        "answer": "",
        "explanation": "You should note that S3 now supports both gateway endpoints as well as the interface endpoints."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB service using the private IP address</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "DynamoDB does not support interface endpoints, so these two options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an API endpoint for S3 and then connect to the S3 service using the private IP address</strong> - There is no such thing as an API endpoint for S3. API endpoints are used with AWS API Gateway. This option has been added as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html"
    ]
  },
  {
    "id": 16,
    "question": "<p>You are a developer working on AWS Lambda functions that are triggered by Amazon API Gateway and would like to perform testing on a low volume of traffic for new API versions.</p>\n\n<p>Which of the following features will accomplish this task?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Custom Authorizers</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Mapping Templates</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Stage Variables</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Canary Deployment</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Canary Deployment</strong></p>\n\n<p>In a canary release deployment, total API traffic is separated at random into a production release and a canary release with a preconfigured ratio. Typically, the canary release receives a small percentage of API traffic and the production release takes up the rest. The updated API features are only visible to API traffic through the canary. You can adjust the canary traffic percentage to optimize test coverage or performance.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q50-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q50-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Stage Variables</strong> - They act like environment variables and can be used in your API setup.</p>\n\n<p><strong>Mapping Templates</strong> - Its a script to map the payload from a method request to the corresponding integration request and also maps the integration response to the corresponding method response.</p>\n\n<p><strong>Custom Authorizers</strong> - Used for authentication purposes and must return AWS Identity and Access Management (IAM) policies.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Canary Deployment</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "In a canary release deployment, total API traffic is separated at random into a production release and a canary release with a preconfigured ratio. Typically, the canary release receives a small percentage of API traffic and the production release takes up the rest. The updated API features are only visible to API traffic through the canary. You can adjust the canary traffic percentage to optimize test coverage or performance."
      },
      {
        "link": "https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Stage Variables</strong> - They act like environment variables and can be used in your API setup."
      },
      {
        "answer": "",
        "explanation": "<strong>Mapping Templates</strong> - Its a script to map the payload from a method request to the corresponding integration request and also maps the integration response to the corresponding method response."
      },
      {
        "answer": "",
        "explanation": "<strong>Custom Authorizers</strong> - Used for authentication purposes and must return AWS Identity and Access Management (IAM) policies."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html"
    ]
  },
  {
    "id": 17,
    "question": "<p>You would like to paginate the results of an S3 List to show 100 results per page to your users and minimize the number of API calls that you will use.</p>\n\n<p>Which CLI options should you use? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>--max-items</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>--page-size</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>--next-token</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>--limit</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>--starting-token</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>--max-items</strong></p>\n\n<p><strong>--starting-token</strong></p>\n\n<p>For commands that can return a large list of items, the AWS Command Line Interface (AWS CLI) has three options to control the number of items included in the output when the AWS CLI calls a service's API to populate the list.</p>\n\n<p><code>--page-size</code></p>\n\n<p><code>--max-items</code></p>\n\n<p><code>--starting-token</code></p>\n\n<p>By default, the AWS CLI uses a page size of 1000 and retrieves all available items. For example, if you run <code>aws s3api list-objects</code> on an Amazon S3 bucket that contains 3,500 objects, the AWS CLI makes four calls to Amazon S3, handling the service-specific pagination logic for you in the background and returning all 3,500 objects in the final output.</p>\n\n<p>Here's an example: <code>aws s3api list-objects --bucket my-bucket --max-items 100 --starting-token eyJNYXJrZXIiOiBudWxsLCAiYm90b190cnVuY2F0ZV9hbW91bnQiOiAxfQ==</code></p>\n\n<p>Incorrect options:</p>\n\n<p>\"--page-size\" - You can use the <code>--page-size</code> option to specify that the AWS CLI requests a smaller number of items from each call to the AWS service. The CLI still retrieves the full list but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call.</p>\n\n<p>\"--next-token\" - This is a made-up option and has been added as a distractor.</p>\n\n<p>\"--limit\" - This is a made-up option and has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html\">https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>--max-items</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>--starting-token</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "For commands that can return a large list of items, the AWS Command Line Interface (AWS CLI) has three options to control the number of items included in the output when the AWS CLI calls a service's API to populate the list."
      },
      {
        "answer": "<code>--page-size</code>",
        "explanation": ""
      },
      {
        "answer": "<code>--max-items</code>",
        "explanation": ""
      },
      {
        "answer": "<code>--starting-token</code>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "By default, the AWS CLI uses a page size of 1000 and retrieves all available items. For example, if you run <code>aws s3api list-objects</code> on an Amazon S3 bucket that contains 3,500 objects, the AWS CLI makes four calls to Amazon S3, handling the service-specific pagination logic for you in the background and returning all 3,500 objects in the final output."
      },
      {
        "answer": "",
        "explanation": "Here's an example: <code>aws s3api list-objects --bucket my-bucket --max-items 100 --starting-token eyJNYXJrZXIiOiBudWxsLCAiYm90b190cnVuY2F0ZV9hbW91bnQiOiAxfQ==</code>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>--page-size</strong> - You can use the <code>--page-size</code> option to specify that the AWS CLI requests a smaller number of items from each call to the AWS service. The CLI still retrieves the full list but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call."
      },
      {
        "answer": "",
        "explanation": "<strong>--next-token</strong> - This is a made-up option and has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>--limit</strong> - This is a made-up option and has been added as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html"
    ]
  },
  {
    "id": 18,
    "question": "<p>A developer has defined a Lambda integration in Amazon API Gateway using a stage variable. However, when the developer invokes the API method, it consistently returns an \"Internal server error\" and a 500 status code.</p>\n\n<p>What steps should the developer take to fix the issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>API calls can't exceed the maximum allowed API request rate per account and per Region. Implement error retries and exponential backoffs to fix the error</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>If you create a stage variable to call a Lambda function through your API, you must add the required permissions. Update your Lambda function's resource-based AWS Identity and Access Management (IAM) policy so that it grants invoke permission to the API Gateway</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>When setting the Lambda function as the value of a stage variable, use the function's ARN and not the function alias for setting up the value</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>If you create a stage variable to call a function through your API, you must add the required permissions. Create an IAM role that your Lambda function can assume when invoking the respective AWS resources</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>If you create a stage variable to call a Lambda function through your API, you must add the required permissions. Update your Lambda function's resource-based AWS Identity and Access Management (IAM) policy so that it grants invoke permission to the API Gateway</strong></p>\n\n<p>If your Lambda function's resource-based policy doesn't include permissions for your API to invoke the function, API Gateway returns an Internal server error message.</p>\n\n<p>If you create a stage variable to call a function through your API, you must add the required permissions by doing one of the following:</p>\n\n<p>Update your Lambda function's resource-based AWS Identity and Access Management (IAM) policy so that it grants invoke permission to API Gateway.</p>\n\n<p>OR</p>\n\n<p>Create an IAM role that API Gateway can assume to invoke your Lambda function.</p>\n\n<p>If you build an API Gateway API with standard Lambda integration using the API Gateway console, the console adds the required permissions automatically.</p>\n\n<p>Example resource-based policy that grants invoke permission to API Gateway:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q48-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q48-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-lambda-stage-variable-500/\">https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-lambda-stage-variable-500/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>API calls can't exceed the maximum allowed API request rate per account and per Region. Implement error retries and exponential backoffs to fix the error</strong> - When API calls exceed the maximum allowed API request rate per account and per Region, the error received is a \"Rate Exceeded\" error and not an \"Internal server error\". Hence, this option is irrelevant to the given use case.</p>\n\n<p><strong>If you create a stage variable to call a function through your API, you must add the required permissions. Create an IAM role that your Lambda function can assume when invoking the respective AWS resources</strong> - This option is incorrect. However, creating an IAM role that API Gateway can assume to invoke the Lambda function is another solution to fix the given problem.</p>\n\n<p><strong>When setting the Lambda function as the value of a stage variable, use the function's ARN and not the function alias for setting up the value</strong> - This statement is incorrect. When setting a Lambda function as the value of a stage variable, use the function's local name, possibly including its alias or version specification. Do not use the function's ARN. The API Gateway console assumes the stage variable value for a Lambda function as the unqualified function name and expands the given stage variable into an ARN.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-set-stage-variables-aws-console.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-set-stage-variables-aws-console.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-lambda-stage-variable-500/\">https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-lambda-stage-variable-500/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>If you create a stage variable to call a Lambda function through your API, you must add the required permissions. Update your Lambda function's resource-based AWS Identity and Access Management (IAM) policy so that it grants invoke permission to the API Gateway</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "If your Lambda function's resource-based policy doesn't include permissions for your API to invoke the function, API Gateway returns an Internal server error message."
      },
      {
        "answer": "",
        "explanation": "If you create a stage variable to call a function through your API, you must add the required permissions by doing one of the following:"
      },
      {
        "answer": "",
        "explanation": "Update your Lambda function's resource-based AWS Identity and Access Management (IAM) policy so that it grants invoke permission to API Gateway."
      },
      {
        "answer": "",
        "explanation": "OR"
      },
      {
        "answer": "",
        "explanation": "Create an IAM role that API Gateway can assume to invoke your Lambda function."
      },
      {
        "answer": "",
        "explanation": "If you build an API Gateway API with standard Lambda integration using the API Gateway console, the console adds the required permissions automatically."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q48-i1.jpg",
        "answer": "",
        "explanation": "Example resource-based policy that grants invoke permission to API Gateway:"
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-lambda-stage-variable-500/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>API calls can't exceed the maximum allowed API request rate per account and per Region. Implement error retries and exponential backoffs to fix the error</strong> - When API calls exceed the maximum allowed API request rate per account and per Region, the error received is a \"Rate Exceeded\" error and not an \"Internal server error\". Hence, this option is irrelevant to the given use case."
      },
      {
        "answer": "",
        "explanation": "<strong>If you create a stage variable to call a function through your API, you must add the required permissions. Create an IAM role that your Lambda function can assume when invoking the respective AWS resources</strong> - This option is incorrect. However, creating an IAM role that API Gateway can assume to invoke the Lambda function is another solution to fix the given problem."
      },
      {
        "answer": "",
        "explanation": "<strong>When setting the Lambda function as the value of a stage variable, use the function's ARN and not the function alias for setting up the value</strong> - This statement is incorrect. When setting a Lambda function as the value of a stage variable, use the function's local name, possibly including its alias or version specification. Do not use the function's ARN. The API Gateway console assumes the stage variable value for a Lambda function as the unqualified function name and expands the given stage variable into an ARN."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-lambda-stage-variable-500/",
      "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-set-stage-variables-aws-console.html"
    ]
  },
  {
    "id": 19,
    "question": "<p>A financial services company uses Amazon S3 to store transformed and anonymized customer data that is generated by a daily batch job. The development team has been tasked to build a solution that analyzes the output of the daily job for any sensitive financial information about the company's customers.</p>\n\n<p>As an AWS Certified Developer Associate, which of the following options would you recommend to address this use case MOST efficiently?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/CustomIdentifier</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure a S3 event notification for every object upload that triggers a Lambda function based Python script to detect sensitive customer information</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/Financial</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/Personal</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/Financial</strong></p>\n\n<p>Amazon Macie is a data security service that discovers sensitive data by using machine learning and pattern matching, provides visibility into data security risks, and enables automated protection against those risks. To help you manage the security posture of your organization's Amazon Simple Storage Service (Amazon S3) data estate, Macie provides you with an inventory of your S3 buckets, and automatically evaluates and monitors the buckets for security and access control. If Macie detects a potential issue with the security or privacy of your data, such as a bucket that becomes publicly accessible, Macie generates a finding for you to review and remediate as necessary.</p>\n\n<p>Macie also automates the discovery and reporting of sensitive data to provide you with a better understanding of the data that your organization stores in Amazon S3. To detect sensitive data, you can use built-in criteria and techniques that Macie provides, custom criteria that you define, or a combination of the two. If Macie detects sensitive data in an S3 object, Macie generates a finding to notify you of the sensitive data that Macie found.</p>\n\n<p>Macie generates a sensitive data finding when it detects sensitive data in an S3 object that it analyzes to discover sensitive data. This includes analysis that Macie performs when you run a sensitive data discovery job and when it performs automated sensitive data discovery.</p>\n\n<p>For the given use case, you can use Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/Financial which implies that the S3 object contains financial information, such as bank account numbers or credit card numbers.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q8-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q8-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/macie/latest/user/findings-types.html\">https://docs.aws.amazon.com/macie/latest/user/findings-types.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/Personal</strong> - The object contains personally identifiable information (such as mailing addresses or driver's license identification numbers), personal health information (such as health insurance or medical identification numbers), or a combination of the two.</p>\n\n<p><strong>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/CustomIdentifier</strong> - The object contains text that matches the detection criteria of one or more custom data identifiers. The object might contain more than one type of sensitive data.</p>\n\n<p>As explained above, for the given use case, you need to look for any sensitive data findings of type SensitiveData:S3Object/Financial. So both these options are incorrect.</p>\n\n<p><strong>Configure a S3 event notification for every object upload that triggers a Lambda function based custom application to detect sensitive customer information</strong> - Maintaining a custom application to detect sensitive information would be highly inefficient as you need to consistently upgrade the application to handle new formats and types of financial information. This is better handled by leveraging Macie's Findings.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/macie/latest/user/what-is-macie.html\">https://docs.aws.amazon.com/macie/latest/user/what-is-macie.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/macie/latest/user/findings-types.html\">https://docs.aws.amazon.com/macie/latest/user/findings-types.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/Financial</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Macie is a data security service that discovers sensitive data by using machine learning and pattern matching, provides visibility into data security risks, and enables automated protection against those risks. To help you manage the security posture of your organization's Amazon Simple Storage Service (Amazon S3) data estate, Macie provides you with an inventory of your S3 buckets, and automatically evaluates and monitors the buckets for security and access control. If Macie detects a potential issue with the security or privacy of your data, such as a bucket that becomes publicly accessible, Macie generates a finding for you to review and remediate as necessary."
      },
      {
        "answer": "",
        "explanation": "Macie also automates the discovery and reporting of sensitive data to provide you with a better understanding of the data that your organization stores in Amazon S3. To detect sensitive data, you can use built-in criteria and techniques that Macie provides, custom criteria that you define, or a combination of the two. If Macie detects sensitive data in an S3 object, Macie generates a finding to notify you of the sensitive data that Macie found."
      },
      {
        "answer": "",
        "explanation": "Macie generates a sensitive data finding when it detects sensitive data in an S3 object that it analyzes to discover sensitive data. This includes analysis that Macie performs when you run a sensitive data discovery job and when it performs automated sensitive data discovery."
      },
      {
        "answer": "",
        "explanation": "For the given use case, you can use Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/Financial which implies that the S3 object contains financial information, such as bank account numbers or credit card numbers."
      },
      {
        "link": "https://docs.aws.amazon.com/macie/latest/user/findings-types.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/Personal</strong> - The object contains personally identifiable information (such as mailing addresses or driver's license identification numbers), personal health information (such as health insurance or medical identification numbers), or a combination of the two."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage Macie to analyze the output of the daily batch job and look for any sensitive data findings of type SensitiveData:S3Object/CustomIdentifier</strong> - The object contains text that matches the detection criteria of one or more custom data identifiers. The object might contain more than one type of sensitive data."
      },
      {
        "answer": "",
        "explanation": "As explained above, for the given use case, you need to look for any sensitive data findings of type SensitiveData:S3Object/Financial. So both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a S3 event notification for every object upload that triggers a Lambda function based custom application to detect sensitive customer information</strong> - Maintaining a custom application to detect sensitive information would be highly inefficient as you need to consistently upgrade the application to handle new formats and types of financial information. This is better handled by leveraging Macie's Findings."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/macie/latest/user/findings-types.html",
      "https://docs.aws.amazon.com/macie/latest/user/what-is-macie.html"
    ]
  },
  {
    "id": 20,
    "question": "<p>You are using AWS SQS FIFO queues to get the ordering of messages on a per <code>user_id</code> basis. On top of this, you would like to make sure that duplicate messages should not be sent to SQS as this would cause application failure.</p>\n\n<p>As a developer, which message parameter should you set for deduplicating messages?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>MessageDeduplicationId</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>ContentBasedDeduplication</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>ReceiveRequestAttemptId</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>MessageGroupId</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS FIFO queues are designed to enhance messaging between applications when the order of operations and events has to be enforced.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q35-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q35-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a><p></p>\n\n<p><strong>MessageDeduplicationId</strong></p>\n\n<p>The message deduplication ID is the token used for the deduplication of sent messages. If a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>MessageGroupId</strong> - The message group ID is the tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are always processed one by one, in a strict order relative to the message group (however, messages that belong to different message groups might be processed out of order).</p>\n\n<p><strong>ReceiveRequestAttemptId</strong> - This parameter applies only to FIFO (first-in-first-out) queues. The token is used for deduplication of ReceiveMessage calls. If a networking issue occurs after a ReceiveMessage action, and instead of a response you receive a generic error, you can retry the same action with an identical ReceiveRequestAttemptId to retrieve the same set of messages, even if their visibility timeout has not yet expired.</p>\n\n<p><strong>ContentBasedDeduplication</strong> - This is not a message parameter, but a queue setting. Enable content-based deduplication to instruct Amazon SQS to use an SHA-256 hash to generate the message deduplication ID using the body of the message - but not the attributes of the message.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS FIFO queues are designed to enhance messaging between applications when the order of operations and events has to be enforced."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html"
      },
      {
        "answer": "<strong>MessageDeduplicationId</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The message deduplication ID is the token used for the deduplication of sent messages. If a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>MessageGroupId</strong> - The message group ID is the tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are always processed one by one, in a strict order relative to the message group (however, messages that belong to different message groups might be processed out of order)."
      },
      {
        "answer": "",
        "explanation": "<strong>ReceiveRequestAttemptId</strong> - This parameter applies only to FIFO (first-in-first-out) queues. The token is used for deduplication of ReceiveMessage calls. If a networking issue occurs after a ReceiveMessage action, and instead of a response you receive a generic error, you can retry the same action with an identical ReceiveRequestAttemptId to retrieve the same set of messages, even if their visibility timeout has not yet expired."
      },
      {
        "answer": "",
        "explanation": "<strong>ContentBasedDeduplication</strong> - This is not a message parameter, but a queue setting. Enable content-based deduplication to instruct Amazon SQS to use an SHA-256 hash to generate the message deduplication ID using the body of the message - but not the attributes of the message."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html"
    ]
  },
  {
    "id": 21,
    "question": "<p>Your Lambda function processes files for your customers and as part of that process, it creates a lot of intermediary files it needs to store on its disk and then discard.</p>\n\n<p>What is the best way to store temporary files for your Lambda functions that will be discarded when the function stops running?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a tmp/ directory in the source zip file and use it</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the local directory /opt</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the local directory /tmp</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use an S3 bucket</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the local directory /tmp</strong></p>\n\n<p>This is 512MB of temporary space you can use for your Lambda functions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a tmp/ directory in the source zip file and use it</strong> - This option has been added as a distractor, as you can't access a directory within a zip file.</p>\n\n<p><strong>Use the local directory /opt</strong> - This option has been added as a distractor. This path is not accessible.</p>\n\n<p><strong>Use an S3 bucket</strong> - This won't be temporary after the Lambda function is deleted, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/limits.html\">https://docs.aws.amazon.com/lambda/latest/dg/limits.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use the local directory /tmp</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This is 512MB of temporary space you can use for your Lambda functions."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a tmp/ directory in the source zip file and use it</strong> - This option has been added as a distractor, as you can't access a directory within a zip file."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the local directory /opt</strong> - This option has been added as a distractor. This path is not accessible."
      },
      {
        "answer": "",
        "explanation": "<strong>Use an S3 bucket</strong> - This won't be temporary after the Lambda function is deleted, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/lambda/latest/dg/limits.html"
    ]
  },
  {
    "id": 22,
    "question": "<p>Your company wants to move away from manually managing Lambda in the AWS console and wants to upload and update them using AWS CloudFormation.</p>\n\n<p>How do you declare an AWS Lambda function in CloudFormation? (Select two)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Upload all the code as a zip to S3 and refer the object in <code>AWS::Lambda::Function</code> block</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block as long as there are no third-party dependencies</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block and reference the dependencies as a zip file stored in S3</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Upload all the code to CodeCommit and refer to the CodeCommit Repository in <code>AWS::Lambda::Function</code> block</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Upload all the code as a folder to S3 and refer the folder in <code>AWS::Lambda::Function</code> block</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct options:</p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a><p></p>\n\n<p><strong>Upload all the code as a zip to S3 and refer the object in <code>AWS::Lambda::Function</code> block</strong></p>\n\n<p>You can upload all the code as a zip to S3 and refer the object in <code>AWS::Lambda::Function</code> block.</p>\n\n<p>The AWS::Lambda::Function resource creates a Lambda function. To create a function, you need a deployment package and an execution role. The deployment package contains your function code.</p>\n\n<p><strong>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block as long as there are no third-party dependencies</strong></p>\n\n<p>The other option is to write the code inline for Node.js and Python as long as there are no dependencies for your code, besides the dependencies already provided by AWS in your Lambda Runtime (aws-sdk and cfn-response and many other AWS related libraries are preloaded via, for example, boto3 (python) in the lambda instances.)</p>\n\n<p>YAML template for creating a Lambda function:</p>\n\n<pre><code>Type: AWS::Lambda::Function\nProperties:\n  Code:\n    Code\n  DeadLetterConfig:\n    DeadLetterConfig\n  Description: String\n  Environment:\n    Environment\n  FileSystemConfigs:\n    - FileSystemConfig\n  FunctionName: String\n  Handler: String\n  KmsKeyArn: String\n  Layers:\n    - String\n  MemorySize: Integer\n  ReservedConcurrentExecutions: Integer\n  Role: String\n  Runtime: String\n  Tags:\n    - Tag\n  Timeout: Integer\n  TracingConfig:\n    TracingConfig\n  VpcConfig:\n    VpcConfig\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upload all the code to CodeCommit and refer to the CodeCommit Repository in <code>AWS::Lambda::Function</code> block</strong></p>\n\n<p><strong>Upload all the code as a folder to S3 and refer the folder in <code>AWS::Lambda::Function</code> block</strong></p>\n\n<p><strong>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block and reference the dependencies as a zip file stored in S3</strong></p>\n\n<p>These three options contradict the explanation provided earlier. So these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume."
      },
      {
        "image": "https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png",
        "answer": "",
        "explanation": "How Lambda function works:"
      },
      {
        "link": "https://aws.amazon.com/lambda/"
      },
      {
        "answer": "<strong>Upload all the code as a zip to S3 and refer the object in <code>AWS::Lambda::Function</code> block</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can upload all the code as a zip to S3 and refer the object in <code>AWS::Lambda::Function</code> block."
      },
      {
        "answer": "",
        "explanation": "The AWS::Lambda::Function resource creates a Lambda function. To create a function, you need a deployment package and an execution role. The deployment package contains your function code."
      },
      {
        "answer": "<strong>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block as long as there are no third-party dependencies</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The other option is to write the code inline for Node.js and Python as long as there are no dependencies for your code, besides the dependencies already provided by AWS in your Lambda Runtime (aws-sdk and cfn-response and many other AWS related libraries are preloaded via, for example, boto3 (python) in the lambda instances.)"
      },
      {
        "answer": "",
        "explanation": "YAML template for creating a Lambda function:"
      },
      {
        "answer": "",
        "explanation": "<pre><code>Type: AWS::Lambda::Function\nProperties:\n  Code:\n    Code\n  DeadLetterConfig:\n    DeadLetterConfig\n  Description: String\n  Environment:\n    Environment\n  FileSystemConfigs:\n    - FileSystemConfig\n  FunctionName: String\n  Handler: String\n  KmsKeyArn: String\n  Layers:\n    - String\n  MemorySize: Integer\n  ReservedConcurrentExecutions: Integer\n  Role: String\n  Runtime: String\n  Tags:\n    - Tag\n  Timeout: Integer\n  TracingConfig:\n    TracingConfig\n  VpcConfig:\n    VpcConfig\n</code></pre>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Upload all the code to CodeCommit and refer to the CodeCommit Repository in <code>AWS::Lambda::Function</code> block</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Upload all the code as a folder to S3 and refer the folder in <code>AWS::Lambda::Function</code> block</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Write the AWS Lambda code inline in CloudFormation in the <code>AWS::Lambda::Function</code> block and reference the dependencies as a zip file stored in S3</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided earlier. So these are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/lambda/",
      "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html"
    ]
  },
  {
    "id": 23,
    "question": "<p>You have created a test environment in Elastic Beanstalk and as part of that environment, you have created an RDS database.</p>\n\n<p>How can you make sure the database can be explored after the environment is destroyed?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Make a snapshot of the database before it gets deleted</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Change the Elastic Beanstalk environment variables</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Convert the Elastic Beanstalk environment to a worker environment</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Make a selective delete in Elastic Beanstalk</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Make a snapshot of the database before it gets deleted</strong></p>\n\n<p>Use an Elastic Beanstalk blue (environment A)/green (environment B) deployment to decouple an RDS DB instance from environment.</p>\n\n<p>Create a new Elastic Beanstalk environment (environment B) with the necessary information to connect to the RDS DB instance.</p>\n\n<p>Note: An RDS DB instance attached to an Elastic Beanstalk environment is ideal for development and testing environments. However, it's not ideal for production environments because the lifecycle of the database instance is tied to the lifecycle of your application environment. If you terminate the environment, then you lose your data because the RDS DB instance is deleted by the environment. For more information, see Using Elastic Beanstalk with Amazon RDS.</p>\n\n<p>This is the only way to recover the database data before it gets deleted by Elastic Beanstalk.</p>\n\n<p>Please review this excellent document that addresses this use-case :</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/\">https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Make a selective delete in Elastic Beanstalk</strong> - This is not a feature in Elastic Beanstalk.</p>\n\n<p><strong>Change the Elastic Beanstalk environment variables</strong> - Environment variables won't help with the provisioned RDS database.</p>\n\n<p><strong>Convert the Elastic Beanstalk environment to a worker environment</strong> - You can't convert Elastic Beanstalk environments, you can only change their settings.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/\">https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Make a snapshot of the database before it gets deleted</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Use an Elastic Beanstalk blue (environment A)/green (environment B) deployment to decouple an RDS DB instance from environment."
      },
      {
        "answer": "",
        "explanation": "Create a new Elastic Beanstalk environment (environment B) with the necessary information to connect to the RDS DB instance."
      },
      {
        "answer": "",
        "explanation": "Note: An RDS DB instance attached to an Elastic Beanstalk environment is ideal for development and testing environments. However, it's not ideal for production environments because the lifecycle of the database instance is tied to the lifecycle of your application environment. If you terminate the environment, then you lose your data because the RDS DB instance is deleted by the environment. For more information, see Using Elastic Beanstalk with Amazon RDS."
      },
      {
        "answer": "",
        "explanation": "This is the only way to recover the database data before it gets deleted by Elastic Beanstalk."
      },
      {
        "answer": "",
        "explanation": "Please review this excellent document that addresses this use-case :"
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/",
        "answer": "",
        "explanation": "<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/\">https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/</a>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Make a selective delete in Elastic Beanstalk</strong> - This is not a feature in Elastic Beanstalk."
      },
      {
        "answer": "",
        "explanation": "<strong>Change the Elastic Beanstalk environment variables</strong> - Environment variables won't help with the provisioned RDS database."
      },
      {
        "answer": "",
        "explanation": "<strong>Convert the Elastic Beanstalk environment to a worker environment</strong> - You can't convert Elastic Beanstalk environments, you can only change their settings."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/"
    ]
  },
  {
    "id": 24,
    "question": "<p>Your company is new to cloud computing and would like to host a static HTML5 website on the cloud and be able to access it via domain www.mycompany.com. You have created a bucket in Amazon Simple Storage Service (S3), enabled website hosting, and set the index.html as the default page. Finally, you create an Alias record in Amazon Route 53 that points to the S3 website endpoint of your S3 bucket.</p>\n\n<p>When you test the domain www.mycompany.com you get the following error: 'HTTP response code 403 (Access Denied)'. What can you do to resolve this error?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable Encryption</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a bucket policy</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Enable CORS</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an IAM role</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct answer</p>\n\n<p><strong>Create a bucket policy</strong></p>\n\n<p>Bucket policy is an access policy option available for you to grant permission to your Amazon S3 resources. It uses JSON-based access policy language.</p>\n\n<p>If you want to configure an existing bucket as a static website that has public access, you must edit block public access settings for that bucket. You may also have to edit your account-level block public access settings.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q52-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q52-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a><p></p>\n\n<p>Incorrect:</p>\n\n<p><strong>Create an IAM role</strong> - This will not help because IAM roles are attached to services and in this case, we have public users.</p>\n\n<p><strong>Enable CORS</strong> - CORS defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. Here we are not dealing with cross domains.</p>\n\n<p><strong>Enable Encryption</strong> - For the most part, encryption does not have an effect on access denied/forbidden errors. On the website endpoint, if a user requests an object that doesn't exist, Amazon S3 returns HTTP response code 404 (Not Found). If the object exists but you haven't granted read permission on it, the website endpoint returns HTTP response code 403 (Access Denied).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteAccessPermissionsReqd.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteAccessPermissionsReqd.html</a></p>\n",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteAccessPermissionsReqd.html"
    ]
  },
  {
    "id": 25,
    "question": "<p>You've just deployed an AWS Lambda function. The lambda function will be invoked via the API Gateway. The API Gateway will need to control access to it.</p>\n\n<p>Which of the following mechanisms is not supported for API Gateway?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Lambda Authorizer</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Cognito User Pools</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>IAM permissions with sigv4</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>STS</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a><p></p>\n\n<p><strong>STS</strong></p>\n\n<p>The AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). However, is not supported at the time with API Gateway.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>IAM permissions with sigv4</strong> - They can be applied to an entire API or individual methods.</p>\n\n<p><strong>Lambda Authorizer</strong> - Control access to REST API methods using bearer token authentication as well as information described by headers, paths, query strings, stage variables, or context variables request parameter.</p>\n\n<p><strong>Cognito User Pools</strong> - Use Cognito User Pools to create customizable authentication and authorization solutions for your REST APIs.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud."
      },
      {
        "image": "https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png",
        "answer": "",
        "explanation": "How API Gateway Works:"
      },
      {
        "link": "https://aws.amazon.com/api-gateway/"
      },
      {
        "answer": "<strong>STS</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). However, is not supported at the time with API Gateway."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>IAM permissions with sigv4</strong> - They can be applied to an entire API or individual methods."
      },
      {
        "answer": "",
        "explanation": "<strong>Lambda Authorizer</strong> - Control access to REST API methods using bearer token authentication as well as information described by headers, paths, query strings, stage variables, or context variables request parameter."
      },
      {
        "answer": "",
        "explanation": "<strong>Cognito User Pools</strong> - Use Cognito User Pools to create customizable authentication and authorization solutions for your REST APIs."
      }
    ],
    "references": [
      "https://aws.amazon.com/api-gateway/"
    ]
  },
  {
    "id": 26,
    "question": "<p>You would like to retrieve a subset of your dataset stored in S3 with the CSV format. You would like to retrieve a month of data and only 3 columns out of the 10.</p>\n\n<p>You need to minimize compute and network costs for this, what should you use?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>S3 Select</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>S3 Analytics</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>S3 Inventory</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>S3 Access Logs</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>S3 Select</strong></p>\n\n<p>S3 Select enables applications to retrieve only a subset of data from an object by using simple SQL expressions. By using S3 Select to retrieve only the data needed by your application, you can achieve drastic performance increases in many cases you can get as much as a 400% improvement.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q19-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q19-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/aws/s3-glacier-select/\">https://aws.amazon.com/blogs/aws/s3-glacier-select/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>S3 Inventory</strong> - Amazon S3 inventory is one of the tools Amazon S3 provides to help manage your storage. You can use it to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs.</p>\n\n<p><strong>S3 Analytics</strong> - By using Amazon S3 analytics storage class analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class.</p>\n\n<p><strong>S3 Access Logs</strong> - Server access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/s3-glacier-select/\">https://aws.amazon.com/blogs/aws/s3-glacier-select/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>S3 Select</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "S3 Select enables applications to retrieve only a subset of data from an object by using simple SQL expressions. By using S3 Select to retrieve only the data needed by your application, you can achieve drastic performance increases in many cases you can get as much as a 400% improvement."
      },
      {
        "link": "https://aws.amazon.com/blogs/aws/s3-glacier-select/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>S3 Inventory</strong> - Amazon S3 inventory is one of the tools Amazon S3 provides to help manage your storage. You can use it to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs."
      },
      {
        "answer": "",
        "explanation": "<strong>S3 Analytics</strong> - By using Amazon S3 analytics storage class analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class."
      },
      {
        "answer": "",
        "explanation": "<strong>S3 Access Logs</strong> - Server access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/aws/s3-glacier-select/"
    ]
  },
  {
    "id": 27,
    "question": "<p>Your company likes to operate multiple AWS accounts so that teams have their environments. Services deployed across these accounts interact with one another, and now there's a requirement to implement X-Ray traces across all your applications deployed on EC2 instances and AWS accounts.</p>\n\n<p>As such, you would like to have a unified account to view all the traces. What should you in your X-Ray daemon set up to make this work? (Select two)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure the X-Ray daemon to use access and secret keys</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable Cross Account collection in the X-Ray console</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a user in the target unified account and generate access and secret keys</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure the X-Ray daemon to use an IAM instance role</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Create a role in the target unified account and allow roles in each sub-account to assume the role.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a><p></p>\n\n<p><strong>Create a role in the target unified account and allow roles in each sub-account to assume the role</strong></p>\n\n<p><strong>Configure the X-Ray daemon to use an IAM instance role</strong></p>\n\n<p>The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account.</p>\n\n<p>X-Ray can also track requests flowing through applications or services across multiple AWS Regions.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q27-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q27-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/xray/faqs/\">https://aws.amazon.com/xray/faqs/</a><p></p>\n\n<p>You can create the necessary configurations for cross-account access via this reference documentation -\n<a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a user in the target unified account and generate access and secret keys</strong></p>\n\n<p><strong>Configure the X-Ray daemon to use access and secret keys</strong></p>\n\n<p>These two options combined together would work but wouldn't be a best-practice security-wise. Therefore these are not correct.</p>\n\n<p><strong>Enable Cross Account collection in the X-Ray console</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/xray/faqs/\">https://aws.amazon.com/xray/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png",
        "answer": "",
        "explanation": "How X-Ray Works:"
      },
      {
        "link": "https://aws.amazon.com/xray/"
      },
      {
        "answer": "",
        "explanation": "<strong>Create a role in the target unified account and allow roles in each sub-account to assume the role</strong>"
      },
      {
        "answer": "<strong>Configure the X-Ray daemon to use an IAM instance role</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account."
      },
      {
        "answer": "",
        "explanation": "X-Ray can also track requests flowing through applications or services across multiple AWS Regions."
      },
      {
        "link": "https://aws.amazon.com/xray/faqs/"
      },
      {
        "link": "https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html",
        "answer": "",
        "explanation": "You can create the necessary configurations for cross-account access via this reference documentation -\n<a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html</a>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Create a user in the target unified account and generate access and secret keys</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Configure the X-Ray daemon to use access and secret keys</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These two options combined together would work but wouldn't be a best-practice security-wise. Therefore these are not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable Cross Account collection in the X-Ray console</strong> - This is a made-up option and has been added as a distractor."
      }
    ],
    "references": [
      "https://aws.amazon.com/xray/",
      "https://aws.amazon.com/xray/faqs/",
      "https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html"
    ]
  },
  {
    "id": 28,
    "question": "<p>One of your Kinesis Stream is experiencing increased traffic due to a sale day. Therefore your Kinesis Administrator has split shards and thus you went from having 6 shards to having 10 shards in your Kinesis Stream. Your consuming application is running a KCL-based application on EC2 instances.</p>\n\n<p>What is the maximum number of EC2 instances that can be deployed to process the shards?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>10</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>20</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>1</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>6</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>10</strong></p>\n\n<p>Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs.</p>\n\n<p>A Kinesis data stream is a set of shards. A shard is a uniquely identified sequence of data records in a stream. A stream is composed of one or more shards, each of which provides a fixed unit of capacity.</p>\n\n<p>Kinesis Data Streams Overview:\n<img src=\"https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a><p></p>\n\n<p>Each KCL consumer application instance uses \"workers\" to process data in Kinesis shards.  At any given time, each shard of data records is bound to a particular worker via a lease. For the given use-case, an EC2 instance acts as the worker for the KCL application.  You can have at most one EC2 instance per shard in Kinesis for the given application. As we have 10 shards, the max number of EC2 instances is 10.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q21-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q21-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html\">https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>1</strong></p>\n\n<p><strong>6</strong></p>\n\n<p><strong>20</strong></p>\n\n<p>These three options contradict the explanation provided earlier. So these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-kcl.html\">https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-kcl.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>10</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs."
      },
      {
        "answer": "",
        "explanation": "A Kinesis data stream is a set of shards. A shard is a uniquely identified sequence of data records in a stream. A stream is composed of one or more shards, each of which provides a fixed unit of capacity."
      },
      {
        "image": "https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png",
        "answer": "",
        "explanation": "Kinesis Data Streams Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html"
      },
      {
        "answer": "",
        "explanation": "Each KCL consumer application instance uses \"workers\" to process data in Kinesis shards.  At any given time, each shard of data records is bound to a particular worker via a lease. For the given use-case, an EC2 instance acts as the worker for the KCL application.  You can have at most one EC2 instance per shard in Kinesis for the given application. As we have 10 shards, the max number of EC2 instances is 10."
      },
      {
        "link": "https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>1</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>6</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>20</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided earlier. So these are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html",
      "https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html",
      "https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-kcl.html"
    ]
  },
  {
    "id": 29,
    "question": "<p>Your team lead has finished creating a CodeBuild project in the management console and a build spec has been defined for the project. After the build is run, CodeBuild fails to pull a Docker image into the build environment.</p>\n\n<p>What is the most likely cause?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Missing IAM permissions for the CodeBuild Service</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>The Docker image is too big</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The Docker image is missing some tags</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>CodeBuild cannot work with custom Docker images</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Missing IAM permissions for the CodeBuild Service</strong></p>\n\n<p>By default, IAM users don't have permission to create or modify Amazon Elastic Container Registry (Amazon ECR) resources or perform tasks using the Amazon ECR API. A user who uses the AWS CodeBuild console must have a minimum set of permissions that allows the user to describe other AWS resources for the AWS account.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q46-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q46-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Docker image is missing some tags</strong> - Tags are optional for naming purposes</p>\n\n<p><strong>CodeBuild cannot work with custom Docker images</strong> - Custom docker images are supported, so this option is incorrect.</p>\n\n<p><strong>The Docker image is too big</strong> - It is good to properly design the image but in this case, it does not affect the CodeBuild. You can also look at multi-stage builds, which are a new feature requiring Docker 17.05 or higher on the daemon and client. Multistage builds are useful to anyone who has struggled to optimize Dockerfiles while keeping them easy to read and maintain.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Missing IAM permissions for the CodeBuild Service</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "By default, IAM users don't have permission to create or modify Amazon Elastic Container Registry (Amazon ECR) resources or perform tasks using the Amazon ECR API. A user who uses the AWS CodeBuild console must have a minimum set of permissions that allows the user to describe other AWS resources for the AWS account."
      },
      {
        "link": "https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The Docker image is missing some tags</strong> - Tags are optional for naming purposes"
      },
      {
        "answer": "",
        "explanation": "<strong>CodeBuild cannot work with custom Docker images</strong> - Custom docker images are supported, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>The Docker image is too big</strong> - It is good to properly design the image but in this case, it does not affect the CodeBuild. You can also look at multi-stage builds, which are a new feature requiring Docker 17.05 or higher on the daemon and client. Multistage builds are useful to anyone who has struggled to optimize Dockerfiles while keeping them easy to read and maintain."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html"
    ]
  },
  {
    "id": 30,
    "question": "<p>A company has a new media application that utilizes an Amazon CloudFront distribution that accesses the S3 bucket by using an origin access identity (OAI). The S3 bucket has an explicit access denial for all other users. A developer wants to allow access to the login page for unauthenticated users while ensuring the security of all private content that has restricted viewer access.</p>\n\n<p>Which of the following will you recommend?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure a second cache behavior to the distribution having the same origin as the default cache behavior and have the path pattern for the second cache behavior as * with viewer access as restricted. Modify the default cache behavior’s path pattern to the path of the login page and have the viewer access as unrestricted</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure a second origin as the failover origin for the default behavior of the original distribution and have the path pattern for the second origin as the path of the login page with viewer access as unrestricted. Keep the behavior for the primary origin unchanged</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure a new distribution having the same origin as the original distribution and set the path pattern for the default cache behavior of the new distribution as the path of the login page with viewer access as unrestricted. Keep the default cache behavior of the original distribution unchanged</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure a second cache behavior to the distribution having the same origin as the default cache behavior and have the path pattern for the second cache behavior as the path of the login page with viewer access as unrestricted. Keep the default cache behavior’s settings unchanged</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a second cache behavior to the distribution having the same origin as the default cache behavior and have the path pattern for the second cache behavior as the path of the login page with viewer access as unrestricted. Keep the default cache behavior’s settings unchanged</strong></p>\n\n<p>Cache behavior describes how CloudFront processes requests. You must create at least as many cache behaviors (including the default cache behavior) as you have origins if you want CloudFront to serve objects from all of the origins. Each cache behavior specifies the one origin from which you want CloudFront to get objects. If you have two origins and only the default cache behavior, the default cache behavior will cause CloudFront to get objects from one of the origins, but the other origin is never used.</p>\n\n<p>The pattern (for example, images/*.jpg) specifies which requests to apply the behavior to. When CloudFront receives a viewer request, the requested path is compared with path patterns in the order in which cache behaviors are listed in the distribution. The path pattern for the default cache behavior is * and cannot be changed. If the request for an object does not match the path pattern for any cache behaviors, CloudFront applies the behavior in the default cache behavior.</p>\n\n<p>For the given use case, you need to add a second cache behavior to the distribution having the same origin as the default cache behavior and list it above the default cache behavior in the distribution. The second cache behavior should have the path pattern as the path of the login page with viewer access set as unrestricted. This would allow access to the login page for unauthenticated users. Since the default cache behavior’s settings remain unchanged, it ensures the security of all private content that continues to have restricted viewer access.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a second cache behavior to the distribution having the same origin as the default cache behavior and have the path pattern for the second cache behavior as * with viewer access as restricted. Modify the default cache behavior’s path pattern to the path of the login page and have the viewer access as unrestricted</strong> - This option is incorrect since the path pattern for the default cache behavior is always * and cannot be changed.</p>\n\n<p><strong>Configure a new distribution having the same origin as the original distribution and set the path pattern for the default cache behavior of the new distribution as the path of the login page with viewer access as unrestricted. Keep the default cache behavior of the original distribution unchanged</strong> - If you have two origins and only the default cache behavior, the default cache behavior will cause CloudFront to get objects from one of the origins, but the other origin is never used. So this option is incorrect.</p>\n\n<p><strong>Configure a second origin as the failover origin for the default behavior of the original distribution and have the path pattern for the second origin as the path of the login page with viewer access as unrestricted. Keep the behavior for the primary origin unchanged</strong> - You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin. To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Finally, you create or update a cache behavior to use the origin group.</p>\n\n<p>This option is incorrect since the failover kicks in only when the primary is unavailable. Therefore, access to the login page and the rest of the content will never work together.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CacheBehavior.html\">https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CacheBehavior.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Configure a second cache behavior to the distribution having the same origin as the default cache behavior and have the path pattern for the second cache behavior as the path of the login page with viewer access as unrestricted. Keep the default cache behavior’s settings unchanged</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Cache behavior describes how CloudFront processes requests. You must create at least as many cache behaviors (including the default cache behavior) as you have origins if you want CloudFront to serve objects from all of the origins. Each cache behavior specifies the one origin from which you want CloudFront to get objects. If you have two origins and only the default cache behavior, the default cache behavior will cause CloudFront to get objects from one of the origins, but the other origin is never used."
      },
      {
        "answer": "",
        "explanation": "The pattern (for example, images/*.jpg) specifies which requests to apply the behavior to. When CloudFront receives a viewer request, the requested path is compared with path patterns in the order in which cache behaviors are listed in the distribution. The path pattern for the default cache behavior is * and cannot be changed. If the request for an object does not match the path pattern for any cache behaviors, CloudFront applies the behavior in the default cache behavior."
      },
      {
        "answer": "",
        "explanation": "For the given use case, you need to add a second cache behavior to the distribution having the same origin as the default cache behavior and list it above the default cache behavior in the distribution. The second cache behavior should have the path pattern as the path of the login page with viewer access set as unrestricted. This would allow access to the login page for unauthenticated users. Since the default cache behavior’s settings remain unchanged, it ensures the security of all private content that continues to have restricted viewer access."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure a second cache behavior to the distribution having the same origin as the default cache behavior and have the path pattern for the second cache behavior as * with viewer access as restricted. Modify the default cache behavior’s path pattern to the path of the login page and have the viewer access as unrestricted</strong> - This option is incorrect since the path pattern for the default cache behavior is always * and cannot be changed."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a new distribution having the same origin as the original distribution and set the path pattern for the default cache behavior of the new distribution as the path of the login page with viewer access as unrestricted. Keep the default cache behavior of the original distribution unchanged</strong> - If you have two origins and only the default cache behavior, the default cache behavior will cause CloudFront to get objects from one of the origins, but the other origin is never used. So this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure a second origin as the failover origin for the default behavior of the original distribution and have the path pattern for the second origin as the path of the login page with viewer access as unrestricted. Keep the behavior for the primary origin unchanged</strong> - You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin. To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Finally, you create or update a cache behavior to use the origin group."
      },
      {
        "answer": "",
        "explanation": "This option is incorrect since the failover kicks in only when the primary is unavailable. Therefore, access to the login page and the rest of the content will never work together."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CacheBehavior.html",
      "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html"
    ]
  },
  {
    "id": 31,
    "question": "<p>A company's e-commerce application becomes slow when traffic spikes. The application has a three-tier architecture (web, application and database tier) that uses synchronous transactions. The development team at the company has identified certain bottlenecks in the application tier and it is looking for a long term solution to improve the application's performance.</p>\n\n<p>As a developer associate, which of the following solutions would you suggest to meet the required application response times while accounting for any traffic spikes?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage SQS with asynchronous AWS Lambda calls to decouple the application and data tiers</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer</strong> - A horizontally scalable system is one that can increase capacity by adding more computers to the system. This is in contrast to a vertically scalable system, which is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory or storage.</p>\n\n<p>Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different computers.</p>\n\n<p>Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed.</p>\n\n<p>To use Elastic Load Balancing with your Auto Scaling group, you attach the load balancer to your Auto Scaling group to register the group with the load balancer. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group.</p>\n\n<p>When you use Elastic Load Balancing with your Auto Scaling group, it's not necessary to register individual EC2 instances with the load balancer. Instances that are launched by your Auto Scaling group are automatically registered with the load balancer. Likewise, instances that are terminated by your Auto Scaling group are automatically deregistered from the load balancer.</p>\n\n<p>This option will require fewer design changes, it's mostly configuration changes and the ability for the web/application tier to be able to communicate across instances. Hence, this is the right solution for the current use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage SQS with asynchronous AWS Lambda calls to decouple the application and data tiers</strong> - This is incorrect as it uses asynchronous AWS Lambda calls and the application uses synchronous transactions. The question says there should be no change in the application architecture.</p>\n\n<p><strong>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS</strong> - The issue is not with the persistence layer at all. This option has only been used as a distractor.</p>\n\n<p>You can deploy scalable Oracle Real Application Clusters (RAC) on Amazon EC2 using Amazon Machine Images (AMI) on AWS Marketplace. Oracle RAC is a shared-everything database cluster technology from Oracle that allows a single database (a set of data files) to be concurrently accessed and served by one or many database server instances.</p>\n\n<p><strong>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size</strong> - Vertical scaling is just a band-aid solution and will not work long term.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/\">https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer</strong> - A horizontally scalable system is one that can increase capacity by adding more computers to the system. This is in contrast to a vertically scalable system, which is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory or storage."
      },
      {
        "answer": "",
        "explanation": "Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different computers."
      },
      {
        "answer": "",
        "explanation": "Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed."
      },
      {
        "answer": "",
        "explanation": "To use Elastic Load Balancing with your Auto Scaling group, you attach the load balancer to your Auto Scaling group to register the group with the load balancer. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group."
      },
      {
        "answer": "",
        "explanation": "When you use Elastic Load Balancing with your Auto Scaling group, it's not necessary to register individual EC2 instances with the load balancer. Instances that are launched by your Auto Scaling group are automatically registered with the load balancer. Likewise, instances that are terminated by your Auto Scaling group are automatically deregistered from the load balancer."
      },
      {
        "answer": "",
        "explanation": "This option will require fewer design changes, it's mostly configuration changes and the ability for the web/application tier to be able to communicate across instances. Hence, this is the right solution for the current use case."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Leverage SQS with asynchronous AWS Lambda calls to decouple the application and data tiers</strong> - This is incorrect as it uses asynchronous AWS Lambda calls and the application uses synchronous transactions. The question says there should be no change in the application architecture."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS</strong> - The issue is not with the persistence layer at all. This option has only been used as a distractor."
      },
      {
        "answer": "",
        "explanation": "You can deploy scalable Oracle Real Application Clusters (RAC) on Amazon EC2 using Amazon Machine Images (AMI) on AWS Marketplace. Oracle RAC is a shared-everything database cluster technology from Oracle that allows a single database (a set of data files) to be concurrently accessed and served by one or many database server instances."
      },
      {
        "answer": "",
        "explanation": "<strong>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size</strong> - Vertical scaling is just a band-aid solution and will not work long term."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html",
      "https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/"
    ]
  },
  {
    "id": 32,
    "question": "<p>A financial services company has developed a REST API which is deployed in an Auto Scaling Group behind an Application Load Balancer. The API stores the data payload in DynamoDB and the static content is served through S3. Upon analyzing the usage pattern, it's found that 80% of the read requests are shared across all users.</p>\n\n<p>As a Developer Associate, how can you improve the application performance while optimizing the cost with the least development effort?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Enable DAX for DynamoDB and ElastiCache Memcached for S3</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</strong></p>\n\n<p>DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second.</p>\n\n<p>DAX is tightly integrated with DynamoDB—you simply provision a DAX cluster, use the DAX client SDK to point your existing DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache DynamoDB reads.</p>\n\n<p>CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content.</p>\n\n<p>So, you can use CloudFront to improve application performance to serve static content from S3.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.</p>\n\n<p>ElastiCache for Redis Overview:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a><p></p>\n\n<p>Although, you can integrate Redis with DynamoDB, it's much more involved from a development perspective. For the given use-case, you should use DAX which is a much better fit.</p>\n\n<p><strong>Enable DAX for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database.</p>\n\n<p>ElastiCache cannot be used as a cache to serve static content from S3, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second."
      },
      {
        "answer": "",
        "explanation": "DAX is tightly integrated with DynamoDB—you simply provision a DAX cluster, use the DAX client SDK to point your existing DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache DynamoDB reads."
      },
      {
        "answer": "",
        "explanation": "CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users."
      },
      {
        "answer": "",
        "explanation": "When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content."
      },
      {
        "answer": "",
        "explanation": "So, you can use CloudFront to improve application performance to serve static content from S3."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store."
      },
      {
        "image": "https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png",
        "answer": "",
        "explanation": "ElastiCache for Redis Overview:"
      },
      {
        "link": "https://aws.amazon.com/elasticache/redis/"
      },
      {
        "answer": "",
        "explanation": "Although, you can integrate Redis with DynamoDB, it's much more involved from a development perspective. For the given use-case, you should use DAX which is a much better fit."
      },
      {
        "answer": "<strong>Enable DAX for DynamoDB and ElastiCache Memcached for S3</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database."
      },
      {
        "answer": "",
        "explanation": "ElastiCache cannot be used as a cache to serve static content from S3, so both these options are incorrect."
      }
    ],
    "references": [
      "https://aws.amazon.com/elasticache/redis/",
      "https://aws.amazon.com/dynamodb/dax/",
      "https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/"
    ]
  },
  {
    "id": 33,
    "question": "<p>The customer feedback functionality for a company's flagship web application is handled via an Amazon API Gateway based REST API that invokes an AWS Lambda function for further processing. Although the performance of the function is satisfactory, the development team has been tasked to optimize the startup time of the Lambda function to further improve the customer experience.</p>\n\n<p>How will you optimize the Lambda function for faster initialization?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure reserved concurrency to guarantee the maximum number of concurrent instances of the Lambda function</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure provisioned concurrency for the Lambda function to respond immediately to the function's invocations</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure an interface VPC endpoint powered by AWS PrivateLink to access the Amazon API Gateway REST API with milliseconds latency</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable API caching in Amazon API Gateway to cache AWS Lambda function response</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure provisioned concurrency for the Lambda function to respond immediately to the function's invocations</strong></p>\n\n<p>When Lambda allocates an instance of your function, the runtime loads your function's code and runs the initialization code that you define outside of the handler. If your code and dependencies are large, or you create SDK clients during initialization, this process can take some time. When your function has not been used for some time, needs to scale up, or when you update a function, Lambda creates new execution environments. This causes the portion of requests that are served by new instances to have higher latency than the rest, otherwise known as a cold start.</p>\n\n<p>By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with low latency. Lambda functions configured with provisioned concurrency run with consistent start-up latency, making them ideal for building interactive mobile or web backends, latency-sensitive microservices, and synchronously invoked APIs.</p>\n\n<p>Functions with Provisioned Concurrency differ from on-demand functions in some important ways:</p>\n\n<ol>\n<li><p>Initialization code does not need to be optimized. Since this happens long before the invocation, lengthy initialization does not impact the latency of invocations. If you are using runtimes that typically take longer to initialize, like Java, the performance of these can benefit from using Provisioned Concurrency.</p></li>\n<li><p>Initialization code is run more frequently than the total number of invocations. Since Lambda is highly available, for every one unit of Provisioned Concurrency, there are a minimum of two execution environments prepared in separate Availability Zones. This is to ensure that your code is available in the event of a service disruption. As environments are reaped and load balancing occurs, Lambda over-provisions environments to ensure availability. You are not charged for this activity. If your code initializer implements logging, you will see additional log files anytime that this code is run, even though the main handler is not invoked.</p></li>\n<li><p>Provisioned Concurrency cannot be used with the $LATEST version. This feature can only be used with published versions and aliases of a function. If you see cold starts for functions configured to use Provisioned Concurrency, you may be invoking the $LATEST version, instead of the version or alias with Provisioned Concurrency configured.</p></li>\n</ol>\n\n<p>Reducing cold starts with Provisioned Concurrency:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q14-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q14-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/\">https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure reserved concurrency to guarantee the maximum number of concurrent instances of the Lambda function</strong> - Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can use that concurrency. There is no charge for configuring reserved concurrency for a function. Whereas, provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations.</p>\n\n<p><strong>Configure an interface VPC endpoint powered by AWS PrivateLink to access the Amazon API Gateway REST API with milliseconds latency</strong> - An interface VPC endpoint can be used to connect your VPC resources to the AWS Lambda function without crossing the public internet. VPC endpoint is irrelevant to the current discussion.</p>\n\n<p><strong>Enable API caching in Amazon API Gateway to cache AWS Lambda function response</strong> - With caching, you can reduce the number of calls made to your AWS Lambda function and also improve the latency of requests to your API. Caching is best-effort and applications making frequent API calls to retrieve static data can benefit from a caching layer. Caching does not reduce the initialization time Lambda takes and hence is not an optimal solution for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/\">https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Configure provisioned concurrency for the Lambda function to respond immediately to the function's invocations</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "When Lambda allocates an instance of your function, the runtime loads your function's code and runs the initialization code that you define outside of the handler. If your code and dependencies are large, or you create SDK clients during initialization, this process can take some time. When your function has not been used for some time, needs to scale up, or when you update a function, Lambda creates new execution environments. This causes the portion of requests that are served by new instances to have higher latency than the rest, otherwise known as a cold start."
      },
      {
        "answer": "",
        "explanation": "By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with low latency. Lambda functions configured with provisioned concurrency run with consistent start-up latency, making them ideal for building interactive mobile or web backends, latency-sensitive microservices, and synchronously invoked APIs."
      },
      {
        "answer": "",
        "explanation": "Functions with Provisioned Concurrency differ from on-demand functions in some important ways:"
      },
      {},
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q14-i1.jpg",
        "answer": "",
        "explanation": "Reducing cold starts with Provisioned Concurrency:"
      },
      {
        "link": "https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure reserved concurrency to guarantee the maximum number of concurrent instances of the Lambda function</strong> - Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can use that concurrency. There is no charge for configuring reserved concurrency for a function. Whereas, provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure an interface VPC endpoint powered by AWS PrivateLink to access the Amazon API Gateway REST API with milliseconds latency</strong> - An interface VPC endpoint can be used to connect your VPC resources to the AWS Lambda function without crossing the public internet. VPC endpoint is irrelevant to the current discussion."
      },
      {
        "answer": "",
        "explanation": "<strong>Enable API caching in Amazon API Gateway to cache AWS Lambda function response</strong> - With caching, you can reduce the number of calls made to your AWS Lambda function and also improve the latency of requests to your API. Caching is best-effort and applications making frequent API calls to retrieve static data can benefit from a caching layer. Caching does not reduce the initialization time Lambda takes and hence is not an optimal solution for this use case."
      }
    ],
    "references": [
      "https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/",
      "https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html",
      "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html"
    ]
  },
  {
    "id": 34,
    "question": "<p>You are deploying Lambda functions that operate on your S3 buckets to read files and extract key metadata. The Lambda functions are managed using SAM.</p>\n\n<p>Which Policy should you insert in your serverless model template to give buckets read access?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>LambdaInvokePolicy</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>S3ReadPolicy</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>SQSPollerPolicy</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>S3CrudPolicy</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>S3ReadPolicy</strong></p>\n\n<p>The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications on AWS.</p>\n\n<p>A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda function—it can include additional resources such as APIs, databases, and event source mappings.</p>\n\n<p>AWS SAM allows you to choose from a list of policy templates to scope the permissions of your Lambda functions to the resources that are used by your application.</p>\n\n<p>AWS SAM applications in the AWS Serverless Application Repository that use policy templates don't require any special customer acknowledgments to deploy the application from the AWS Serverless Application Repository.</p>\n\n<p>S3ReadPolicy =&gt; Gives read-only permission to objects in an Amazon S3 bucket.</p>\n\n<p>S3CrudPolicy =&gt; Gives create, read, update, and delete permission to objects in an Amazon S3 bucket.</p>\n\n<p>SQSPollerPolicy =&gt; Permits to poll an Amazon SQS Queue.</p>\n\n<p>LambdaInvokePolicy =&gt; Permits to invoke a Lambda function, alias, or version.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SQSPollerPolicy</strong></p>\n\n<p><strong>S3CrudPolicy</strong></p>\n\n<p><strong>LambdaInvokePolicy</strong></p>\n\n<p>These three options contradict the explanation provided earlier. So these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-policy-templates.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-policy-templates.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>S3ReadPolicy</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications on AWS."
      },
      {
        "answer": "",
        "explanation": "A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda function—it can include additional resources such as APIs, databases, and event source mappings."
      },
      {
        "answer": "",
        "explanation": "AWS SAM allows you to choose from a list of policy templates to scope the permissions of your Lambda functions to the resources that are used by your application."
      },
      {
        "answer": "",
        "explanation": "AWS SAM applications in the AWS Serverless Application Repository that use policy templates don't require any special customer acknowledgments to deploy the application from the AWS Serverless Application Repository."
      },
      {
        "answer": "",
        "explanation": "S3ReadPolicy =&gt; Gives read-only permission to objects in an Amazon S3 bucket."
      },
      {
        "answer": "",
        "explanation": "S3CrudPolicy =&gt; Gives create, read, update, and delete permission to objects in an Amazon S3 bucket."
      },
      {
        "answer": "",
        "explanation": "SQSPollerPolicy =&gt; Permits to poll an Amazon SQS Queue."
      },
      {
        "answer": "",
        "explanation": "LambdaInvokePolicy =&gt; Permits to invoke a Lambda function, alias, or version."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>SQSPollerPolicy</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>S3CrudPolicy</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>LambdaInvokePolicy</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three options contradict the explanation provided earlier. So these are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-policy-templates.html"
    ]
  },
  {
    "id": 35,
    "question": "<p>An e-commerce application posts its order transactions in bulk to an accounting application for further processing. Due to changes in the compliance rules, all the transactions are being encrypted with AWS Key Management Service (AWS KMS) key before posting to the accounting application. Post this change, the testers have raised tickets regarding the application receiving a ThrottlingException error.</p>\n\n<p>What measures should a developer take to fix this issue MOST optimally? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the data key caching feature with the AWS Encryption SDK encryption library</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use a bucket-level key for SSE-KMS which will decrease the requested traffic to AWS KMS thereby avoiding the ThrottlingException error</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Write queries in Amazon CloudWatch Logs Insights to track your API request usage and submit an AWS Support case to request a quota increase</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Send AWS CloudTrail events generated by AWS KMS to Amazon CloudWatch Logs</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Reduce the rate of requests and consider using the backoff and retry logic</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Security",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Reduce the rate of requests and consider using the backoff and retry logic</strong></p>\n\n<p>Each AWS SDK implements automatic retry logic. The AWS SDK for Java automatically retries requests, and you can configure the retry settings. In addition to simple retries, each AWS SDK implements an exponential backoff algorithm for better flow control. The idea behind exponential backoff is to use progressively longer waits between retries for consecutive error responses. You should implement a maximum delay interval, as well as a maximum number of retries. The maximum delay interval and the maximum number of retries are not necessarily fixed values and should be set based on the operation being performed, as well as other local factors, such as network latency.</p>\n\n<p><strong>Use the data key caching feature with the AWS Encryption SDK encryption library</strong></p>\n\n<p>Data key caching stores data keys and related cryptographic material in a cache. When you encrypt or decrypt data, the AWS Encryption SDK looks for a matching data key in the cache. If it finds a match, it uses the cached data key rather than generating a new one. Data key caching can improve performance, reduce cost, and help you stay within service limits as your application scales.</p>\n\n<p>Your application can benefit from data key caching if:\n1. It can reuse data keys.\n2. It generates numerous data keys.\n3. Your cryptographic operations are unacceptably slow, expensive, limited, or resource-intensive.</p>\n\n<p>Data key caching is an optional feature of the AWS Encryption SDK that you should use cautiously. By default, the AWS Encryption SDK generates a new data key for every encryption operation. This technique supports cryptographic best practices, which discourage excessive reuse of data keys. In general, use data key caching only when it is required to meet your performance goals. Then, use the data key caching security thresholds to ensure that you use the minimum amount of caching required to meet your cost and performance goals.</p>\n\n<p>Best practices to troubleshoot ThrottlingException errors:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q16-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q16-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/kms-throttlingexception-error/\">https://aws.amazon.com/premiumsupport/knowledge-center/kms-throttlingexception-error/</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Send AWS CloudTrail events generated by AWS KMS to Amazon CloudWatch Logs</strong> - Deeper analysis of CloudTrail data sent to CloudWatch logs can help spot throttled API calls. While it is certainly possible to track API usage using CloudTrail data, it is not an optimal way to fix the ThrottlingException error.</p>\n\n<p><strong>Write queries in Amazon CloudWatch Logs Insights to track your API request usage and submit an AWS Support case to request a quota increase</strong> - Historically, to understand how close to a request rate quota you were, you had to perform three tasks: (i) send AWS CloudTrail events generated by AWS KMS to Amazon CloudWatch Logs; (ii) write queries in Amazon CloudWatch Logs Insights to track your API request usage; and (iii) submit an AWS Support case to request a quota increase. Now, you can view your AWS KMS API usage and request quota increases within the AWS Service Quotas console itself without doing any special configuration.</p>\n\n<p><strong>Use a bucket-level key for SSE-KMS which will decrease the requested traffic to AWS KMS thereby avoiding the ThrottlingException error</strong> - Amazon S3 bucket has not been mentioned in the given use case and hence this option is irrelevant.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/general/latest/gr/api-retries.html\">https://docs.aws.amazon.com/general/latest/gr/api-retries.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/data-key-caching.html\">https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/data-key-caching.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/manage-your-aws-kms-api-request-rates-using-service-quotas-and-amazon-cloudwatch/\">https://aws.amazon.com/blogs/security/manage-your-aws-kms-api-request-rates-using-service-quotas-and-amazon-cloudwatch/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Reduce the rate of requests and consider using the backoff and retry logic</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Each AWS SDK implements automatic retry logic. The AWS SDK for Java automatically retries requests, and you can configure the retry settings. In addition to simple retries, each AWS SDK implements an exponential backoff algorithm for better flow control. The idea behind exponential backoff is to use progressively longer waits between retries for consecutive error responses. You should implement a maximum delay interval, as well as a maximum number of retries. The maximum delay interval and the maximum number of retries are not necessarily fixed values and should be set based on the operation being performed, as well as other local factors, such as network latency."
      },
      {
        "answer": "<strong>Use the data key caching feature with the AWS Encryption SDK encryption library</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Data key caching stores data keys and related cryptographic material in a cache. When you encrypt or decrypt data, the AWS Encryption SDK looks for a matching data key in the cache. If it finds a match, it uses the cached data key rather than generating a new one. Data key caching can improve performance, reduce cost, and help you stay within service limits as your application scales."
      },
      {
        "answer": "",
        "explanation": "Your application can benefit from data key caching if:\n1. It can reuse data keys.\n2. It generates numerous data keys.\n3. Your cryptographic operations are unacceptably slow, expensive, limited, or resource-intensive."
      },
      {
        "answer": "",
        "explanation": "Data key caching is an optional feature of the AWS Encryption SDK that you should use cautiously. By default, the AWS Encryption SDK generates a new data key for every encryption operation. This technique supports cryptographic best practices, which discourage excessive reuse of data keys. In general, use data key caching only when it is required to meet your performance goals. Then, use the data key caching security thresholds to ensure that you use the minimum amount of caching required to meet your cost and performance goals."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q16-i1.jpg",
        "answer": "",
        "explanation": "Best practices to troubleshoot ThrottlingException errors:"
      },
      {
        "link": "https://aws.amazon.com/premiumsupport/knowledge-center/kms-throttlingexception-error/"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Send AWS CloudTrail events generated by AWS KMS to Amazon CloudWatch Logs</strong> - Deeper analysis of CloudTrail data sent to CloudWatch logs can help spot throttled API calls. While it is certainly possible to track API usage using CloudTrail data, it is not an optimal way to fix the ThrottlingException error."
      },
      {
        "answer": "",
        "explanation": "<strong>Write queries in Amazon CloudWatch Logs Insights to track your API request usage and submit an AWS Support case to request a quota increase</strong> - Historically, to understand how close to a request rate quota you were, you had to perform three tasks: (i) send AWS CloudTrail events generated by AWS KMS to Amazon CloudWatch Logs; (ii) write queries in Amazon CloudWatch Logs Insights to track your API request usage; and (iii) submit an AWS Support case to request a quota increase. Now, you can view your AWS KMS API usage and request quota increases within the AWS Service Quotas console itself without doing any special configuration."
      },
      {
        "answer": "",
        "explanation": "<strong>Use a bucket-level key for SSE-KMS which will decrease the requested traffic to AWS KMS thereby avoiding the ThrottlingException error</strong> - Amazon S3 bucket has not been mentioned in the given use case and hence this option is irrelevant."
      }
    ],
    "references": [
      "https://aws.amazon.com/premiumsupport/knowledge-center/kms-throttlingexception-error/",
      "https://docs.aws.amazon.com/general/latest/gr/api-retries.html",
      "https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/data-key-caching.html",
      "https://aws.amazon.com/blogs/security/manage-your-aws-kms-api-request-rates-using-service-quotas-and-amazon-cloudwatch/"
    ]
  },
  {
    "id": 36,
    "question": "<p>As part of your video processing application, you are looking to perform a set of repetitive and scheduled tasks asynchronously. Your application is deployed on Elastic Beanstalk.</p>\n\n<p>Which Elastic Beanstalk environment should you set up for performing the repetitive tasks?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Setup a Web Server environment and a <code>cron.yaml</code> file</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Setup a Worker environment and a <code>cron.yaml</code> file</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Setup a Web Server environment and a <code>.ebextensions</code> file</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Setup a Worker environment and a <code>.ebextensions</code> file</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>Elastic BeanStalk Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q25-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q25-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a><p></p>\n\n<p><strong>Setup a Worker environment and a <code>cron.yaml</code> file</strong></p>\n\n<p>An environment is a collection of AWS resources running an application version. An environment that pulls tasks from an Amazon Simple Queue Service (Amazon SQS) queue runs in a worker environment tier.</p>\n\n<p>If your AWS Elastic Beanstalk application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load.</p>\n\n<p>For a worker environment, you need a <code>cron.yaml</code> file to define the cron jobs and do repetitive tasks.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q25-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q25-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Setup a Web Server environment and a <code>cron.yaml</code> file</strong></p>\n\n<p><strong>Setup a Worker environment and a <code>.ebextensions</code> file</strong></p>\n\n<p><strong>Setup a Web Server environment and a <code>.ebextensions</code> file</strong></p>\n\n<p><code>.ebextensions/</code> won't work to define cron jobs, and Web Server environments cannot be set up to perform repetitive and scheduled tasks. So these three options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q25-i1.jpg",
        "answer": "",
        "explanation": "Elastic BeanStalk Key Concepts:"
      },
      {
        "link": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html"
      },
      {
        "answer": "<strong>Setup a Worker environment and a <code>cron.yaml</code> file</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "An environment is a collection of AWS resources running an application version. An environment that pulls tasks from an Amazon Simple Queue Service (Amazon SQS) queue runs in a worker environment tier."
      },
      {
        "answer": "",
        "explanation": "If your AWS Elastic Beanstalk application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load."
      },
      {
        "answer": "",
        "explanation": "For a worker environment, you need a <code>cron.yaml</code> file to define the cron jobs and do repetitive tasks."
      },
      {
        "link": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Setup a Web Server environment and a <code>cron.yaml</code> file</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Setup a Worker environment and a <code>.ebextensions</code> file</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Setup a Web Server environment and a <code>.ebextensions</code> file</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "<code>.ebextensions/</code> won't work to define cron jobs, and Web Server environments cannot be set up to perform repetitive and scheduled tasks. So these three options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html"
    ]
  },
  {
    "id": 37,
    "question": "<p>You would like to run the X-Ray daemon for your Docker containers deployed using AWS Fargate.</p>\n\n<p>What do you need to do to ensure the setup will work? (Select two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy the X-Ray daemon agent as a daemon set on ECS</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy the X-Ray daemon agent as a sidecar container</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Deploy the X-Ray daemon agent as a process on your EC2 instance</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Provide the correct IAM instance role to the EC2 instance</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Provide the correct IAM task role to the X-Ray container</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Deploy the X-Ray daemon agent as a sidecar container</strong></p>\n\n<p>In Amazon ECS, create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file to allow your application to communicate with the daemon container.</p>\n\n<p>As we are using AWS Fargate, we do not have control over the underlying EC2 instance and thus we can't deploy the agent on the EC2 instance or run an X-Ray agent container as a daemon set (only available for ECS classic).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q17-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q17-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html</a><p></p>\n\n<p><strong>Provide the correct IAM task role to the X-Ray container</strong></p>\n\n<p>For Fargate, we can only provide IAM roles to tasks, which is also the best security practice should we use EC2 instances.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy the X-Ray daemon agent as a daemon set on ECS</strong> - As explained above, since we are using AWS Fargate, we do not have control over the underlying EC2 instance and thus we can't run an X-Ray agent container as a daemon set.</p>\n\n<p><strong>Deploy the X-Ray daemon agent as a process on your EC2 instance</strong></p>\n\n<p><strong>Provide the correct IAM instance role to the EC2 instance</strong></p>\n\n<p>As we are using AWS Fargate, we do not have control over the underlying EC2 instance, so both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Deploy the X-Ray daemon agent as a sidecar container</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "In Amazon ECS, create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file to allow your application to communicate with the daemon container."
      },
      {
        "answer": "",
        "explanation": "As we are using AWS Fargate, we do not have control over the underlying EC2 instance and thus we can't deploy the agent on the EC2 instance or run an X-Ray agent container as a daemon set (only available for ECS classic)."
      },
      {
        "link": "https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html"
      },
      {
        "answer": "<strong>Provide the correct IAM task role to the X-Ray container</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "For Fargate, we can only provide IAM roles to tasks, which is also the best security practice should we use EC2 instances."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Deploy the X-Ray daemon agent as a daemon set on ECS</strong> - As explained above, since we are using AWS Fargate, we do not have control over the underlying EC2 instance and thus we can't run an X-Ray agent container as a daemon set."
      },
      {
        "answer": "<strong>Deploy the X-Ray daemon agent as a process on your EC2 instance</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Provide the correct IAM instance role to the EC2 instance</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As we are using AWS Fargate, we do not have control over the underlying EC2 instance, so both these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html"
    ]
  },
  {
    "id": 38,
    "question": "<p>You are looking to invoke an AWS Lambda function every hour (similar to a cron job) in a serverless way.</p>\n\n<p>Which event source should you use for your AWS Lambda function?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>CloudWatch Events</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon S3</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Kinesis</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SQS</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>CloudWatch Events</strong></p>\n\n<p>You can create a Lambda function and direct CloudWatch Events to execute it on a regular schedule. You can specify a fixed rate (for example, execute a Lambda function every hour or 15 minutes), or you can specify a Cron expression.</p>\n\n<p>CloudWatch Events Key Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q38-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q38-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a><p></p>\n\n<p>Schedule Expressions for CloudWatch Events Rules:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q38-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q38-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3</strong></p>\n\n<p><strong>SQS</strong></p>\n\n<p><strong>Kinesis</strong></p>\n\n<p>These three AWS services don't have cron capabilities, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>CloudWatch Events</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can create a Lambda function and direct CloudWatch Events to execute it on a regular schedule. You can specify a fixed rate (for example, execute a Lambda function every hour or 15 minutes), or you can specify a Cron expression."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q38-i1.jpg",
        "answer": "",
        "explanation": "CloudWatch Events Key Concepts:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q38-i2.jpg",
        "answer": "",
        "explanation": "Schedule Expressions for CloudWatch Events Rules:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Amazon S3</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>SQS</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Kinesis</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "These three AWS services don't have cron capabilities, so these options are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html",
      "https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html"
    ]
  },
  {
    "id": 39,
    "question": "<p>You are creating a web application in which users can follow each other. Some users will be more popular than others and thus their data will be requested very often. Currently, the user data sits in RDS and it has been recommended by your Developer to use ElastiCache as a caching layer to improve the read performance. The whole dataset of users cannot sit in ElastiCache without incurring tremendous costs and therefore you would like to cache only the most often requested users profiles there. As your website is high traffic, it is accepted to have stale data for users for a while, as long as the stale data is less than a minute old.</p>\n\n<p>What caching strategy do you recommend implementing?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a Lazy Loading strategy without TTL</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use a Write Through strategy without TTL</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use a Write Through strategy with TTL</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a Lazy Loading strategy with TTL</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option</p>\n\n<p><strong>Use a Lazy Loading strategy with TTL</strong></p>\n\n<p>Lazy loading is a caching strategy that loads data into the cache only when necessary. Whenever your application requests data, it first requests the ElastiCache cache. If the data exists in the cache and is current, ElastiCache returns the data to your application. If the data doesn't exist in the cache or has expired, your application requests the data from your data store. Your datastore then returns the data to your application.</p>\n\n<p>In this case, data that is actively requested by users will be cached in ElastiCache, and thanks to the TTL, we can expire that data after a minute to limit the data staleness.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q42-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q42-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading</a><p></p>\n\n<p>Incorrect option:</p>\n\n<p><strong>Use a Lazy Loading strategy without TTL</strong> - This fits the read requirements, but won't help expiring stale data, so we need TTL.</p>\n\n<p><strong>Use a Write Through strategy with TTL</strong></p>\n\n<p><strong>Use a Write Through strategy without TTL</strong></p>\n\n<p>The problem with these two options for the write-through strategy is that we would fill up the cache with unnecessary data and as mentioned in the question we don't have enough space in the cache to fit all the dataset. Therefore we can't use a write-through strategy.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use a Lazy Loading strategy with TTL</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Lazy loading is a caching strategy that loads data into the cache only when necessary. Whenever your application requests data, it first requests the ElastiCache cache. If the data exists in the cache and is current, ElastiCache returns the data to your application. If the data doesn't exist in the cache or has expired, your application requests the data from your data store. Your datastore then returns the data to your application."
      },
      {
        "answer": "",
        "explanation": "In this case, data that is actively requested by users will be cached in ElastiCache, and thanks to the TTL, we can expire that data after a minute to limit the data staleness."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use a Lazy Loading strategy without TTL</strong> - This fits the read requirements, but won't help expiring stale data, so we need TTL."
      },
      {
        "answer": "<strong>Use a Write Through strategy with TTL</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Use a Write Through strategy without TTL</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The problem with these two options for the write-through strategy is that we would fill up the cache with unnecessary data and as mentioned in the question we don't have enough space in the cache to fit all the dataset. Therefore we can't use a write-through strategy."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.LazyLoading"
    ]
  },
  {
    "id": 40,
    "question": "<p>Your organization has set up a full CI/CD pipeline leveraging CodePipeline and the deployment is done on Elastic Beanstalk. This pipeline has worked for over a year now but you are approaching the limits of Elastic Beanstalk in terms of how many versions can be stored in the service.</p>\n\n<p>How can you remove older versions that are not used by Elastic Beanstalk so that new versions can be created for your applications?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Setup an <code>.ebextensions</code> file</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Define a Lambda function</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use a Lifecycle Policy</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Worker Environments</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use a Lifecycle Policy</strong></p>\n\n<p>Each time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application version. If you don't delete versions that you no longer use, you will eventually reach the application version limit and be unable to create new versions of that application.</p>\n\n<p>You can avoid hitting the limit by applying an application version lifecycle policy to your applications. A lifecycle policy tells Elastic Beanstalk to delete old application versions or to delete application versions when the total number of versions for an application exceeds a specified number.</p>\n\n<p>Elastic Beanstalk applies an application's lifecycle policy each time you create a new application version and deletes up to 100 versions each time the lifecycle policy is applied. Elastic Beanstalk deletes old versions after creating the new version and does not count the new version towards the maximum number of versions defined in the policy.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q15-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q15-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html</a><p></p>\n\n<p>Incorrect options:\n<strong>Setup an <code>.ebextensions</code> files</strong> - You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains. This does not help with managing versions.</p>\n\n<p><strong>Define a Lambda function</strong> - This could work but would require a lot of manual scripting, to achieve the same desired effect as the Lifecycle Policy EB feature.</p>\n\n<p><strong>Use Worker Environments</strong> - This won't help. If your application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use a Lifecycle Policy</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Each time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application version. If you don't delete versions that you no longer use, you will eventually reach the application version limit and be unable to create new versions of that application."
      },
      {
        "answer": "",
        "explanation": "You can avoid hitting the limit by applying an application version lifecycle policy to your applications. A lifecycle policy tells Elastic Beanstalk to delete old application versions or to delete application versions when the total number of versions for an application exceeds a specified number."
      },
      {
        "answer": "",
        "explanation": "Elastic Beanstalk applies an application's lifecycle policy each time you create a new application version and deletes up to 100 versions each time the lifecycle policy is applied. Elastic Beanstalk deletes old versions after creating the new version and does not count the new version towards the maximum number of versions defined in the policy."
      },
      {
        "link": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html"
      },
      {
        "answer": "",
        "explanation": "Incorrect options:\n<strong>Setup an <code>.ebextensions</code> files</strong> - You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains. This does not help with managing versions."
      },
      {
        "answer": "",
        "explanation": "<strong>Define a Lambda function</strong> - This could work but would require a lot of manual scripting, to achieve the same desired effect as the Lifecycle Policy EB feature."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Worker Environments</strong> - This won't help. If your application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load."
      }
    ],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html"
    ]
  },
  {
    "id": 41,
    "question": "<p>Your AWS account is now growing to 200 users and you would like to provide each of these users a personal space in the S3 bucket 'my_company_space' with the prefix <code>/home/&lt;username&gt;</code>, where they have read/write access.</p>\n\n<p>How can you do this efficiently?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create inline policies for each user as they are onboarded</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create one customer-managed policy with policy variables and attach it to a group of all users</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create one customer-managed policy per user and attach them to the relevant users</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an S3 bucket policy and change it as users are added and removed</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create one customer-managed policy with policy variables and attach it to a group of all users</strong></p>\n\n<p>You can assign access to \"dynamically calculated resources\" by using policy variables, a feature that lets you specify placeholders in a policy. When the policy is evaluated, the policy variables are replaced with values that come from the context of the request itself.</p>\n\n<p>This is ideal when you want want to generalize the policy so it works for many users without having to make a unique copy of the policy for each user. For example, consider writing a policy to allow each user to have access to his or her own objects in an Amazon S3 bucket, as in the previous example. But don't create a separate policy for each user that explicitly specifies the user's name as part of the resource. Instead, create a single group policy that works for any user in that group.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q13-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q13-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an S3 bucket policy and change it as users are added and removed</strong></p>\n\n<p>This doesn't scale and the S3 bucket policy size may be maxed out. The IAM policies bump up against a size limit (up to 2 kb for users, 5 kb for groups, and 10 kb for roles). S3 supports bucket policies of up 20 kb.</p>\n\n<p><strong>Create inline policies for each user as they are onboarded</strong>: This would work but doesn't scale and it's inefficient.</p>\n\n<p><strong>Create one customer-managed policy per user and attach them to the relevant users</strong>: This would work but doesn't scale and would be a nightmare to manage.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Create one customer-managed policy with policy variables and attach it to a group of all users</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can assign access to \"dynamically calculated resources\" by using policy variables, a feature that lets you specify placeholders in a policy. When the policy is evaluated, the policy variables are replaced with values that come from the context of the request itself."
      },
      {
        "answer": "",
        "explanation": "This is ideal when you want want to generalize the policy so it works for many users without having to make a unique copy of the policy for each user. For example, consider writing a policy to allow each user to have access to his or her own objects in an Amazon S3 bucket, as in the previous example. But don't create a separate policy for each user that explicitly specifies the user's name as part of the resource. Instead, create a single group policy that works for any user in that group."
      },
      {
        "link": "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Create an S3 bucket policy and change it as users are added and removed</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This doesn't scale and the S3 bucket policy size may be maxed out. The IAM policies bump up against a size limit (up to 2 kb for users, 5 kb for groups, and 10 kb for roles). S3 supports bucket policies of up 20 kb."
      },
      {
        "answer": "",
        "explanation": "<strong>Create inline policies for each user as they are onboarded</strong>: This would work but doesn't scale and it's inefficient."
      },
      {
        "answer": "",
        "explanation": "<strong>Create one customer-managed policy per user and attach them to the relevant users</strong>: This would work but doesn't scale and would be a nightmare to manage."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html"
    ]
  },
  {
    "id": 42,
    "question": "<p>Which environment variable can be used by AWS X-Ray SDK to ensure that the daemon is correctly discovered on ECS?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS_XRAY_DEBUG_MODE</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS_XRAY_DAEMON_ADDRESS</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS_XRAY_TRACING_NAME</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS_XRAY_CONTEXT_MISSING</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>AWS_XRAY_DAEMON_ADDRESS</strong></p>\n\n<p>Set the host and port of the X-Ray daemon listener. By default, the SDK uses 127.0.0.1:2000 for both trace data (UDP) and sampling (TCP). Use this variable if you have configured the daemon to listen on a different port or if it is running on a different host.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS_XRAY_TRACING_NAME</strong> - This sets a service name that the SDK uses for segments.</p>\n\n<p><strong>AWS_XRAY_CONTEXT_MISSING</strong> - This should be set to LOG_ERROR to avoid throwing exceptions when your instrumented code attempts to record data when no segment is open.</p>\n\n<p><strong>AWS_XRAY_DEBUG_MODE</strong> - This should be set to TRUE to configure the SDK to output logs to the console, instead of configuring a logger.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-configuration.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-configuration.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>AWS_XRAY_DAEMON_ADDRESS</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Set the host and port of the X-Ray daemon listener. By default, the SDK uses 127.0.0.1:2000 for both trace data (UDP) and sampling (TCP). Use this variable if you have configured the daemon to listen on a different port or if it is running on a different host."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>AWS_XRAY_TRACING_NAME</strong> - This sets a service name that the SDK uses for segments."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS_XRAY_CONTEXT_MISSING</strong> - This should be set to LOG_ERROR to avoid throwing exceptions when your instrumented code attempts to record data when no segment is open."
      },
      {
        "answer": "",
        "explanation": "<strong>AWS_XRAY_DEBUG_MODE</strong> - This should be set to TRUE to configure the SDK to output logs to the console, instead of configuring a logger."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-configuration.html"
    ]
  },
  {
    "id": 43,
    "question": "<p>You have been collecting AWS X-Ray traces across multiple applications and you would now like to index your XRay traces to search and filter through them efficiently.</p>\n\n<p>What should you use in your instrumentation?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Metadata</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Annotations</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Segments</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Sampling</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Annotations</strong></p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.</p>\n\n<p>You can use X-Ray to collect data across AWS Accounts. The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a><p></p>\n\n<p>Annotations are simple key-value pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API.</p>\n\n<p>X-Ray indexes up to 50 annotations per trace.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Metadata</strong> - Metadata are key-value pairs with values of any type, including objects and lists, but that is not indexed. Use metadata to record data you want to store in the trace but don't need to use for searching traces.</p>\n\n<p><strong>Segments</strong> - The computing resources running your application logic send data about their work as segments. A segment provides the resource's name, details about the request, and details about the work done.</p>\n\n<p><strong>Sampling</strong> - To ensure efficient tracing and provide a representative sample of the requests that your application serves, the X-Ray SDK applies a sampling algorithm to determine which requests get traced. By default, the X-Ray SDK records the first request each second, and five percent of any additional requests.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Annotations</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components."
      },
      {
        "answer": "",
        "explanation": "You can use X-Ray to collect data across AWS Accounts. The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account."
      },
      {
        "image": "https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png",
        "answer": "",
        "explanation": "How X-Ray Works:"
      },
      {
        "link": "https://aws.amazon.com/xray/"
      },
      {
        "answer": "",
        "explanation": "Annotations are simple key-value pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API."
      },
      {
        "answer": "",
        "explanation": "X-Ray indexes up to 50 annotations per trace."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Metadata</strong> - Metadata are key-value pairs with values of any type, including objects and lists, but that is not indexed. Use metadata to record data you want to store in the trace but don't need to use for searching traces."
      },
      {
        "answer": "",
        "explanation": "<strong>Segments</strong> - The computing resources running your application logic send data about their work as segments. A segment provides the resource's name, details about the request, and details about the work done."
      },
      {
        "answer": "",
        "explanation": "<strong>Sampling</strong> - To ensure efficient tracing and provide a representative sample of the requests that your application serves, the X-Ray SDK applies a sampling algorithm to determine which requests get traced. By default, the X-Ray SDK records the first request each second, and five percent of any additional requests."
      }
    ],
    "references": [
      "https://aws.amazon.com/xray/",
      "https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html"
    ]
  },
  {
    "id": 44,
    "question": "<p>You are using AWS SQS FIFO queues to get the ordering of messages on a per <code>user_id</code> basis.</p>\n\n<p>As a developer, which message parameter should you set the value of <code>user_id</code> to guarantee the ordering?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>MessageGroupId</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>MessageHash</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>MessageOrderId</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>MessageDeduplicationId</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>AWS FIFO queues are designed to enhance messaging between applications when the order of operations and events has to be enforced.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q33-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q33-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a><p></p>\n\n<p><strong>MessageGroupId</strong></p>\n\n<p>The message group ID is the tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are always processed one by one, in a strict order relative to the message group (however, messages that belong to different message groups might be processed out of order).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>MessageDeduplicationId</strong> - The message deduplication ID is the token used for the deduplication of sent messages. If a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval.</p>\n\n<p><strong>MessageOrderId</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p><strong>MessageHash</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagegroupid-property.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagegroupid-property.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "AWS FIFO queues are designed to enhance messaging between applications when the order of operations and events has to be enforced."
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html"
      },
      {
        "answer": "<strong>MessageGroupId</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The message group ID is the tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are always processed one by one, in a strict order relative to the message group (however, messages that belong to different message groups might be processed out of order)."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>MessageDeduplicationId</strong> - The message deduplication ID is the token used for the deduplication of sent messages. If a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval."
      },
      {
        "answer": "",
        "explanation": "<strong>MessageOrderId</strong> - This is a made-up option and has been added as a distractor."
      },
      {
        "answer": "",
        "explanation": "<strong>MessageHash</strong> - This is a made-up option and has been added as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagegroupid-property.html"
    ]
  },
  {
    "id": 45,
    "question": "<p>You are responsible for an application that runs on multiple Amazon EC2 instances. In front of the instances is an Internet-facing load balancer that takes requests from clients over the internet and distributes them to the EC2 instances. A health check is configured to ping the index.html page found in the root directory for the health status. When accessing the website via the internet visitors of the website receive timeout errors.</p>\n\n<p>What should be checked first to resolve the issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The application is down</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Security Groups</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The ALB is warming up</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>IAM Roles</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Security Groups</strong></p>\n\n<p>A security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance.</p>\n\n<p>Check the security group rules of your EC2 instance. You need a security group rule that allows inbound traffic from your public IPv4 address on the proper port.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>IAM Roles</strong> - Usually you run into issues with authorization of APIs with roles but not for timeout, so this option does not fit the given use-case.</p>\n\n<p><strong>The application is down</strong> - Although you can set a health check for application ping or HTTP, timeouts are usually caused by blocked firewall access.</p>\n\n<p><strong>The ALB is warming up</strong> - ALB has a slow start mode which allows a warm-up period before being able to respond to requests with optimal performance. So this is not the issue.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html#TroubleshootingInstancesConnectionTimeout\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html#TroubleshootingInstancesConnectionTimeout</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Security Groups</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "A security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance."
      },
      {
        "answer": "",
        "explanation": "Check the security group rules of your EC2 instance. You need a security group rule that allows inbound traffic from your public IPv4 address on the proper port."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>IAM Roles</strong> - Usually you run into issues with authorization of APIs with roles but not for timeout, so this option does not fit the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>The application is down</strong> - Although you can set a health check for application ping or HTTP, timeouts are usually caused by blocked firewall access."
      },
      {
        "answer": "",
        "explanation": "<strong>The ALB is warming up</strong> - ALB has a slow start mode which allows a warm-up period before being able to respond to requests with optimal performance. So this is not the issue."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html#TroubleshootingInstancesConnectionTimeout"
    ]
  },
  {
    "id": 46,
    "question": "<p>A business-critical mobile application uses Amazon Cognito user pools with multi-factor authentication (MFA) enabled for all its users. The application manages confidential data about the company's sales forecasts and product launches. Considering the highly critical nature of the application, the company wants to track every user login activity via a notification sent as an email to the security team.</p>\n\n<p>Which of the following would you recommend as the MOST optimal way of implementing this requirement within a short period?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an AWS Lambda function that uses Amazon Simple Email Service to send an email notification to the concerned security team. Configure this function as Amazon Cognito post-authentication Lambda trigger</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create an AWS Lambda function that uses Amazon Simple Email Service to send an email notification to the concerned security team. Configure this function as Amazon Cognito pre-authentication Lambda trigger</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure an AWS Lambda function as a trigger to Amazon Cognito identity pools authenticated API operations. Create the Lambda function to utilize the Amazon Simple Email Service to send an email notification to the concerned security team</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure Amazon Cognito user pools authenticated API operations and MFA API operations to send all login data to Amazon Kinesis Data Streams. Configure an AWS Lambda function to analyze these streams and trigger an SNS notification to the security team based on user access</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Lambda function that uses Amazon Simple Email Service to send an email notification to the concerned security team. Configure this function as Amazon Cognito post-authentication Lambda trigger</strong></p>\n\n<p>Amazon Cognito invokes Post authentication Lambda trigger after signing in a user, you can add custom logic after Amazon Cognito authenticates the user.</p>\n\n<p>Post-authentication Lambda flows:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q37-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q37-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Lambda function that uses Amazon Simple Email Service to send an email notification to the concerned security team. Configure this function as Amazon Cognito pre-authentication Lambda trigger</strong> - Pre-authentication Lambda trigger: Amazon Cognito invokes this trigger when a user attempts to sign in so that you can create custom validation that accepts or denies the authentication request. This is not useful for the current use case since we want to track user login activity which happens post-authentication.</p>\n\n<p><strong>Configure an AWS Lambda function as a trigger to Amazon Cognito identity pools authenticated API operations. Create the Lambda function to utilize the Amazon Simple Email Service to send an email notification to the concerned security team</strong> - This statement is incorrect. Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. Cognito identity pools are for authorization and not for authentication.</p>\n\n<p><strong>Configure Amazon Cognito user pools authenticated API operations and MFA API operations to send all login data to Amazon Kinesis Data Streams. Configure an AWS Lambda function to analyze these streams and trigger an SNS notification to the security team based on user access</strong> - This is a made-up option given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/user-pools-API-operations.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/user-pools-API-operations.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Create an AWS Lambda function that uses Amazon Simple Email Service to send an email notification to the concerned security team. Configure this function as Amazon Cognito post-authentication Lambda trigger</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon Cognito invokes Post authentication Lambda trigger after signing in a user, you can add custom logic after Amazon Cognito authenticates the user."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q37-i1.jpg",
        "answer": "",
        "explanation": "Post-authentication Lambda flows:"
      },
      {
        "link": "https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an AWS Lambda function that uses Amazon Simple Email Service to send an email notification to the concerned security team. Configure this function as Amazon Cognito pre-authentication Lambda trigger</strong> - Pre-authentication Lambda trigger: Amazon Cognito invokes this trigger when a user attempts to sign in so that you can create custom validation that accepts or denies the authentication request. This is not useful for the current use case since we want to track user login activity which happens post-authentication."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure an AWS Lambda function as a trigger to Amazon Cognito identity pools authenticated API operations. Create the Lambda function to utilize the Amazon Simple Email Service to send an email notification to the concerned security team</strong> - This statement is incorrect. Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. Cognito identity pools are for authorization and not for authentication."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Amazon Cognito user pools authenticated API operations and MFA API operations to send all login data to Amazon Kinesis Data Streams. Configure an AWS Lambda function to analyze these streams and trigger an SNS notification to the security team based on user access</strong> - This is a made-up option given only as a distractor."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html",
      "https://docs.aws.amazon.com/cognito/latest/developerguide/user-pools-API-operations.html"
    ]
  },
  {
    "id": 47,
    "question": "<p>A company ingests real-time data into its on-premises data center and subsequently a daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 GB.</p>\n\n<p>Which of the following is the fastest way to upload the daily compressed file into S3?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Upload the compressed file using multipart upload with S3 transfer acceleration</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Upload the compressed file in a single operation</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>FTP the compressed file into an EC2 instance that runs in the same region as the S3 bucket. Then transfer the file from the EC2 instance into the S3 bucket</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Upload the compressed file using multipart upload</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Upload the compressed file using multipart upload with S3 transfer acceleration</strong></p>\n\n<p>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</p>\n\n<p>Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance. If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upload the compressed file in a single operation</strong> - In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput - you can upload parts in parallel to improve throughput. Therefore, this option is not correct.</p>\n\n<p><strong>Upload the compressed file using multipart upload</strong> - Although using multipart upload would certainly speed up the process, combining with S3 transfer acceleration would further improve the transfer speed. Therefore just using multipart upload is not the correct option.</p>\n\n<p><strong>FTP the compressed file into an EC2 instance that runs in the same region as the S3 bucket. Then transfer the file from the EC2 instance into the S3 bucket</strong> -  This is a roundabout process of getting the file into S3 and added as a distractor. Although it is technically feasible to follow this process, it would involve a lot of scripting and certainly would not be the fastest way to get the file into S3.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Upload the compressed file using multipart upload with S3 transfer acceleration</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path."
      },
      {
        "answer": "",
        "explanation": "Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance. If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Upload the compressed file in a single operation</strong> - In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput - you can upload parts in parallel to improve throughput. Therefore, this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>Upload the compressed file using multipart upload</strong> - Although using multipart upload would certainly speed up the process, combining with S3 transfer acceleration would further improve the transfer speed. Therefore just using multipart upload is not the correct option."
      },
      {
        "answer": "",
        "explanation": "<strong>FTP the compressed file into an EC2 instance that runs in the same region as the S3 bucket. Then transfer the file from the EC2 instance into the S3 bucket</strong> -  This is a roundabout process of getting the file into S3 and added as a distractor. Although it is technically feasible to follow this process, it would involve a lot of scripting and certainly would not be the fastest way to get the file into S3."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html",
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html"
    ]
  },
  {
    "id": 48,
    "question": "<p>A developer has created a new Application Load Balancer but has not registered any targets with the target groups.</p>\n\n<p>Which of the following errors would be generated by the Load Balancer?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>HTTP 500: Internal server error</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>HTTP 504: Gateway timeout</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>HTTP 502: Bad gateway</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>HTTP 503: Service unavailable</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>HTTP 503: Service unavailable</strong></p>\n\n<p>The Load Balancer generates the <code>HTTP 503: Service unavailable</code> error when the target groups for the load balancer have no registered targets.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>HTTP 500: Internal server error</strong></p>\n\n<p><strong>HTTP 502: Bad gateway</strong></p>\n\n<p><strong>HTTP 504: Gateway timeout</strong></p>\n\n<p>Here is a summary of the possible causes for these error types:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q59-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q59-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html</a><p></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>HTTP 503: Service unavailable</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The Load Balancer generates the <code>HTTP 503: Service unavailable</code> error when the target groups for the load balancer have no registered targets."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>HTTP 500: Internal server error</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>HTTP 502: Bad gateway</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>HTTP 504: Gateway timeout</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Here is a summary of the possible causes for these error types:"
      },
      {
        "link": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html"
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html"
    ]
  },
  {
    "id": 49,
    "question": "<p>A security company is requiring all developers to perform server-side encryption with customer-provided encryption keys when performing operations in AWS S3. Developers should write software with C# using the AWS SDK and implement the requirement in the PUT, GET, Head, and Copy operations.</p>\n\n<p>Which of the following encryption methods meets this requirement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SSE-KMS</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>SSE-C</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>SSE-S3</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Client-Side Encryption</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>SSE-C</strong></p>\n\n<p>You have the following options for protecting data at rest in Amazon S3:</p>\n\n<p>Server-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.</p>\n\n<p>Client-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p>For the given use-case, the company wants to manage the encryption keys via its custom application and let S3 manage the encryption, therefore you must use Server-Side Encryption with Customer-Provided Keys (SSE-C).</p>\n\n<p>Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it writes to disks, and decryption, when you access your objects.</p>\n\n<p>Please review these three options for Server Side Encryption on S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q55-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q55-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-KMS</strong> - Server-Side Encryption with Customer Master Keys (CMKs) stored in AWS Key Management Service (SSE-KMS) is similar to SSE-S3. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Additionally, you can create and manage customer-managed CMKs or use AWS managed CMKs that are unique to you, your service, and your Region.</p>\n\n<p><strong>Client-Side Encryption</strong> - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p><strong>SSE-S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>SSE-C</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You have the following options for protecting data at rest in Amazon S3:"
      },
      {
        "answer": "",
        "explanation": "Server-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects."
      },
      {
        "answer": "",
        "explanation": "Client-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools."
      },
      {
        "answer": "",
        "explanation": "For the given use-case, the company wants to manage the encryption keys via its custom application and let S3 manage the encryption, therefore you must use Server-Side Encryption with Customer-Provided Keys (SSE-C)."
      },
      {
        "answer": "",
        "explanation": "Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it writes to disks, and decryption, when you access your objects."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q55-i1.jpg",
        "answer": "",
        "explanation": "Please review these three options for Server Side Encryption on S3:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>SSE-KMS</strong> - Server-Side Encryption with Customer Master Keys (CMKs) stored in AWS Key Management Service (SSE-KMS) is similar to SSE-S3. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Additionally, you can create and manage customer-managed CMKs or use AWS managed CMKs that are unique to you, your service, and your Region."
      },
      {
        "answer": "",
        "explanation": "<strong>Client-Side Encryption</strong> - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools."
      },
      {
        "answer": "",
        "explanation": "<strong>SSE-S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html"
    ]
  },
  {
    "id": 50,
    "question": "<p>You would like your Elastic Beanstalk environment to expose an HTTPS endpoint and an HTTP endpoint. The HTTPS endpoint should be used to get in-flight encryption between your clients and your web servers, while the HTTP endpoint should only be used to redirect traffic to HTTPS and support URLs starting with http://.</p>\n\n<p>What must be done to configure this setup? (Select three)</p>",
    "corrects": [
      4,
      5,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Only open up port 443</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure your EC2 instances to redirect HTTPS traffic to HTTP</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Only open up port 80</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Assign an SSL certificate to the Load Balancer</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Configure your EC2 instances to redirect HTTP traffic to HTTPS</p>",
        "correct": true
      },
      {
        "id": 6,
        "answer": "<p>Open up port 80 &amp; port 443</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Security",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Assign an SSL certificate to the Load Balancer</strong></p>\n\n<p>This ensures that the Load Balancer can expose an HTTPS endpoint.</p>\n\n<p><strong>Open up port 80 &amp; port 443</strong></p>\n\n<p>This ensures that the Load Balancer will allow both the HTTP (80) and HTTPS (443) protocol for incoming connections</p>\n\n<p><strong>Configure your EC2 instances to redirect HTTP traffic to HTTPS</strong></p>\n\n<p>This ensures traffic originating from HTTP onto the Load Balancer forces a redirect to HTTPS by the EC2 instances before being correctly served, thus ensuring the traffic served is fully encrypted.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Only open up port 80</strong> - This is not correct as it would not allow HTTPS traffic (port 443).</p>\n\n<p><strong>Only open up port 443</strong> - This is not correct as it would not allow HTTP traffic (port 80).</p>\n\n<p><strong>Configure your EC2 instances to redirect HTTPS traffic to HTTP</strong> - This is not correct as it would force HTTP traffic, instead of HTTPS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-httpredirect.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-httpredirect.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Assign an SSL certificate to the Load Balancer</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This ensures that the Load Balancer can expose an HTTPS endpoint."
      },
      {
        "answer": "",
        "explanation": "<strong>Open up port 80 &amp; port 443</strong>"
      },
      {
        "answer": "",
        "explanation": "This ensures that the Load Balancer will allow both the HTTP (80) and HTTPS (443) protocol for incoming connections"
      },
      {
        "answer": "<strong>Configure your EC2 instances to redirect HTTP traffic to HTTPS</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This ensures traffic originating from HTTP onto the Load Balancer forces a redirect to HTTPS by the EC2 instances before being correctly served, thus ensuring the traffic served is fully encrypted."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Only open up port 80</strong> - This is not correct as it would not allow HTTPS traffic (port 443)."
      },
      {
        "answer": "",
        "explanation": "<strong>Only open up port 443</strong> - This is not correct as it would not allow HTTP traffic (port 80)."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure your EC2 instances to redirect HTTPS traffic to HTTP</strong> - This is not correct as it would force HTTP traffic, instead of HTTPS."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-httpredirect.html",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html"
    ]
  },
  {
    "id": 51,
    "question": "<p>A media company wants to migrate a video editing service to Amazon EC2 while following security best practices. The videos are sourced and read from a non-public S3 bucket.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an S3 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an IAM user with read-only permissions for the S3 bucket. Configure the IAM user credentials in the user data of the EC2 instance</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up an EC2 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up an IAM user with read-only permissions for the S3 bucket. Configure AWS credentials for this user via AWS CLI on the EC2 instance</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an EC2 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</strong></p>\n\n<p>As an AWS security best practice, you should not create an IAM user and pass the user's credentials to the application or embed the credentials in the application. Instead, create an IAM role that you attach to the EC2 instance to give temporary security credentials to applications running on the instance. When an application uses these credentials in AWS, it can perform all of the operations that are allowed by the policies attached to the role.</p>\n\n<p>So for the given use-case, you should create an IAM role with read-only permissions for the S3 bucket and apply it to the EC2 instance profile.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q1-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q1-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an IAM user with read-only permissions for the S3 bucket. Configure AWS credentials for this user via AWS CLI on the EC2 instance</strong></p>\n\n<p><strong>Set up an IAM user with read-only permissions for the S3 bucket. Configure the IAM user credentials in the user data of the EC2 instance</strong></p>\n\n<p>As mentioned in the explanation above, it is dangerous to pass an IAM user's credentials to the application or embed the credentials in the application or even configure these credentials in the user data of the EC2 instance. So both these options are incorrect.</p>\n\n<p><strong>Set up an S3 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</strong> - As the application is running on EC2 instances, therefore you need to set up an EC2 service role, not an S3 service role.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Set up an EC2 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As an AWS security best practice, you should not create an IAM user and pass the user's credentials to the application or embed the credentials in the application. Instead, create an IAM role that you attach to the EC2 instance to give temporary security credentials to applications running on the instance. When an application uses these credentials in AWS, it can perform all of the operations that are allowed by the policies attached to the role."
      },
      {
        "answer": "",
        "explanation": "So for the given use-case, you should create an IAM role with read-only permissions for the S3 bucket and apply it to the EC2 instance profile."
      },
      {
        "link": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Set up an IAM user with read-only permissions for the S3 bucket. Configure AWS credentials for this user via AWS CLI on the EC2 instance</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Set up an IAM user with read-only permissions for the S3 bucket. Configure the IAM user credentials in the user data of the EC2 instance</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "As mentioned in the explanation above, it is dangerous to pass an IAM user's credentials to the application or embed the credentials in the application or even configure these credentials in the user data of the EC2 instance. So both these options are incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Set up an S3 service role with read-only permissions for the S3 bucket and attach the role to the EC2 instance profile</strong> - As the application is running on EC2 instances, therefore you need to set up an EC2 service role, not an S3 service role."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html"
    ]
  },
  {
    "id": 52,
    "question": "<p>The development team at an e-commerce company is preparing for the upcoming Thanksgiving sale. The product manager wants the development team to implement appropriate caching strategy on Amazon ElastiCache to withstand traffic spikes on the website during the sale. A key requirement is to facilitate consistent updates to the product prices and product description, so that the cache never goes out of sync with the backend.</p>\n\n<p>As a Developer Associate, which of the following solutions would you recommend for the given use-case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a caching strategy to write to the cache directly and sync the backend at a later time</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use a caching strategy to write to the backend first and then invalidate the cache</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use a caching strategy to update the cache and the backend at the same time</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a caching strategy to write to the backend first and wait for the cache to expire via TTL</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.</p>\n\n<p>Broadly, you can set up two types of caching strategies:</p>\n\n<ol>\n<li><p>Lazy Loading</p></li>\n<li><p>Write-Through</p></li>\n</ol>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q62-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q62-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a><p></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q62-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q62-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a><p></p>\n\n<p><strong>Use a caching strategy to write to the backend first and then invalidate the cache</strong></p>\n\n<p>This option is similar to the write-through strategy wherein the application writes to the backend first and then invalidate the cache. As the cache gets invalidated, the caching engine would then fetch the latest value from the backend, thereby making sure that the product prices and product description stay consistent with the backend.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a caching strategy to update the cache and the backend at the same time</strong> - The cache and the backend cannot be updated at the same time via a single atomic operation as these are two separate systems. Therefore this option is incorrect.</p>\n\n<p><strong>Use a caching strategy to write to the backend first and wait for the cache to expire via TTL</strong> - This strategy could work if the TTL is really short. However, for the duration of this TTL, the cache would be out of sync with the backend, hence this option is not correct for the given use-case.</p>\n\n<p><strong>Use a caching strategy to write to the cache directly and sync the backend at a later time</strong> - This option is given as a distractor as this strategy is not viable to address the given use-case. The product prices and description on the cache must always stay consistent with the backend. You cannot sync the backend at a later time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing."
      },
      {
        "answer": "",
        "explanation": "Broadly, you can set up two types of caching strategies:"
      },
      {},
      {
        "link": "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html"
      },
      {
        "answer": "<strong>Use a caching strategy to write to the backend first and then invalidate the cache</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "This option is similar to the write-through strategy wherein the application writes to the backend first and then invalidate the cache. As the cache gets invalidated, the caching engine would then fetch the latest value from the backend, thereby making sure that the product prices and product description stay consistent with the backend."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use a caching strategy to update the cache and the backend at the same time</strong> - The cache and the backend cannot be updated at the same time via a single atomic operation as these are two separate systems. Therefore this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use a caching strategy to write to the backend first and wait for the cache to expire via TTL</strong> - This strategy could work if the TTL is really short. However, for the duration of this TTL, the cache would be out of sync with the backend, hence this option is not correct for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use a caching strategy to write to the cache directly and sync the backend at a later time</strong> - This option is given as a distractor as this strategy is not viable to address the given use-case. The product prices and description on the cache must always stay consistent with the backend. You cannot sync the backend at a later time."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html"
    ]
  },
  {
    "id": 53,
    "question": "<p>You have created a DynamoDB table to support your application and provisioned RCU and WCU to it so that your application has been running for over a year now without any throttling issues. Your application now requires a second type of query over your table and as such, you have decided to create an LSI and a GSI on a new table to support that use case. One month after having implemented such indexes, it seems your table is experiencing throttling.</p>\n\n<p>Upon looking at the table's metrics, it seems the RCU and WCU provisioned are still sufficient. What's happening?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The GSI is throttling so you need to provision more RCU and WCU to the GSI</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>The LSI is throttling so you need to provision more RCU and WCU to the LSI</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Metrics are lagging in your CloudWatch dashboard and you should see the RCU and WCU peaking for the main table in a few minutes</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Adding both an LSI and a GSI to a table is not recommended by AWS best practices as this is a known cause for creating throttles</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>The GSI is throttling so you need to provision more RCU and WCU to the GSI</strong></p>\n\n<p>DynamoDB supports two types of secondary indexes:</p>\n\n<p>Global secondary index — An index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions. A global secondary index is stored in its own partition space away from the base table and scales separately from the base table.</p>\n\n<p>Local secondary index — An index that has the same partition key as the base table, but a different sort key. A local secondary index is \"local\" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value.</p>\n\n<p>Differences between GSI and LSI:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q30-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q30-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a><p></p>\n\n<p>If you perform heavy write activity on the table, but a global secondary index on that table has insufficient write capacity, then the write activity on the table will be throttled. To avoid potential throttling, the provisioned write capacity for a global secondary index should be equal or greater than the write capacity of the base table since new updates will write to both the base table and global secondary index.</p>\n\n<p>Incorrect options</p>\n\n<p><strong>The LSI is throttling so you need to provision more RCU and WCU to the LSI</strong> - LSI use the RCU and WCU of the main table, so you can't provision more RCU and WCU to the LSI.</p>\n\n<p><strong>Adding both an LSI and a GSI to a table is not recommended by AWS best practices as this is a known cause for creating throttles</strong> - This option has been added as a distractor. It is fine to have LSI and GSI together.</p>\n\n<p><strong>Metrics are lagging in your CloudWatch dashboard and you should see the RCU and WCU peaking for the main table in a few minutes</strong> - This could be a reason, but in this case, the GSI is at fault as the application has been running fine for months.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.ThroughputConsiderations\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.ThroughputConsiderations</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>The GSI is throttling so you need to provision more RCU and WCU to the GSI</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "DynamoDB supports two types of secondary indexes:"
      },
      {
        "answer": "",
        "explanation": "Global secondary index — An index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions. A global secondary index is stored in its own partition space away from the base table and scales separately from the base table."
      },
      {
        "answer": "",
        "explanation": "Local secondary index — An index that has the same partition key as the base table, but a different sort key. A local secondary index is \"local\" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q30-i1.jpg",
        "answer": "",
        "explanation": "Differences between GSI and LSI:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html"
      },
      {
        "answer": "",
        "explanation": "If you perform heavy write activity on the table, but a global secondary index on that table has insufficient write capacity, then the write activity on the table will be throttled. To avoid potential throttling, the provisioned write capacity for a global secondary index should be equal or greater than the write capacity of the base table since new updates will write to both the base table and global secondary index."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>The LSI is throttling so you need to provision more RCU and WCU to the LSI</strong> - LSI use the RCU and WCU of the main table, so you can't provision more RCU and WCU to the LSI."
      },
      {
        "answer": "",
        "explanation": "<strong>Adding both an LSI and a GSI to a table is not recommended by AWS best practices as this is a known cause for creating throttles</strong> - This option has been added as a distractor. It is fine to have LSI and GSI together."
      },
      {
        "answer": "",
        "explanation": "<strong>Metrics are lagging in your CloudWatch dashboard and you should see the RCU and WCU peaking for the main table in a few minutes</strong> - This could be a reason, but in this case, the GSI is at fault as the application has been running fine for months."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html",
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.ThroughputConsiderations"
    ]
  },
  {
    "id": 54,
    "question": "<p>A business-critical application is hosted on an Amazon EC2 instance and the latest update is in the testing phase. The business has requested a solution to track the average response time of the application and send a notification to the manager if it exceeds a particular threshold. The update will eventually be implemented in the production environment where the solution will be deployed on multiple EC2 instances.</p>\n\n<p>Which of the following options would you combine to address the requirements of the business? (Select two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a CloudWatch alarm to send an Amazon Simple Notification Service (Amazon SNS) notification when the average of the response time metric exceeds the threshold</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Install and configure AWS Systems Manager Agent (SSM Agent) on the EC2 instances to monitor the response time and send the data to Amazon CloudWatch as a custom metric</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure the application to write the response time to a log file on the instance. Install and configure the Amazon Inspector agent on the EC2 instances to read the logs and send the response time to Amazon EventBridge</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure an EventBridge custom rule to send an Amazon Simple Notification Service (Amazon SNS) notification when the average of the response time metric exceeds the threshold</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Configure the application to write the response time to a log file on the EC2 instance. Install and configure the Amazon CloudWatch agent on the EC2 instance to stream the application logs to CloudWatch Logs. Create a metric filter for the response time from the log file</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure the application to write the response time to a log file on the EC2 instance. Install and configure the Amazon CloudWatch agent on the EC2 instance to stream the application logs to CloudWatch Logs. Create a metric filter for the response time from the log file</strong></p>\n\n<p><strong>Create a CloudWatch alarm to send an Amazon Simple Notification Service (Amazon SNS) notification when the average of the response time metric exceeds the threshold</strong></p>\n\n<p>You can collect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch agent. The unified CloudWatch agent enables you to do the following:</p>\n\n<ol>\n<li><p>Collect internal system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances.</p></li>\n<li><p>Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.</p></li>\n<li><p>Retrieve custom metrics from your applications or services using the StatsD and collectd protocols. StatsD is supported on both Linux servers and servers running Windows Server. collectd is supported only on Linux servers.</p></li>\n<li><p>Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p></li>\n</ol>\n\n<p>You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is CWAgent, although you can specify a different namespace when you configure the agent.</p>\n\n<p>A metric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. The action can be sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Amazon EC2 Auto Scaling action, or creating an OpsItem or incident in Systems Manager. For this use case, we will configure the metric alarm action to send an SNS notification when the alarm is in an ALARM state.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the application to write the response time to a log file on the instance. Install and configure the Amazon Inspector agent on the EC2 instances to read the logs and send the response time to Amazon EventBridge</strong> - This statement is incorrect. Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container workloads for software vulnerabilities and unintended network exposure. Inspector is not a log-collecting agent.</p>\n\n<p><strong>Configure an EventBridge custom rule to send an Amazon Simple Notification Service (Amazon SNS) notification when the average of the response time metric exceeds the threshold</strong> - Amazon EventBridge integrates with Amazon CloudWatch so that when CloudWatch alarms are triggered, a matching EventBridge rule can execute targets. So we are using CloudWatch alarms in this scenario too and hence a straightforward way would be to use CloudWatch alarm.</p>\n\n<p><strong>Install and configure AWS Systems Manager Agent (SSM Agent) on the EC2 instances to monitor the response time and send the data to Amazon CloudWatch as a custom metric</strong> - This statement is incorrect. AWS Systems Manager Agent (SSM Agent) is Amazon software that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, on-premises servers, and virtual machines (VMs). SSM Agent makes it possible for the Systems Manager to update, manage, and configure these resources. SSM agent is required on each instance that needs to be managed from the Systems Manager service. SSM agents cannot collect log data and push it to CloudWatch logs. However, Systems Manager can be used to automate the task of CloudWatch agent installation on the instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Configure the application to write the response time to a log file on the EC2 instance. Install and configure the Amazon CloudWatch agent on the EC2 instance to stream the application logs to CloudWatch Logs. Create a metric filter for the response time from the log file</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>Create a CloudWatch alarm to send an Amazon Simple Notification Service (Amazon SNS) notification when the average of the response time metric exceeds the threshold</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can collect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch agent. The unified CloudWatch agent enables you to do the following:"
      },
      {},
      {
        "answer": "",
        "explanation": "You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is CWAgent, although you can specify a different namespace when you configure the agent."
      },
      {
        "answer": "",
        "explanation": "A metric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. The action can be sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Amazon EC2 Auto Scaling action, or creating an OpsItem or incident in Systems Manager. For this use case, we will configure the metric alarm action to send an SNS notification when the alarm is in an ALARM state."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Configure the application to write the response time to a log file on the instance. Install and configure the Amazon Inspector agent on the EC2 instances to read the logs and send the response time to Amazon EventBridge</strong> - This statement is incorrect. Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container workloads for software vulnerabilities and unintended network exposure. Inspector is not a log-collecting agent."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure an EventBridge custom rule to send an Amazon Simple Notification Service (Amazon SNS) notification when the average of the response time metric exceeds the threshold</strong> - Amazon EventBridge integrates with Amazon CloudWatch so that when CloudWatch alarms are triggered, a matching EventBridge rule can execute targets. So we are using CloudWatch alarms in this scenario too and hence a straightforward way would be to use CloudWatch alarm."
      },
      {
        "answer": "",
        "explanation": "<strong>Install and configure AWS Systems Manager Agent (SSM Agent) on the EC2 instances to monitor the response time and send the data to Amazon CloudWatch as a custom metric</strong> - This statement is incorrect. AWS Systems Manager Agent (SSM Agent) is Amazon software that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, on-premises servers, and virtual machines (VMs). SSM Agent makes it possible for the Systems Manager to update, manage, and configure these resources. SSM agent is required on each instance that needs to be managed from the Systems Manager service. SSM agents cannot collect log data and push it to CloudWatch logs. However, Systems Manager can be used to automate the task of CloudWatch agent installation on the instances."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html",
      "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html"
    ]
  },
  {
    "id": 55,
    "question": "<p>Your client wants to deploy a service on EC2 instances, and as EC2 instances are added into an ASG, each EC2 instance should be running 3 different Docker Containers simultaneously.</p>\n\n<p>What Elastic Beanstalk platform should they choose?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Docker multi-container platform</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Third-party platform</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Docker single-container platform</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Custom platform</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Docker multi-container platform</strong></p>\n\n<p>Docker is a container platform that allows you to define your software stack and store it in an image that can be downloaded from a remote repository. Use the Multicontainer Docker platform if you need to run multiple containers on each instance. The Multicontainer Docker platform does not include a proxy server. Elastic Beanstalk uses Amazon Elastic Container Service (Amazon ECS) to coordinate container deployments to multi-container Docker environments.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Docker single-container platform</strong> - Docker is a container platform that allows you to define your software stack and store it in an image that can be downloaded from a remote repository. Use the Single Container Docker platform if you only need to run a single Docker container on each instance in your environment. The single container platform includes an Nginx proxy server.</p>\n\n<p><strong>Custom Platform</strong> - Elastic Beanstalk supports custom platforms. A custom platform provides more advanced customization than a custom image in several ways. A custom platform lets you develop an entirely new platform from scratch, customizing the operating system, additional software, and scripts that Elastic Beanstalk runs on platform instances. This flexibility enables you to build a platform for an application that uses a language or other infrastructure software, for which Elastic Beanstalk doesn't provide a managed platform. Compare that to custom images, where you modify an Amazon Machine Image (AMI) for use with an existing Elastic Beanstalk platform, and Elastic Beanstalk still provides the platform scripts and controls the platform's software stack. Besides, with custom platforms, you use an automated, scripted way to create and maintain your customization, whereas with custom images you make the changes manually over a running instance.</p>\n\n<p><strong>Third Party Platform</strong> - This is a made-up option.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/platforms/platforms-supported.html#platforms-supported.mcdocker\">https://docs.aws.amazon.com/elasticbeanstalk/latest/platforms/platforms-supported.html#platforms-supported.mcdocker</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Docker multi-container platform</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Docker is a container platform that allows you to define your software stack and store it in an image that can be downloaded from a remote repository. Use the Multicontainer Docker platform if you need to run multiple containers on each instance. The Multicontainer Docker platform does not include a proxy server. Elastic Beanstalk uses Amazon Elastic Container Service (Amazon ECS) to coordinate container deployments to multi-container Docker environments."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Docker single-container platform</strong> - Docker is a container platform that allows you to define your software stack and store it in an image that can be downloaded from a remote repository. Use the Single Container Docker platform if you only need to run a single Docker container on each instance in your environment. The single container platform includes an Nginx proxy server."
      },
      {
        "answer": "",
        "explanation": "<strong>Custom Platform</strong> - Elastic Beanstalk supports custom platforms. A custom platform provides more advanced customization than a custom image in several ways. A custom platform lets you develop an entirely new platform from scratch, customizing the operating system, additional software, and scripts that Elastic Beanstalk runs on platform instances. This flexibility enables you to build a platform for an application that uses a language or other infrastructure software, for which Elastic Beanstalk doesn't provide a managed platform. Compare that to custom images, where you modify an Amazon Machine Image (AMI) for use with an existing Elastic Beanstalk platform, and Elastic Beanstalk still provides the platform scripts and controls the platform's software stack. Besides, with custom platforms, you use an automated, scripted way to create and maintain your customization, whereas with custom images you make the changes manually over a running instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Third Party Platform</strong> - This is a made-up option."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/platforms/platforms-supported.html#platforms-supported.mcdocker"
    ]
  },
  {
    "id": 56,
    "question": "<p>You are implementing a banking application in which you need to update the Exchanges DynamoDB table and the AccountBalance DynamoDB table at the same time or not at all.</p>\n\n<p>Which DynamoDB feature should you use?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>DynamoDB Streams</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>DynamoDB Indexes</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>DynamoDB Transactions</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>DynamoDB TTL</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>DynamoDB Transactions</strong></p>\n\n<p>You can use DynamoDB transactions to make coordinated all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, helping you to maintain data correctness in your applications.</p>\n\n<p>DynamoDB Transactions Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q24-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q24-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DynamoDB TTL</strong> - DynamoDB TTL allows you to expire data based on a timestamp, so this option is not correct.</p>\n\n<p><strong>DynamoDB Streams</strong> - DynamoDB Streams gives a changelog of changes that happened to your tables and then may even relay these to a Lambda function for further processing.</p>\n\n<p><strong>DynamoDB Indexes</strong> - GSI and LSI are used to allow you to query your tables using different partition/sort keys.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>DynamoDB Transactions</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "You can use DynamoDB transactions to make coordinated all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, helping you to maintain data correctness in your applications."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q24-i1.jpg",
        "answer": "",
        "explanation": "DynamoDB Transactions Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>DynamoDB TTL</strong> - DynamoDB TTL allows you to expire data based on a timestamp, so this option is not correct."
      },
      {
        "answer": "",
        "explanation": "<strong>DynamoDB Streams</strong> - DynamoDB Streams gives a changelog of changes that happened to your tables and then may even relay these to a Lambda function for further processing."
      },
      {
        "answer": "",
        "explanation": "<strong>DynamoDB Indexes</strong> - GSI and LSI are used to allow you to query your tables using different partition/sort keys."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html"
    ]
  },
  {
    "id": 57,
    "question": "<p>You would like to have a one-stop dashboard for all the CI/CD needs of one of your projects. You don't need heavy control of the individual configuration of each component in your CI/CD, but need to be able to get a holistic view of your projects.</p>\n\n<p>Which service do you recommend?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>CodeStar</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>CodeBuild</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>CodeDeploy</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>CodePipeline</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>CodeStar</strong></p>\n\n<p>AWS CodeStar enables you to quickly develop, build, and deploy applications on AWS. AWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place. With AWS CodeStar, you can set up your entire continuous delivery toolchain in minutes, allowing you to start releasing code faster. AWS CodeStar makes it easy for your whole team to work together securely, allowing you to easily manage access and add owners, contributors, and viewers to your projects. Each AWS CodeStar project comes with a project management dashboard, including an integrated issue tracking capability powered by Atlassian JIRA Software. With the AWS CodeStar project dashboard, you can easily track progress across your entire software development process, from your backlog of work items to teams’ recent code deployments.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CodeBuild</strong></p>\n\n<p><strong>CodeDeploy</strong></p>\n\n<p><strong>CodePipeline</strong></p>\n\n<p>All these options are individual services encompassed by CodeStar when you deploy a project. They have to be used individually and don't provide a unified \"project\" view.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/codestar/\">https://aws.amazon.com/codestar/</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>CodeStar</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS CodeStar enables you to quickly develop, build, and deploy applications on AWS. AWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place. With AWS CodeStar, you can set up your entire continuous delivery toolchain in minutes, allowing you to start releasing code faster. AWS CodeStar makes it easy for your whole team to work together securely, allowing you to easily manage access and add owners, contributors, and viewers to your projects. Each AWS CodeStar project comes with a project management dashboard, including an integrated issue tracking capability powered by Atlassian JIRA Software. With the AWS CodeStar project dashboard, you can easily track progress across your entire software development process, from your backlog of work items to teams’ recent code deployments."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>CodeBuild</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>CodeDeploy</strong>",
        "explanation": ""
      },
      {
        "answer": "<strong>CodePipeline</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "All these options are individual services encompassed by CodeStar when you deploy a project. They have to be used individually and don't provide a unified \"project\" view."
      }
    ],
    "references": [
      "https://aws.amazon.com/codestar/"
    ]
  },
  {
    "id": 58,
    "question": "<p>You are running a public DNS service on an EC2 instance where the DNS name is pointing to the IP address of the instance. You wish to upgrade your DNS service but would like to do it without any downtime.</p>\n\n<p>Which of the following options will help you accomplish this?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a Load Balancer and an auto scaling group</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Elastic IP</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Route 53</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Provide a static private IP</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p>Route 53 is a DNS managed by AWS, but nothing prevents you from running your own DNS (it's just a software) on an EC2 instance. The trick of this question is that it's about EC2, running some software that needs a fixed IP, and not about Route 53 at all.</p>\n\n<p><strong>Elastic IP</strong></p>\n\n<p>DNS services are identified by a public IP, so you need to use Elastic IP.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Load Balancer and an auto-scaling group</strong> - Load balancers do not provide an IP, instead they provide a DNS name, so this option is ruled out.</p>\n\n<p><strong>Provide a static private IP</strong> - If you provide a private IP it will not be accessible from the internet, so this option is incorrect.</p>\n\n<p><strong>Use Route 53</strong> - Route 53 is a DNS service from AWS but the use-case talks about offering a DNS service using an EC2 instance, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html#using-instance-addressing-eips-associating-different\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html#using-instance-addressing-eips-associating-different</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "Route 53 is a DNS managed by AWS, but nothing prevents you from running your own DNS (it's just a software) on an EC2 instance. The trick of this question is that it's about EC2, running some software that needs a fixed IP, and not about Route 53 at all."
      },
      {
        "answer": "<strong>Elastic IP</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "DNS services are identified by a public IP, so you need to use Elastic IP."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create a Load Balancer and an auto-scaling group</strong> - Load balancers do not provide an IP, instead they provide a DNS name, so this option is ruled out."
      },
      {
        "answer": "",
        "explanation": "<strong>Provide a static private IP</strong> - If you provide a private IP it will not be accessible from the internet, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Use Route 53</strong> - Route 53 is a DNS service from AWS but the use-case talks about offering a DNS service using an EC2 instance, so this option is incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html#using-instance-addressing-eips-associating-different"
    ]
  },
  {
    "id": 59,
    "question": "<p>You are running a web application where users can author blogs and share them with their followers. Most of the workflow is read based, but when a blog is updated, you would like to ensure that the latest data is served to the users (no stale data). The Developer has already suggested using ElastiCache to cope with the read load but has asked you to implement a caching strategy that complies with the requirements of the site.</p>\n\n<p>Which strategy would you recommend?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a Write Through strategy</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use a Lazy Loading strategy without TTL</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use DAX</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a Lazy Loading strategy with TTL</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Use a Write Through strategy</strong></p>\n\n<p>The write-through strategy adds data or updates data in the cache whenever data is written to the database.</p>\n\n<p>In a Write Through strategy, any new blog or update to the blog will be written to both the database layer and the caching layer, thus ensuring that the latest data is always served from the cache.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q29-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q29-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a Lazy Loading strategy without TTL</strong></p>\n\n<p>Lazy Loading is a caching strategy that loads data into the cache only when necessary. Whenever your application requests data, it first requests the ElastiCache cache. If the data exists in the cache and is current, ElastiCache returns the data to your application. If the data doesn't exist in the cache or has expired, your application requests the data from your data store.</p>\n\n<p><strong>Use a Lazy Loading strategy with TTL</strong></p>\n\n<p>In the case of Lazy Loading, the data is loaded onto the cache whenever the data is missing from the cache. In case the blog gets updated, it won't be updated from the cache unless that cache expires (in case you used a TTL). Time to live (TTL) is an integer value that specifies the number of seconds until the key expires. When an application attempts to read an expired key, it is treated as though the key is not found. The database is queried for the key and the cache is updated. Therefore, for a while, old data will be served to users which is a problem from a requirements perspective as we don't want any stale data.</p>\n\n<p><strong>Use DAX</strong> - This is a cache for DynamoDB based implementations, but in this question, we are considering ElastiCache. Therefore this option is not relevant.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Use a Write Through strategy</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The write-through strategy adds data or updates data in the cache whenever data is written to the database."
      },
      {
        "answer": "",
        "explanation": "In a Write Through strategy, any new blog or update to the blog will be written to both the database layer and the caching layer, thus ensuring that the latest data is always served from the cache."
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "<strong>Use a Lazy Loading strategy without TTL</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Lazy Loading is a caching strategy that loads data into the cache only when necessary. Whenever your application requests data, it first requests the ElastiCache cache. If the data exists in the cache and is current, ElastiCache returns the data to your application. If the data doesn't exist in the cache or has expired, your application requests the data from your data store."
      },
      {
        "answer": "<strong>Use a Lazy Loading strategy with TTL</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "In the case of Lazy Loading, the data is loaded onto the cache whenever the data is missing from the cache. In case the blog gets updated, it won't be updated from the cache unless that cache expires (in case you used a TTL). Time to live (TTL) is an integer value that specifies the number of seconds until the key expires. When an application attempts to read an expired key, it is treated as though the key is not found. The database is queried for the key and the cache is updated. Therefore, for a while, old data will be served to users which is a problem from a requirements perspective as we don't want any stale data."
      },
      {
        "answer": "",
        "explanation": "<strong>Use DAX</strong> - This is a cache for DynamoDB based implementations, but in this question, we are considering ElastiCache. Therefore this option is not relevant."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html"
    ]
  },
  {
    "id": 60,
    "question": "<p>You are working with a t2.small instance bastion host that has the AWS CLI installed to help manage all the AWS services installed on it. You would like to know the security group and the instance id of the current instance.</p>\n\n<p>Which of the following will help you fetch the needed information?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Query the metadata at http://169.254.169.254/latest/meta-data</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Query the user data at http://254.169.254.169/latest/meta-data</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Query the user data at http://169.254.169.254/latest/user-data</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an IAM role and attach it to your EC2 instance that helps you perform a 'describe' API call</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Query the metadata at http://169.254.169.254/latest/meta-data</strong> - Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you're writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application. To view all categories of instance metadata from within a running instance, use the following URI - <code>http://169.254.169.254/latest/meta-data/</code>. The IP address 169.254.169.254 is a link-local address and is valid only from the instance. All instance metadata is returned as text (HTTP content type text/plain).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM role and attach it to your EC2 instance that helps you perform a 'describe' API call</strong> - The AWS CLI has a describe-instances API call needs instance ID as an input. So, this will not work for the current use case wherein we do not know the instance ID.</p>\n\n<p><strong>Query the user data at http://169.254.169.254/latest/user-data</strong> - This address retrieves the user data that you specified when launching your instance.</p>\n\n<p><strong>Query the user data at http://254.169.254.169/latest/meta-data</strong> - The IP address specified is wrong.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html</a></p>\n\n<p><a href=\"https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/describe-instances.html\">https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/describe-instances.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Query the metadata at http://169.254.169.254/latest/meta-data</strong> - Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you're writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application. To view all categories of instance metadata from within a running instance, use the following URI - <code>http://169.254.169.254/latest/meta-data/</code>. The IP address 169.254.169.254 is a link-local address and is valid only from the instance. All instance metadata is returned as text (HTTP content type text/plain)."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Create an IAM role and attach it to your EC2 instance that helps you perform a 'describe' API call</strong> - The AWS CLI has a describe-instances API call needs instance ID as an input. So, this will not work for the current use case wherein we do not know the instance ID."
      },
      {
        "answer": "",
        "explanation": "<strong>Query the user data at http://169.254.169.254/latest/user-data</strong> - This address retrieves the user data that you specified when launching your instance."
      },
      {
        "answer": "",
        "explanation": "<strong>Query the user data at http://254.169.254.169/latest/meta-data</strong> - The IP address specified is wrong."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html",
      "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/describe-instances.html"
    ]
  },
  {
    "id": 61,
    "question": "<p>Applications running on EC2 instances process messages from an SQS queue but sometimes they experience errors due to messages not being processed.</p>\n\n<p>To isolate the messages, which option will help with further debugging?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Increase the VisibilityTimeout</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement a Dead Letter Queue</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use DeleteMessage</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Reduce the VisibilityTimeout</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Troubleshooting and Optimization",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement a Dead Letter Queue</strong></p>\n\n<p>Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.</p>\n\n<p>Sometimes, messages can’t be processed because of a variety of possible issues, such as when a user comments on a story but it remains unprocessed because the original story itself is deleted by the author while the comments were being posted. In such a case, the dead-letter queue can be used to handle message processing failures.</p>\n\n<p>How do dead-letter queues work?\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q44-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q44-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a><p></p>\n\n<p>Use-cases for dead-letter queues:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q44-i2.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q44-i2.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a><p></p>\n\n<p>Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that cannot be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.</p>\n\n<p>Incorrect:</p>\n\n<p><strong>Use DeleteMessage</strong> - This API call deletes the message in the queue but does not help you find the issue.</p>\n\n<p><strong>Reduce the VisibilityTimeout</strong> - Amazon SQS uses a visibility timeout to prevent other consumers from receiving and processing the same message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. If you reduce the VisibilityTimeout, more consumers will get the failed message</p>\n\n<p><strong>Increase the VisibilityTimeout</strong> - It won't help because you don't need more time but rather an isolated place to debug.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Implement a Dead Letter Queue</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed."
      },
      {
        "answer": "",
        "explanation": "Sometimes, messages can’t be processed because of a variety of possible issues, such as when a user comments on a story but it remains unprocessed because the original story itself is deleted by the author while the comments were being posted. In such a case, the dead-letter queue can be used to handle message processing failures."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q44-i1.jpg",
        "answer": "",
        "explanation": "How do dead-letter queues work?"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q44-i2.jpg",
        "answer": "",
        "explanation": "Use-cases for dead-letter queues:"
      },
      {
        "link": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
      },
      {
        "answer": "",
        "explanation": "Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that cannot be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed."
      },
      {
        "answer": "",
        "explanation": "Incorrect:"
      },
      {
        "answer": "",
        "explanation": "<strong>Use DeleteMessage</strong> - This API call deletes the message in the queue but does not help you find the issue."
      },
      {
        "answer": "",
        "explanation": "<strong>Reduce the VisibilityTimeout</strong> - Amazon SQS uses a visibility timeout to prevent other consumers from receiving and processing the same message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. If you reduce the VisibilityTimeout, more consumers will get the failed message"
      },
      {
        "answer": "",
        "explanation": "<strong>Increase the VisibilityTimeout</strong> - It won't help because you don't need more time but rather an isolated place to debug."
      }
    ],
    "incorrectAnswerExplanations": [],
    "references": [
      "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"
    ]
  },
  {
    "id": 62,
    "question": "<p>A media analytics company has built a streaming application on Lambda using Serverless Application Model (SAM).</p>\n\n<p>As a Developer Associate, which of the following would you identify as the correct order of execution to successfully deploy the application?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Develop the SAM template locally =&gt; upload the template to Lambda =&gt; deploy your application to the cloud</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Develop the SAM template locally =&gt; upload the template to S3 =&gt; deploy your application to the cloud</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Develop the SAM template locally =&gt; upload the template to CodeCommit =&gt; deploy your application to CodeDeploy</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Develop the SAM template locally =&gt; deploy the template to S3 =&gt; use your application in the cloud</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Develop the SAM template locally =&gt; upload the template to S3 =&gt; deploy your application to the cloud</strong></p>\n\n<p>The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML.</p>\n\n<p>You can develop and test your serverless application locally, and then you can deploy your application by using the sam deploy command. The sam deploy command zips your application artifacts, uploads them to Amazon Simple Storage Service (Amazon S3), and deploys your application to the AWS Cloud. AWS SAM uses AWS CloudFormation as the underlying deployment mechanism.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q2-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q2-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Develop the SAM template locally =&gt; upload the template to Lambda =&gt; deploy your application to the cloud</strong></p>\n\n<p><strong>Develop the SAM template locally =&gt; upload the template to CodeCommit =&gt; deploy your application to CodeDeploy</strong></p>\n\n<p><strong>Develop the SAM template locally =&gt; deploy the template to S3 =&gt; use your application in the cloud</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Develop the SAM template locally =&gt; upload the template to S3 =&gt; deploy your application to the cloud</strong>"
      },
      {
        "answer": "",
        "explanation": "The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML."
      },
      {
        "answer": "",
        "explanation": "You can develop and test your serverless application locally, and then you can deploy your application by using the sam deploy command. The sam deploy command zips your application artifacts, uploads them to Amazon Simple Storage Service (Amazon S3), and deploys your application to the AWS Cloud. AWS SAM uses AWS CloudFormation as the underlying deployment mechanism."
      },
      {
        "link": "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Develop the SAM template locally =&gt; upload the template to Lambda =&gt; deploy your application to the cloud</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Develop the SAM template locally =&gt; upload the template to CodeCommit =&gt; deploy your application to CodeDeploy</strong>"
      },
      {
        "answer": "",
        "explanation": "<strong>Develop the SAM template locally =&gt; deploy the template to S3 =&gt; use your application in the cloud</strong>"
      },
      {
        "answer": "",
        "explanation": "These three options contradict the details provided in the explanation above, so these are incorrect."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html"
    ]
  },
  {
    "id": 63,
    "question": "<p>You would like your Elastic Beanstalk environment to expose an HTTPS endpoint instead of an HTTP endpoint to get in-flight encryption between your clients and your web servers.</p>\n\n<p>What must be done to set up HTTPS on Beanstalk?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a separate CloudFormation template to load the SSL certificate onto the Load Balancer</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a config file in the .ebextensions folder to configure the Load Balancer</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure Health Checks</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Open up the port 80 for the security group</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p>The simplest way to use HTTPS with an Elastic Beanstalk environment is to assign a server certificate to your environment's load balancer. When you configure your load balancer to terminate HTTPS, the connection between the client and the load balancer is secure. Backend connections between the load balancer and EC2 instances use HTTP, so no additional configuration of the instances is required.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q12-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q12-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html</a><p></p>\n\n<p><strong>Create a config file in the .ebextensions folder to configure the Load Balancer</strong></p>\n\n<p>To update your AWS Elastic Beanstalk environment to use HTTPS, you need to configure an HTTPS listener for the load balancer in your environment. Two types of load balancers support an HTTPS listener: Classic Load Balancer and Application Load Balancer.</p>\n\n<p>Example <code>.ebextensions/securelistener-alb.config</code></p>\n\n<p>Use this example when your environment has an Application Load Balancer. The example uses options in the aws:elbv2:listener namespace to configure an HTTPS listener on port 443 with the specified certificate. The listener routes traffic to the default process.</p>\n\n<pre><code>option_settings:\n  aws:elbv2:listener:443:\n    ListenerEnabled: 'true'\n    Protocol: HTTPS\n    SSLCertificateArns: arn:aws:acm:us-east-2:1234567890123:certificate/####################################\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a separate CloudFormation template to load the SSL certificate onto the Load Balancer</strong> - A separate CloudFormation template won't be able to mutate the state of a Load Balancer managed by Elastic Beanstalk, so this option is incorrect.</p>\n\n<p><strong>Open up the port 80 for the security group</strong> - Port 80 is for HTTP traffic, so this option is incorrect.</p>\n\n<p><strong>Configure Health Checks</strong> - Health Checks are not related to SSL certificates, so this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "",
        "explanation": "The simplest way to use HTTPS with an Elastic Beanstalk environment is to assign a server certificate to your environment's load balancer. When you configure your load balancer to terminate HTTPS, the connection between the client and the load balancer is secure. Backend connections between the load balancer and EC2 instances use HTTP, so no additional configuration of the instances is required."
      },
      {
        "link": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html"
      },
      {
        "answer": "<strong>Create a config file in the .ebextensions folder to configure the Load Balancer</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "To update your AWS Elastic Beanstalk environment to use HTTPS, you need to configure an HTTPS listener for the load balancer in your environment. Two types of load balancers support an HTTPS listener: Classic Load Balancer and Application Load Balancer."
      },
      {
        "answer": "",
        "explanation": "Example <code>.ebextensions/securelistener-alb.config</code>"
      },
      {
        "answer": "",
        "explanation": "Use this example when your environment has an Application Load Balancer. The example uses options in the aws:elbv2:listener namespace to configure an HTTPS listener on port 443 with the specified certificate. The listener routes traffic to the default process."
      },
      {
        "answer": "",
        "explanation": "<pre><code>option_settings:\n  aws:elbv2:listener:443:\n    ListenerEnabled: 'true'\n    Protocol: HTTPS\n    SSLCertificateArns: arn:aws:acm:us-east-2:1234567890123:certificate/####################################\n</code></pre>"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use a separate CloudFormation template to load the SSL certificate onto the Load Balancer</strong> - A separate CloudFormation template won't be able to mutate the state of a Load Balancer managed by Elastic Beanstalk, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Open up the port 80 for the security group</strong> - Port 80 is for HTTP traffic, so this option is incorrect."
      },
      {
        "answer": "",
        "explanation": "<strong>Configure Health Checks</strong> - Health Checks are not related to SSL certificates, so this option is ruled out."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html",
      "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html"
    ]
  },
  {
    "id": 64,
    "question": "<p>Your organization has a single Amazon Simple Storage Service (S3) bucket that contains folders labeled with customer names. Several administrators have IAM access to the S3 bucket and versioning is enabled to easily recover from unintended user actions.</p>\n\n<p>Which of the following statements about versioning is <strong>NOT</strong> true based on this scenario?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Any file that was unversioned before enabling versioning will have the 'null' version</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Overwriting a file increases its versions</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deleting a file is a recoverable operation</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Versioning can be enabled only for a specific folder</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Development with AWS Services",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Versioning can be enabled only for a specific folder</strong></p>\n\n<p>The versioning state applies to all (never some) of the objects in that bucket. The first time you enable a bucket for versioning, objects in it are thereafter always versioned and given a unique version ID.</p>\n\n<p>Versioning Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q10-i1.jpg\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"></span></p><div class=\"open-full-size-image--wrapper--R4gIm\" data-purpose=\"open-full-size-image\"><img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q10-i1.jpg\" alt=\"\" loading=\"eager\"><button type=\"button\" class=\"ud-btn ud-btn-medium ud-btn-link ud-heading-sm open-full-size-image--backdrop--Zor3j\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"ud-icon ud-icon-large ud-icon-color-neutral\"><use xlink:href=\"#icon-search\"></use></svg></button></div>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a><p></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Overwriting a file increases its versions</strong> - If you overwrite an object (file), it results in a new object version in the bucket. You can always restore the previous version.</p>\n\n<p><strong>Deleting a file is a recoverable operation</strong> - Correct, when you delete an object (file), Amazon S3 inserts a delete marker, which becomes the current object version and you can restore the previous version.</p>\n\n<p><strong>Any file that was unversioned before enabling versioning will have the 'null' version</strong> - Objects stored in your bucket before you set the versioning state have a version ID of null. Those existing objects in your bucket do not change.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Versioning can be enabled only for a specific folder</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "The versioning state applies to all (never some) of the objects in that bucket. The first time you enable a bucket for versioning, objects in it are thereafter always versioned and given a unique version ID."
      },
      {
        "image": "https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q10-i1.jpg",
        "answer": "",
        "explanation": "Versioning Overview:"
      },
      {
        "link": "https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html"
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Overwriting a file increases its versions</strong> - If you overwrite an object (file), it results in a new object version in the bucket. You can always restore the previous version."
      },
      {
        "answer": "",
        "explanation": "<strong>Deleting a file is a recoverable operation</strong> - Correct, when you delete an object (file), Amazon S3 inserts a delete marker, which becomes the current object version and you can restore the previous version."
      },
      {
        "answer": "",
        "explanation": "<strong>Any file that was unversioned before enabling versioning will have the 'null' version</strong> - Objects stored in your bucket before you set the versioning state have a version ID of null. Those existing objects in your bucket do not change."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html"
    ]
  },
  {
    "id": 65,
    "question": "<p>You were assigned to a project that requires the use of the AWS CLI to build a project with AWS CodeBuild. Your project's root directory includes the buildspec.yml file to run build commands and would like your build artifacts to be automatically encrypted at the end.</p>\n\n<p>How should you configure CodeBuild to accomplish this?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an AWS Lambda Hook</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use In Flight encryption (SSL)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Specify a KMS key to use</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use the AWS Encryption SDK</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Security",
    "explanation": "<p>Correct option:</p>\n\n<p><strong>Specify a KMS key to use</strong></p>\n\n<p>AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.</p>\n\n<p>For AWS CodeBuild to encrypt its build output artifacts, it needs access to an AWS KMS customer master key (CMK). By default, AWS CodeBuild uses the AWS-managed CMK for Amazon S3 in your AWS account. The following environment variable provides these details:</p>\n\n<p>CODEBUILD_KMS_KEY_ID: The identifier of the AWS KMS key that CodeBuild is using to encrypt the build output artifact (for example, arn:aws:kms:region-ID:account-ID:key/key-ID or alias/key-alias).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an AWS Lambda Hook</strong> - Code hook is used for integration with Lambda and is not relevant for the given use-case.</p>\n\n<p><strong>Use the AWS Encryption SDK</strong> - The SDK just makes it easier for you to implement encryption best practices in your application and is not relevant for the given use-case.</p>\n\n<p><strong>Use In-Flight encryption (SSL)</strong> - SSL is usually for internet traffic which in this case will be using internal traffic through AWS and is not relevant for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html</a></p>\n",
    "correctAnswerExplanations": [
      {
        "answer": "<strong>Specify a KMS key to use</strong>",
        "explanation": ""
      },
      {
        "answer": "",
        "explanation": "AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications."
      },
      {
        "answer": "",
        "explanation": "For AWS CodeBuild to encrypt its build output artifacts, it needs access to an AWS KMS customer master key (CMK). By default, AWS CodeBuild uses the AWS-managed CMK for Amazon S3 in your AWS account. The following environment variable provides these details:"
      },
      {
        "answer": "",
        "explanation": "CODEBUILD_KMS_KEY_ID: The identifier of the AWS KMS key that CodeBuild is using to encrypt the build output artifact (for example, arn:aws:kms:region-ID:account-ID:key/key-ID or alias/key-alias)."
      }
    ],
    "incorrectAnswerExplanations": [
      {
        "answer": "",
        "explanation": "<strong>Use an AWS Lambda Hook</strong> - Code hook is used for integration with Lambda and is not relevant for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use the AWS Encryption SDK</strong> - The SDK just makes it easier for you to implement encryption best practices in your application and is not relevant for the given use-case."
      },
      {
        "answer": "",
        "explanation": "<strong>Use In-Flight encryption (SSL)</strong> - SSL is usually for internet traffic which in this case will be using internal traffic through AWS and is not relevant for the given use-case."
      }
    ],
    "references": [
      "https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html",
      "https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html"
    ]
  }
]