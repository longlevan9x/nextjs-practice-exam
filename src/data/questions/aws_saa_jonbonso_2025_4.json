[
  {
    "id": 1,
    "question": "<p>A company conducts performance testing on a large instance MySQL RDS DB instance twice a week. They use Performance Insights to analyze and fine-tune expensive queries. The company needs to reduce its operational expenses in running the tests without compromising the tests' integrity.</p><p>Which of the following is the most cost-effective solution?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p><strong>Amazon RDS</strong> creates a storage volume snapshot that backs up the entire DB instance, not just individual databases. It's important to keep in mind that when creating a DB snapshot on a Single-AZ DB instance, a brief I/O suspension may occur. The duration of the suspension can vary from a few seconds to a few minutes, depending on the size and class of your DB instance.<br><img src=\"https://media.tutorialsdojo.com/public/db-snapshot-06-20-23.png\"></p><p>In the scenario, taking a snapshot allows you to capture the current state of the database, including all data and configurations. By terminating the database, you can avoid incurring costs during the idle period between tests. You can simply restore the database from the snapshot when you need to run the tests again. This approach helps minimize costs by only paying for the database instance during the actual testing periods.</p><p>Hence, the correct answer is: <strong>Once the testing is completed, take a snapshot of the database and terminate it. Restore the database from the snapshot when necessary.</strong></p><p>The option that says: <strong>Stop the database once the test is done and restart it only when necessary</strong> is incorrect. While stopping the database instance can save costs on compute resources, it's important to note that a stopped DB instance does continue to incur costs on its provisioned storage.</p><p>The option that says: <strong>Downgrade the database to a smaller instance. </strong>is incorrect. The downsized instance may not have sufficient resources to accurately simulate real-world scenarios or handle the workload generated by the tests. Therefore, while it may save costs, it may compromise the integrity of the tests.</p><p>The option that says: <strong>Perform a </strong><code><strong>mysqldump</strong></code><strong> to get a copy of the database on a local machine. Use MySQL Workbench to analyze the queries </strong>is incorrect. Although this approach would totally save costs, testing on a local machine may not accurately replicate the resources and environment of a large RDS DB instance. Thus, the tests might produce unreliable results.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Once the testing is completed, take a snapshot of the database and terminate it. Restore the database from the snapshot when necessary.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Stop the database once the test is done and restart it only when necessary.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Downgrade the database to a smaller instance.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Perform a <code>mysqldump</code> to get a copy of the database on a local machine. Use MySQL Workbench to analyze the queries.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 2,
    "question": "<p>A company has an Application Load Balancer (ALB) that accepts HTTP and HTTPS traffic on ports 80 and 443, respectively. Recently, the company associated a new domain for its website, and they want to ensure that all HTTP traffic for this new domain is automatically redirected to HTTPS to improve security.</p><p>Which ALB configuration should be done to satisfy the requirement?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>Application Load Balancer</strong> operates at the application layer (layer 7), routing traffic to targets such as EC2 instances, containers, IP addresses, and Lambda functions based on the requested content. It is ideal for advanced load balancing of HTTP and HTTPS traffic, providing enhanced request routing for modern application architectures like microservices and container-based applications. Additionally, it simplifies and improves application security by always utilizing the latest SSL/TLS ciphers and protocols.</p><p><img src=\"https://media.tutorialsdojo.com/public/ALB-06-01-23.png\"></p><p>Configuring the existing ALB listener on port 80 to redirect traffic to port 443 is the most appropriate solution for redirecting HTTP to HTTPS traffic. This can be achieved by setting up a redirect rule for the listener. By doing this, all HTTP traffic will be automatically redirected to HTTPS, thus improving the security of the website.</p><p>Hence, the correct answer in this scenario is: <strong>Configure the existing HTTP listener to redirect traffic to port 443.</strong></p><p>The option that says: <strong>Create a new HTTP listener on port 80 and add a redirect action to the HTTPS protocol on port 443</strong> while this is the correct way of redirecting HTTP to HTTPS, creating a new HTTP listener on port 80 is not possible because there is already an existing listener assigned to that port.</p><p>The option that says: <strong>Create a new ALB listener on port 443 and configure it to redirect HTTP traffic to HTTPS </strong>is not possible because you can’t have duplicate ports opened in ALB. Take note that there’s already an existing HTTPS listener on port 443.</p><p>The option that says: <strong>Configure the existing on port 443 and add a redirect action to HTTP on port 80 </strong>is incorrect as this configuration goes against the objective stated in the question. The requirement is to automatically redirect HTTP traffic to HTTPS, not the other way around.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p><p><a href=\"https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\">https://aws.amazon.com/elasticloadbalancing/application-load-balancer/</a></p><p><br></p><p><strong>Check out this AWS Elastic Load Balancing Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a new HTTP listener on port 80 and add a redirect action to the HTTPS protocol on port 443.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure the existing HTTP listener to redirect traffic to port 443.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a new ALB listener on port 443 and configure it to redirect HTTP traffic to HTTPS.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure the existing on port 443 and add a redirect action to HTTP on port 80.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 3,
    "question": "<p>A tech company currently has an on-premises infrastructure. They are currently running low on storage and want to have the ability to extend their storage using the AWS cloud.</p><p>Which AWS service can help them achieve this requirement?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>AWS Storage Gateway connects an on-premises software appliance with cloud-based storage to provide seamless integration with data security features between your on-premises IT environment and the AWS storage infrastructure. You can use the service to store data in the AWS Cloud for scalable and cost-effective storage that helps maintain data security.</p><p><img src=\"https://media.tutorialsdojo.com/file-gateway.jpg\"></p><p>Hence, the correct answer is:<strong> AWS Storage Gateway.</strong></p><p>The option that says:<strong> Amazon EC2</strong> is incorrect since this is a compute service, not a storage service.</p><p>The option that says:<strong> Amazon Elastic Block Storage</strong> is incorrect since EBS is primarily used as a storage of your EC2 instances.</p><p>The option that says:<strong> Amazon SQS</strong> is incorrect since this is a message queuing service and does not extend your on-premises storage capacity.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"http://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html\">http://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html</a></p><p><br></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c02/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c02/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Amazon EC2",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Storage Gateway</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon Elastic Block Storage</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Amazon SQS",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 4,
    "question": "<p>A startup launched a new FTP server using an On-Demand EC2 instance in a newly created VPC with default settings. The server should not be accessible publicly but only through the IP address <code>175.45.116.100</code> and nowhere else.</p><p>Which of the following is the most suitable way to implement this requirement?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>The FTP protocol uses TCP via ports 20 and 21. This should be configured in your security groups or in your Network ACL inbound rules. As required by the scenario, you should only allow the individual IP of the client and not the entire network. Therefore, in the <strong>Source, </strong>the proper CIDR notation should be used. The <strong>/32</strong> denotes one IP address and the <strong>/0</strong> refers to the entire network.</p><p>It is stated in the scenario that you launched the EC2 instances in a newly created VPC with default settings. Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic. Hence, you actually don't need to explicitly add inbound rules to your Network ACL to allow inbound traffic, if your VPC has a default setting.</p><p><img src=\"https://media.tutorialsdojo.com/security-diagram.png\"></p><p>The below option is incorrect:</p><p><strong>Create a new inbound rule in the security group of the EC2 instance with the following details:</strong></p><p><strong>Protocol: UDP</strong></p><p><strong>Port Range: 20 - 21</strong></p><p><strong>Source: </strong><code><strong>175.45.116.100/32</strong></code></p><p>Although the configuration of the Security Group is valid, the provided Protocol is incorrect. Take note that FTP uses TCP and not UDP.</p><p>The below option is also incorrect:</p><p><strong>Create a new Network ACL inbound rule in the subnet of the EC2 instance with the following details:</strong></p><p><strong>Protocol: TCP</strong></p><p><strong>Port Range: 20 - 21</strong></p><p><strong>Source: </strong><code><strong>175.45.116.100/0</strong></code></p><p><strong>Allow/Deny: ALLOW</strong></p><p>Although setting up an inbound Network ACL is valid, the source is invalid since it must be an IPv4 or IPv6 CIDR block. In the provided IP address, the <strong>/0</strong> refers to the entire network and not a specific IP address. In addition, it is stated in the scenario that the newly created VPC has default settings and by default, the Network ACL allows all traffic. This means that there is actually no need to configure your Network ACL.</p><p>Likewise, the below option is also incorrect:</p><p><strong>Create a new Network ACL inbound rule in the subnet of the EC2 instance with the following details:</strong></p><p><strong>Protocol: UDP</strong></p><p><strong>Port Range: 20 - 21</strong></p><p><strong>Source: </strong><code><strong>175.45.116.100/0</strong></code></p><p><strong>Allow/Deny: ALLOW</strong></p><p>Just like in the above, the source is also invalid. Take note that FTP uses TCP and not UDP, which is one of the reasons why this option is wrong. In addition, it is stated in the scenario that the newly created VPC has default settings and by default, the Network ACL allows all traffic. This means that there is actually no need to configure your Network ACL.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a new inbound rule in the security group of the EC2 instance with the following details: </p><p>Protocol: TCP </p><p>Port Range: 20 - 21 </p><p>Source: <code>175.45.116.100/32</code></p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a new inbound rule in the security group of the EC2 instance with the following details:</p><p>Protocol: UDP </p><p>Port Range: 20 - 21 </p><p>Source: <code>175.45.116.100/32</code></p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a new Network ACL inbound rule in the subnet of the EC2 instance with the following details: </p><p>Protocol: TCP </p><p>Port Range: 20 - 21 </p><p>Source: <code>175.45.116.100/0</code></p><p>Allow/Deny: ALLOW</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a new Network ACL inbound rule in the subnet of the EC2 instance with the following details: </p><p>Protocol: UDP </p><p>Port Range: 20 - 21 </p><p>Source: <code>175.45.116.100/0</code> </p><p>Allow/Deny: ALLOW</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 5,
    "question": "<p>A Solutions Architect is designing a monitoring application which generates audit logs of all operational activities of the company's cloud infrastructure. Their IT Security and Compliance team mandates that the application retain the logs for 5 years before the data can be deleted. </p><p>How can the Architect meet the above requirement?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>An <strong>Amazon S3 Glacier (Glacier) vault</strong> can have one resource-based vault access policy and one Vault Lock policy attached to it. A <em>Vault Lock policy</em> is a vault access policy that you can lock. Using a Vault Lock policy can help you enforce regulatory and compliance requirements. Amazon S3 Glacier provides a set of API operations for you to manage the Vault Lock policies.</p><p><img src=\"https://media.tutorialsdojo.com/glacier_lock_policy_4.png\"></p><p>As an example of a Vault Lock policy, suppose that you are required to retain archives for one year before you can delete them. To implement this requirement, you can create a Vault Lock policy that denies users permission to delete an archive until the archive has existed for one year. You can test this policy before locking it down. After you lock the policy, the policy becomes immutable. For more information about the locking process, see Amazon S3 Glacier Vault Lock. If you want to manage other user permissions that can be changed, you can use the vault access policy</p><p>Amazon S3 Glacier supports the following archive operations: Upload, Download, and Delete. Archives are immutable and cannot be modified. Hence, the correct answer is to <strong>store the audit logs in a Glacier vault and use the Vault Lock feature</strong>.</p><p><strong>Storing the audit logs in an EBS volume and then taking EBS snapshots every month</strong> is incorrect because this is not a suitable and secure solution. Anyone who has access to the EBS Volume can simply delete and modify the audit logs. Snapshots can be deleted too.</p><p><strong>Storing the audit logs in an Amazon S3 bucket and enabling Multi-Factor Authentication Delete (MFA Delete) on the S3 bucket</strong> is incorrect because this would still not meet the requirement. If someone has access to the S3 bucket and also has the proper MFA privileges, then the audit logs can be edited.</p><p><strong>Storing the audit logs in an EFS volume and using Network File System version 4 (NFSv4) file-locking mechanism</strong> is incorrect because the data integrity of the audit logs can still be compromised if it is stored in an EFS volume with Network File System version 4 (NFSv4) file-locking mechanism and hence, not suitable as storage for the files. Although it will provide some sort of security, the file lock can still be overridden and the audit logs might be edited by someone else.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock-policy.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock-policy.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/glacier-vault-lock/\">https://aws.amazon.com/blogs/aws/glacier-vault-lock/</a></p><p><br></p><p><strong>Check out this Amazon S3 Glacier Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-glacier/?src=udemy\">https://tutorialsdojo.com/amazon-glacier/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the audit logs in a Glacier vault and use the Vault Lock feature.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Store the audit logs in an EBS volume and then take EBS snapshots every month.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Store the audit logs in an Amazon S3 bucket and enable Multi-Factor Authentication Delete (MFA Delete) on the S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the audit logs in an EFS volume and use Network File System version 4 (NFSv4) file-locking mechanism.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 6,
    "question": "<p>A food company bought 50 licenses of Windows Server to be used by the developers when launching Amazon EC2 instances to deploy and test applications. The developers are free to provision EC2 instances as long as there is a license available. The licenses are tied to the total CPU count of each virtual machine. The company wants to ensure that developers won’t be able to launch new instances once the licenses are exhausted. The company wants to receive notifications when all licenses are in use.</p><p>Which of the following options is the recommended solution to meet the company's requirements?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS License Manager</strong> is a service that makes it easier for you to manage your software licenses from software vendors (for example, Microsoft, SAP, Oracle, and IBM) centrally across AWS and your on-premises environments. This provides control and visibility into the usage of your licenses, enabling you to limit licensing overages and reduce the risk of non-compliance and misreporting.</p><p>As you build out your cloud infrastructure on AWS, you can save costs by using Bring Your Own License model (BYOL) opportunities. That is, you can re-purpose your existing license inventory for use with your cloud resources.</p><p><img src=\"https://media.tutorialsdojo.com/public/aws_licence_manager.png\"></p><p>If you are responsible for managing licenses in your organization, you can use License Manager to set up licensing rules, attach them to your launches, and keep track of usage. The users in your organization can then add and remove license-consuming resources without additional work.</p><p><strong>License Manager</strong> reduces the risk of licensing overages and penalties with inventory tracking that is tied directly into AWS services. License Manager's built-in dashboards provide ongoing visibility into license usage and assistance with vendor audits.</p><p>You can prevent license usage when the available licenses are exhausted by selecting the “Enforce license limit” option in license configuration. When this limit exceeds, the instance launch is blocked to control overages.</p><p>Therefore, the correct answer is:<strong> Define licensing rules on AWS License Manager to track and control license usage. Enable the option to “Enforce license limit” to prevent going over the number of allocated licenses. Add an Amazon SNS topic to send notifications and alerts.</strong> You can use AWS License Manager to set up licensing rules, attach them to your EC2 launches, and keep track of usage.</p><p>The option that says: <strong>Define license configuration rules on AWS Certificate Manager to track and control license usage. Enable the option to “Enforce certificate limit” to prevent going over the number of allocated licenses. Add an Amazon SQS queue with ChangeVisibility Timeout configured to send notifications and alerts</strong> is incorrect. AWS Certificate Manager is used to provision, manage and deploy SSL certificates, not software licenses. In addition, you should use Amazon SNS for notification and not an Amazon SQS queue.</p><p>The option that says: <strong>Upload the licenses on AWS Systems Manager Fleet Manager to be encrypted and distributed to Amazon EC2 instances. Attach an IAM role on the EC2 instances to request a license from the Fleet Manager. Set up an Amazon SNS to send notifications and alerts once all licenses are used</strong> is incorrect. The Fleet Manager is just one of the many capabilities of the AWS Systems Manager that provides a unified user interface (UI) experience in remotely managing your nodes/EC2 instances running on AWS or on-premises. This service is not capable of managing the software licenses of your EC2 instances.</p><p>The option that says: <strong>Configure AWS Resource Access Manager (AWS RAM) to track and control the licenses used by AWS resources. Configure AWS RAM to provide available licenses for Amazon EC2 instances. Set up an Amazon SNS to send notifications and alerts once all licenses are used</strong> is incorrect. AWS RAM is used securely share your resources across AWS accounts in your AWS organization and not for tracking or controlling software licenses.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/license-manager/latest/userguide/license-manager.html\">https://docs.aws.amazon.com/license-manager/latest/userguide/license-manager.html</a></p><p><a href=\"https://docs.aws.amazon.com/license-manager/latest/userguide/license-manager-overview.html\">https://docs.aws.amazon.com/license-manager/latest/userguide/license-manager-overview.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/mechanisms-to-govern-license-usage-with-aws-license-manager/\">https://aws.amazon.com/blogs/mt/mechanisms-to-govern-license-usage-with-aws-license-manager/</a></p><p><br></p><p><strong>Check out this AWS License Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-license-manager/?src=udemy\">https://tutorialsdojo.com/aws-license-manager/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Define licensing rules on AWS License Manager to track and control license usage. Enable the option to “Enforce license limit” to prevent going over the number of allocated licenses. Add an Amazon SNS topic to send notifications and alerts.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Define license configuration rules on AWS Certificate Manager to track and control license usage. Enable the option to “Enforce certificate limit” to prevent going over the number of allocated licenses. Add an Amazon SQS queue with ChangeVisibility Timeout configured to send notifications and alerts.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Upload the licenses on AWS Systems Manager Fleet Manager to be encrypted and distributed to Amazon EC2 instances. Attach an IAM role on the EC2 instances to request a license from the Fleet Manager. Set up an Amazon SNS to send notifications and alerts once all licenses are used</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure AWS Resource Access Manager (AWS RAM) to track and control the licenses used by AWS resources. Configure AWS RAM to provide available licenses for Amazon EC2 instances. Set up an Amazon SNS to send notifications and alerts once all licenses are used.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 7,
    "question": "<p>An organization uses a Microsoft SQL Server database to support its suite of applications. The organization plans to transition to an Amazon Aurora PostgreSQL database while minimizing application code modifications.</p><p>Which combination of actions will achieve these objectives? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>AWS Schema Conversion Tool (AWS SCT)</strong> is a service that helps convert database schemas from one database engine to another. It supports conversion from various source databases, including Microsoft SQL Server, to target databases like Amazon Aurora PostgreSQL.</p><p><strong>AWS Database Migration Service (AWS DMS)</strong> is a service that helps migrate databases to AWS quickly and securely. It supports continuous data replication with minimal downtime.</p><p><strong>Babelfish for Aurora PostgreSQL</strong> is an extension designed for Amazon Aurora PostgreSQL. It allows the database to interpret commands from applications developed for Microsoft SQL Server. By serving as a translation layer, it converts T-SQL (Microsoft's SQL dialect) into PostgreSQL, enabling SQL Server applications to run directly on Aurora PostgreSQL with minimal or no changes required to the code.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-babelfish-for-aurora-postgressql-01-06-25.png\"></p><p>In the given scenario, where an organization is transitioning from Microsoft SQL Server to Amazon Aurora PostgreSQL, Babelfish is a crucial tool. By turning on the Babelfish, the organization can ensure its existing applications continue functioning with minimal modifications. Babelfish will translate the SQL Server queries to be compatible with Aurora PostgreSQL. This addresses one of the primary objectives of the migration, which is to minimize application code modifications.</p><p>Furthermore, using the AWS Schema Conversion Tool (SCT) and AWS Database Migration Service (DMS) together is a logical approach. AWS SCT will handle the conversion of the Microsoft SQL Server schema to the Aurora PostgreSQL schema, while AWS DMS will facilitate the migration of data from the existing database to the new Aurora PostgreSQL database. This combination of services effectively addresses another primary goal of the migration: successfully converting the schema and migrating the data.</p><p>Hence, the correct answers are:</p><p><strong>- Turn on Babelfish on Aurora PostgreSQL to allow applications to continue using existing SQL queries.</strong></p><p><strong>- Use AWS Schema Conversion Tool (AWS SCT) to convert the database schema and AWS Database Migration Service (AWS DMS) to migrate the data.</strong></p><p>The option that says: <strong>Use AWS AppConfig to manage configuration updates during the migration</strong> is incorrect because AWS AppConfig is primarily designed to manage and deploy application configuration changes in a controlled and safe manner. It does not address the key challenges of schema conversion, data migration, or SQL query compatibility crucial for transitioning from Microsoft SQL Server to Amazon Aurora PostgreSQL.</p><p>The option that says: <strong>Use Amazon Kinesis Data Streams for real-time data replication to Aurora PostgreSQL</strong> is incorrect. Kinesis Data Streams is typically used for real-time data ingestion and processing, not for database schema conversion or migration. While it can handle real-time data replication, it does not address the need to convert the database schema or ensure SQL query compatibility.</p><p>The option that says: <strong>Use AWS Glue to transform the SQL queries from the applications for compatibility with Aurora PostgreSQL</strong> is incorrect. AWS Glue is primarily an extract, transform, and load (ETL) service designed for data preparation and loading for analytics. It is not intended for transforming SQL queries to ensure compatibility with a different database engine.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish.html</a></p><p><a href=\"https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Welcome.html\">https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html\">https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html</a></p><p><br></p><p><strong>Check out these AWS Database Migration Service and Amazon Aurora Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-database-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-database-migration-service/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora/?src=udemy\">https://tutorialsdojo.com/amazon-aurora/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS AppConfig to manage configuration updates during the migration.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Turn on Babelfish on Aurora PostgreSQL to allow applications to continue using existing SQL queries.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Schema Conversion Tool (AWS SCT) to convert the database schema and AWS Database Migration Service (AWS DMS) to migrate the data.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Kinesis Data Streams for real-time data replication to Aurora PostgreSQL.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use AWS Glue to transform the SQL queries from the applications for compatibility with Aurora PostgreSQL.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2,
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 8,
    "question": "<p>A company has a set of Linux servers running on multiple On-Demand EC2 Instances. The Audit team wants to collect and process the application log files generated from these servers for their report.</p><p>Which of the following services is best to use in this case?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>Amazon EMR</strong> is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data. By using these frameworks and related open-source projects such as Apache Hive and Apache Pig, you can process data for analytics purposes and business intelligence workloads. Additionally, you can use Amazon EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service (Amazon S3) and Amazon DynamoDB.</p><p><img src=\"https://media.tutorialsdojo.com/Big-Data-Redesign_Diagram_Enterprise-Data-Warehouse.52d3e98fc79bf05e60c0f8278f067de595d5d3b2.png\"></p><p>Hence, the correct answer is: <strong>Amazon S3 for storing the application log files and Amazon Elastic MapReduce for processing the log files.</strong></p><p>The option that says: <strong>Amazon S3 Glacier for storing the application log files and Spot EC2 Instances for processing them</strong> is incorrect as Amazon S3 Glacier is used for data archive only.</p><p>The option that says: <strong>A single On-Demand Amazon EC2 instance for both storing and processing the log files</strong> is incorrect as an EC2 instance is not a recommended storage service. In addition, Amazon EC2 does not have a built-in data processing engine to process large amounts of data.</p><p>The option that says: <strong>Amazon S3 Glacier Deep Archive for storing the application log files and AWS ParallelCluster for processing the log files</strong> is incorrect because the long retrieval time of Amazon S3 Glacier Deep Archive makes this option unsuitable. Moreover, AWS ParallelCluster is just an AWS-supported open-source cluster management tool that makes it easy for you to deploy and manage High-Performance Computing (HPC) clusters on AWS. ParallelCluster uses a simple text file to model and provision all the resources needed for your HPC applications in an automated and secure manner.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html\">http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html</a></p><p><a href=\"https://aws.amazon.com/hpc/parallelcluster/\">https://aws.amazon.com/hpc/parallelcluster/</a></p><p><br></p><p><strong>Check out this Amazon EMR Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-emr/?src=udemy\">https://tutorialsdojo.com/amazon-emr/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Amazon S3 for storing the application log files and Amazon Elastic MapReduce for processing the log files.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon S3 Glacier for storing the application log files and Spot EC2 Instances for processing them.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "A single On-Demand Amazon EC2 instance for both storing and processing the log files",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon S3 Glacier Deep Archive for storing the application log files and AWS ParallelCluster for processing the log files.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 9,
    "question": "<p>To save costs, your manager instructed you to analyze and review the setup of your AWS cloud infrastructure. You should also provide an estimate of how much your company will pay for all of the AWS resources that they are using. In this scenario, which of the following will incur costs? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p>Billing commences when Amazon EC2 initiates the boot sequence of an AMI instance. Billing ends when the instance terminates, which could occur through a web services command, by running \"shutdown -h\", or through instance failure. When you stop an instance, AWS shuts it down but doesn't charge hourly usage for a stopped instance or data transfer fees. However, AWS does charge for the storage of any Amazon EBS volumes.</p><p>Hence, <strong>a running EC2 Instance</strong> and <strong>EBS Volumes attached to stopped EC2 Instances</strong> are the right answers and conversely, <strong>a stopped On-Demand EC2 Instance</strong> is incorrect as there is no charge for a stopped EC2 instance that you have shut down.</p><p><strong>Using Amazon VPC</strong> is incorrect because there are no additional charges for creating and using the VPC itself. Usage charges for other Amazon Web Services, including Amazon EC2, still apply at published rates for those resources, including data transfer charges.</p><p><strong>Public Data Set</strong> is incorrect due to the fact that Amazon stores the data sets at no charge to the community and, as with all AWS services, you pay only for the compute and storage you use for your own applications.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p><p><a href=\"https://aws.amazon.com/vpc/faqs\">https://aws.amazon.com/vpc/faqs</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "A running EC2 Instance",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>A stopped On-Demand EC2 Instance</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "EBS Volumes attached to stopped EC2 Instances",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Using an Amazon VPC",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Public Data Set",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 10,
    "question": "<p>A multinational bank is storing its confidential files in an S3 bucket. The security team recently performed an audit, and the report shows that multiple files have been uploaded without 256-bit Advanced Encryption Standard (AES) server-side encryption. For added protection, the encryption key must be automatically rotated every year. The solutions architect must ensure that there would be no other unencrypted files uploaded in the S3 bucket in the future.</p><p>Which of the following will meet these requirements with the LEAST operational overhead?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>A <strong>bucket policy</strong> is a resource-based policy that you can use to grant access permissions to your bucket and the objects in it. Server-side encryption protects data at rest. Amazon S3 encrypts each object with a unique key. Amazon S3 server-side encryption uses one of the strongest block ciphers available to encrypt your data, 256-bit Advanced Encryption Standard (<strong>AES-256</strong>).</p><p><img src=\"https://media.tutorialsdojo.com/s3-deny-policy.png\"></p><p>If you need server-side encryption for all of the objects that are stored in a bucket, use a bucket policy. You can create a bucket policy that denies permissions to upload an object unless the request includes the <code>x-amz-server-side-encryption</code> header to request server-side encryption.</p><p><img src=\"https://media.tutorialsdojo.com/amazons3-server-side-encryption-upload.png\"></p><p>Automatic key rotation is disabled by default on customer managed keys but authorized users can enable and disable it. When you enable (or re-enable) automatic key rotation, AWS KMS automatically rotates the KMS key one year (approximately 365 days) after the enable date and every year thereafter.</p><p>AWS KMS automatically rotates AWS managed keys every year (approximately 365 days). You cannot enable or disable key rotation for AWS managed keys.</p><p>Hence, the correct answer is <strong>Create an S3 bucket policy that denies permissions to upload an object unless the request includes the </strong><code><strong>s3:x-amz-server-side-encryption\": \"AES256\"</strong></code><strong> header. Enable server-side encryption with Amazon S3-managed encryption keys (SSE-S3) and rely on the built-in key rotation feature of the SSE-S3 encryption keys.</strong></p><p>The option that says:<strong> Create a Service Control Policy (SCP) for the S3 bucket that rejects any object uploads unless the request includes the </strong><code><strong>s3:x-amz-server-side-encryption\": \"AES256\"</strong></code><strong> header. Enable server-side encryption with Amazon S3-managed encryption keys (SSE-S3) and modify the built-in key rotation feature of the SSE-S3 encryption keys to rotate the key yearly </strong>is incorrect. Although it could work, you cannot modify the built-in key rotation feature manually in SSE-S3. These keys are managed by AWS, and not you. Moreover, you have to use a bucket policy in Amazon S3 and not a Service Control Policy, since the latter is primarily used for controlling multiple AWS accounts.</p><p>The option that says:<strong> Create a new customer-managed key from the AWS Key Management Service (AWS KMS). Configure the default encryption behavior of the bucket to use the customer-managed key. Manually rotate the KMS key and every year </strong>is incorrect because this solution is missing a bucket policy that denies S3 object uploads without any encryption. Conversely, the act of manually rotating the KMS key each and every year adds to the manual management overhead for this solution.</p><p>The option that says:<strong> Create an S3 bucket policy for the S3 bucket that rejects any object uploads unless the request includes the </strong><code><strong>s3:x-amz-server-side-encryption\":\"aws:kms\"</strong></code><strong> header. Enable the S3 Object Lock in compliance mode for all objects to automatically rotate the built-in AES256 customer-managed key of the bucket</strong> is incorrect because the scenario says that the encryption required is 256-bit Advanced Encryption Standard (AES-256), not AWS Key Management Service (SSE-KMS). Take note that that if the value of the s3:x-amz-server-side-encryption header is aws:kms, then the S3 encryption type is SSE-KMS. This type of server-side encryption does not use a 256-bit Advanced Encryption Standard (AES-256), unlike the SSE-S3 type. In addition, an S3 Object Lock feature is not used for encryption but instead, for preventing the objects from being deleted or overwritten for a fixed amount of time or indefinitely.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policies.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html</a></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a Service Control Policy (SCP) for the S3 bucket that rejects any object uploads unless the request includes the <code>s3:x-amz-server-side-encryption\": \"AES256\"</code> header. Enable server-side encryption with Amazon S3-managed encryption keys (SSE-S3) and modify the built-in key rotation feature of the SSE-S3 encryption keys to rotate the key yearly.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create an S3 bucket policy that denies permissions to upload an object unless the request includes the <code>s3:x-amz-server-side-encryption\": \"AES256\"</code> header. Enable server-side encryption with Amazon S3-managed encryption keys (SSE-S3) and rely on the built-in key rotation feature of the SSE-S3 encryption keys.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a new customer-managed key from the AWS Key Management Service (AWS KMS). Configure the default encryption behavior of the bucket to use the customer-managed key. Manually rotate the KMS key and every year.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an S3 bucket policy for the S3 bucket that rejects any object uploads unless the request includes the <code>s3:x-amz-server-side-encryption\":\"aws:kms\"</code> header. Enable the S3 Object Lock in compliance mode for all objects to automatically rotate the built-in AES256 customer-managed key of the bucket.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 11,
    "question": "<p>A company is using the AWS Directory Service to integrate their on-premises Microsoft Active Directory (AD) domain with their Amazon EC2 instances via an AD connector. The below identity-based policy is attached to the IAM Identities that use the AWS Directory service:</p><p><br></p><pre class=\"prettyprint linenums\">{\n \"Version\":\"2012-10-17\",\n \"Statement\":[\n  {\n   \"Sid\":\"DirectoryTutorialsDojo1234\",\n   \"Effect\":\"Allow\",\n   \"Action\":[\n    \"ds:*\"\n   ],\n   \"Resource\":\"arn:aws:ds:us-east-1:987654321012:directory/d-1234567890\"\n  },\n  {\n   \"Effect\":\"Allow\",\n   \"Action\":[\n   \"ec2:*\"\n   ],\n   \"Resource\":\"*\"\n  }\n ]\n}</pre><p><br></p><p>Which of the following BEST describes what the above resource policy does?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS Directory Service</strong> provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other AWS services. Directories store information about users, groups, and devices, and administrators use them to manage access to information and resources. AWS Directory Service provides multiple directory choices for customers who want to use existing Microsoft AD or Lightweight Directory Access Protocol (LDAP)–aware applications in the cloud. It also offers those same choices to developers who need a directory to manage users, groups, devices, and access.</p><p><img src=\"https://media.tutorialsdojo.com/directory_service_howitworks.80bfccbf2f5d1d63558ec3c086aff247147258f1.png\"></p><p>Every AWS resource is owned by an AWS account, and permissions to create or access the resources are governed by permissions policies. An account administrator can attach permissions policies to IAM identities (that is, users, groups, and roles), and some services (such as AWS Lambda) also support attaching permissions policies to resources.</p><p>The following resource policy example allows all <code>ds</code> calls as long as the resource contains the directory ID \"<code>d-1234567890</code>\".</p><pre class=\"prettyprint linenums\">{\n \"Version\":\"2012-10-17\",\n \"Statement\":[\n    {\n        \"Sid\":\"VisualEditor0\",\n        \"Effect\":\"Allow\",\n        \"Action\":[\n            \"ds:*\"\n         ],\n                 \"Resource\":\"arn:aws:ds:us-east-1:123456789012:directory/d-1234567890\"\n    },\n    {\n    \"Effect\":\"Allow\",\n    \"Action\":[\n        \"ec2:*\"\n    ],\n    \"Resource\":\"*\"\n    }\n  ]\n}</pre><p><br></p><p>Hence, the correct answer is the option that says: <strong>Allows all AWS Directory Service </strong><code><strong>(ds)</strong></code><strong> calls as long as the resource contains the directory ID: </strong><code><strong>d-1234567890</strong></code>.</p><p>The option that says: <strong>Allows all AWS Directory Service </strong><code><strong>(ds)</strong></code><strong> calls as long as the resource contains the directory ID: </strong><code><strong>DirectoryTutorialsDojo1234</strong></code> is incorrect because <code>DirectoryTutorialsDojo1234</code> is the Statement ID (SID) and not the Directory ID.</p><p>The option that says: <strong>Allows all AWS Directory Service </strong><code><strong>(ds)</strong></code><strong> calls as long as the resource contains the directory ID: </strong><code><strong>987654321012</strong></code> is incorrect because the numbers: <code>987654321012</code> is the Account ID and not the Directory ID.</p><p>The option that says: <strong>Allows all AWS Directory Service </strong><code><strong>(ds)</strong></code><strong> calls as long as the resource contains the directory name of: </strong><code><strong>DirectoryTutorialsDojo1234</strong></code> is incorrect because <code>DirectoryTutorialsDojo1234</code> is the Statement ID (SID) and not the Directory name.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/IAM_Auth_Access_IdentityBased.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/IAM_Auth_Access_IdentityBased.html</a></p><p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/IAM_Auth_Access_Overview.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/IAM_Auth_Access_Overview.html</a></p><p><br></p><p><strong>Check out this AWS Identity &amp; Access Management (IAM) Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Allows all AWS Directory Service (<code>ds</code>) calls as long as the resource contains the directory ID: <code>d-1234567890</code></p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Allows all AWS Directory Service (<code>ds</code>) calls as long as the resource contains the directory ID: <code>DirectoryTutorialsDojo1234</code></p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Allows all AWS Directory Service (<code>ds</code>) calls as long as the resource contains the directory ID:&nbsp; <code>987654321012</code></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Allows all AWS Directory Service (<code>ds</code>) calls as long as the resource contains the directory name of: <code>DirectoryTutorialsDojo1234</code></p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 12,
    "question": "<p>A technology company is building a new cryptocurrency trading platform that allows the buying and selling of Bitcoin, Ethereum, Ripple, Tether, and many others. A Cloud Engineer was hired to build the infrastructure required for this platform. During the first week, the engineer began creating CloudFormation YAML scripts to define all of the AWS resources needed for the application. The manager was shocked that the engineer hadn't set up the EC2 instances, S3 buckets, and other AWS resources immediately. He doesn't understand the purpose of the text-based scripts the engineer has written and has asked for clarification.</p><p>In this scenario, what are the benefits of using Amazon CloudFormation that the manager should know to address his concerns? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>AWS CloudFormation</strong> provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This file serves as the single source of truth for your cloud environment. AWS CloudFormation is available at no additional charge, and you pay only for the AWS resources needed to run your applications.</p><p><img src=\"https://media.tutorialsdojo.com/create-stack-diagram.png\"></p><p>Hence, the correct answers are:</p><p><strong>- Enables modeling, provisioning, and version-controlling of your entire AWS infrastructure</strong></p><p><strong>- Allows you to model your entire infrastructure in a text file</strong></p><p>The option that says: <strong>Provides highly durable and scalable data storage</strong> is incorrect because CloudFormation is not a data storage service.</p><p>The option that says: <strong>A storage location for the code of your application</strong> is incorrect because CloudFormation is not primarily used to store your application code.</p><p>The option that says: <strong>Using CloudFormation itself is free, including the AWS resources that have been created</strong> is incorrect because although the use of CloudFormation service is only free, you have to pay for the AWS resources that you created.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p><p><a href=\"https://aws.amazon.com/cloudformation/faqs/\">https://aws.amazon.com/cloudformation/faqs/</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Provides highly durable and scalable data storage</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "A storage location for the code of your application",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enables modeling, provisioning, and version-controlling of your entire AWS infrastructure</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Allows you to model your entire infrastructure in a text file",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Using CloudFormation itself is free, including the AWS resources that have been created.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3,
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 13,
    "question": "<p>A company has a web application hosted in AWS cloud where the application logs are sent to Amazon CloudWatch. Lately, the web application has recently been encountering some errors which can be resolved simply by restarting the instance.</p><p>What will you do to automatically restart the EC2 instances whenever the same application error occurs?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>In this scenario, you can look at the existing CloudWatch logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which invokes an action to restart the EC2 instance.</p><p><img src=\"https://media.tutorialsdojo.com/AWS-CW-Alarm.PNG\"></p><p>You can create alarms that automatically stop, terminate, reboot, or recover your EC2 instances using Amazon CloudWatch alarm actions. You can use the stop or terminate actions to help you save money when you no longer need an instance to be running. You can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs.</p><p>Hence, the correct answer is: <strong>First, look at the existing CloudWatch logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which invokes an action to restart the EC2 instance.</strong></p><p>The option that says: <strong>First, look at the existing CloudWatch logs for keywords related to the application error to create a custom metric. Then, create an alarm in Amazon SNS for that custom metric which invokes an action to restart the EC2 instance</strong> is incorrect because you can't create an alarm in Amazon SNS.</p><p>The following options are incorrect because Flow Logs are used in VPC and not on specific EC2 instance:</p><p><strong>- First, look at the existing Flow logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which invokes an action to restart the EC2 instance.</strong></p><p><strong>First, look at the existing Flow logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which calls a Lambda function that invokes an action to restart the EC2 instance.</strong></p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "First, look at the existing CloudWatch logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which invokes an action to restart the EC2 instance.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "First, look at the existing CloudWatch logs for keywords related to the application error to create a custom metric. Then, create an alarm in Amazon SNS for that custom metric which invokes an action to restart the EC2 instance.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "First, look at the existing Flow logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which invokes an action to restart the EC2 instance.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "First, look at the existing Flow logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which calls a Lambda function that invokes an action to restart the EC2 instance.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 14,
    "question": "<p>A real-time data analytics application is using AWS Lambda to process data and store results in JSON format to an S3 bucket. To speed up the existing workflow, you have to use a service where you can run sophisticated Big Data analytics on your data without moving them into a separate analytics system.   </p><p>Which of the following group of services can you use to meet this requirement?  </p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>Amazon Athena, Amazon Redshift Spectrum, and AWS Glue are highly relevant services for performing Big Data analytics directly on data stored in Amazon S3, without needing to move the data to a separate system.</p><p><strong>Amazon Athena</strong> allows users to query data directly in Amazon S3 using SQL, providing a serverless approach to perform analytics on S3-stored data in formats such as JSON.</p><p><img src=\"https://media.tutorialsdojo.com/public/Amazon-Athena-02Oct24.png\"></p><p><strong>Amazon Redshift Spectrum</strong> extends the querying capabilities of Amazon Redshift to also access and analyze structured and semi-structured data in S3, supporting large-scale analytics.</p><p><img src=\"https://media.tutorialsdojo.com/public/Amazon-Redshift-2Oct2024.png\"></p><p><strong>AWS Glue</strong> is a fully managed ETL (Extract, Transform, Load) service that helps catalog, prepare, and transform data in S3 for analytics, simplifying Big Data workflows.</p><p><img src=\"https://media.tutorialsdojo.com/public/TD-AWS-Glue-4Sept2024.png\"></p><p>These services together provide an efficient way to perform sophisticated analytics on S3-stored data without moving it, which fits the scenario requirements.</p><p>Hence, the correct answer is: <strong>Amazon Athena, Amazon Redshift Spectrum, AWS Glue.</strong></p><p>The option that says: <strong>Amazon Neptune, DynamoDB DAX, Amazon Redshift Spectrum</strong> is incorrect because Amazon Neptune and DynamoDB DAX simply do not support analytics or querying capabilities on data stored in S3. They are used for graph databases and caching, respectively.</p><p>The option that says: <strong>Amazon X-Ray, Amazon Neptune, DynamoDB</strong> is incorrect because Amazon X-Ray is only a tracing service, and Neptune and DynamoDB are not relevant for analytics on S3 data.</p><p>The option that says: <strong>Amazon Glue, Amazon Redshift, Amazon S3</strong> is incorrect. This option is only partially correct. While Glue can transform and catalog data in S3 and S3 provides storage, Redshift requires data to be loaded into its cluster for analytics. This contradicts the requirement to perform analytics directly on data in S3 without moving it.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/what-is.html\">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html</a></p><p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html\">https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html</a></p><p><br></p><p><strong>Amazon Redshift Overview:</strong></p><p><br></p><p><strong>Check out these Amazon Athena, Amazon Redshift, and AWS Glue Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-athena/?src=udemy\">https://tutorialsdojo.com/amazon-athena/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-redshift/?src=udemy\">https://tutorialsdojo.com/amazon-redshift/</a></p><p><a href=\"https://tutorialsdojo.com/aws-glue/?src=udemy\">https://tutorialsdojo.com/aws-glue/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Neptune, DynamoDB DAX, Amazon Redshift Spectrum</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon X-Ray, Amazon Neptune, DynamoDB  </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Glue, Amazon Redshift, Amazon S3</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Athena, Amazon Redshift Spectrum, AWS Glue</p>",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 15,
    "question": "<p>A company has a web application hosted in an On-Demand EC2 instance. You are creating a shell script that needs the instance's public and private IP addresses.</p><p>What is the best way to get the instance's associated IP addresses which your shell script can use?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>Instance metadata is data about your EC2 instance that you can use to configure or manage the running instance. Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you're writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application.</p><p><img src=\"https://media.tutorialsdojo.com/AWS-EC2-Metadata.png\"></p><p>To view the private IPv4 address, public IPv4 address, and all other categories of instance metadata from within a running instance, use the following URL:</p><pre class=\"prettyprint linenums\">http://169.254.169.254/latest/meta-data/</pre><p><br></p><p><strong>Reference:</strong></p><p><a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "By using IAM.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>By using a CloudWatch metric.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "By using a Curl or Get Command to get the latest metadata information from http://169.254.169.254/latest/meta-data/",
        "correct": true
      },
      {
        "id": 4,
        "answer": "By using a Curl or Get Command to get the latest user data information from http://169.254.169.254/latest/user-data/",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 16,
    "question": "<p>A technical lead of the Cloud Infrastructure team was consulted by a software developer regarding the required AWS resources of the web application that he is building. The developer knows that an Instance Store only provides ephemeral storage where the data is automatically deleted when the instance is terminated. To ensure that the data of the web application persists, the app should be launched in an EC2 instance that has a durable, block-level storage volume attached. The developer knows that they need to use an EBS volume, but they are not sure what type they need to use.</p><p>In this scenario, which of the following is true about Amazon EBS volume types and their respective usage? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon EBS</strong> provides three volume types to best meet the needs of your workloads: General Purpose (SSD), Provisioned IOPS (SSD), and Magnetic.</p><p><img src=\"https://media.tutorialsdojo.com/overview_getting_started.png\"></p><p>General Purpose (SSD) is the new, SSD-backed, general purpose EBS volume type that is recommended as the default choice for customers. General Purpose (SSD) volumes are suitable for a broad range of workloads, including small to medium-sized databases, development and test environments, and boot volumes.</p><p>Provisioned IOPS (SSD) volumes offer storage with consistent and low-latency performance and are designed for I/O intensive applications such as large relational or NoSQL databases. Magnetic volumes provide the lowest cost per gigabyte of all EBS volume types.</p><p>Magnetic volumes are ideal for workloads where data are accessed infrequently, and applications where the lowest storage cost is important. Take note that this is a Previous Generation Volume. The latest low-cost magnetic storage types are Cold HDD (sc1) and Throughput Optimized HDD (st1) volumes.</p><p>Hence, the correct answers are:</p><p><strong>- Provisioned IOPS volumes offer storage with consistent and low-latency performance, and are designed for I/O intensive applications such as large relational or NoSQL databases.</strong></p><p><strong>- Magnetic volumes provide the lowest cost per gigabyte of all EBS volume types and are ideal for workloads where data is accessed infrequently, and applications where the lowest storage cost is important.</strong></p><p>The option that says: <strong>Spot volumes provide the lowest cost per gigabyte of all EBS volume types and are ideal for workloads where data is accessed infrequently, and applications where the lowest storage cost is important</strong> is incorrect because there is no EBS type called a \"Spot volume\" however, there is an Instance purchasing option for Spot Instances.</p><p>The option that says: <strong>General Purpose SSD (gp3) volumes with multi-attach enabled offer consistent and low-latency performance, and are designed for applications requiring multi-az resiliency</strong> is incorrect because the multi-attach feature can only be enabled on EBS Provisioned IOPS io2 or io1 volumes. In addition, multi-attach won't offer multi-az resiliency because this feature only allows an EBS volume to be attached on multiple instances <strong>within</strong> an availability zone.</p><p>The option that says: <strong>Single root I/O virtualization (SR-IOV) volumes are suitable for a broad range of workloads, including small to medium-sized databases, development and test environments, and boot volumes</strong> is incorrect because SR-IOV is related with Enhanced Networking on Linux and not in EBS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html</a></p><p><br></p><p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-ebs/?src=udemy\">https://tutorialsdojo.com/amazon-ebs/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Spot volumes provide the lowest cost per gigabyte of all EBS volume types and are ideal for workloads where data is accessed infrequently, and applications where the lowest storage cost is important.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Provisioned IOPS volumes offer storage with consistent and low-latency performance, and are designed for I/O intensive applications such as large relational or NoSQL databases. </p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Magnetic volumes provide the lowest cost per gigabyte of all EBS volume types and are ideal for workloads where data is accessed infrequently, and applications where the lowest storage cost is important.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>General Purpose SSD (gp3) volumes with multi-attach enabled offer consistent and low-latency performance, and are designed for applications requiring multi-az resiliency.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Single root I/O virtualization (SR-IOV) volumes are suitable for a broad range of workloads, including small to medium-sized databases, development and test environments, and boot volumes.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2,
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 17,
    "question": "<p>A company has an OLTP (Online Transactional Processing) application that is hosted in an Amazon ECS cluster using the Fargate launch type. It has an Amazon RDS database that stores data of its production website. The Data Analytics team needs to run queries against the database to track and audit all user transactions. These query operations against the production database must not impact application performance in any way.</p><p>Which of the following is the MOST suitable and cost-effective solution that you should implement?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>Amazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.</p><p>You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, Oracle and PostgreSQL, as well as Amazon Aurora.</p><p><img src=\"https://media.tutorialsdojo.com/public/2020-01-16_09-06-35-c09d77a9e6c2998538885b574784c9df.png\"></p><p>You can reduce the load on your source DB instance by routing read queries from your applications to the read replica. These replicas allow you to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.</p><p>Because read replicas can be promoted to master status, they are useful as part of a sharding implementation. To shard your database, add a read replica and promote it to master status, then, from each of the resulting DB Instances, delete the data that belongs to the other shard.</p><p>Hence, the correct answer is:<strong> Set up a new Amazon RDS Read Replica of the production database. Direct the Data Analytics team to query the production data from the replica.</strong></p><p>The option that says: <strong>Set up a new Amazon Redshift database cluster. Migrate the product database into Redshift and allow the Data Analytics team to fetch data from it</strong> is incorrect because Redshift is primarily used for OLAP (Online Analytical Processing) applications and not for OLTP.</p><p>The option that says: <strong>Set up a Multi-AZ deployments configuration of your production database in RDS. Direct the Data Analytics team to query the production data from the standby instance</strong> is incorrect because you can't directly connect to the standby instance. This is only used in the event of a database failover when your primary instance encountered an outage.</p><p>The option that says: <strong>Upgrade the instance type of the RDS database to a large instance</strong> is incorrect because this entails a significant amount of cost. Moreover, the production database could still be affected by the queries done by the Data Analytics team. A better solution for this scenario is to use a Read Replica instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/caching/database-caching/ \">https://aws.amazon.com/caching/database-caching/</a></p><p><a href=\"https://aws.amazon.com/rds/details/read-replicas/\">https://aws.amazon.com/rds/details/read-replicas/</a></p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a new Amazon Redshift database cluster. Migrate the product database into Redshift and allow the Data Analytics team to fetch data from it.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up a new Amazon RDS Read Replica of the production database. Direct the Data Analytics team to query the production data from the replica.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up a Multi-AZ deployments configuration of your production database in RDS. Direct the Data Analytics team to query the production data from the standby instance.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Upgrade the instance type of the RDS database to a large instance.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 18,
    "question": "<p>A company has a global news website hosted in a fleet of EC2 Instances. Lately, the load on the website has increased which resulted in slower response time for the site visitors. This issue impacts the revenue of the company as some readers tend to leave the site if it does not load after 10 seconds. The goal is to address the issue in a cost-effective manner.”</p><p>Which of the below services in AWS can be used to solve this problem? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>The global news website has a problem with latency considering that there are a lot of readers of the site from all parts of the globe. In this scenario, you can use a content delivery network (CDN) which is a geographically distributed group of servers that work together to provide fast delivery of Internet content. And since this is a news website, most of its data are read-only, which can be cached to improve the read throughput and avoid repetitive requests from the server.</p><p><img src=\"https://media.tutorialsdojo.com/how-you-configure-cf.png\"></p><p>In AWS, Amazon CloudFront is the global content delivery network (CDN) service that you can use, and for web caching, Amazon ElastiCache is the suitable service.</p><p>Hence, the correct answers are:</p><p><strong>- Use Amazon CloudFront with website as the custom origin.</strong></p><p><strong>- Use Amazon ElastiCache for the website's in-memory data store or cache.</strong></p><p>The option that says: <strong>For better read throughput, use AWS Storage Gateway to distribute the content across multiple regions</strong> is incorrect as AWS Storage Gateway is only used for storage.</p><p>The option that says:<strong> Deploy the website to all regions in different VPCs for faster processing</strong> is incorrect as this typically would be costly and totally unnecessary, considering that you can use Amazon CloudFront and ElastiCache to improve the performance of the website.</p><p>The option that says: <strong>Use Amazon RDS Multi-AZ deployments for database read scalability </strong>is incorrect.<strong> </strong>While Amazon RDS Multi-AZ deployments provide high availability and automatic failover, they primarily focus on database redundancy rather than improving website performance. Multi-AZ deployments are not directly related to reducing response times for the website.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><a href=\"http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon CloudFront with website as the custom origin.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "For better read throughput, use AWS Storage Gateway to distribute the content across multiple regions.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Use Amazon ElastiCache for the website's in-memory data store or cache.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Deploy the website to all regions in different VPCs for faster processing.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use Amazon RDS Multi-AZ deployments for database read scalability.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 19,
    "question": "<p>A company has a High Performance Computing (HPC) cluster that is composed of EC2 Instances with Provisioned IOPS (io1) volume to process transaction-intensive, low-latency workloads. The Solutions Architect must maintain high IOPS while keeping the latency down by setting the optimal queue length for the volume. The size of each volume is 10 GiB.</p><p>Which of the following is the MOST suitable configuration that the Architect should set up?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Provisioned IOPS SSD (</strong><code><strong>io1</strong></code><strong>) volumes</strong> are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike <code>gp2</code>, which uses a bucket and credit model to calculate performance, an <code>io1</code> volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers within 10 percent of the provisioned IOPS performance 99.9 percent of the time over a given year.</p><p>An <code>io1</code> volume can range in size from 4 GiB to 16 TiB. You can provision from 100 IOPS up to 64,000 IOPS per volume on <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances\">Nitro system</a> instance families and up to 32,000 on other instance families. The maximum ratio of provisioned IOPS to the requested volume size (in GiB) is 50:1.</p><p>For example, a 100 GiB volume can be provisioned with up to 5,000 IOPS. On a supported instance type, any volume 1,280 GiB in size or greater allows provisioning up to the 64,000 IOPS maximum (50 × 1,280 GiB = 64,000).</p><p><img src=\"https://media.tutorialsdojo.com/public/io1_throughput.png\"></p><p>An <code>io1</code> volume provisioned with up to 32,000 IOPS supports a maximum I/O size of 256 KiB and yields as much as 500 MiB/s of throughput. With the I/O size at the maximum, peak throughput is reached at 2,000 IOPS. A volume provisioned with more than 32,000 IOPS (up to the cap of 64,000 IOPS) supports a maximum I/O size of 16 KiB and yields as much as 1,000 MiB/s of throughput.</p><p>The volume queue length is the number of pending I/O requests for a device. Latency is the true end-to-end client time of an I/O operation, in other words, the time elapsed between sending an I/O to EBS and receiving an acknowledgment from EBS that the I/O read or write is complete. Queue length must be correctly calibrated with I/O size and latency to avoid creating bottlenecks either on the guest operating system or on the network link to EBS.</p><p>Optimal queue length varies for each workload, depending on your particular application's sensitivity to IOPS and latency. If your workload is not delivering enough I/O requests to fully use the performance available to your EBS volume, then your volume might not deliver the IOPS or throughput that you have provisioned.</p><p>Transaction-intensive applications are sensitive to increased I/O latency and are well-suited for SSD-backed <code>io1</code> and <code>gp2</code> volumes. You can maintain high IOPS while keeping latency down by maintaining a low queue length and a high number of IOPS available to the volume. Consistently driving more IOPS to a volume than it has available can cause increased I/O latency.</p><p>Throughput-intensive applications are less sensitive to increased I/O latency and are well-suited for HDD-backed <code>st1</code> and <code>sc1</code> volumes. You can maintain high throughput to HDD-backed volumes by maintaining a high queue length when performing large, sequential I/O.</p><p>Therefore, for instance, a 10 GiB volume can be provisioned with up to <strong>500</strong> IOPS. Any volume 640 GiB in size or greater allows provisioning up to a maximum of 32,000 IOPS (50 × 640 GiB = 32,000). Hence, the correct answer is to <strong>set the IOPS to 500 then maintain a low queue length</strong>.</p><p><strong>Setting the IOPS to 400 then maintaining a low queue length</strong> is incorrect. Although a value of 400 is an acceptable value, it is not the maximum value for the IOPS. You will not fully utilize the available IOPS that the volume can offer if you just set it to 400.</p><p>The options that say: <strong>Set the IOPS to 600 then maintain a high queue length</strong> and<strong> Set the IOPS to 800 then maintain a low queue length</strong> are both incorrect because the maximum IOPS for the 10 GiB volume is only 500. Therefore, any value greater than the maximum amount, such as 600 or 800, is wrong. Moreover, you should keep the latency down by maintaining a low queue length, and not higher.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html\">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-io-characteristics.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-io-characteristics.html</a></p><p><br></p><p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-ebs/?src=udemy\">https://tutorialsdojo.com/amazon-ebs/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set the IOPS to 400 then maintain a low queue length.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set the IOPS to 500 then maintain a low queue length.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set the IOPS to 600 then maintain a high queue length.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set the IOPS to 800 then maintain a low queue length.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 20,
    "question": "<p>A company is running a web application on AWS. The application is made up of an Auto-Scaling group that sits behind an Application Load Balancer and an Amazon DynamoDB table where user data is stored. The solutions architect must design the application to remain available in the event of a regional failure. A solution to automatically monitor the status of your workloads across your AWS account, conduct architectural reviews and check for AWS best practices.</p><p>Which configuration meets the requirement with the least amount of downtime possible?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>When you have more than one resource performing the same function—for example, more than one HTTP serve—you can configure Amazon Route 53 to check the health of your resources and respond to DNS queries using only the healthy resources. For example, suppose your website, example.com, is hosted on six servers, two each in three data centers around the world. You can configure Route 53 to check the health of those servers and to respond to DNS queries for example.com using only the servers that are currently healthy.</p><p><strong><img src=\"https://media.tutorialsdojo.com/amazon-route-53-dns-failover.jpg\"></strong></p><p>In this scenario, you can replicate the process layer (EC2 instances, Application Load Balancer) to a different region and create a global table based on the existing DynamoDB table (data layer). Amazon DynamoDB will handle data synchronization between the tables in different regions. This way, the state of the application is preserved even in the event of an outage. Lastly, configure Route 53 DNS failover and set the DNS name of the backup application load balancer as a target.</p><p>You can also use the Well-Architected Tool to automatically monitor the status of your workloads across your AWS account, conduct architectural reviews and check for AWS best practices.</p><p><img src=\"https://media.tutorialsdojo.com/well-architected-tool-aws-saa-c03.png\"></p><p>This tool is based on the AWS Well-Architected Framework, which was developed to help cloud architects build secure, high-performing, resilient, and efficient application infrastructures. The Framework has been used in tens of thousands of workload reviews by AWS solutions architects, and it provides a consistent approach for evaluating your cloud architecture and implementing designs that will scale with your application needs over time.</p><p>Hence, the correct answer is: <strong>In a secondary region, create a global table of the DynamoDB table and replicate the auto-scaling group and application load balancer. Use Route 53 DNS failover to automatically route traffic to the resources in the secondary region. Set up the AWS Well-Architected Tool to easily get recommendations for improving your workloads based on the AWS best practices</strong></p><p>The option that says: <strong>In a secondary region, create a global secondary index of the DynamoDB table and replicate the auto-scaling group and application load balancer. Use Route 53 DNS failover to automatically route traffic to the resources in the secondary region. Set up the AWS Compute Optimizer to automatically get recommendations for improving your workloads based on the AWS best practices</strong> is incorrect because this configuration is impossible to implement. A global secondary index can only be created in the region where its parent table resides. Moreover, the AWS Compute Optimizer simply helps you to identify the optimal AWS resource configurations, such as Amazon Elastic Compute Cloud (EC2) instance types, Amazon Elastic Block Store (EBS) volume configurations, and AWS Lambda function memory sizes. It is not capable of providing recommendations to improve your workloads based on AWS best practices.</p><p>The option that says: <strong>Write a CloudFormation template that includes the auto-scaling group, application load balancer, and DynamoDB table. In the event of a failure, deploy the template in a secondary region. Use Route 53 DNS failover to automatically route traffic to the resources in the secondary region. Set up and configure the Amazon Managed Service for Prometheus service to receive insights for improving your workloads based on the AWS best practices </strong>is incorrect. This solution describes a situation in which the environment is provisioned only after a regional failure occurs. It won't work because to enable Route 53 DNS failover, you'd need to target an existing environment. The use of the Amazon Managed Service for Prometheus service is irrelevant as well. This is just a serverless, Prometheus-compatible monitoring service for container metrics that makes it easier to securely monitor container environments at scale.</p><p>The option that says: <strong>Write a CloudFormation template that includes the auto-scaling group, application load balancer, and DynamoDB table. In the event of a failure, deploy the template in a secondary region. Configure Amazon EventBridge (Amazon CloudWatch Events) to trigger a Lambda function that updates the application’s Route 53 DNS record. Launch an Amazon Managed Grafana workspace to automatically receive tips and action items for improving your workloads based on the AWS best practices </strong>is incorrect. This could work, but it won't deliver the shortest downtime possible since resource provisioning takes minutes to complete. Switching traffic to a standby environment is a faster method, albeit more expensive. Amazon Managed Grafana is a fully managed service with rich, interactive data visualizations to help customers analyze, monitor, and alarm on metrics, logs, and traces across multiple data sources. This service does not provide recommendations based on AWS best practices. You have to use the AWS Well-Architected Tool instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/creating-disaster-recovery-mechanisms-using-amazon-route-53/\">https://aws.amazon.com/blogs/networking-and-content-delivery/creating-disaster-recovery-mechanisms-using-amazon-route-53/</a></p><p><a href=\"https://aws.amazon.com/well-architected-tool\">https://aws.amazon.com/well-architected-tool</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>In a secondary region, create a global table of the DynamoDB table and replicate the auto-scaling group and application load balancer. Use Route 53 DNS failover to automatically route traffic to the resources in the secondary region. Set up the AWS Well-Architected Tool to easily get recommendations for improving your workloads based on the AWS best practices</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>In a secondary region, create a global secondary index of the DynamoDB table and replicate the auto-scaling group and application load balancer. Use Route 53 DNS failover to automatically route traffic to the resources in the secondary region. Set up the AWS Compute Optimizer to automatically get recommendations for improving your workloads based on the AWS best practices</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Write a CloudFormation template that includes the auto-scaling group, application load balancer, and DynamoDB table. In the event of a failure, deploy the template in a secondary region. Use Route 53 DNS failover to automatically route traffic to the resources in the secondary region. Set up and configure the Amazon Managed Service for Prometheus service to receive insights for improving your workloads based on the AWS best practices.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Write a CloudFormation template that includes the auto-scaling group, application load balancer, and DynamoDB table. In the event of a failure, deploy the template in a secondary region. Configure Amazon EventBridge (Amazon CloudWatch Events) to trigger a Lambda function that updates the application’s Route 53 DNS record. Launch an Amazon Managed Grafana workspace to automatically receive tips and action items for improving your workloads based on the AWS best practices</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 21,
    "question": "<p>A solutions architect is tasked with designing a scalable infrastructure solution for a business that runs uses Amazon Elastic Kubernetes Service (Amazon EKS) to execute container applications. Since the company's workload varies throughout the day, they want to make sure that its underlying infrastructure automatically scales in and out in response to demand.</p><p>Which of the following would meet the requirements with the LEAST amount of operational overhead?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>Autoscaling is a function that automatically scales your resources up or down to meet changing demands. This is a major Kubernetes function that would otherwise require extensive human resources to perform manually.</p><p>Amazon EKS supports two autoscaling solutions:</p><p>Karpenter is a flexible, high-performance Kubernetes cluster autoscaler that launches appropriately sized compute resources, like Amazon EC2 instances, in response to changing application load. It integrates with AWS to provision compute resources that precisely match workload requirements.</p><p>Cluster Autoscaler, on the other hand, automatically adjusts the number of nodes in a cluster based on pod failures or rescheduling. It utilizes Auto Scaling groups for managing the cluster's node capacity.</p><p><img src=\"https://media.tutorialsdojo.com/public/horizontal-pod-autoscaling-071923-0659PM.jpg\"></p><p>The Kubernetes Horizontal Pod Autoscaler (HPA) automatically scales the number of Pods based on CPU utilization, allowing applications to scale in or out to meet demand. It is a standard API resource in Kubernetes that requires a metrics source like the Kubernetes metrics server to be installed on the Amazon EKS cluster. The HPA does not need additional deployment or installation on the cluster to start scaling applications.</p><p>Hence, the correct answer is: <strong>Use a combination of Kubernetes Metrics and Kubernetes Cluster Autoscaler to manage the number of nodes.</strong></p><p>The option that says: <strong>Integrate an edge-optimized API endpoint in Amazon API Gateway with Amazon EKS to manage and expose APIs for the containerized applications running on EKS </strong>is incorrect because the question specifically asks for a solution that enables automatic scaling in response to demand. An edge-optimized API endpoint in Amazon API Gateway is primarily used for geographically distributed clients wherein the API requests are routed to the nearest CloudFront Point of Presence (POP). It doesn't specifically address the need for scaling.</p><p>The option that says: <strong>Use Amazon EC2 Auto Scaling Groups with custom scaling policies to manage the scaling of EKS worker nodes </strong>is incorrect. While Amazon EC2 Auto Scaling Groups can be used to manage EKS worker nodes, this approach requires more configuration and management of scaling policies. It does not offer the same level of seamless integration and automation as Kubernetes Cluster Autoscaler, which is more optimized for Kubernetes workloads.</p><p>The option that says: <strong>Set up CloudWatch alarms for CPU utilization or request count to monitor the relevant metrics of the container applications running on Amazon EKS</strong> is incorrect. While Amazon CloudWatch alarms can monitor the metrics of the Amazon EKS cluster, this approach alone does not enable automatic scaling of the EKS cluster in response to demand.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html\">https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html</a></p><p><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html\">https://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html</a></p><p><br></p><p><strong>Check out this Amazon EKS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-kubernetes-service-eks/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-kubernetes-service-eks/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a combination of Kubernetes Metrics and Kubernetes Cluster Autoscaler to manage the number of nodes.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Integrate an edge-optimized API endpoint in Amazon API Gateway with Amazon EKS to manage and expose APIs for the containerized applications running on EKS.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilize AWS App Mesh to control and monitor the communication between services deployed on Amazon EKS. Set up Amazon CloudWatch Application Insights to trigger autoscaling in the EKS cluster.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up CloudWatch alarms for CPU utilization or request count to monitor the relevant metrics of the container applications running on Amazon EKS.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 22,
    "question": "<p>A financial firm is designing an application architecture for its online trading platform that must have high availability and fault tolerance. Their Solutions Architect configured the application to use an Amazon S3 bucket located in the us-east-1 region to store large amounts of intraday financial data. The stored financial data in the bucket must not be affected even if there is an outage in one of the Availability Zones or if there's a regional service failure. </p><p>What should the Architect do to avoid any costly service disruptions and ensure data durability?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>In this scenario, you need to enable Cross-Region Replication to ensure that your S3 bucket would not be affected even if there is an outage in one of the Availability Zones or a regional service failure in us-east-1. When you upload your data in S3, your objects are redundantly stored on multiple devices across multiple facilities within the region only, where you created the bucket. Thus, if there is an outage on the entire region, your S3 bucket will be unavailable if you do not enable Cross-Region Replication, which should make your data available to another region.</p><p><img src=\"https://media.tutorialsdojo.com/public/S3_Replication_Diagram.7860a04edc2ba93d290ffb9a21a9718574dc08e4.jpg\"></p><p>Note that an Availability Zone (AZ) is more related with Amazon EC2 instances rather than Amazon S3 so if there is any outage in the AZ, the S3 bucket is usually not affected but only the EC2 instances deployed on that zone.</p><p>Hence, the correct answer is: <strong>Enable Cross-Region Replication.</strong></p><p>The option that says: <strong>Copy the S3 bucket to an EBS-backed EC2 instance</strong> is incorrect because EBS is not as durable as Amazon S3. Moreover, if the Availability Zone where the volume is hosted goes down then the data will also be inaccessible.</p><p>The option that says: <strong>Create a Lifecycle Policy to regularly backup the S3 bucket to Amazon Glacier</strong> is incorrect because Glacier is primarily used for data archival. You also need to replicate your data to another region for better durability.</p><p>The option that says: <strong>Create a new S3 bucket in another region and configure Cross-Account Access to the bucket located in us-east-1</strong> is incorrect because Cross-Account Access in Amazon S3 is primarily used if you want to grant access to your objects to another AWS account, and not just to another AWS Region. For example, Account <code>MANILA</code> can grant another AWS account (Account <code>CEBU)</code> permission to access its resources such as buckets and objects. S3 Cross-Account Access does not replicate data from one region to another. A better solution is to enable Cross-Region Replication (CRR) instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p><p><a href=\"https://aws.amazon.com/s3/features/replication/\">https://aws.amazon.com/s3/features/replication/</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Copy the S3 bucket to an EBS-backed EC2 instance.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Create a Lifecycle Policy to regularly backup the S3 bucket to Amazon Glacier.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a new S3 bucket in another region and configure Cross-Account Access to the bucket located in us-east-1.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable Cross-Region Replication.</p>",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 23,
    "question": "<p>A popular augmented reality (AR) mobile game is heavily using a RESTful API which is hosted in AWS. The API uses Amazon API Gateway and a DynamoDB table with a preconfigured read and write capacity. Based on your systems monitoring, the DynamoDB table begins to throttle requests during high peak loads which causes the slow performance of the game.  </p><p>Which of the following can you do to improve the performance of your app?  </p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>DynamoDB auto scaling</strong> uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling. When the workload decreases, Application Auto Scaling decreases the throughput so that you don't pay for unused provisioned capacity.</p><p><strong>Using DynamoDB Auto Scaling</strong> is the best answer. DynamoDB Auto Scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf.</p><p><strong>Integrating an Application Load Balancer with your DynamoDB table</strong> is incorrect because an Application Load Balancer is not suitable to be used with DynamoDB and in addition, this will not increase the throughput of your DynamoDB table.</p><p><strong>Adding the DynamoDB table to an Auto Scaling Group</strong> is incorrect because you usually put EC2 instances on an Auto Scaling Group, and not a DynamoDB table.</p><p><strong>Creating an SQS queue in front of the DynamoDB table</strong> is incorrect because this is not a design principle for high throughput DynamoDB table. Using SQS is for handling queuing and polling the request. This will not increase the throughput of DynamoDB which is required in this situation.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/ ?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p><p><br></p><p><strong>Amazon DynamoDB Overview:</strong></p><p><a href=\"https://www.youtube.com/watch?v=3ZOyUNIeorU\">https://www.youtube.com/watch?v=3ZOyUNIeorU</a></p><p><br></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Integrate an Application Load Balancer with your DynamoDB table.  </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add the DynamoDB table to an Auto Scaling Group.  </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use DynamoDB Auto Scaling  </p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Create an SQS queue in front of the DynamoDB table.  </p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 24,
    "question": "<p>A data analytics company, which uses machine learning to collect and analyze consumer data, is using Redshift cluster as their data warehouse. You are instructed to implement a disaster recovery plan for their systems to ensure business continuity even in the event of an AWS region outage.   </p><p>Which of the following is the best approach to meet this requirement?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>You can configure Amazon Redshift to copy snapshots for a cluster to another region. To configure cross-region snapshot copy, you need to enable this copy feature for each cluster and configure where to copy snapshots and how long to keep copied automated snapshots in the destination region. When a cross-region copy is enabled for a cluster, all new manual and automatic snapshots are copied to the specified region.</p><p><img src=\"https://media.tutorialsdojo.com/public/rs-mgmt-restore-table-from-snapshot.png\"> The option that says: <strong>Create a scheduled job that will automatically take the snapshot of your Redshift Cluster and store it to an S3 bucket. Restore the snapshot in case of an AWS region outage</strong> is incorrect. Although this option is possible, this entails a lot of manual work and hence, not the best option. You should configure cross-region snapshot copy instead.</p><p>The option that says: <strong>Do nothing because Amazon Redshift is a highly available, fully-managed data warehouse which can withstand an outage of an entire AWS region</strong> is incorrect. Although Amazon Redshift is a fully-managed data warehouse, you will still need to configure cross-region snapshot copy to ensure that your data is properly replicated to another region.</p><p><strong>Using Automated snapshots of your Redshift Cluster</strong> is incorrect because using automated snapshots is not enough and will not be available in case the entire AWS region is down.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html</a></p><p><br></p><p><strong>Amazon Redshift Overview:</strong></p><p><a href=\"https://youtu.be/jlLERNzhHOg\">https://youtu.be/jlLERNzhHOg</a><br></p><p><strong>Check out this Amazon Redshift Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-redshift/?src=udemy\">https://tutorialsdojo.com/amazon-redshift/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a scheduled job that will automatically take the snapshot of your Redshift Cluster and store it to an S3 bucket. Restore the snapshot in case of an AWS region outage.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Do nothing because Amazon Redshift is a highly available, fully-managed data warehouse which can withstand an outage of an entire AWS region.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Automated snapshots of your Redshift Cluster.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable Cross-Region Snapshots Copy in your Amazon Redshift Cluster.</p>",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 25,
    "question": "<p>There is a new compliance rule in your company that audits every Windows and Linux EC2 instances each month to view any performance issues. They have more than a hundred EC2 instances running in production, and each must have a logging function that collects various system details regarding that instance. The SysOps team will periodically review these logs and analyze their contents using AWS Analytics tools, and the result will need to be retained in an S3 bucket. </p><p>In this scenario, what is the most efficient way to collect and analyze logs from the instances with minimal effort?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>To collect logs from your Amazon EC2 instances and on-premises servers into <strong>CloudWatch Logs</strong>, AWS offers both a new unified CloudWatch agent, and an older CloudWatch Logs agent. It is recommended to use the unified CloudWatch agent which has the following advantages:</p><p>- You can collect both logs and advanced metrics with the installation and configuration of just one agent.</p><p>- The unified agent enables the collection of logs from servers running Windows Server.</p><p>- If you are using the agent to collect CloudWatch metrics, the unified agent also enables the collection of additional system metrics, for in-guest visibility.</p><p>- The unified agent provides better performance.</p><p><img src=\"https://d1.awsstatic.com/product-marketing/cloudwatch/Product%20Page%20Diagrams/LogsInsights-workflow.0d4b31f451e7cc4dc05e2b40240bcd0d178f1283.png\"></p><p><strong>CloudWatch Logs Insights</strong> enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you quickly and effectively respond to operational issues. If an issue occurs, you can use CloudWatch Logs Insights to identify potential causes and validate deployed fixes.</p><p>CloudWatch Logs Insights includes a purpose-built query language with a few simple but powerful commands. CloudWatch Logs Insights provides sample queries, command descriptions, query autocompletion, and log field discovery to help you get started quickly. Sample queries are included for several types of AWS service logs.</p><p>The option that says: <strong>Install AWS SDK in each instance and create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Enable CloudWatch detailed monitoring and use CloudWatch Logs Insights to analyze the log data of all instances</strong> is incorrect. Although this is a valid solution, this entails a lot of effort to implement as you have to allocate time to install the AWS SDK to each instance and develop a custom monitoring solution. Remember that the question is specifically looking for a solution that can be implemented with minimal effort. In addition, it is unnecessary and not cost-efficient to enable detailed monitoring in CloudWatch in order to meet the requirements of this scenario since this can be done using CloudWatch Logs.</p><p>The option that says: <strong>Install the AWS Systems Manager Agent (SSM Agent) in each instance which will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights</strong> is incorrect. Although this is also a valid solution, it is more efficient to use CloudWatch agent than an SSM agent. Manually connecting to an instance to view log files and troubleshoot an issue with SSM Agent is time-consuming hence, for more efficient instance monitoring, you can use the CloudWatch Agent instead to send the log data to Amazon CloudWatch Logs.</p><p>The option that says: <strong>Install AWS Inspector Agent in each instance which will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all instances</strong> is incorrect because AWS Inspector is simply a security assessments service which only helps you in checking for unintended network accessibility of your EC2 instances and for vulnerabilities on those EC2 instances. Furthermore, setting up an Amazon CloudWatch dashboard is not suitable since its primarily used for scenarios where you have to monitor your resources in a single view, even those resources that are spread across different AWS Regions. It is better to use CloudWatch Logs Insights instead since it enables you to interactively search and analyze your log data.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html \">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html</a></p><p><br></p><p><strong>Amazon CloudWatch Overview:</strong></p><p><a href=\"https://www.youtube.com/watch?v=q0DmxfyGkeU\">https://www.youtube.com/watch?v=q0DmxfyGkeU</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/ ?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>CloudWatch Agent vs SSM Agent vs Custom Daemon Scripts:</strong></p><p><a href=\"https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/ ?src=udemy\">https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Install the unified CloudWatch Logs agent in each instance which will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Install AWS SDK in each instance and create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Enable CloudWatch detailed monitoring and use CloudWatch Logs Insights to analyze the log data of all instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Install the AWS Systems Manager Agent (SSM Agent) in each instance which will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Install AWS Inspector Agent in each instance which will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all instances.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 26,
    "question": "<p>A large multinational investment bank has a web application that requires a minimum of 4 EC2 instances to run to ensure that it can cater to its users across the globe. You are instructed to ensure fault tolerance of this system.</p><p>Which of the following is the best option?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>Fault Tolerance</strong> is the ability of a system to remain in operation even if some of the components used to build the system fail. In AWS, this means that in the event of server fault or system failures, the number of running EC2 instances should not fall below the minimum number of instances required by the system for it to work properly. So if the application requires a minimum of 4 instances, there should be at least 4 instances running in case there is an outage in one of the Availability Zones or if there are server issues.</p><p><img src=\"https://media.tutorialsdojo.com/AWS-ASG-ALB-EC2.png\"></p><p>One of the differences between <strong>Fault Tolerance</strong> and <strong>High Availability</strong> is that the former refers to the minimum number of running instances. For example, you have a system that requires a minimum of 4 running instances and currently has 6 running instances deployed in two Availability Zones. There was a component failure in one of the Availability Zones which knocks out 3 instances. In this case, the system can still be regarded as Highly Available since there are still instances running that can accommodate the requests. However, it is not Fault-Tolerant since the required minimum of four instances has not been met.</p><p>Hence, the correct answer is: <strong>Deploy an Auto Scaling group with 2 instances in each of 3 Availability Zones behind an Application Load Balancer.</strong></p><p>The option that says: <strong>Deploy an Auto Scaling group with 2 instances in each of 2 Availability Zones behind an Application Load Balancer</strong> is incorrect because if one Availability Zone went out, there will only be 2 running instances available out of the required 4 minimum instances. Although the Auto Scaling group can spin up another 2 instances, the fault tolerance of the web application has already been compromised.</p><p>The option that says: <strong>Deploy an Auto Scaling group with 4 instances in one Availability Zone behind an Application Load Balancer</strong> is incorrect because if the Availability Zone went out, there will be no running instance available to accommodate the request.</p><p>The option that says: <strong>Deploy an Auto Scaling group with 1 instance in each of 4 Availability Zones behind an Application Load Balancer</strong> is incorrect because if one Availability Zone went out, there will only be 3 instances available to accommodate the request.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf\">https://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf\">https://d1.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf</a></p><p><br></p><p><strong>AWS Overview Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheets-overview/?src=udemy\">https://tutorialsdojo.com/aws-cheat-sheets-overview/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Deploy an Auto Scaling group with 2 instances in each of 3 Availability Zones behind an Application Load Balancer.",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Deploy an Auto Scaling group with 2 instances in each of 2 Availability Zones behind an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Deploy  an Auto Scaling group with 4 instances in one Availability Zone behind an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Deploy an Auto Scaling group with 1 instance in each of 4 Availability Zones behind an Application Load Balancer.",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 27,
    "question": "<p>A media company is using Amazon EC2, ELB, and S3 for its video-sharing portal for filmmakers. They are using a standard S3 storage class to store all high-quality videos that are frequently accessed only during the first three months of posting.</p><p>As a Solutions Architect, what should you do if the company needs to automatically transfer or archive media data from an S3 bucket to Glacier?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p>You can create a lifecycle policy in S3 to automatically transfer your data to Glacier.</p><p>Lifecycle configuration enables you to specify the lifecycle management of objects in a bucket. The configuration is a set of one or more rules, where each rule defines an action for Amazon S3 to apply to a group of objects.</p><p><img src=\"https://media.tutorialsdojo.com/public/s3_transition_int_to_2.png\"></p><p>These actions can be classified as follows:</p><p><strong>Transition actions</strong> – In which you define when objects transition to another storage class. For example, you may choose to transition objects to the STANDARD_IA (IA, for infrequent access) storage class 30 days after creation or archive objects to the GLACIER storage class one year after creation.</p><p><strong>Expiration actions</strong> – In which you specify when the objects expire. Then Amazon S3 deletes the expired objects on your behalf.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Use a custom shell script that transfers data from the S3 bucket to Glacier",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Use Lifecycle Policies",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon SQS</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon SWF</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 28,
    "question": "<p>A company recently launched an e-commerce application that is running in eu-east-2 region, which strictly requires six EC2 instances running at all times. In that region, there are 3 Availability Zones (AZ) that you can use - eu-east-2a, eu-east-2b, and eu-east-2c.</p><p>Which of the following deployments provide 100% fault tolerance if any single AZ in the region becomes unavailable? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>Fault Tolerance is the ability of a system to remain in operation even if some of the components used to build the system fail. In AWS, this means that in the event of server fault or system failures, the number of running EC2 instances should not fall below the minimum number of instances required by the system for it to work properly. So if the application requires a minimum of 6 instances, there should be at least 6 instances running in case there is an outage in one of the Availability Zones or if there are server issues.</p><p><img src=\"https://media.tutorialsdojo.com/autoscaling-group-4-03.jpg\"></p><p>In this scenario, you have to simulate a situation where one Availability Zone became unavailable for each option and check whether it still has 6 running instances.</p><p>Hence, the correct answers are: <strong>eu-east-2a with six EC2 instances, eu-east-2b with six EC2 instances, and eu-east-2c with no EC2 instances</strong> and <strong>eu-east-2a with three EC2 instances, eu-east-2b with three EC2 instances, and eu-east-2c with three EC2 instances</strong> because even if one of the availability zones were to go down, there would still be 6 active instances.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf\">https://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>eu-east-2a with two EC2 instances, eu-east-2b with four EC2 instances, and eu-east-2c with two EC2 instances</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "eu-east-2a with two EC2 instances, eu-east-2b with two EC2 instances, and eu-east-2c with two EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "answer": "eu-east-2a with four EC2 instances, eu-east-2b with two EC2 instances, and eu-east-2c with two EC2 instances",
        "correct": false
      },
      {
        "id": 4,
        "answer": "eu-east-2a with six EC2 instances, eu-east-2b with six EC2 instances, and eu-east-2c with no EC2 instances",
        "correct": true
      },
      {
        "id": 5,
        "answer": "eu-east-2a with three EC2 instances, eu-east-2b with three EC2 instances, and eu-east-2c with three EC2 instances",
        "correct": true
      }
    ],
    "corrects": [
      4,
      5
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 29,
    "question": "<p>A company plans to develop a custom messaging service that will be used to train an AI for an automatic response feature. The service is expected to receive thousands of messages per day, all of which will be processed by an Amazon EMR cluster. It is crucial that none of the messages are lost, no duplicates are produced, and that the messages are processed in EMR in the same order as their arrival.</p><p>Which of the following options can satisfy the given requirement?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>Two important requirements that the chosen AWS service should fulfill is that data should not go missing, is durable, and streams data in the sequence of arrival. Kinesis can do the job just fine because of its architecture. A Kinesis data stream is a set of shards that has a sequence of data records, and each data record has a sequence number that is assigned by Kinesis Data Streams. Kinesis can also easily handle the high volume of messages being sent to the service.</p><p><img src=\"https://media.tutorialsdojo.com/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\"><br>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</p><p>The option that says: <strong>Set up a default Amazon SQS queue to handle the messages</strong> is incorrect because although SQS is a valid messaging service, it is not suitable for scenarios where you need to process the data based on the order they were received. Take note that a default queue in SQS is just a standard queue and not a FIFO (First-In-First-Out) queue. In addition, SQS does not guarantee that no duplicates will be sent.</p><p>The option that says: <strong>Set up an Amazon SNS Topic to handle the messages</strong> is incorrect because SNS is a pub-sub messaging service in AWS. SNS might not be capable of handling such a large volume of messages being received and sent at a time. It does not also guarantee that the data will be transmitted in the same order they were received.</p><p>The option that says: <strong>Create an Amazon Data Firehose to handle the messages</strong> is incorrect because Amazon Data Firehose is primarily designed for delivering real-time streaming data to destinations such as data lakes, data stores, and analytics services. It ensures reliable data delivery, but it doesn't guarantee the order of message delivery and processing.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/introduction.html \">https://docs.aws.amazon.com/streams/latest/dev/introduction.html</a></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p><p><br></p><p><strong>Check out this Amazon Kinesis Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon Kinesis Data Stream to collect the messages.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up a default Amazon SQS queue to handle the messages.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up an Amazon SNS Topic to handle the messages.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon Data Firehose to handle the messages.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 30,
    "question": "<p>A company has several web applications with users all around the world. Each application is hosted in an Auto Scaling group of EC2 instances in multiple AZs behind an Application Load Balancer (ALB). All applications have their own fully qualified domain name. For added security, the applications must use a publicly trusted SSL certificate.</p><p>Which solution will meet this requirement with the LEAST operational overhead?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>There are two AWS services for issuing and deploying X.509 certificates. Choose the one that best fits your needs. Considerations include whether you need public- or private-facing certificates, customized certificates, certificates you want to deploy into other AWS services, or automated certificate management and renewal.</p><p><strong>ACM Private CA</strong>—This service is for enterprise customers building a public key infrastructure (PKI) inside the AWS cloud and is intended for private use within an organization. With ACM Private CA, you can create your own CA hierarchy and issue certificates with it for authenticating internal users, computers, applications, services, servers, and other devices and for signing computer code. Certificates issued by a private CA are trusted only within your organization, not on the internet.</p><p><img src=\"https://media.tutorialsdojo.com/aws-certificate-manager-private-certificate-authority.png\"></p><p>AWS Certificate Manager (ACM) — This service manages certificates for enterprise customers who need a publicly trusted secure web presence using TLS. You can deploy ACM certificates into AWS Elastic Load Balancing, Amazon CloudFront, Amazon API Gateway, and other integrated services. The most common application of this kind is a secure public website with significant traffic requirements.</p><p>With this service, you can use public certificates provided by ACM (ACM certificates) or certificates that you import into ACM. If you use ACM Private CA to create a CA, ACM can manage certificate issuance from that private CA and automate certificate renewals.</p><p><img src=\"https://media.tutorialsdojo.com/aws-certificate-manager-public-ssl-tls-certificate.png\"></p><p>One of the key differences between the regular ACM and ACM Private Certificate Authority(PCA) is the ability to create a Root CA (for establishing new CA hierarchy). This enables the ACM PCA service to generate private certificates. ACM PCA can also generate a private subordinate CA.</p><p>In addition to requesting SSL/TLS certificates provided by AWS Certificate Manager (ACM), you can import certificates that you obtained outside of AWS. You might do this because you already have a certificate from a third-party certificate authority (CA) or because you have application-specific requirements that are not met by ACM issued certificates.</p><p>To renew an imported certificate, you can obtain a new certificate from your certificate issuer and then manually reimport it into ACM. This action preserves the certificate's association and its Amazon Resource Name (ARN). Alternatively, you can import a completely new certificate. Multiple certificates with the same domain name can be imported, but they must be imported one at a time.</p><p>Take note that the scenario asked for a publicly trusted SSL certificate and not a private certificate. Hence, the correct answer is: <strong>Use the AWS Certificate Manager (ACM) to generate a public SSL/TLS certificate. Associate the new SSL/TLS certificate on the HTTPS listener of the ALBs.</strong></p><p>The option that says: <strong>Launch a self-hosted certificate authority (CA) using the Let's Encrypt tool in an Amazon EC2 instance. Utilize the built-in ISRG Root X1 trusted root CA certificate. Generate a new SSL/TLS certificate using the </strong><code><strong>certbot</strong></code><strong> CLI utility. Associate the new certificate on the HTTPS listener of the ALBs</strong> is incorrect because you cannot directly associate an SSL/TLS certificate to an Application Load Balancer. The certificate must be first imported to ACM before it can be associated to the elastic load balancers in your account.</p><p>The option that says: <strong>Use OpenSSL to generate a self-signed certificate. Import the SSL/TLS certificate to the AWS Certificate Manager (ACM) and associate it with the HTTPS listener of the ALBs</strong> is incorrect. Although this is a valid solution, this entails a lot of operational overhead since you have to generate multiple certificates yourself and manually import them to ACM. In addition, you have to manually reimport your certificate every year since self-signed certificates in ACM are not automatically renewed, unlike those generated from ACM.</p><p>The option that says: <strong>Issue an SSL/TLS certificate using the AWS Certificate Manager Private Certificate Authority. Associate the new certificate on the HTTPS listener of the ALBs</strong> is incorrect because the scenario explicitly mentioned that you have to use a publicly trusted SSL certificate. Remember that the ACM PCA provides a private certificate, not a public one.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html\">https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html</a></p><p><a href=\"https://docs.aws.amazon.com/acm-pca/latest/userguide/PcaWelcome.html\">https://docs.aws.amazon.com/acm-pca/latest/userguide/PcaWelcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html\">https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html</a></p><p><br></p><p><strong>Check out this AWS Certificate Manager (ACM) Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certificate-manager/?src=udemy\">https://tutorialsdojo.com/aws-certificate-manager/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Launch a self-hosted certificate authority (CA) using the Let's Encrypt tool in an Amazon EC2 instance. Utilize the built-in ISRG Root X1 trusted root CA certificate. Generate a new SSL/TLS certificate using the <code>certbot</code> CLI utility. Associate the new certificate on the HTTPS listener of the ALBs.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the AWS Certificate Manager (ACM) to generate a public SSL/TLS certificate. Associate the new SSL/TLS certificate on the HTTPS listener of the ALBs.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use OpenSSL to generate a self-signed certificate. Import the SSL/TLS certificate to the AWS Certificate Manager (ACM) and associate it with the HTTPS listener of the ALBs</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Issue an SSL/TLS certificate using the AWS Certificate Manager Private Certificate Authority. Associate the new certificate on the HTTPS listener of the ALBs.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 31,
    "question": "<p>A leading IT consulting company has an application which processes a large stream of financial data by an Amazon ECS Cluster then stores the result to a DynamoDB table. You have to design a solution to detect new entries in the DynamoDB table then automatically trigger a Lambda function to run some tests to verify the processed data. </p><p>What solution can be easily implemented to alert the Lambda function of new entries while requiring minimal configuration change to your architecture?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>Amazon DynamoDB is integrated with AWS Lambda so that you can create <em>triggers</em>—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.</p><p>If you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.</p><p><img src=\"https://media.tutorialsdojo.com/StreamsAndTriggers.png\"></p><p>You can create a Lambda function which can perform a specific action that you specify, such as sending a notification or initiating a workflow. For instance, you can set up a Lambda function to simply copy each stream record to persistent storage, such as EFS or S3, to create a permanent audit trail of write activity in your table.</p><p>Suppose you have a mobile gaming app that writes to a <code>TutorialsDojoCourses</code> table. Whenever the <code>TopCourse</code> attribute of the <code>TutorialsDojoScores</code> table is updated, a corresponding stream record is written to the table's stream. This event could then trigger a Lambda function that posts a congratulatory message on a social media network. (The function would simply ignore any stream records that are not updated to <code>TutorialsDojoCourses</code> or that do not modify the <code>TopCourse</code> attribute.)</p><p>Hence, <strong>enabling DynamoDB Streams to capture table activity and automatically trigger the Lambda function</strong> is the correct answer because the requirement can be met with minimal configuration change using DynamoDB streams, which can automatically trigger Lambda functions whenever there is a new entry.</p><p><strong>Using CloudWatch Alarms to trigger the Lambda function whenever a new entry is created in the DynamoDB table</strong> is incorrect because CloudWatch Alarms only monitor service metrics, not changes in DynamoDB table data.</p><p><strong>Invoking the Lambda functions using SNS each time that the ECS Cluster successfully processed financial data</strong> is incorrect because you don't need to create an SNS topic just to invoke Lambda functions. You can enable DynamoDB streams instead to meet the requirement with less configuration.</p><p><strong>Using Systems Manager Automation to detect new entries in the DynamoDB table then automatically invoking the Lambda function for processing</strong> is incorrect because the Systems Manager Automation service is primarily used to simplify common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. It does not have the capability to detect new entries in a DynamoDB table.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB cheat sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use CloudWatch Alarms to trigger the Lambda function whenever a new entry is created in the DynamoDB table.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Invoke the Lambda functions using SNS each time that the ECS Cluster successfully processed financial data.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable DynamoDB Streams to capture table activity and automatically trigger the Lambda function.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Systems Manager Automation to detect new entries in the DynamoDB table then automatically invoke the Lambda function for processing.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 32,
    "question": "<p>A company has hundreds of VPCs with multiple VPN connections to their data centers spanning 5 AWS Regions. As the number of its workloads grows, the company must be able to scale its networks across multiple accounts and VPCs to keep up. A Solutions Architect is tasked to interconnect all of the company's on-premises networks, VPNs, and VPCs into a single gateway, which includes support for inter-region peering across multiple AWS regions.</p><p>Which of the following is the BEST solution that the architect should set up to support the required interconnectivity?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>AWS Transit Gateway</strong> is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. As you grow the number of workloads running on AWS, you need to be able to scale your networks across multiple accounts and Amazon VPCs to keep up with the growth.</p><p>Today, you can connect pairs of Amazon VPCs using peering. However, managing point-to-point connectivity across many Amazon VPCs without the ability to centrally manage the connectivity policies can be operationally costly and cumbersome. For on-premises connectivity, you need to attach your AWS VPN to each individual Amazon VPC. This solution can be time-consuming to build and hard to manage when the number of VPCs grows into the hundreds.</p><p>With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway to each Amazon VPC, on-premises data center, or remote office across your network. Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks which act like spokes. This hub and spoke model significantly simplifies management and reduces operational costs because each network only has to connect to the Transit Gateway and not to every other network. Any new VPC is simply connected to the Transit Gateway and is then automatically available to every other network that is connected to the Transit Gateway. This ease of connectivity makes it easy to scale your network as you grow.</p><p><img src=\"https://media.tutorialsdojo.com/transit-gateway-Inter-Region-Peering.jpg\"></p><p>It acts as a Regional virtual router for traffic flowing between your virtual private clouds (VPC) and VPN connections. A transit gateway scales elastically based on the volume of network traffic. Routing through a transit gateway operates at layer 3, where the packets are sent to a specific next-hop attachment, based on their destination IP addresses.</p><p>A transit gateway attachment is both a source and a destination of packets. You can attach the following resources to your transit gateway:</p><p>- One or more VPCs</p><p>- One or more VPN connections</p><p>- One or more AWS Direct Connect gateways</p><p>- One or more transit gateway peering connections</p><p>If you attach a transit gateway peering connection, the transit gateway must be in a different Region.</p><p>Hence, the correct answer is: <strong>Set up an AWS Transit Gateway in each region to interconnect all networks within it. Then, route traffic between the transit gateways through a peering connection.</strong></p><p>The option that says: <strong>Set up an AWS Direct Connect Gateway to achieve inter-region VPC access to all of the AWS resources and on-premises data centers. Set up a link aggregation group (LAG) to aggregate multiple connections at a single AWS Direct Connect endpoint in order to treat them as a single, managed connection. Launch a virtual private gateway in each VPC and then create a public virtual interface for each AWS Direct Connect connection to the Direct Connect Gateway</strong> is incorrect. You can only create a <strong>private</strong> virtual interface to a Direct Connect gateway and not a <strong>public</strong> virtual interface. Using a link aggregation group (LAG) is also irrelevant in this scenario because it is just a logical interface that uses the Link Aggregation Control Protocol (LACP) to aggregate multiple connections at a single AWS Direct Connect endpoint, allowing you to treat them as a single, managed connection.</p><p>The option that says: <strong>Enable inter-region VPC peering which allows peering relationships to be established between VPCs across different AWS regions. This will ensure that the traffic will always stay on the global AWS backbone and will never traverse the public Internet</strong> is incorrect. This would require a lot of manual set up and management overhead to successfully build a functional, error-free inter-region VPC network compared with just using a Transit Gateway. Although the Inter-Region VPC Peering provides a cost-effective way to share resources between regions or replicate data for geographic redundancy, its connections are not dedicated and highly available. Moreover, it doesn't support the company's on-premises data centers in multiple AWS Regions.</p><p>The option that says: <strong>Set up an AWS VPN CloudHub for inter-region VPC access and a Direct Connect gateway for the VPN connections to the on-premises data centers. Create a virtual private gateway in each VPC, then create a private virtual interface for each AWS Direct Connect connection to the Direct Connect gateway</strong> is incorrect. This option doesn't meet the requirement of interconnecting all of the company's on-premises networks, VPNs, and VPCs into a <strong>single</strong> gateway, which includes support for inter-region peering across multiple AWS regions. As its name implies, the AWS VPN CloudHub is only for <strong>VPNs</strong> and not for VPCs. It is also not capable of managing hundreds of VPCs with multiple VPN connections to their data centers that span multiple AWS Regions.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/how-transit-gateways-work.html\">https://docs.aws.amazon.com/vpc/latest/tgw/how-transit-gateways-work.html</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/building-a-global-network-using-aws-transit-gateway-inter-region-peering/\">https://aws.amazon.com/blogs/networking-and-content-delivery/building-a-global-network-using-aws-transit-gateway-inter-region-peering/</a></p><p><br></p><p><strong>Check out this AWS Transit Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-transit-gateway/?src=udemy\">https://tutorialsdojo.com/aws-transit-gateway/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an AWS Direct Connect Gateway to achieve inter-region VPC access to all of the AWS resources and on-premises data centers. Set up a link aggregation group (LAG) to aggregate multiple connections at a single AWS Direct Connect endpoint in order to treat them as a single, managed connection. Launch a virtual private gateway in each VPC and then create a public virtual interface for each AWS Direct Connect connection to the Direct Connect Gateway.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an AWS Transit Gateway in each region to interconnect all networks within it. Then, route traffic between the transit gateways through a peering connection.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up an AWS VPN CloudHub for inter-region VPC access and a Direct Connect gateway for the VPN connections to the on-premises data centers. Create a virtual private gateway in each VPC, then create a private virtual interface for each AWS Direct Connect connection to the Direct Connect gateway.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable inter-region VPC peering that allows peering relationships to be established between multiple VPCs across different AWS regions. Set up a networking configuration that ensures that the traffic will always stay on the global AWS backbone and never traverse the public Internet.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 33,
    "question": "<p>A company is looking to store its confidential financial files in AWS, which are accessed every week.<strong> </strong>The Architect was instructed to set up the storage system, which uses envelope encryption and automates key rotation. It should also provide an audit trail that shows who used the encryption key and by whom for security purposes.</p><p>Which combination of actions should the Architect implement to satisfy the requirement in the most cost-effective way? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p>Server-side encryption is the encryption of data at its destination by the application or service that receives it. AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. Amazon S3 uses AWS KMS keys to encrypt your Amazon S3 objects. SSE-KMS encrypts only the object data. Any object metadata is not encrypted. If you use KMS keys, you use AWS KMS via the AWS Management Console or AWS KMS APIs to centrally create encryption keys, define the policies that control how keys can be used, and audit key usage to prove that they are being used correctly. You can use these keys to protect your data in Amazon S3 buckets.</p><p><img src=\"https://media.tutorialsdojo.com/s3-server-side-encryption.jpg\"></p><p>A KMS key is a logical representation of a cryptographic key. The KMS key includes metadata, such as the key ID, creation date, description, and key state. The KMS key also contains the key material used to encrypt and decrypt data. You can use a KMS key to encrypt and decrypt up to 4 KB (4096 bytes) of data. Typically, you use KMS keys to generate, encrypt, and decrypt the data keys that you use outside of AWS KMS to encrypt your data. This strategy is known as <strong>envelope encryption.</strong></p><p>You have three mutually exclusive options depending on how you choose to manage the encryption keys:</p><p><strong>Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</strong> – Each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</p><p><strong>Use Server-Side Encryption with KMS Key Stored in AWS Key Management Service (SSE-KMS)</strong> – Similar to SSE-S3, but with some additional benefits and charges for using this service. There are separate permissions for the use of a KMS key that provides added protection against unauthorized access of your objects in Amazon S3. SSE-KMS also provides you with an audit trail that shows when your KMS key was used and by whom. Additionally, you can create and manage customer-managed key or use AWS managed KMS keys that are unique to you, your service, and your Region.</p><p><strong>Use Server-Side Encryption with Customer-Provided Keys (SSE-C)</strong> – You manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects.</p><p>In the scenario, the company needs to store financial files in AWS which are accessed every week and the solution should use envelope encryption. This requirement can be fulfilled by using an Amazon S3 configured with Server-Side Encryption with AWS KMS keys (SSE-KMS).</p><p>Hence, the correct answers are:</p><p>- <strong>Use Amazon S3 to store the data</strong>.</p><p>- <strong>Configure Server-Side Encryption with AWS KMS Keys (SSE-KMS).</strong></p><p>The option that says: <strong>Use Amazon S3 Glacier Deep Archive to store the data</strong> is incorrect. Although this primarily provides the most cost-effective storage solution, it is not the appropriate service to use if the files being stored are frequently accessed every week.</p><p>The option that says: <strong>Configure Server-Side Encryption with Customer-Provided Keys (SSE-C)</strong> is incorrect. Although you can typically configure automatic key rotation, it does not provide an audit trail showing when your KMS Key was used and by whom, unlike Server-Side Encryption with AWS KMS Keys (SSE-KMS).</p><p>The option that says: <strong>Configure Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) </strong>is incorrect. Just like the previous option, it does not provide an audit log detailing key usage. Unlike Server-Side Encryption with AWS KMS Keys (SSE-KMS), SSE-S3 does not allow tracking of encryption and decryption events through AWS CloudTrail.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon S3 to store the data.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon S3 Glacier Deep Archive to store the data.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure Server-Side Encryption with Customer-Provided Keys (SSE-C).</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3).</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Configure Server-Side Encryption with AWS KMS Keys (SSE-KMS).</p>",
        "correct": true
      }
    ],
    "corrects": [
      1,
      5
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 34,
    "question": "<p>A company needs to implement a solution that will process real-time streaming data of its users across the globe. This will enable them to track and analyze globally-distributed user activity on their website and mobile applications, including clickstream analysis. The solution should process the data in close geographical proximity to their users and respond to user requests at low latencies.</p><p>Which of the following is the most suitable solution for this scenario?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency. With Lambda@Edge, you don't have to provision or manage infrastructure in multiple locations around the world. You pay only for the compute time you consume - there is no charge when your code is not running.</p><p>With Lambda@Edge, you can enrich your web applications by making them globally distributed and improving their performance — all with zero server administration. Lambda@Edge runs your code in response to events generated by the Amazon CloudFront content delivery network (CDN). Just upload your code to AWS Lambda, which takes care of everything required to run and scale your code with high availability at an AWS location closest to your end user.</p><p><img src=\"https://media.tutorialsdojo.com/public/AWS-Lambda-at-Edge_User-Tracking-Analytics-diagram%20Oct%202018.5cc920f99c9450467b7290c20a8a4eb7c444e915.png\"></p><p>By using Lambda@Edge and Kinesis together, you can process real-time streaming data so that you can track and analyze globally-distributed user activity on your website and mobile applications, including clickstream analysis. Hence, the correct answer in this scenario is the option that says: <strong>Integrate CloudFront with Lambda@Edge in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Kinesis and durably store the results to an Amazon S3 bucket.</strong></p><p>The options that say: <strong>Use a CloudFront web distribution and Route 53 with a latency-based routing policy, in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Kinesis and durably store the results to an Amazon S3 bucket</strong> and <strong>Use a CloudFront web distribution and Route 53 with a Geoproximity routing policy in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Kinesis and durably store the results to an Amazon S3 bucket</strong> are both incorrect because you can only route traffic using Route 53 since it does not have any computing capability. This solution would not be able to process and return the data in close geographical proximity to your users since it is not using Lambda@Edge.</p><p>The option that says: <strong>Integrate CloudFront with Lambda@Edge in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Amazon Athena and durably store the results to an Amazon S3 bucket</strong> is incorrect. Although using Lambda@Edge is correct, Amazon Athena is just an interactive query service that enables you to easily analyze data in Amazon S3 using standard SQL. Kinesis should be used to process the streaming data in real time.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/lambda/edge/\">https://aws.amazon.com/lambda/edge/</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/global-data-ingestion-with-amazon-cloudfront-and-lambdaedge/\">https://aws.amazon.com/blogs/networking-and-content-delivery/global-data-ingestion-with-amazon-cloudfront-and-lambdaedge/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a CloudFront web distribution and Route 53 with a latency-based routing policy, in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Kinesis and durably store the results to an Amazon S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Integrate CloudFront with Lambda@Edge in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Amazon Athena and durably store the results to an Amazon S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use a CloudFront web distribution and Route 53 with a Geoproximity routing policy in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Kinesis and durably store the results to an Amazon S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Integrate CloudFront with Lambda@Edge in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Kinesis and durably store the results to an Amazon S3 bucket.</p>",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 35,
    "question": "<p>A company launched a global news website that is deployed to AWS and is using MySQL RDS. The website has millions of viewers from all over the world, which means that the website has a read-heavy database workload. All database transactions must be ACID compliant to ensure data integrity.</p><p>In this scenario, which of the following is the best option to use to increase the read-throughput on the MySQL database?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon RDS Read Replicas</strong> provide enhanced performance and durability for database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, Oracle, and PostgreSQL as well as Amazon Aurora.</p><p><img src=\"https://media.tutorialsdojo.com/public/MySQLConnector.png\"></p><p><strong>Enabling Multi-AZ deployments</strong> is incorrect because the Multi-AZ deployments feature is mainly used to achieve high availability and failover support for your database.</p><p><strong>Enabling Amazon RDS Standby Replicas</strong> is incorrect because a Standby replica is used in Multi-AZ deployments and hence, it is not a solution to reduce read-heavy database workloads.</p><p><strong>Using SQS to queue up the requests</strong> is incorrect. Although an SQS queue can effectively manage the requests, it won't be able to entirely improve the read-throughput of the database by itself.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/details/read-replicas/\">https://aws.amazon.com/rds/details/read-replicas/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Enable Multi-AZ deployments",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Enable Amazon RDS Standby Replicas",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Enable Amazon RDS Read Replicas",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use SQS to queue up the requests",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 36,
    "question": "<p>A company deployed an online enrollment system database on a prestigious university, which is hosted in RDS. The Solutions Architect is required to monitor the database metrics in Amazon CloudWatch to ensure the availability of the enrollment system.</p><p>What are the enhanced monitoring metrics that Amazon CloudWatch gathers from Amazon RDS DB instances which provide more accurate information? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>Amazon RDS</strong> provides metrics in real-time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB instance using the console or consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice.</p><p><strong>CloudWatch</strong> gathers metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance. As a result, you might find differences between the measurements because the hypervisor layer performs a small amount of work. The differences can be greater if your DB instances use smaller instance classes because then there are likely more virtual machines (VMs) that are managed by the hypervisor layer on a single physical instance. Enhanced Monitoring metrics are useful when you want to see how different processes or threads on a DB instance use the CPU.</p><p><img src=\"https://media.tutorialsdojo.com/public/metrics1.png\"></p><p>In RDS, the Enhanced Monitoring metrics shown in the Process List view are organized as follows:</p><p><strong>RDS child processes</strong> – Shows a summary of the RDS processes that support the DB instance, for example <code>aurora</code> for Amazon Aurora DB clusters and <code>mysqld</code> for MySQL DB instances. Process threads appear nested beneath the parent process. Process threads show CPU utilization only as other metrics are the same for all threads for the process. The console displays a maximum of 100 processes and threads. The results are a combination of the top CPU-consuming and memory-consuming processes and threads. If there are more than 50 processes and more than 50 threads, the console displays the top 50 consumers in each category. This display helps you identify which processes are having the greatest impact on performance.</p><p><strong>RDS processes</strong> – Shows a summary of the resources used by the RDS management agent, diagnostics monitoring processes, and other AWS processes that are required to support RDS DB instances.</p><p><strong>OS processes</strong> – Shows a summary of the kernel and system processes, which generally have minimal impact on performance.</p><p><strong>CPU Utilization, Database Connections, </strong>and<strong> Freeable Memory</strong> are incorrect because these are just the regular items provided by Amazon RDS Metrics in CloudWatch. Remember that the scenario is asking for the Enhanced Monitoring metrics.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/rds-metricscollected.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/rds-metricscollected.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.CloudWatchLogs\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.CloudWatchLogs</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>CPU Utilization</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Database Connections</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Freeable Memory</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "RDS child processes.",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>OS processes</p>",
        "correct": true
      }
    ],
    "corrects": [
      4,
      5
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 37,
    "question": "<p>A newly hired Solutions Architect is checking all of the security groups and network access control list rules of the company's AWS resources. For security purposes, the MS SQL connection via port 1433 of the database tier should be secured. Below is the security group configuration of their Microsoft SQL Server database:</p><p><br></p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2020-01-11_08-59-07-8cf0adda84f8f9ecd6201220e6901cc5.png\"></p><p><br></p><p>The application tier hosted in an Auto Scaling group of EC2 instances is the only identified resource that needs to connect to the database. The Architect should ensure that the architecture complies with the best practice of granting least privilege. </p><p>Which of the following changes should be made to the security group configuration?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>A <strong>security group</strong> acts as a virtual firewall for your instance to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC can be assigned to a different set of security groups.</p><p>If you launch an instance using the Amazon EC2 API or a command line tool and you don't specify a security group, the instance is automatically assigned to the default security group for the VPC. If you launch an instance using the Amazon EC2 console, you have an option to create a new security group for the instance.</p><p><img src=\"https://media.tutorialsdojo.com/public/2020-01-11_09-55-33-102a3438068e9bb4c45fa670155c2044.png\"></p><p>For each security group, you add <em>rules</em> that control the inbound traffic to instances and a separate set of rules that control the outbound traffic. This section describes the basic things that you need to know about security groups for your VPC and their rules.</p><p>Amazon security groups and network ACLs don't filter traffic to or from link-local addresses (<code>169.254.0.0/16</code>) or AWS reserved IPv4 addresses (these are the first four IPv4 addresses of the subnet, including the Amazon DNS server address for the VPC). Similarly, flow logs do not capture IP traffic to or from these addresses.</p><p>In the scenario, the security group configuration allows any server (0.0.0.0/0) from anywhere to establish an MS SQL connection to the database via the 1433 port. The most suitable solution here is to change the <code><strong>Source</strong></code> field to the security group ID attached to the application tier.</p><p>Hence, the correct answer is the option that says: <strong>For the MS SQL rule, change the </strong><code><strong>Source</strong></code><strong> to the security group ID attached to the application tier</strong>.</p><p>The option that says: <strong>For the MS SQL rule, change the </strong><code><strong>Source</strong></code><strong> to the EC2 instance IDs of the underlying instances of the Auto Scaling group</strong> is incorrect because using the EC2 instance IDs of the underlying instances of the Auto Scaling group as the source can cause intermittent issues. New instances will be added, and old instances will be removed from the Auto Scaling group over time, which means that you have to manually update the security group setting once again. A better solution is to use the security group ID of the Auto Scaling group of EC2 instances.</p><p>The option that says: <strong>For the MS SQL rule, change the </strong><code><strong>Source</strong></code><strong> to the static AnyCast IP address attached to the application tier</strong> is incorrect because a static AnyCast IP address is primarily used for AWS Global Accelerator and not for security group configurations.</p><p>The option that says: <strong>For the MS SQL rule, change the </strong><code><strong>Source</strong></code><strong> to the Network ACL ID attached to the application tier</strong> is incorrect because you have to use the security group ID instead of the Network ACL ID of the application tier. Take note that the Network ACL covers the entire subnet which means that other applications that use the same subnet will also be affected.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>For the MS SQL rule, change the <code>Source</code> to the security group ID attached to the application tier.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>For the MS SQL rule, change the <code>Source</code> to the EC2 instance IDs of the underlying instances of the Auto Scaling group.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>For the MS SQL rule, change the <code>Source</code> to the static AnyCast IP address attached to the application tier.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>For the MS SQL rule, change the <code>Source</code> to the Network ACL ID attached to the application tier.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 38,
    "question": "<p>An automotive company is working on an autonomous vehicle development and deployment project using AWS. The solution requires High Performance Computing (HPC) in order to collect, store and manage massive amounts of data as well as to support deep learning frameworks. The Linux EC2 instances that will be used should have a lower latency and higher throughput than the TCP transport traditionally used in cloud-based HPC systems. It should also enhance the performance of inter-instance communication and must include an OS-bypass functionality to allow the HPC to communicate directly with the network interface hardware to provide low-latency, reliable transport functionality. </p><p>Which of the following is the MOST suitable solution that you should implement to achieve the above requirements?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>An Elastic Fabric Adapter (EFA)</strong> is a network device that you can attach to your Amazon EC2 instance to accelerate High Performance Computing (HPC) and machine learning applications. EFA enables you to achieve the application performance of an on-premises HPC cluster with the scalability, flexibility, and elasticity provided by the AWS Cloud.</p><p>EFA provides lower and more consistent latency and higher throughput than the TCP transport traditionally used in cloud-based HPC systems. It enhances the performance of inter-instance communication which is critical for scaling HPC and machine learning applications. It is optimized to work on the existing AWS network infrastructure, and it can scale depending on application requirements.</p><p>EFA integrates with Libfabric 1.9.0, and it supports Open MPI 4.0.2 and Intel MPI 2019 Update 6 for HPC applications and Nvidia Collective Communications Library (NCCL) for machine learning applications.</p><p><img src=\"https://media.tutorialsdojo.com/public/efa_stack.png\"></p><p>The OS-bypass capabilities of EFAs are not supported on Windows instances. If you attach an EFA to a Windows instance, the instance functions as an Elastic Network Adapter without the added EFA capabilities.</p><p>Elastic Network Adapters (ENAs) provide traditional IP networking features that are required to support VPC networking. EFAs provide all of the same traditional IP networking features as ENAs, and they also support OS-bypass capabilities. OS-bypass enables HPC and machine learning applications to bypass the operating system kernel and communicate directly with the EFA device.</p><p>Hence, the correct answer is to <strong>attach an Elastic Fabric Adapter (EFA) on each Amazon EC2 instance to accelerate High Performance Computing (HPC).</strong></p><p><strong>Attaching an Elastic Network Adapter (ENA) on each Amazon EC2 instance to accelerate High Performance Computing (HPC)</strong> is incorrect because Elastic Network Adapter (ENA) doesn't have OS-bypass capabilities, unlike EFA.</p><p><strong>Attaching an Elastic Network Interface (ENI) on each Amazon EC2 instance to accelerate High Performance Computing (HPC)</strong> is incorrect because an Elastic Network Interface (ENI) is simply a logical networking component in a VPC that represents a virtual network card. It doesn't have OS-bypass capabilities that allow the HPC to communicate directly with the network interface hardware to provide low latency, reliable transport functionality.</p><p><strong>Attaching a Private Virtual Interface (VIF) on each Amazon EC2 instance to accelerate High Performance Computing (HPC)</strong> is incorrect because Private Virtual Interface just allows you to connect to your VPC resources on your private IP address or endpoint.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena</a></p><p><br></p><p><strong>Check out this Elastic Fabric Adapter (EFA) Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/elastic-fabric-adapter-efa/?src=udemy\">https://tutorialsdojo.com/elastic-fabric-adapter-efa/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Attach an Elastic Network Adapter (ENA) on each Amazon EC2 instance to accelerate High Performance Computing (HPC).</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Attach an Elastic Fabric Adapter (EFA) on each Amazon EC2 instance to accelerate High Performance Computing (HPC).</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Attach an Elastic Network Interface (ENI) on each Amazon EC2 instance to accelerate High Performance Computing (HPC).</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Attach a Private Virtual Interface (VIF) on each Amazon EC2 instance to accelerate High Performance Computing (HPC).</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 39,
    "question": "<p>A web application is hosted in an Auto Scaling group of EC2 instances deployed across multiple Availability Zones behind an Application Load Balancer. You need to implement an SSL solution for your system to improve its security which is why you requested an SSL/TLS certificate from a third-party certificate authority (CA).</p><p>Where can you safely import the SSL/TLS certificate of your application? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p>If you got your certificate from a third-party CA, import the certificate into ACM or upload it to the IAM certificate store. Hence, <strong>AWS Certificate Manager</strong> and <strong>IAM certificate store</strong> are the correct answers.</p><p><img src=\"https://media.tutorialsdojo.com/public/dark-mode-01.png\"></p><p>ACM lets you import third-party certificates from the ACM console, as well as programmatically. If ACM is not available in your region, use AWS CLI to upload your third-party certificate to the IAM certificate store.</p><p><strong>A private S3 bucket with versioning enabled</strong> and <strong>an S3 bucket configured with server-side encryption with customer-provided encryption keys (SSE-C)</strong> are both incorrect as S3 is not a suitable service to store the SSL certificate.</p><p><strong>CloudFront</strong> is incorrect. Although you can upload certificates to CloudFront, it doesn't mean that you can import SSL certificates on it. You would not be able to export the certificate that you have loaded in CloudFront nor assign them to your EC2 or ELB instances as it would be tied to a single CloudFront distribution.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-procedures.html#cnames-and-https-uploading-certificates\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-procedures.html#cnames-and-https-uploading-certificates</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c02/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c02/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Certificate Manager</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>IAM certificate store</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>A private S3 bucket with versioning enabled</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>An S3 bucket configured with server-side encryption with customer-provided encryption keys (SSE-C) </p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>CloudFront</p>",
        "correct": false
      }
    ],
    "corrects": [
      1,
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 40,
    "question": "<p>A web application requires a minimum of six Amazon Elastic Compute Cloud (EC2) instances running at all times. You are tasked to deploy the application to three availability zones in the EU Ireland region (eu-west-1a, eu-west-1b, and eu-west-1c). It is required that the system is fault-tolerant up to the loss of one Availability Zone. </p><p>Which of the following setup is the most cost-effective solution which also maintains the fault-tolerance of your system?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p>Basically, fault-tolerance is the ability of a system to remain in operation even in the event that some of its components fail without any service degradation. In AWS, it can also refer to the minimum number of running EC2 instances or resources which should be running at all times in order for the system to properly operate and serve its consumers. Take note that this is quite different from the concept of High Availability, which is just concerned with having at least one running instance or resource in case of failure.</p><p><img src=\"https://media.tutorialsdojo.com/public/RegionsAndAZs.png\">In this scenario, <strong>3 instances in eu-west-1a, 3 instances in eu-west-1b, and 3 instances in eu-west-1c</strong> is the correct answer because even if there was an outage in one of the Availability Zones, the system still satisfies the requirement of having a minimum of 6 running instances. It is also the most cost-effective solution among other options.</p><p>The option that says: <strong>6 instances in eu-west-1a, 6 instances in eu-west-1b, and 6 instances in eu-west-1c</strong> is incorrect. Although this solution provides the maximum fault-tolerance for the system, it entails a significant cost to maintain a total of 18 instances across 3 AZs.</p><p>The option that says: <strong>2 instances in eu-west-1a, 2 instances in eu-west-1b, and 2 instances in eu-west-1c</strong> is incorrect because if one Availability Zone goes down, there will only be 4 running instances available. Although this is the most cost-effective solution, it does not provide fault-tolerance.</p><p>The option that says: <strong>6 instances in eu-west-1a, 6 instances in eu-west-1b, and no instances in eu-west-1c</strong> is incorrect. Although it provides fault-tolerance, it is not the most cost-effective solution as compared with the options above. This solution has 12 running instances, unlike the correct answer which only has 9 instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-increase-availability.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-increase-availability.html</a></p><p><a href=\"https://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf\">https://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>3 instances in eu-west-1a, 3 instances in eu-west-1b, and 3 instances in eu-west-1c</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>6 instances in eu-west-1a, 6 instances in eu-west-1b, and 6 instances in eu-west-1c</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>2 instances in eu-west-1a, 2 instances in eu-west-1b, and 2 instances in eu-west-1c</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>6 instances in eu-west-1a, 6 instances in eu-west-1b, and no instances in eu-west-1c</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 41,
    "question": "<p>A Solutions Architect is working for a large global media company with multiple office locations all around the world. The Architect is instructed to build a system to distribute training videos to all employees.</p><p>Using Amazon CloudFront, what method would be used to serve content that is stored in Amazon S3 but not publicly accessible from S3 directly?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>When you create or update a distribution in CloudFront, you can add an origin access Control (OAC) and automatically update the bucket policy to give the origin access control permission to access your bucket. Alternatively, you can choose to manually change the bucket policy or change ACLs, which control permissions on individual objects in your bucket.</p><p><img src=\"https://media.tutorialsdojo.com/4-v-2.png\"></p><p>You can update the Amazon S3 bucket policy using either the AWS Management Console or the Amazon S3 API:</p><p>- Grant the CloudFront origin access control the applicable permissions on the bucket.</p><p>- Deny access to anyone that you don't want to have access using Amazon S3 URLs.</p><p>Hence, the correct answer is: <strong>Create an Origin Access Control (OAC) for CloudFront and grant access to the objects in your S3 bucket to that OAC.</strong></p><p>The option that says:<strong> Create an Identity and Access Management (IAM) user for CloudFront and grant access to the objects in your S3 bucket to that IAM user</strong> is incorrect because you cannot directly create an IAM User for a specific Amazon CloudFront distribution. You have to use an origin access control (OAC) instead.</p><p>The option that says: <strong>Create an S3 bucket policy that lists the CloudFront distribution ID as the principal and the target bucket as the Amazon Resource Name (ARN)</strong> is incorrect. While you can typically specify AWS accounts, IAM users, and IAM roles as principals in an S3 bucket policy, you cannot directly use a CloudFront distribution ID as a principal in an S3 bucket policy. Instead, you must define the CloudFront service as the Principal and restrict access to your CF distribution using the Condition policy. Therefore, this is not the correct method for the given scenario.</p><p>The option that says: <strong>Create a web ACL in AWS WAF to block any public S3 access and attach it to the CloudFront distribution</strong> is incorrect because AWS WAF is primarily used to protect your applications from common web vulnerabilities and not to ensure exclusive access to CloudFront.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/SecurityAndPrivateContent.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/SecurityAndPrivateContent.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Origin Access Control (OAC) for CloudFront and grant access to the objects in your S3 bucket to that OAC.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "Create an Identity and Access Management (IAM) user for CloudFront and grant access to the objects in your S3 bucket to that IAM user.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an S3 bucket policy that lists the CloudFront distribution ID as the principal and the target bucket as the Amazon Resource Name (ARN).</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a web ACL in AWS WAF to block any public S3 access and attach it to the CloudFront distribution.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 42,
    "question": "<p>A company has a fixed set of Amazon EC2 instances inside a VPC in the AWS cloud. The instances run a mission-critical application. In a recent incident, one of the EC2 instances suddenly powered down which affected the availability of the application. To avoid this incident in the future, the management wants to get notified of any upcoming AWS events that may affect these EC2 instances.</p><p>Which of the following options is the recommended action to meet the above requirements?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p><strong>AWS Health</strong> provides ongoing visibility into your resource performance and the availability of your AWS services and accounts. You can use AWS Health <em>events</em> to learn how service and resource changes might affect your applications running on AWS. AWS Health provides relevant and timely information to help you manage events in progress. AWS Health also helps you be aware of and to prepare for planned activities.</p><p>You can use <strong>Amazon EventBridge</strong> to detect and react to AWS Health events. Then, based on the rules that you create, EventBridge invokes one or more target actions when an event matches the values that you specify in a rule. For example, you can use AWS Health to receive email notifications if you have AWS resources in your AWS account that are scheduled for updates, such as Amazon Elastic Compute Cloud (Amazon EC2) instances.</p><p><img src=\"https://media.tutorialsdojo.com/saa_personal_health_dashboard.png\"></p><p><strong>Your account events</strong> – This page shows events that are specific to your account. You can view open, recent, and scheduled changes. You can also view notifications, as well as an event log that shows all events from the past 90 days.</p><p>Therefore, the correct answer is: <strong>Create an Amazon EventBridge (Amazon CloudWatch Events) rule to check for AWS Personal Health Dashboard events that are related to Amazon EC2 instances. To send notifications, set an Amazon SNS topic as a target for the rule. </strong>You can use Amazon EventBridge to detect and react to AWS Health events and then send messages using AWS SNS.</p><p>The option that says: <strong>Create an Amazon EventBridge (Amazon CloudWatch Events) rule that is scheduled to run every 24 hours. Set the target to an AWS Lambda function that will check AWS Service Health Dashboard and send notifications for any events that may affect Amazon EC2 instances</strong> is incorrect. First of all, the AWS Service Health Dashboard doesn't show events related to specific EC2 instances on individual accounts. This service shows the public events that may affect several customers in particular regions.</p><p>The option that says: <strong>Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to check for any status change for Amazon EC2 instances. Set the target to an AWS Lambda function that will send a notification and restart the affected Amazon EC2 instances</strong> is incorrect. Take note that the requirement is to send notifications for upcoming or scheduled AWS events. This solution will send notifications after the EC2 instances have stopped and not show upcoming events in AWS.</p><p>The option that says: <strong>Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to check for AWS Service Health Dashboard events that are related to Amazon EC2 instances. To send notifications, set an Amazon SNS topic as a target for the rule </strong>is incorrect. AWS Service Health Dashboard shows public events that may affect several customers in particular regions. It doesn't show events related to specific EC2 instances on individual AWS accounts. You have to check the events on the AWS Personal Health Dashboard instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html\">https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html</a></p><p><a href=\"https://docs.aws.amazon.com/health/latest/ug/what-is-aws-health.html\">https://docs.aws.amazon.com/health/latest/ug/what-is-aws-health.html</a></p><p><a href=\"https://docs.aws.amazon.com/health/latest/ug/getting-started-health-dashboard.html\">https://docs.aws.amazon.com/health/latest/ug/getting-started-health-dashboard.html</a></p><p><br></p><p><strong>Check out these Amazon CloudWatch and AWS Health Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><a href=\"https://tutorialsdojo.com/aws-health/?src=udemy\">https://tutorialsdojo.com/aws-health/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create an Amazon EventBridge (Amazon CloudWatch Events) rule that is scheduled to run every 24 hours. Set the target to an AWS Lambda function that will check AWS Service Health Dashboard and send notifications for any events that may affect Amazon EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to check for any status change for Amazon EC2 instances. Set the target to an AWS Lambda function that will send a notification and restart the affected Amazon EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon EventBridge (Amazon CloudWatch Events) rule to check for AWS Personal Health Dashboard events that are related to Amazon EC2 instances. To send notifications, set an Amazon SNS topic as a target for the rule.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to check for AWS Service Health Dashboard events that are related to Amazon EC2 instances. To send notifications, set an Amazon SNS topic as a target for the rule.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 43,
    "question": "<p>An online survey startup is collecting real estate data in the United States for several years. The startup already has a total of 5 TB of data stored in an Amazon S3 bucket located in the us-east-1 Region. All real estate data must be shared with a European AWS Managed Service Provider (MSP) Partner which also uses Amazon S3 for storage. Due to budget constraints, the startup must keep its data transfer costs in S3 as low as possible and disable anonymous access.</p><p>Which solution meets this requirement MOST cost-effectively?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>In general, bucket owners pay for all Amazon S3 storage and data transfer costs that are associated with their bucket. However, you can configure a bucket to be a <em>Requester Pays</em> bucket. With Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket. The bucket owner always pays the cost of storing data.</p><p>Typically, you configure buckets to be Requester Pays buckets when you want to share data but not incur charges associated with others accessing the data. For example, you might use Requester Pays buckets when making available large datasets, such as zip code directories, reference data, geospatial information, or web crawling data.</p><p><img src=\"https://media.tutorialsdojo.com/s3-requester-pays-bucket-aws.png\"></p><p>Take note that if you enable Requester Pays on a bucket, anonymous access to that bucket will not be allowed anymore.</p><p>You must authenticate all requests involving Requester Pays buckets. The request authentication enables Amazon S3 to identify and charge the requester for their use of the Requester Pays bucket. When the requester assumes an AWS Identity and Access Management (IAM) role before making their request, the account to which the role belongs is charged for the request.</p><p>After you configure a bucket to be a Requester Pays bucket, requesters must include <code>x-amz-request-payer</code> in their API request header, for DELETE, GET, HEAD, POST, and PUT requests, or as a parameter in a REST request to show that they understand that they will be charged for the request and the data download.</p><p>Hence, the correct answer is: <strong>Enable the Requester Pays feature on the Amazon S3 bucket to lower data transfer costs and disable anonymous access</strong></p><p>The option that says: <strong>Enable Cross-Region Replication(CRR) on the startup’s S3 bucket to automatically copy the S3 content to the partner’s S3 bucket in Europe</strong> is incorrect because CRR incurs data request and transfer fee for inter-region Data Transfer Out (DTO) from your S3 bucket to your destination region. This will effectively increase your data transfer costs instead of lowering it.</p><p>The option that says: <strong>Enable cross-account access of the startup’s S3 bucket to allow the data downloads and exclusive access from the partner’s AWS account</strong> is incorrect because if the European partner downloads the data from the S3 bucket, then the startup will still be billed for the data transfer costs and not the AWS account of the partner.</p><p>The option that says: <strong>Enable S3 Object Lock in governance mode to lower data transfer costs and set a Legal Hold for each object to disable anonymous access</strong> is incorrect because the S3 Object Lock feature only prevents objects from being deleted or overwritten for a fixed amount of time. This option will not decrease the data transfer costs at all. Furthermore, the \"Legal Hold\" option is just a write-once-read-many (WORM) lock retention period for an S3 object that will be applied indefinitely until you explicitly remove it.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysExamples.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysExamples.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable the Requester Pays feature on the Amazon S3 bucket to lower data transfer costs and disable anonymous access</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable Cross-Region Replication(CRR) on the startup’s S3 bucket to automatically copy the S3 content to the partner’s S3 bucket in Europe.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable cross-account access of the startup’s S3 bucket to allow the data downloads and exclusive access from the partner’s AWS account</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable S3 Object Lock in governance mode to lower data transfer costs and set a Legal Hold for each object to disable anonymous access</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 44,
    "question": "<p>A company hosts all its applications on its data center on the US East Coast. Most of the workloads are legacy applications that are hosted on individual virtual machines running in Linux and Windows operating systems. The company plans to migrate all of its VM workloads to the AWS cloud. To minimize changes in the applications during the migration process, it has been decided that the company will use a “lift-and-shift” strategy. The company also wants to minimize downtime during the migration process.</p><p>Which of the following options should the Solutions Architect implement for this scenario?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>AWS Application Migration Service (AWS MGN)</strong> is the primary migration service recommended for lift-and-shift migrations to AWS. AWS encourages customers who are currently using AWS Elastic Disaster Recovery to switch to AWS MGN for future migrations. AWS MGN enables organizations to move applications to AWS without having to make any changes to the applications, their architecture, or the migrated servers.</p><p><strong>AWS Application Migration Service</strong> minimizes time-intensive, error-prone manual processes by automatically converting your source servers from physical, virtual machines, and cloud infrastructure to run natively on AWS.</p><p>The service simplifies your migration by enabling you to use the same automated process for a wide range of applications. By launching non-disruptive tests before migrating, you can be confident that your most critical applications such as SAP, Oracle, and SQL Server, will work seamlessly on AWS.</p><p>Implementation begins by installing the <strong>AWS Replication Agent</strong> on your source servers. When you launch Test or Cutover instances, AWS Application Migration Service automatically converts your source servers to boot and runs natively on AWS.</p><p><img src=\"https://media.tutorialsdojo.com/saa_application_migration_service.jpg\"></p><p>Therefore, the correct answer is: <strong>Install the AWS Replication Agent on each of the on-premises VMs to continuously replicate the servers to AWS. Use AWS Migration Service (AWS MGN) to launch test instances and perform cutover once testing is completed.</strong></p><p>The option that says: <strong>Export the on-premises VMs and upload the images to an Amazon S3 bucket. Use VM Import/Export service to import the images and launch them as Amazon EC2 instances</strong> is incorrect. This approach will take a lot of downtime since you will need to import the VMs manually, and they will most likely be outdated if there are a lot of changes on the source VMs after the upload is complete.</p><p>The option that says: <strong>Use the AWS Application Discovery Service for lift-and-shift migrations. Deploy the AWS Application Discovery Agent to the on-premises data center to start the replication process. After the replication task is completed, launch Amazon EC2 instances based on the created AMIs </strong>is incorrect. The AWS Application Discovery Service is primarily used to track the migration status of your on-premises applications from the Migration Hub console in your home Region. This service is not capable of doing the actual migration.</p><p>The option that says: <strong>Utilize AWS DataSync to migrate the application workloads to AWS. Deploy the AWS DataSync VM on the on-premises data center. Once replication is completed, launch Amazon EC2 instances based on the created AMIs</strong> is incorrect. AWS DataSync is designed to facilitate data transfer from on-premises to AWS storage systems, not for migrating/syncing Virtual Machines.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/how-to-use-the-new-aws-application-migration-service-for-lift-and-shift-migrations/\">https://aws.amazon.com/blogs/aws/how-to-use-the-new-aws-application-migration-service-for-lift-and-shift-migrations/</a></p><p><a href=\"https://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html\">https://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html</a></p><p><a href=\"https://docs.aws.amazon.com/mgn/latest/ug/first-time-setup-gs.html\">https://docs.aws.amazon.com/mgn/latest/ug/first-time-setup-gs.html</a></p><p><br></p><p><strong>Check out this AWS Application Migration Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-application-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-application-migration-service/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Export the on-premises VMs and upload the images to an Amazon S3 bucket. Use VM Import/Export service to import the images and launch them as Amazon EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Install the AWS Replication Agent on each of the on-premises VMs to continuously replicate the servers to AWS. Use AWS Migration Service (AWS MGN) to launch test instances and perform cutover once testing is completed.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use the AWS Application Discovery Service for lift-and-shift migrations. Deploy the AWS Application Discovery Agent to the on-premises data center to start the replication process. After the replication task is completed, launch Amazon EC2 instances based on the created AMIs.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Utilize AWS DataSync to migrate the application workloads to AWS. Deploy the AWS DataSync VM on the on-premises data center. Once replication is completed, launch Amazon EC2 instances based on the created AMIs.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 45,
    "question": "<p>A company is using an On-Demand EC2 instance to host a legacy web application that uses an Amazon Instance Store-Backed AMI. The web application should be decommissioned as soon as possible and hence, you need to terminate the EC2 instance.</p><p>When the instance is terminated, what happens to the data on the root volume?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AMIs</strong> are categorized as either <em>backed by Amazon EBS</em> or <em>backed by instance store</em>. The former means that the root device for an instance launched from the AMI is an Amazon EBS volume created from an Amazon EBS snapshot. The latter means that the root device for an instance launched from the AMI is an instance store volume created from a template stored in Amazon S3.</p><p><img src=\"https://media.tutorialsdojo.com/instance_lifecycle.png\"></p><p>The data on instance store volumes persist only during the life of the instance which means that if the instance is terminated, the data will be automatically deleted.</p><p>Hence, the correct answer is: <strong>Data is automatically deleted.</strong></p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Data is automatically saved as an EBS snapshot.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Data is automatically saved as an EBS volume.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Data is unavailable until the instance is restarted.",
        "correct": false
      },
      {
        "id": 4,
        "answer": "Data is automatically deleted.",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 46,
    "question": "<p>A company decided to change its third-party data analytics tool to a cheaper solution. They sent a full data export on a CSV file which contains all of their analytics information. You then save the CSV file to an S3 bucket for storage. Your manager asked you to do some validation on the provided data export.</p><p>In this scenario, what is the most cost-effective and easiest way to analyze export data using standard SQL?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>Amazon Athena</strong> is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds.</p><p><img src=\"https://media.tutorialsdojo.com/public/StepsToMigration.jpg\"></p><p>Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run. Athena scales automatically—executing queries in parallel—so results are fast, even with large datasets and complex queries.</p><p>Athena helps you analyze unstructured, semi-structured, and structured data stored in Amazon S3. Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC. You can use Athena to run ad-hoc queries using ANSI SQL without the need to aggregate or load the data into Athena.</p><p>Hence, the correct answer is: <strong>To be able to run SQL queries, use Amazon Athena to analyze the export data file in S3.</strong></p><p>The rest of the options are all incorrect because it is not necessary to set up a database to be able to analyze the CSV export file. You can use a cost-effective option (AWS Athena), which is a serverless service that enables you to pay only for the queries you run.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/what-is.html\">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p><p><br></p><p><strong>Check out this Amazon Athena Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-athena/?src=udemy\">https://tutorialsdojo.com/amazon-athena/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a migration tool to load the CSV export file from S3 to a DynamoDB instance. Once the data has been loaded, run queries using DynamoDB.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use mysqldump client utility to load the CSV export file from S3 to a MySQL RDS instance. Run some SQL queries once the data has been loaded to complete your validation.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>To be able to run SQL queries, use AWS Athena to analyze the export data file in S3.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use a migration tool to load the CSV export file from S3 to a database that is designed for online analytic processing (OLAP) such as AWS RedShift. Run some queries once the data has been loaded to complete your validation.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 47,
    "question": "<p>A startup plans to develop a multiplayer game that uses UDP as the protocol for communication between clients and game servers. The data of the users will be stored in a key-value store. As the Solutions Architect, you need to implement a solution that will distribute the traffic across a number of servers.</p><p>Which of the following could help you achieve this requirement?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>A <strong>Network Load Balancer</strong> functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. For UDP traffic, the load balancer selects a target using a flow hash algorithm based on the protocol, source IP address, source port, destination IP address, and destination port. A UDP flow has the same source and destination, so it is consistently routed to a single target throughout its lifetime. Different UDP flows have a different source IP addresses and ports, so they can be routed to different targets.</p><p><img src=\"https://media.tutorialsdojo.com/public/2020-05-30_11-27-17-6b6ca142e32995f1af0d40df8d45dbff.png\"></p><p>In this scenario, a startup plans to create a multiplayer game that uses UDP as the protocol for communications. Since UDP is a Layer 4 traffic, we can limit the option that uses Network Load Balancer. The data of the users will be stored in a key-value store. This means that we should select Amazon DynamoDB since it supports both document and key-value store models.</p><p>Hence, the correct answer is:<strong> Distribute the traffic using Network Load Balancer and store the data in Amazon DynamoDB</strong>.</p><p>The option that says:<strong> Distribute the traffic using Application Load Balancer and store the data in Amazon DynamoDB </strong>is incorrect because UDP is not supported in Application Load Balancer. Remember that UDP is a Layer 4 traffic. Therefore, you should use a Network Load Balancer.</p><p>The option that says:<strong> Distribute the traffic using Network Load Balancer and store the data in Amazon Aurora</strong> is incorrect because Amazon Aurora is a relational database service. Instead of Aurora, you should use Amazon DynamoDB.</p><p>The option that says:<strong> Distribute the traffic using Application Load Balancer and store the data in Amazon RDS</strong> is incorrect because Application Load Balancer only supports application traffic (Layer 7). Also, Amazon RDS is not suitable as a key-value store. You should use DynamoDB since it supports both document and key-value store models.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-udp-load-balancing-for-network-load-balancer/\">https://aws.amazon.com/blogs/aws/new-udp-load-balancing-for-network-load-balancer/</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Distribute the traffic using Application Load Balancer and store the data in Amazon DynamoDB.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Distribute the traffic using Network Load Balancer and store the data in Amazon Aurora.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Distribute the traffic using Application Load Balancer and store the data in Amazon RDS.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Distribute the traffic using Network Load Balancer and store the data in Amazon DynamoDB.</p>",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 48,
    "question": "<p>A media company needs to configure an Amazon S3 bucket to serve static assets for the public-facing web application. Which methods ensure that all of the objects uploaded to the S3 bucket can be read publicly all over the Internet? (Select TWO.)</p>",
    "multiple": false,
    "domain": "Design Secure Architectures",
    "explanation": "<p>By default, all Amazon S3 resources such as buckets, objects, and related subresources are private, which means that only the AWS account holder (resource owner) that created it has access to the resource. The resource owner can optionally grant access permissions to others by writing an access policy. In S3, you also set the permissions of the object during upload to make it public.</p><p>Amazon S3 offers access policy options broadly categorized as resource-based policies and user policies. Access policies you attach to your resources (buckets and objects) are referred to as resource-based policies.</p><p>For example, bucket policies and access control lists (ACLs) are resource-based policies. You can also attach access policies to users in your account. These are called user policies. You may choose to use resource-based policies, user policies, or some combination of these to manage permissions to your Amazon S3 resources.</p><p>You can also manage the public permissions of your objects during upload. Under <strong><em>Manage public permissions</em></strong><em>,</em> you can grant read access to your objects to the general public (everyone in the world) for all of the files that you're uploading. Granting public read access is applicable to a small subset of use cases, such as when buckets are used for websites.</p><p><img src=\"https://media.tutorialsdojo.com/public/2020-05-22_03-44-35-8faafd3a03fc514d237f5eb8ded2faee.png\"></p><p>Hence, the correct answers are:</p><p><strong>- Grant public read access to the object when uploading it using the S3 Console.</strong></p><p><strong>- Configure the S3 bucket policy to set all objects to public read.</strong></p><p>The option that says: <strong>Configure the cross-origin resource sharing (CORS) of the S3 bucket to allow objects to be publicly accessible from all domains</strong> is incorrect. CORS will only allow objects from one domain (travel.cebu.com) to be loaded and accessible to a different domain (palawan.com). It won't necessarily expose objects for public access all over the internet.</p><p>The option that says: <strong>Creating an IAM role to set the objects inside the S3 bucket to public read</strong> is incorrect. You can create an IAM role and attach it to an EC2 instance in order to retrieve objects from the S3 bucket or add new ones. An IAM Role, in itself, cannot directly make the S3 objects public or change the permissions of each individual object.</p><p>The option that says: <strong>Do nothing. Amazon S3 objects are already public by default</strong> is incorrect because, by default, all the S3 resources are private, so only the AWS account that created the resources can access them.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html\">http://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Grant public read access to the object when uploading it using the S3 Console.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure the cross-origin resource sharing (CORS) of the S3 bucket to allow objects to be publicly accessible from all domains.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "Configure the S3 bucket policy to set all objects to public read.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Create an IAM role to set the objects inside the S3 bucket to public read.",
        "correct": false
      },
      {
        "id": 5,
        "answer": "Do nothing. Amazon S3 objects are already public by default.",
        "correct": false
      }
    ],
    "corrects": [
      1,
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 49,
    "question": "<p>A company must integrate the Lightweight Directory Access Protocol (LDAP) directory service from the on-premises data center to the AWS VPC using IAM. The identity store which is currently being used is not compatible with SAML.</p><p>Which of the following provides the most valid approach to implement the integration?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>If your identity store is not compatible with SAML 2.0 then you can build a custom identity broker application to perform a similar function. The broker application authenticates users, requests temporary credentials for users from AWS, and then provides them to the user to access AWS resources.</p><p>The application verifies that employees are signed into the existing corporate network's identity and authentication system, which might use LDAP, Active Directory, or another system. The identity broker application then obtains temporary security credentials for the employees.</p><p>To get temporary security credentials, the identity broker application calls either <code><strong>AssumeRole</strong></code> or <code><strong>GetFederationToken</strong></code> to obtain temporary security credentials, depending on how you want to manage the policies for users and when the temporary credentials should expire. The call returns temporary security credentials consisting of an AWS access key ID, a secret access key, and a session token. The identity broker application makes these temporary security credentials available to the internal company application. The app can then use the temporary credentials to make calls to AWS directly. The app caches the credentials until they expire, and then requests a new set of temporary credentials.</p><p><img src=\"https://media.tutorialsdojo.com/public/enterprise-authentication-with-identity-broker-application.diagram.png\"></p><p>Hence, the correct answer is: <strong>Develop an on-premises custom identity broker application and use STS to issue short-lived AWS credentials.</strong></p><p>The option that says: <strong>Use an IAM policy that references the LDAP identifiers and AWS credentials</strong> is incorrect because simply using an IAM policy is not enough to integrate your LDAP service to IAM. You need to use SAML, STS, or a custom identity broker.</p><p>The option that says: <strong>Use AWS IAM Identity Center to manage access between AWS and your LDAP</strong> is incorrect. While AWS IAM Identity Center can integrate with external identity providers, it primarily supports SAML 2.0-based identity providers. The question explicitly states that the identity store is not compatible with SAML.</p><p>The option that says: <strong>Use IAM roles to rotate the IAM credentials whenever LDAP credentials are updated</strong> is incorrect because manually rotating the IAM credentials is not an optimal solution to integrate your on-premises and VPC network. You need to use SAML, STS, or a custom identity broker.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/aws-identity-and-access-management-now-with-identity-federation/\">https://aws.amazon.com/blogs/aws/aws-identity-and-access-management-now-with-identity-federation/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate/</a></p><p><br></p><p><strong>Check out this AWS Identity and Access Management Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "Use an IAM policy that references the LDAP identifiers and AWS credentials.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS IAM Identity Center to manage access between AWS and your LDAP.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Develop an on-premises custom identity broker application and use STS to issue short-lived AWS credentials.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "Use IAM roles to rotate the IAM credentials whenever LDAP credentials are updated.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 50,
    "question": "<p>A company has a team of developers that provisions their own resources on the AWS cloud. The developers use IAM user access keys to automate their resource provisioning and application testing processes in AWS. To ensure proper security compliance, the security team wants to automate the process of deactivating and deleting any IAM user access key that is over 90 days old.</p><p>Which solution will meet these requirements with the LEAST operational effort?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>AWS Config</strong> provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.</p><p>You can use AWS Config rules to evaluate the configuration settings of your AWS resources. When AWS Config detects that a resource violates the conditions in one of your rules, AWS Config flags the resource as non-compliant and sends a notification. AWS Config continuously evaluates your resources as they are created, changed, or deleted.</p><p>To analyze potential security weaknesses, you need detailed historical information about your AWS resource configurations, such as the AWS Identity and Access Management (IAM) permissions that are granted to your users or the Amazon EC2 security group rules that control access to your resources.</p><p><strong>Amazon EventBridge (Amazon CloudWatch Events)</strong> is a serverless event bus service that you can use to connect your applications with data from a variety of sources.</p><p>You can use an Amazon EventBridge (Amazon CloudWatch Events) rule with a custom event pattern and an input transformer to match an AWS Config evaluation rule output as NON_COMPLIANT. Then, route the response to an Amazon Simple Notification Service (Amazon SNS) topic.</p><p><img src=\"https://media.tutorialsdojo.com/saa_aws_config_eventbridge.jpg\"></p><p>Therefore, the correct answer is: <strong>Use the AWS Config managed rule to check if the IAM user access keys are not rotated within 90 days. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the non-compliant keys, and define a target to invoke a custom Lambda function to deactivate and delete the keys. </strong>Amazon EventBridge (Amazon CloudWatch Events) can check for AWS Config non-compliant events and then trigger a Lambda for remediation.</p><p>The option that says: <strong>Create an Amazon EventBridge (Amazon CloudWatch Events) rule to filter IAM user access keys older than 90 days. Schedule an AWS Batch job that runs every 24 hours to delete all the specified access keys</strong> is incorrect. Amazon EventBridge (Amazon CloudWatch Events) cannot directly check for IAM events that show the age of IAM access keys. The use of the AWS Batch service is not warranted here as it is primarily used to easily and efficiently run hundreds of thousands of batch computing jobs on AWS.</p><p>The option that says: <strong>Create a custom AWS Config rule to check for the max-age of IAM access keys. Schedule an AWS Batch job that runs every 24 hours to delete all the non-compliant access keys</strong> is incorrect. AWS Config can only use Systems Manager Automation documents as remediation actions. Moreover, the AWS Batch service cannot be used for remediating non-compliant resources like IAM access keys.</p><p>The option that says: <strong>Create an Amazon EventBridge (Amazon CloudWatch Events) rule to filter IAM user access keys older than 90 days. Define a target to invoke a Lambda function to deactivate and delete the old access keys</strong> is incorrect. Amazon EventBridge (Amazon CloudWatch Events) cannot directly check for IAM events that contain the age of IAM access keys. You have to use the AWS Config service to detect the non-compliant IAM keys.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/managing-aged-access-keys-through-aws-config-remediations/\">https://aws.amazon.com/blogs/mt/managing-aged-access-keys-through-aws-config-remediations/</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/config-resource-non-compliant/\">https://aws.amazon.com/premiumsupport/knowledge-center/config-resource-non-compliant/</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html\">https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html</a></p><p><br></p><p><strong>Check out these Amazon CloudWatch and AWS Config Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the AWS Config managed rule to check if the IAM user access keys are not rotated within 90 days. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the non-compliant keys, and define a target to invoke a custom Lambda function to deactivate and delete the keys.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create an Amazon EventBridge (Amazon CloudWatch Events) rule to filter IAM user access keys older than 90 days. Schedule an AWS Batch job that runs every 24 hours to delete all the specified access keys.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a custom AWS Config rule to check for the max-age of IAM access keys. Schedule an AWS Batch job that runs every 24 hours to delete all the non-compliant access keys.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create an Amazon EventBridge (Amazon CloudWatch Events) rule to filter IAM user access keys older than 90 days. Define a target to invoke a Lambda function to deactivate and delete the old access keys.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 51,
    "question": "<p>A company launched an EC2 instance in the newly created VPC. They noticed that the generated instance does not have an associated DNS hostname.</p><p>Which of the following options could be a valid reason for this issue?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>When you launch an EC2 instance into a default VPC, AWS provides it with public and private DNS hostnames that correspond to the public IPv4 and private IPv4 addresses for the instance.</p><p><img src=\"https://media.tutorialsdojo.com/public/DrewDennis_DNSresolutionDNShostnames.png\"></p><p>However, when you launch an instance into a non-default VPC, AWS provides the instance with a private DNS hostname only. New instances will only be provided with a public DNS hostname depending on these two DNS attributes: the <strong>DNS resolution</strong> and <strong>DNS hostnames</strong> that you have specified for your VPC and if your instance has a public IPv4 address.</p><p>In this case, the new EC2 instance does not automatically get a DNS hostname because the <strong>DNS resolution</strong> and <strong>DNS hostnames</strong> attributes are disabled in the newly created VPC.</p><p>Hence, the correct answer is: <strong>The DNS resolution and DNS hostname of the VPC configuration should be enabled.</strong></p><p>The option that says: <strong>The newly created VPC has an invalid CIDR block</strong> is incorrect since it's very unlikely that a VPC has an invalid CIDR block because of AWS validation schemes.</p><p>The option that says: <strong>Amazon Route 53 is not enabled</strong> is incorrect since Route 53 does not need to be enabled. Route 53 is the DNS service of AWS, but the VPC is the one that enables assigning of instance hostnames.</p><p>The option that says: <strong>The security group of the EC2 instance needs to be modified</strong> is incorrect since security groups are just firewalls for your instances. They filter traffic based on a set of security group rules.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-dns.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-dns.html</a></p><p><a href=\"https://aws.amazon.com/vpc/\">https://aws.amazon.com/vpc/</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "The newly created VPC has an invalid CIDR block.",
        "correct": false
      },
      {
        "id": 2,
        "answer": "Amazon Route53 is not enabled.",
        "correct": false
      },
      {
        "id": 3,
        "answer": "The DNS resolution and DNS hostname of the VPC configuration should be enabled.",
        "correct": true
      },
      {
        "id": 4,
        "answer": "The security group of the EC2 instance needs to be modified.",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 52,
    "question": "<p>A hospital has a mission-critical application that uses a RESTful API powered by Amazon API Gateway and AWS Lambda. The medical officers upload PDF reports to the system which are then stored as static media content in an Amazon S3 bucket.</p><p>The security team wants to improve its visibility when it comes to cyber-attacks and ensure HIPAA (Health Insurance Portability and Accountability Act) compliance. The company is searching for a solution that continuously monitors object-level S3 API operations and identifies protected health information (PHI) in the reports, with minimal changes in the existing Lambda function.</p><p>Which of the following solutions will meet these requirements with the LEAST operational overhead?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>Amazon Comprehend Medical is a natural language processing service that makes it easy to use machine learning to extract relevant medical information from unstructured text. Using Amazon Comprehend Medical, you can quickly and accurately gather information, such as medical conditions, medication, dosage, strength, and frequency from a variety of sources, like doctors’ notes, clinical trial reports, and patient health records.</p><p><img src=\"https://media.tutorialsdojo.com/amazon-comprehend-medical.png\"></p><p>Amazon Comprehend Medical uses advanced machine learning models to accurately and quickly identify medical information such as medical conditions and medication and determine their relationship to each other, for instance, medication and dosage. You access Comprehend Medical through a simple API call, no machine learning expertise is required, no complicated rules to write, and no models to train.</p><p>You can use the extracted medical information and their relationships to build applications for use cases, like clinical decision support, revenue cycle management (medical coding), and clinical trial management. Because Comprehend Medical is HIPAA-eligible and can quickly identify protected health information (PHI), such as name, age, and medical record number, you can also use it to create applications that securely process, maintain, and transmit PHI.</p><p>Hence, the correct answer is: <strong>Use Amazon Textract to extract the text from the PDF reports. Integrate Amazon Comprehend Medical with the existing Lambda function to identify the PHI from the extracted text.</strong></p><p>The option that says: <strong>Use Amazon Textract Medical with PII redaction turned on to extract and filter sensitive text from the PDF reports. Create a new Lambda function that calls the regular Amazon Comprehend API to identify the PHI from the extracted text</strong> is incorrect. The PII (Personally Identifiable Information) redaction feature of the Amazon Textract Medical service is not enough to identify all the Protected Health Information (PHI) in the PDF reports. Take note that PII is quite different from PHI. In addition, this option proposes to create a brand new Lambda function, which is contrary to what the company explicitly requires of having a minimal change in their existing Lambda function.</p><p>The option that says: <strong>Use Amazon Transcribe to read and analyze the PDF reports using the </strong><code><strong>StartTranscriptionJob API</strong></code><strong> operation. Use Amazon CloudWatch Logs to detect protected health information (PHI) content by tracking access logs and security events </strong>is incorrect because Amazon Transcribe is an automatic speech recognition (ASR) service designed primarily to help developers integrate speech-to-text capability to their applications, and not for extracting text from image files or PDFs. Moreover, CloudWatch Logs cannot perform PHI detection directly.</p><p>The option that says: <strong>Use Amazon Textract with the </strong><code><strong>StartDocumentTextDetection API</strong></code><strong> operation to extract text from PDF reports. Analyze the extracted data with a custom-built PHI detection algorithm within the Lambda function </strong>is incorrect because creating and maintaining a custom PHI detection algorithm typically increases operational overhead. This approach is inefficient compared to integrating a pre-built, HIPAA-compliant service like Amazon Comprehend Medical.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/comprehend/medical/\">https://aws.amazon.com/comprehend/medical/</a></p><p><a href=\"https://docs.aws.amazon.com/comprehend-medical/latest/dev/textanalysis-phi.html\">https://docs.aws.amazon.com/comprehend-medical/latest/dev/textanalysis-phi.html</a></p><p><br></p><p><strong>Check out this Amazon Comprehend Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-comprehend/?src=udemy\">https://tutorialsdojo.com/amazon-comprehend</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Textract Medical with PII redaction turned on to extract and filter sensitive text from the PDF reports. Create a new Lambda function that calls the regular Amazon Comprehend API to identify the PHI from the extracted text.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Textract to extract the text from the PDF reports. Integrate Amazon Comprehend Medical with the existing Lambda function to identify the PHI from the extracted text.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Transcribe to read and analyze the PDF reports using the <code>StartTranscriptionJob API</code> operation. Use Amazon CloudWatch Logs to detect protected health information (PHI) content by tracking access logs and security events.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Textract with the <code>StartDocumentTextDetection</code> API operation to extract text from PDF reports. Analyze the extracted data with a custom-built PHI detection algorithm within the Lambda function.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 53,
    "question": "<p>A company has a regional API Gateway in the us-east-2 region that serves as a proxy to a backend service. Clients connect to the service using the invoke URL of the API stage. To improve usability, the company wants to associate a custom domain name (<code>api.tutorialsdojo.com</code>) with the API. Moreover, the domain name must support HTTPS to ensure secure connections. The company has an existing hosted zone for its domain on Amazon Route 53.</p><p>Which of the following would be the next step to achieve the company's objective?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p>In API Gateway, A Regional API is an API deployed to a specified Region and intended to serve clients, such as EC2 instances, in the same AWS Region. API requests are targeted directly to the Region-specific API Gateway API without going through any CloudFront distribution. For in-Region requests, a Regional endpoint bypasses the unnecessary round trip to a CloudFront distribution.</p><p><img src=\"https://media.tutorialsdojo.com/public/api-custom-domain-07-05-23.png\"></p><p>When using a regional API, any custom domain name you choose is exclusive to the specific region where the API is deployed. If you deploy the same regional API in multiple regions, it can have the same custom domain name across all regions. Also, you must request or import the SSL certificate in the same Region as your API.</p><p>To meet the requirement in the scenario, you must first request a public certificate for your custom domain from the AWS Certificate Manager (ACM) in the same region as your API Gateway. This certificate will secure HTTPS connections to your API. Then, you configure a regional API Gateway domain name, and associate it with your custom domain and the ACM certificate. Finally, you create an alias record in Route 53, which maps your custom domain to the API Gateway's domain name. With this setup, client requests to your custom domain will be securely routed to the correct API Gateway.</p><p>Hence the correct answer is: <strong>Request a public certificate in the us-east-2 region for </strong><code><strong>api.tutorialsdojo.com</strong></code><strong> using AWS Certificate Manager (ACM). Create a regional API Gateway domain name and associate it with </strong><code><strong>api.tutorialsdojo.com</strong></code><strong> and the ACM certificate. In Route 53, create an alias record for </strong><code><strong>api.tutorialsdojo.com</strong></code><strong> that points to the API Gateway domain name.</strong></p><p>The option that says: <strong>Use the AWS Certificate Manager Private Certificate Authority (ACM PCA) to generate a private certificate for </strong><code><strong>api.tutorialsdojo.com</strong></code><strong>. Override the invoke URL using stage variables </strong>is incorrect. Private certificates are used within an internal network. A public certificate is required to enable secure connections from the public internet to the API. Also, overriding the invoke URL using stage variables is not relevant to the scenario.</p><p>The option that says: <strong>Import an existing public certificate for </strong><code><strong>api.tutorialsdojo.com</strong></code><strong> into AWS Certificate Manager (ACM) in the us-east-2. In Route 53, create a CNAME record for </strong><code><strong>api.tutorialsdojo.com</strong></code><strong> that points to the invoke URL of the API Gateway stage </strong>is incorrect. This won't work. To use a custom domain name with Amazon API Gateway, you should use API Gateway's Custom Domain Names feature. This will allow you to specify your desired domain name, map it to your API, and correctly configure an SSL/TLS certificate.</p><p>The option that says: <strong>Request a public certificate in the us-east-1 region for </strong><code><strong>api.tutorialsdojo.com</strong></code><strong> using AWS Certificate Manager (ACM). Create a regional API Gateway domain name and associate it with </strong><code><strong>api.tutorialsdojo.com</strong></code><strong> and the ACM certificate. In Route 53, create an alias record for </strong><code><strong>api.tutorialsdojo.com</strong></code><strong> that points to the API Gateway domain name</strong> is incorrect because it suggests configuring the Regional API Gateway endpoint in the wrong region. For a regional API Gateway, the certificate must be requested/imported in the same region as your API.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-endpoint-types.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-endpoint-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html\">https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html</a></p><p><a href=\"https://docs.aws.amazon.com/route53/\">https://docs.aws.amazon.com/route53/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam – SAA-C03 Study Path:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c03/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c03/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Request a public certificate in the us-east-1 region for <code>api.tutorialsdojo.com</code> using AWS Certificate Manager (ACM). Create a regional API Gateway domain name and associate it with <code>api.tutorialsdojo.com</code> and the ACM certificate. In Route 53, create an alias record for <code>api.tutorialsdojo.com</code> that points to the API Gateway domain name.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Import an existing public certificate for <code>api.tutorialsdojo.com</code> into AWS Certificate Manager (ACM) in the us-east-2. In Route 53, create a CNAME record for <code>api.tutorialsdojo.com</code> that points to the invoke URL of the API Gateway stage.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use the AWS Certificate Manager Private Certificate Authority (ACM PCA) to generate a private certificate for <code>api.tutorialsdojo.com</code>. Override the invoke URL using stage variables.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Request a public certificate in the us-east-2 region for <code>api.tutorialsdojo.com</code> using AWS Certificate Manager (ACM). Create a regional API Gateway domain name and associate it with <code>api.tutorialsdojo.com</code> and the ACM certificate. In Route 53, create an alias record for <code>api.tutorialsdojo.com</code> that points to the API Gateway domain name.</p>",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 54,
    "question": "<p>There are a few, easily reproducible but confidential files that your client wants to store in AWS without worrying about storage capacity. For the first month, all of these files will be accessed frequently but after that, they will rarely be accessed at all. The old files will only be accessed by developers so there is no set retrieval time requirement. However, the files under a specific <code>tdojo-finance</code> prefix in the S3 bucket will be used for post-processing that requires millisecond retrieval time.</p><p>Given these conditions, which of the following options would be the most cost-effective solution for your client's storage needs?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p>Initially, the files will be accessed frequently, and S3 is a durable and highly available storage solution for that. After a month has passed, the files won't be accessed frequently anymore, so it is a good idea to use lifecycle policies to move them to a storage class that would have a lower cost for storing them.</p><p><img src=\"https://media.tutorialsdojo.com/public/lifecycle-transitions-v2.png\"></p><p>Since the files are easily reproducible and some of them are needed to be retrieved quickly based on a specific prefix filter (<code>tdojo-finance</code>), S3-One Zone IA would be a good choice for storing them. The other files that do not contain such prefix would then be moved to Glacier for low-cost archival. This setup would also be the most cost-effective for the client.</p><p>Hence, the correct answer is: <strong>Store the files in S3 then after a month, change the storage class of the </strong><code><strong>tdojo-finance</strong></code><strong> prefix to One Zone-IA while the remaining go to Glacier using lifecycle policy</strong>.</p><p><img src=\"https://media.tutorialsdojo.com/public/2019-04-02_04-26-21-158b6e1eb7ef7be3b91049e88f056f3a.gif\"></p><p>The option that says: <strong>Storing the files in S3 then after a month, changing the storage class of the bucket to S3-IA using lifecycle policy</strong> is incorrect. Although it is valid to move the files to S3-IA, this solution still costs more compared with using a combination of S3-One Zone IA and Glacier.</p><p>The option that says: <strong>Storing the files in S3 then after a month, changing the storage class of the bucket to Intelligent-Tiering using lifecycle policy</strong> is incorrect. While S3 Intelligent-Tiering can automatically move data between two access tiers (frequent access and infrequent access) when access patterns change, it is more suitable for scenarios where you don't know the access patterns of your data. It may take some time for S3 Intelligent-Tiering to analyze the access patterns before it moves the data to a cheaper storage class like S3-IA which means you may still end up paying more in the beginning. In addition, you already know the access patterns of the files which means you can directly change the storage class immediately and save cost right away.</p><p>The option that says: <strong>Storing the files in S3 then after a month, changing the storage class of the </strong><code><strong>tdojo-finance</strong></code><strong> prefix to S3-IA while the remaining go to Glacier using lifecycle policy</strong> is incorrect. Even though S3-IA costs less than the S3 Standard storage class, it is still more expensive than S3-One Zone IA. Remember that the files are easily reproducible so you can safely move the data to S3-One Zone IA and in case there is an outage, you can simply generate the missing data again.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/amazon-s3-adds-prefix-and-suffix-filters-for-lambda-function-triggering\">https://aws.amazon.com/blogs/compute/amazon-s3-adds-prefix-and-suffix-filters-for-lambda-function-triggering</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-configuration-examples.html \">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-configuration-examples.html</a></p><p><a href=\"https://aws.amazon.com/s3/pricing\">https://aws.amazon.com/s3/pricing</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the files in S3 then after a month, change the storage class of the bucket to S3-IA using lifecycle policy.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store the files in S3 then after a month, change the storage class of the bucket to Intelligent-Tiering using lifecycle policy.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Store the files in S3 then after a month, change the storage class of the <code>tdojo-finance</code> prefix to One Zone-IA while the remaining go to Glacier using lifecycle policy.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Store the files in S3 then after a month, change the storage class of the <code>tdojo-finance</code> prefix to S3-IA while the remaining go to Glacier using lifecycle policy.</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 55,
    "question": "<p>A Solutions Architect is designing the cloud architecture for the enterprise application suite of the company. Both the web and application tiers need to access the Internet to fetch data from public APIs. However, these servers should be inaccessible from the Internet. </p><p>Which of the following steps should the Architect implement to meet the above requirements?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p>You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services but prevent the internet from initiating a connection with those instances. You are charged for creating and using a NAT gateway in your account.</p><p>NAT gateway hourly usage and data processing rates apply. Amazon EC2 charges for data transfer also apply. NAT gateways are not supported for IPv6 traffic—use an egress-only internet gateway instead.</p><p><img src=\"https://media.tutorialsdojo.com/nat-gateway-diagram.png\"></p><p>To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside. You must also specify an Elastic IP address to associate with the NAT gateway when you create it. The Elastic IP address cannot be changed once you associate it with the NAT Gateway.</p><p>After you've created a NAT gateway, you must update the route table associated with one or more of your private subnets to point Internet-bound traffic to the NAT gateway. This enables instances in your private subnets to communicate with the internet. Each NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone. You have a limit on the number of NAT gateways you can create in an Availability Zone.</p><p>Hence, the correct answer is to <strong>deploy a NAT gateway in the public subnet and add a route to it from the private subnet where the web and application tiers are hosted.</strong></p><p><strong>Deploying the web and application tier instances to a private subnet and then allocating an Elastic IP address to each EC2 instance</strong> is incorrect because an Elastic IP address is just a static, public IPv4 address. In this scenario, you have to use a NAT Gateway instead.</p><p><strong>Deploying a NAT gateway in the private subnet and adding a route to it from the public subnet where the web and application tiers are hosted</strong> is incorrect because you have to deploy a NAT gateway in the public subnet instead and not on a private one.</p><p><strong>Deploying the web and application tier instances to a public subnet and then allocating an Elastic IP address to each EC2 instance</strong> is incorrect because having an EIP address is irrelevant as it is only a static, public IPv4 address. Moreover, you should deploy the web and application tier in the private subnet instead of a public subnet to make it inaccessible from the Internet and then just add a NAT Gateway to allow outbound Internet connection.</p><p><strong>Reference</strong>:</p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c03/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c03/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy the web and application tier instances to a private subnet and then allocate an Elastic IP address to each EC2 instance.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy a NAT gateway in the private subnet and add a route to it from the public subnet where the web and application tiers are hosted.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy the web and application tier instances to a public subnet and then allocate an Elastic IP address to each EC2 instance.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy a NAT gateway in the public subnet and add a route to it from the private subnet where the web and application tiers are hosted.</p>",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 56,
    "question": "<p>A company has a web-based order processing system that is currently using a standard queue in Amazon SQS. The IT Manager noticed that there are a lot of cases where an order was processed twice. This issue has caused a lot of trouble in processing and made the customers very unhappy. The goal is to ensure that this issue will not recur.</p><p>What solution should be implemented to prevent this from happening again in the future?</p>",
    "multiple": false,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon SQS FIFO (First-In-First-Out) Queues</strong> have all the capabilities of the standard queue with additional capabilities designed to enhance messaging between applications when the order of operations and events is critical or where <strong>duplicates can't be tolerated</strong>, for example:</p><p>- Ensure that user-entered commands are executed in the right order. <br>- Display the correct product price by sending price modifications in the right order.<br>- Prevent a student from enrolling in a course before registering for an account.</p><p><img src=\"https://media.tutorialsdojo.com/public/2020-03-09_11-14-42-e24d31f607e373f989c2829e2805b01e.png\"></p><p>Amazon Simple Queue Service (SQS) provides two types of queues: <strong>Standard Queues</strong> and <strong>FIFO (First-In-First-Out) Queues</strong>. Standard Queues allow for <strong>at-least-once delivery</strong>, meaning messages might sometimes be duplicated. This can lead to situations where an order is processed more than once.</p><p>Amazon SQS FIFO Queues guarantee that messages are processed <strong>exactly once</strong> and in the same order they were sent. FIFO queues eliminate the issue of duplicated messages by ensuring that each message is delivered <strong>only once</strong> using deduplication and message sequencing mechanisms.</p><p>The main issue in this scenario is that the order management system produces duplicate orders at times. Since the company is using a standard queue in Amazon SQS, there is a possibility that a message can have a duplicate in case an EC2 instance fails to delete the already processed message.</p><p>Hence, the correct answer is: <strong>Use an SQS FIFO Queue instead.</strong></p><p>The option that says: <strong>Alter the retention period in SQS</strong> is incorrect because the retention period simply specifies if Amazon SQS should delete the messages that have been in a queue for a certain period of time.</p><p>The option that says: <strong>Alter the visibility timeout of SQS</strong> is incorrect because, for standard queues, the visibility timeout isn't a guarantee against receiving a message twice. To avoid duplicate SQS messages, it is better to design your applications to be <em>idempotent</em> (they should not be affected adversely when processing the same message more than once).</p><p>The option that says: <strong>Change the message size in SQS</strong> is incorrect because this is not related at all in this scenario. Adjusting the message size does not impact message duplication. The issue is primarily caused by at-least-once delivery, not by message size limitations.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-queue-types.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-queue-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues-exactly-once-processing.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues-exactly-once-processing.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p><p><br></p><p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Alter the retention period in SQS.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Alter the visibility timeout of SQS.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an SQS FIFO Queue instead.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Change the message size in SQS.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use an Amazon SQS FIFO Queue instead.</p>",
        "correct": true
      }
    ],
    "corrects": [
      3,
      5
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 57,
    "question": "<p>A new company policy requires IAM users to change their passwords’ minimum length to 12 characters. After a random inspection, you found out that there are still employees who do not follow the policy.</p><p>How can you automatically check and evaluate whether the current password policy for an account complies with the company password policy?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS Config</strong> is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.</p><p><img src=\"https://media.tutorialsdojo.com/public/how-AWSconfig-works.png\"></p><p>In the scenario given, we can utilize AWS Config to check for compliance on the password policy by configuring the Config rule to check the IAM_PASSWORD_POLICY on an account. Additionally, because Config integrates with AWS Organizations, we can improve the setup to aggregate compliance information across accounts to a central dashboard.</p><p>Hence, the correct answer is: <strong>Configure AWS Config to trigger an evaluation that will check the compliance for a user’s password periodically</strong>.</p><p><strong>Create a CloudTrail trail. Filter the result by setting the attribute to “Event Name” and lookup value to “ChangePassword”. This easily gives you the list of users who have made changes to their passwords </strong>is incorrect because this setup will just give you the name of the users who have made changes to their respective passwords. It will not give you the ability to check whether their passwords have met the required minimum length.</p><p><strong>Create a Scheduled Lambda function that will run a custom script to check compliance against changes made to the passwords periodically </strong>is a valid solution but still incorrect. AWS Config is already integrated with AWS Lambda. You don't have to create and manage your own Lambda function. You just have to define a Config rule where you will check compliance, and Lambda will process the evaluation. Moreover, you can't directly create a scheduled function by using Lambda itself. You have to create a rule in AWS CloudWatch Events to run the Lambda functions on the schedule that you define.</p><p><strong>Create a rule in the Amazon CloudWatch event. Build an event pattern to match events on IAM. Set the event name to “ChangePassword” in the event pattern. Configure SNS to send notifications to you whenever a user has made changes to his password</strong> is incorrect because this setup will just alert you whenever a user changes his password. Sure, you’ll have information about who made changes, but that is not enough to check whether it complies with the required minimum password length. This can be easily done in AWS Config.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config-rules.html</a></p><p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure AWS Config to trigger an evaluation that will check the compliance for a user’s password periodically.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a CloudTrail trail. Filter the result by setting the attribute to “Event Name” and lookup value to “ChangePassword”. This easily gives you the list of users who have made changes to their passwords.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a Scheduled Lambda Function that will run a custom script to check compliance against changes made to the passwords periodically.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a rule in the Amazon CloudWatch event. Build an event pattern to match events on IAM. Set the event name to “ChangePassword” in the event pattern. Configure SNS to send notifications to you whenever a user has made changes to his password.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 58,
    "question": "<p>An organization plans to use an AWS Direct Connect connection to establish a dedicated connection between its on-premises network and AWS. The organization needs to launch a fully managed solution that will automate and accelerate the replication of data to and from various AWS storage services.</p><p>Which of the following solutions would you recommend?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>AWS DataSync</strong> allows you to copy large datasets with millions of files, without having to build custom solutions with open source tools or license and manage expensive commercial network acceleration software. You can use DataSync to migrate active data to AWS, transfer data to the cloud for analysis and processing, archive data to free up on-premises storage capacity, or replicate data to AWS for business continuity.</p><p>AWS DataSync simplifies, automates, and accelerates copying large amounts of data to and from AWS storage services over the internet or AWS Direct Connect. DataSync can copy data between Network File System (NFS), Server Message Block (SMB) file servers, self-managed object storage, or AWS Snowcone, and Amazon Simple Storage Service (Amazon S3) buckets, Amazon EFS file systems, and Amazon FSx for Windows File Server file systems.</p><p><img src=\"https://media.tutorialsdojo.com/AWS-DataSync-How-it-Works-Diagram.d27ec6f14be8ca9c2d434f94dd4a98c3fdf5c88c.png\"></p><p>You deploy an AWS DataSync agent to your on-premises hypervisor or in Amazon EC2. To copy data to or from an on-premises file server, you download the agent virtual machine image from the AWS Console and deploy to your on-premises VMware ESXi, Linux Kernel-based Virtual Machine (KVM), or Microsoft Hyper-V hypervisor. To copy data to or from an in-cloud file server, you create an Amazon EC2 instance using a DataSync agent AMI. In both cases the agent must be deployed so that it can access your file server using the NFS, SMB protocol, or the Amazon S3 API. To set up transfers between your AWS Snowcone device and AWS storage, use the DataSync agent AMI that comes pre-installed on your device.</p><p>Since the scenario plans to use AWS Direct Connect for private connectivity between on-premises and AWS, you can use DataSync to automate and accelerate online data transfers to AWS storage services. The AWS DataSync agent will be deployed in your on-premises network to accelerate data transfer to AWS. To connect programmatically to an AWS service, you will need to use an AWS Direct Connect service endpoint.<br></p><p>Hence, the correct answer is: <strong>Use an AWS DataSync agent to rapidly move the data over a service endpoint</strong>.</p><p>The option that says: <strong>Use AWS DataSync agent to rapidly move the data over the Internet</strong> is incorrect because the organization will be using an AWS Direct Connect connection for private connectivity. This means that the connection will not pass through the public Internet.</p><p>The options that say: <strong>Use AWS Storage Gateway tape gateway to store data on virtual tape cartridges and asynchronously copy your backups to AWS </strong>and <strong>Use AWS Storage Gateway file gateway to store and retrieve files directly using the SMB file system protocol</strong> are both incorrect because, in the scenario, you need to accelerate the replication of data, and not establish a hybrid cloud storage architecture. AWS Storage Gateway only supports a few AWS storage services as a target based on the type of gateway that you launched. AWS DataSync is more suitable in automating and accelerating online data transfers to a variety of AWS storage services.<br></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/datasync/faqs/\">https://aws.amazon.com/datasync/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html\">https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html</a></p><p><a href=\"https://docs.aws.amazon.com/general/latest/gr/dc.html\">https://docs.aws.amazon.com/general/latest/gr/dc.html</a></p><p><br></p><p><strong>Check out this AWS DataSync Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-datasync/?src=udemy\">https://tutorialsdojo.com/aws-datasync/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an AWS DataSync agent to rapidly move the data over a service endpoint.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use an AWS Storage Gateway tape gateway to store data on virtual tape cartridges and asynchronously copy your backups to AWS.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use an AWS DataSync agent to rapidly move the data over the Internet.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use an AWS Storage Gateway file gateway to store and retrieve files directly using the SMB file system protocol.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 59,
    "question": "<p>A Solutions Architect is designing a setup for a database that will run on Amazon RDS for MySQL. He needs to ensure that the database can automatically failover to an RDS instance to continue operating in the event of failure. The architecture should also be as highly available as possible.</p><p>Which among the following actions should the Solutions Architect do?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>You can run an Amazon RDS DB instance in several AZs with Multi-AZ deployment. Amazon automatically provisions and maintains a secondary standby DB instance in a different AZ. Your primary DB instance is synchronously replicated across AZs to the secondary instance to provide data redundancy, failover support, eliminate I/O freezes, and minimize latency spikes during systems backup.</p><p><img src=\"https://media.tutorialsdojo.com/public/multi-az-deployments.bda9d7bf45a74103d0331a985baf2c5fb838a0fa.png\"></p><p>As described in the scenario, the architecture must meet two requirements:</p><ol><li><p>The database should automatically fail over to an RDS instance in case of failures.</p></li><li><p>The architecture should be as highly available as possible.</p></li></ol><p>Hence, the correct answer is: <strong>Create a standby replica in another availability zone by enabling Multi-AZ deployment </strong>because it meets both of the requirements.</p><p>The option that says: <strong>Create a read replica in the same region where the DB instance resides. In addition, create a read replica in a different region to survive a region's failure. In the event of an Availability Zone outage, promote any replica to become the primary instance </strong>is incorrect. Although this architecture provides higher availability since it can survive a region failure, it still does not meet the first requirement since the process is not automated. The architecture should also support automatic failover to an RDS instance in case of failures.</p><p>Both the following options are incorrect:</p><p><strong>- Create five read replicas across different availability zones. In the event of an Availability Zone outage, promote any replica to become the primary instance</strong></p><p><strong>- Create five cross-region read replicas in each region. In the event of an Availability Zone outage, promote any replica to become the primary instance</strong></p><p>Although it is possible to achieve high availability with these architectures by promoting a read replica into the primary instance in an event of failure, it does not support automatic failover to an RDS instance which is also a requirement in the problem.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a standby replica in another availability zone by enabling Multi-AZ deployment.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Create a read replica in the same region where the DB instance resides. In addition, create a read replica in a different region to survive a region’s failure. In the event of an Availability Zone outage, promote any replica to become the primary instance.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create five read replicas across different availability zones. In the event of an Availability Zone outage, promote any replica to become the primary instance.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create five cross-region read replicas in each region. In the event of an Availability Zone outage, promote any replica to become the primary instance.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 60,
    "question": "<p>An Auto Scaling group (ASG) of Amazon EC2 Linux instances has an Amazon FSx for OpenZFS file system with basic monitoring enabled in Amazon CloudWatch. The Solutions Architect noticed that the legacy web application hosted in the ASG takes a long time to load. After checking the instances, the Architect noticed that the ASG is not launching more instances as it should be, even though the servers already have high memory usage.</p><p>Which of the following options should the Architect implement to solve this issue?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon CloudWatch agent</strong> enables you to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The agent supports both Windows Server and Linux and allows you to select the metrics to be collected, including sub-resource metrics such as per-CPU core.</p><p><img src=\"https://media.tutorialsdojo.com/auto-scaling-custom-metric-cloudwatch-systems-manager-saa-c03.png\"></p><p>The premise of the scenario is that the EC2 servers have <strong>high memory</strong> usage, but since this specific metric is not tracked by the Auto Scaling group by default, the scaling out activity is not being triggered. Remember that by default, CloudWatch doesn't monitor memory usage but only the CPU utilization, Network utilization, Disk performance, and Disk Reads/Writes.</p><p>This is the reason why you have to install a CloudWatch agent in your EC2 instances to collect and monitor the custom metric (memory usage), which will be used by your Auto Scaling Group as a trigger for scaling activities.</p><p>The AWS Systems Manager Parameter Store is one of the capabilities of AWS Systems Manager. It provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter.</p><p>Hence, the correct answer is: <strong>Install the CloudWatch unified agent to the EC2 instances. Set up a custom parameter in AWS Systems Manager Parameter Store with the CloudWatch agent configuration to create an aggregated metric on memory usage percentage. Scale the Auto Scaling group based on the aggregated metric</strong></p><p>The option that says:<strong> Implement an AI solution that leverages Amazon Comprehend to track the near-real-time memory usage of each and every EC2 instance. Use Amazon SageMaker AI to automatically trigger the Auto Scaling event if there is high memory usage</strong> is incorrect because Amazon Comprehend cannot track near-real-time memory usage in Amazon EC2. This is just a natural-language processing (NLP) service that uses machine learning to uncover valuable insights and connections in text. Also, the use of an Amazon SageMaker AI in this scenario is not warranted since there is no machine learning requirement involved.</p><p>The option that says:<strong> Enable detailed monitoring on the EC2 instances of the Auto Scaling group. Use Auto Scaling with custom metrics to scale out the Auto Scaling group based on the aggregated memory usage of EC2 instances</strong> is incorrect because detailed monitoring in CloudWatch primarily enhances the granularity of standard metrics like CPU utilization. While custom metrics can be integrated with AWS Auto Scaling, setting this up requires more overhead compared to installing the CloudWatch unified agent, which directly handles memory metrics.</p><p>The option that says: <strong>Set up Amazon Rekognition to automatically identify and recognize the cause of the high memory usage. Use the AWS Well-Architected Tool to automatically trigger the scale-out event in the ASG based on the overall memory usage</strong> is incorrect because Amazon Rekognition is simply an image recognition service that detects objects, scenes, and faces; extracts text; recognizes celebrities; and identifies inappropriate content in images. It can't be used to track the high memory usage of your Amazon EC2 instances. The AWS Well-Architected Tool, on the other hand, is designed to help you review the state of your applications and workloads. It merely provides a central place for architectural best practices in AWS and nothing more.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/create-amazon-ec2-auto-scaling-policy-memory-utilization-metric-linux/\">https://aws.amazon.com/blogs/mt/create-amazon-ec2-auto-scaling-policy-memory-utilization-metric-linux/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p><p><br></p><p><strong>Check out these Amazon EC2 and CloudWatch Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement an AI solution that leverages Amazon Comprehend to track the near-real-time memory usage of each and every EC2 instance. Use Amazon SageMaker AI to automatically trigger the Auto Scaling event if there is high memory usage.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Install the CloudWatch unified agent to the EC2 instances. Set up a custom parameter in AWS Systems Manager Parameter Store with the CloudWatch agent configuration to create an aggregated metric on memory usage percentage. Scale the Auto Scaling group based on the aggregated metric.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Enable detailed monitoring on the EC2 instances of the Auto Scaling group. Use Auto Scaling with custom metrics to scale out the Auto Scaling group based on the aggregated memory usage of EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Set up Amazon Rekognition to automatically identify and recognize the cause of the high memory usage. Use the AWS Well-Architected Tool to automatically trigger the scale-out event in the ASG based on the overall memory usage.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 61,
    "question": "<p>A financial services organization is developing a cloud-native application on AWS to process and analyze customer transaction data. The application utilizes Amazon Aurora for the database, Amazon EFS for file storage, and Amazon EventBridge to trigger AWS Step Functions for workflow orchestration.</p><p>The organization has implemented AWS IAM Identity Center for user authentication. The data science, engineering, and compliance teams require secure access to Amazon Aurora and Amazon EFS while maintaining strict data privacy standards. The solution must adhere to the principle of least privilege and minimize administrative overhead.</p><p>Which approach best satisfies these requirements?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS IAM Identity Center</strong> centralizes access management for various AWS accounts and applications. It provides single sign-on (SSO) access, enabling users to manage all their assigned accounts from a single location. Users can synchronize with an existing identity provider or create new users and groups directly within the service.</p><p>IAM Identity Center utilizes permission sets—collections of IAM policies—to manage user access across AWS Organizations. It includes a user-friendly AWS Access Portal for easy access to applications and supports deployment for both organization and account instances. Designed for high availability across multiple availability zones, the service ensures secure access through AWS Identity and Access Management (IAM) roles and policies.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-iam-identity-center-12-13-24.png\"></p><p>Hence, the correct answer is: <strong>Enable the IAM Identity Center with an Identity Center directory and create permission sets for granular access for Amazon Aurora and Amazon EFS. Assign teams to groups linked to specific permission sets based on their roles. </strong>This approach provides centralized user management and granular access control and integrates well with the existing IAM Identity Center setup. This approach allows for easy assignment of permissions based on team roles and adheres to the principle of least privilege.</p><p>The option that says: <strong>Set up AWS Control Tower to manage multi-account access. Use Service Control Policies (SCPs) to restrict access to Aurora and EFS at the organizational level. Create IAM roles in each account with specific permissions </strong>is incorrect. AWS Control Tower is typically used for multi-account governance and high-level policy enforcement using Service Control Policies (SCPs). While SCPs can restrict access, they operate at the account or organizational level and cannot provide the fine-grained access required for individual resources like Amazon Aurora or Amazon EFS. This approach increases administrative complexity and does not minimize overhead.</p><p>The option that says: <strong>Use Amazon Cognito User Pools for authentication and use Cognito Identity Pools to provide temporary AWS credentials. Create fine-grained IAM roles for Aurora and EFS access in each team </strong>is incorrect. Amazon Cognito typically provides user authentication and access to AWS resources through temporary credentials. However, Cognito is not inherently integrated with the IAM Identity Center, which the organization is already using. This approach introduces additional complexity and does not align with the organization's current authentication strategy.</p><p>The option that says: <strong>Create separate AWS accounts for each team using AWS Organizations. Set up cross-account IAM roles with least privilege and assign specific Aurora and EFS permissions based on team roles </strong>is incorrect. Using separate AWS accounts and cross-account roles can enforce strict access boundaries, but it adds significant administrative overhead. Managing multiple accounts for each team is unnecessary for this scenario and does not align with the requirement to minimize complexity.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2023/05/amazon-sagemaker-data-wrangler-image-data-preparation/https://docs.aws.amazon.com/singlesignon/latest/userguide/get-set-up-for-idc.html\">https://aws.amazon.com/about-aws/whats-new/2023/05/amazon-sagemaker-data-wrangler-image-data-preparation/https://docs.aws.amazon.com/singlesignon/latest/userguide/get-set-up-for-idc.html</a></p><p><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html</a></p><p><br></p><p><strong>Check out this AWS Identity and Access Management Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up AWS Control Tower to manage multi-account access. Use Service Control Policies (SCPs) to restrict access to Aurora and EFS at the organizational level. Create IAM roles in each account with specific permissions.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable the IAM Identity Center with an Identity Center directory and create permission sets for granular access for Amazon Aurora and Amazon EFS. Assign teams to groups linked to specific permission sets based on their roles.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Cognito User Pools for authentication and use Cognito Identity Pools to provide temporary AWS credentials. Create fine-grained IAM roles for Aurora and EFS access in each team.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create separate AWS accounts for each team using AWS Organizations. Set up cross-account IAM roles with least privilege and assign specific Aurora and EFS permissions based on team roles.</p>",
        "correct": false
      }
    ],
    "corrects": [
      2
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 62,
    "question": "<p>A company plans to migrate a NoSQL database to an EC2 instance. The database is configured to replicate the data automatically to keep multiple copies of data for redundancy. The Solutions Architect needs to launch an instance that has a high IOPS and sequential read/write access.</p><p>Which of the following options fulfills the requirement if I/O throughput is the highest priority?</p>",
    "multiple": true,
    "domain": "Design High-Performing Architectures",
    "explanation": "<p><strong>Amazon EC2 </strong>provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload.</p><p><img src=\"https://media.tutorialsdojo.com/AWS-Instance-Types.png\"></p><p>A storage optimized instance is designed for workloads that require high, sequential read and write access to very large data sets on local storage. They are optimized to deliver tens of thousands of low-latency, random I/O operations per second (IOPS) to applications. Some instance types can drive more I/O throughput than what you can provision for a single EBS volume. You can join multiple volumes together in a RAID 0 configuration to use the available bandwidth for these instances.</p><p>Based on the given scenario, the NoSQL database will be migrated to an EC2 instance. The suitable instance type for NoSQL database is I3 and I3en instances. Also, the primary data storage for I3 and I3en instances is non-volatile memory express (NVMe) SSD instance store volumes. Since the data is replicated automatically, there will be no problem using an instance store volume.</p><p>Hence, the correct answer is:<strong> Use Storage optimized instances with instance store volume.</strong></p><p>The option that says: <strong>Use Compute optimized instances with instance store volume </strong>is incorrect because this type of instance is ideal for compute-bound applications that benefit from high-performance processors. It is not suitable for a NoSQL database.</p><p>The option that says: <strong>Use General purpose instances with EBS volume</strong> is incorrect because this instance only provides a balance of computing, memory, and networking resources. Take note that the requirement in the scenario is high sequential read and write access. Therefore, you must use a storage optimized instance.</p><p>The option that says:<strong> Use Memory optimized instances with EBS volume</strong> is incorrect. Although this type of instance is suitable for a NoSQL database, it is not designed for workloads that require high, sequential read and write access to very large data sets on local storage.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage-optimized-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage-optimized-instances.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Storage optimized instances with instance store volume.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use General purpose instances with EBS volume.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Compute optimized instance with instance store volume.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Memory optimized instances with EBS volume.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 63,
    "question": "<p>A company has stored 200 TB of backup files in Amazon S3. The files are in a vendor-proprietary format. The Solutions Architect needs to use the vendor's proprietary file conversion software to retrieve the files from their Amazon S3 bucket, transform the files to an industry-standard format, and re-upload the files back to Amazon S3. The solution must minimize the data transfer costs.</p><p>Which of the following options can satisfy the given requirement?</p>",
    "multiple": true,
    "domain": "Design Cost-Optimized Architectures",
    "explanation": "<p><strong>Amazon S3</strong> is object storage built to store and retrieve any amount of data from anywhere on the Internet. It’s a simple storage service that offers industry-leading durability, availability, performance, security, and virtually unlimited scalability at very low costs. Amazon S3 is also designed to be highly flexible. Store any type and amount of data that you want; read the same piece of data a million times or only for emergency disaster recovery; build a simple FTP application or a sophisticated web application.</p><p><img src=\"https://media.tutorialsdojo.com/AWS-EC2-S3.png\"></p><p>You pay for all bandwidth into and out of Amazon S3, except for the following:</p><p>- Data transferred in from the Internet.</p><p>- Data transferred out to an Amazon EC2 instance, when the instance is in the same AWS Region as the S3 bucket (including to a different account in the same AWS region).</p><p>- Data transferred out to Amazon CloudFront.</p><p>To minimize the data transfer charges, you need to deploy the EC2 instance in the same Region as Amazon S3. Take note that there is no data transfer cost between S3 and EC2 in the same AWS Region. Install the conversion software on the instance to perform data transformation and re-upload the data to Amazon S3.</p><p>Hence, the correct answer is: <strong>Deploy the EC2 instance in the same Region as Amazon S3. Install the file conversion software on the instance. Perform data transformation and re-upload it to Amazon S3.</strong></p><p>The option that says: <strong>Install the file conversion software in Amazon S3. Use S3 Batch Operations to perform data transformation</strong> is incorrect because it is not possible to install the software in Amazon S3. The S3 Batch Operations just runs multiple S3 operations in a single request. It can’t be integrated with your conversion software.</p><p>The option that says: <strong>Export the data using AWS Snowball Edge device. Install the file conversion software on the device. Transform the data and re-upload it to Amazon S3 </strong>is incorrect. Although this is possible, it is not mentioned in the scenario that the company has an on-premises data center. Thus, there's no need for Snowball.</p><p>The option that says: <strong>Deploy the EC2 instance in a different Region. Install the file conversion software on the instance. Perform data transformation and re-upload it to Amazon S3</strong> is incorrect because this approach wouldn't minimize the data transfer costs. You should deploy the instance in the same Region as Amazon S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/pricing/\">https://aws.amazon.com/s3/pricing/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonS3.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonS3.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Install the file conversion software in Amazon S3. Use S3 Batch Operations to perform data transformation.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Export the data using AWS Snowball Edge device. Install the file conversion software on the device. Transform the data and re-upload it to Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy the EC2 instance in a different Region. Install the conversion software on the instance. Perform data transformation and re-upload it to Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy the EC2 instance in the same Region as Amazon S3. Install the file conversion software on the instance. Perform data transformation and re-upload it to Amazon S3.</p>",
        "correct": true
      }
    ],
    "corrects": [
      4
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 64,
    "question": "<p>Every week, an e-commerce company announces a sales promotion, causing its application hosted on an Auto Scaling group to experience intermittent downtime. Because of long initialization times, the application only becomes operational minutes before a new EC2 instance turns into <code>RUNNING</code> state. A solutions architect must devise a solution that launches capacity in advance based on a forecasted load in order to scale faster.</p><p>Which solution meets the requirements with the least amount of effort?</p>",
    "multiple": true,
    "domain": "Design Resilient Architectures",
    "explanation": "<p>Predictive scaling uses machine learning to predict capacity requirements based on historical data from CloudWatch. The machine learning algorithm consumes the available historical data and calculates capacity that best fits the historical load pattern, and then continuously learns based on new data to make future forecasts more accurate. <br><img src=\"https://media.tutorialsdojo.com/aws-asg-predictive-scaling-policy.JPG\"></p><p>In general, if you have regular patterns of traffic increases and applications that take a long time to initialize, you should consider using predictive scaling. Predictive scaling can help you scale faster by launching capacity in advance of forecasted load, compared to using only dynamic scaling, which is reactive in nature. Predictive scaling can also potentially save you money on your EC2 bill by helping you avoid the need to overprovision capacity. You also don't have to spend time reviewing your application's load patterns and trying to schedule the right amount of capacity using scheduled scaling.</p><p>Hence, the correct answer is: <strong>Configure the Auto Scaling group to use predictive scaling.</strong></p><p>The option that says: <strong>Use Amazon SageMaker Clarify to analyze and predict the workload pattern of the application. Create a scheduled scaling policy based on the prediction results</strong> is incorrect because SageMaker Clarify is a tool designed for detecting bias and providing explainability for machine learning models. Using SageMaker Clarify would only involve unnecessary complexity and does not directly solve the problem of predicting traffic for Auto Scaling.</p><p>The option that says: <strong>Create a dynamic scaling policy based on the historical average CPU load of the application</strong>is incorrect. This solution alone is primarily useful if you want to adjust capacity when certain CPU thresholds are met. It won't be able to react to forecasted load patterns.</p><p>The option that says: <strong>Create a Scheduled Amazon EventBridge (Amazon CloudWatch Events) Rule that runs a scaling job on a Lambda function every midnight </strong>is incorrect. This solution takes more effort to implement and is subjected to rescheduling since the schedule for a sales promotion can change.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/plans/userguide/how-it-works.html\">https://docs.aws.amazon.com/autoscaling/plans/userguide/how-it-works.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-predictive-scaling-for-ec2-powered-by-machine-learning/\">https://aws.amazon.com/blogs/aws/new-predictive-scaling-for-ec2-powered-by-machine-learning/</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Group Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure the Auto Scaling group to use predictive scaling.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon SageMaker Clarify to analyze and predict the workload pattern of the application. Create a scheduled scaling policy based on the prediction results.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a dynamic scaling policy based on the historical average CPU load of the application.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a Scheduled Amazon EventBridge (Amazon CloudWatch Events) Rule that runs a scaling job on a Lambda function every midnight.</p>",
        "correct": false
      }
    ],
    "corrects": [
      1
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 65,
    "question": "<p>A startup is planning to set up and govern a secure, compliant, multi-account AWS environment in preparation for its upcoming projects. The IT Manager requires the solution to have a dashboard for continuous detection of policy non-conformance and non-compliant resources across the enterprise, as well as to comply with the AWS multi-account strategy best practices.</p><p>Which of the following offers the easiest way to fulfill this task?</p>",
    "multiple": true,
    "domain": "Design Secure Architectures",
    "explanation": "<p><strong>AWS Control Tower</strong> offers a straightforward way to set up and govern an AWS multi-account environment, following prescriptive best practices. AWS Control Tower orchestrates the capabilities of several other AWS services, including AWS Organizations, AWS Service Catalog, and AWS Single Sign-On, to build a landing zone in less than an hour. It offers a dashboard to see provisioned accounts across your enterprise, guardrails enabled for policy enforcement, guardrails enabled for continuous detection of policy non-conformance, and non-compliant resources organized by accounts and OUs.</p><p><img src=\"https://media.tutorialsdojo.com/aws-control-tower-landing-zone.png\"></p><p>A guardrail is a high-level rule that provides ongoing governance for your overall AWS environment. It's expressed in plain language. Through guardrails, AWS Control Tower implements <em>preventive</em> or <em>detective</em> controls that help you govern your resources and monitor compliance across groups of AWS accounts.</p><p>A guardrail applies to an entire organizational unit (OU), and every AWS account within the OU is affected by the guardrail. Therefore, when users perform work in any AWS account in your landing zone, they're always subject to the guardrails that are governing their account's OU.</p><p><img src=\"https://media.tutorialsdojo.com/aws-control-tower-guard-rails.png\"></p><p>Hence, the correct answer in this scenario is: <strong>Use AWS Control Tower to launch a landing zone to automatically provision and configure new accounts through an Account Factory. Utilize the AWS Control Tower dashboard to monitor provisioned accounts across your enterprise. Set up preventive and detective guardrails for policy enforcement.</strong></p><p><strong>Use AWS Organizations to build a landing zone to automatically provision new AWS accounts. Utilize the AWS Personal Health Dashboard to see provisioned accounts across your enterprise. Enable preventive and detective guardrails enabled for policy enforcement </strong>is incorrect. The AWS Organizations service neither has the capability to build a landing zone nor a built-in dashboard for continuous detection of policy non-conformance and non-compliant resources across the enterprise. Moreover, the AWS Personal Health Dashboard simply provides alerts and remediation guidance when AWS is experiencing events that may impact your resources. This type of dashboard is not meant for monitoring the newly provisioned AWS accounts. The most suitable solution here is to use AWS Control Tower and its various features.</p><p>The option that says:<strong> Launch new AWS member accounts using the AWS CloudFormation StackSets. Use AWS Config to continuously track the configuration changes and set rules to monitor non-compliant resources. Set up a Multi-Account Multi-Region Data Aggregator to monitor compliance data for rules and accounts in an aggregated view </strong>is incorrect. Although the solution might work to monitor non-compliant resources, this is not the easiest way to fulfill the multi-account requirement in the scenario. It still takes a lot of time to configure CloudFormation StackSets templates and set up AWS Config for all your AWS member accounts.</p><p>The option that says: <strong>Use AWS Service Catalog to launch new AWS member accounts. Configure AWS Service Catalog Launch Constraints to continuously track configuration changes and monitor non-compliant resources. Set up a Multi-Account Multi-Region Data Aggregator to monitor compliance data for rules and accounts in an aggregated view </strong>is incorrect. AWS Service Catalog is not used to detect non-compliant resources and is only used to manage catalogs of IT services from virtual machine images, servers, software, databases, and other resources. This service is primarily used centrally to curate and share commonly deployed templates with their teams to achieve consistent governance and meet compliance requirements.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/controltower/latest/userguide/what-is-control-tower.html\">https://docs.aws.amazon.com/controltower/latest/userguide/what-is-control-tower.html</a></p><p><a href=\"https://docs.aws.amazon.com/controltower/latest/userguide/how-control-tower-works.html\">https://docs.aws.amazon.com/controltower/latest/userguide/how-control-tower-works.html</a></p><p><a href=\"https://docs.aws.amazon.com/controltower/latest/userguide/aws-multi-account-landing-zone.html#multi-account-guidance\">https://docs.aws.amazon.com/controltower/latest/userguide/aws-multi-account-landing-zone.html#multi-account-guidance</a></p><p><br></p><p><strong>Check out this AWS Control Tower Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-control-tower/?src=udemy\">https://tutorialsdojo.com/aws-control-tower/</a></p>",
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Organizations to build a landing zone to automatically provision new AWS accounts. Utilize the AWS Personal Health Dashboard to see provisioned accounts across your enterprise. Enable preventive and detective guardrails enabled for policy enforcement.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Launch new AWS member accounts using the AWS CloudFormation StackSets. Use AWS Config to continuously track the configuration changes and set rules to monitor non-compliant resources. Set up a Multi-Account Multi-Region Data Aggregator to monitor compliance data for rules and accounts in an aggregated view</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Control Tower to launch a landing zone to automatically provision and configure new accounts through an Account Factory. Utilize the AWS Control Tower dashboard to monitor provisioned accounts across your enterprise. Set up preventive and detective guardrails for policy enforcement.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Service Catalog to launch new AWS member accounts. Configure AWS Service Catalog Launch Constraints to continuously track configuration changes and monitor non-compliant resources. Set up a Multi-Account Multi-Region Data Aggregator to monitor compliance data for rules and accounts in an aggregated view</p>",
        "correct": false
      }
    ],
    "corrects": [
      3
    ],
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  }
]