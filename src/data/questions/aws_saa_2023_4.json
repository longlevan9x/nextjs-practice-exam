[
    {
      "id": 1,
      "question": "A business recently moved its data processing platform to AWS. The platform ingests data into a RabbitMQ queue hosted on an Amazon EC2 instance. A processing application running on another Amazon EC2 instance takes this data, processes it, and writes the results to a PostgreSQL database, also running on an EC2 instance. The business requires this setup to be highly resilient with minimal operational effort.\n\nWhich architectural design would provide the HIGHEST level of availability?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon MQ with active/standby brokers set up across two AZs. Add an additional EC2 instance running the processing application in a different AZ. Use Amazon RDS for PostgreSQL with Multi-AZ enabled.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use Amazon MQ with active/standby brokers set up across two AZs. Use an Auto Scaling group for the EC2 instances running the processing application across two AZs. Use Amazon RDS for PostgreSQL with Multi-AZ enabled.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Use Amazon MQ with active/standby brokers set up across two AZs. Add an additional EC2 instance running the processing application in a different AZ. Replicate the PostgreSQL database in a different AZ.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Add a secondary RabbitMQ server in a different Availability Zone (AZ). Add an additional EC2 instance running the processing application in a different AZ. Replicate the PostgreSQL database in a different AZ.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nUse Amazon MQ with active/standby brokers set up across two AZs. Use an Auto Scaling group for the EC2 instances running the processing application across two AZs. Use Amazon RDS for PostgreSQL with Multi-AZ enabled.\n\nThis solutions design maximizes availability by using Amazon MQ with active/standby brokers across multiple AZs, ensuring queue availability. Using an Auto Scaling group for the processing application instances across two AZs allows the system to dynamically adjust capacity to maintain performance. Furthermore, using Amazon RDS for PostgreSQL with Multi-AZ enabled ensures high availability and automatic failover support for DB instances.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAdd a secondary RabbitMQ server in a different Availability Zone (AZ). Add an additional EC2 instance running the processing application in a different AZ. Replicate the PostgreSQL database in a different AZ.\n\nThis option doesn't take full advantage of the managed services AWS offers, which leads to higher operational complexity and potential downtime during failovers.\n\n\n\n\nUse Amazon MQ with active/standby brokers set up across two AZs. Add an additional EC2 instance running the processing application in a different AZ. Replicate the PostgreSQL database in a different AZ.\n\nWhile this option starts to use Amazon MQ, it still requires manual database replication, adding unnecessary operational complexity.\n\n\n\n\nUse Amazon MQ with active/standby brokers set up across two AZs. Add an additional EC2 instance running the processing application in a different AZ. Use Amazon RDS for PostgreSQL with Multi-AZ enabled.\n\nThis option provide little improvement by using Amazon RDS with Multi-AZ enabled, but it doesn't ensure the automatic scaling of the processing application based on load.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazon-mq/latest/developer-guide/active-standby-broker-deployment.html\n\nhttps://aws.amazon.com/rds/features/multi-az/\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use Amazon MQ with active/standby brokers set up across two AZs. Use an Auto Scaling group for the EC2 instances running the processing application across two AZs. Use Amazon RDS for PostgreSQL with Multi-AZ enabled.",
          "explanation": "This solutions design maximizes availability by using Amazon MQ with active/standby brokers across multiple AZs, ensuring queue availability. Using an Auto Scaling group for the processing application instances across two AZs allows the system to dynamically adjust capacity to maintain performance. Furthermore, using Amazon RDS for PostgreSQL with Multi-AZ enabled ensures high availability and automatic failover support for DB instances."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Add a secondary RabbitMQ server in a different Availability Zone (AZ). Add an additional EC2 instance running the processing application in a different AZ. Replicate the PostgreSQL database in a different AZ.",
          "explanation": "This option doesn't take full advantage of the managed services AWS offers, which leads to higher operational complexity and potential downtime during failovers."
        },
        {
          "answer": "Use Amazon MQ with active/standby brokers set up across two AZs. Add an additional EC2 instance running the processing application in a different AZ. Replicate the PostgreSQL database in a different AZ.",
          "explanation": "While this option starts to use Amazon MQ, it still requires manual database replication, adding unnecessary operational complexity."
        },
        {
          "answer": "Use Amazon MQ with active/standby brokers set up across two AZs. Add an additional EC2 instance running the processing application in a different AZ. Use Amazon RDS for PostgreSQL with Multi-AZ enabled.",
          "explanation": "This option provide little improvement by using Amazon RDS with Multi-AZ enabled, but it doesn't ensure the automatic scaling of the processing application based on load."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/active-standby-broker-deployment.html",
        "https://aws.amazon.com/rds/features/multi-az/",
        "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html"
      ]
    },
    {
      "id": 2,
      "question": "A media company is designing a video streaming service which uses an Amazon S3 bucket for storage of video files. The company aims to stream all videos via an Amazon CloudFront distribution and intends to prevent the files from being directly accessed through the S3 URL.\n\nWhat should a solutions architect implement to achieve these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Formulate an S3 bucket policy that sets the CloudFront distribution ID as the Principal and assigns the S3 bucket as the Amazon Resource Name (ARN).",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Construct individual policies for every S3 bucket to grant read-only permissions to CloudFront.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Create an origin access identity (OAI), assign this OAI to the CloudFront distribution, and adjust the S3 bucket permissions so that only the OAI has read access.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Generate an IAM user, provide read access to the objects in the S3 bucket to this user, and link this user to CloudFront.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nCreate an origin access identity (OAI), assign this OAI to the CloudFront distribution, and adjust the S3 bucket permissions so that only the OAI has read access.\n\nWhen using Amazon CloudFront with an Amazon S3 bucket, it is recommended to restrict direct access to the S3 bucket and instead route all requests through CloudFront for better control and security. To achieve this, an origin access identity (OAI) can be created, which is a special CloudFront user. By assigning this OAI to the CloudFront distribution and configuring the S3 bucket permissions to grant read access only to the OAI, direct access to the S3 bucket is prevented.\n\nCloudFront acts as a proxy for the S3 bucket, serving the video files on behalf of the bucket. This ensures that all requests for the video files are routed through CloudFront, allowing for additional caching, security, and customization options.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConstruct individual policies for every S3 bucket to grant read-only permissions to CloudFront.\n\nThis option does not prevent direct access to S3 URLs.\n\n\n\n\nGenerate an IAM user, provide read access to the objects in the S3 bucket to this user, and link this user to CloudFront.\n\nIAM users are not directly associated with CloudFront distributions. CloudFront distributions use Origin Access Identities (OAIs).\n\n\n\n\nFormulate an S3 bucket policy that sets the CloudFront distribution ID as the Principal and assigns the S3 bucket as the Amazon Resource Name (ARN).\n\nCloudFront distribution IDs cannot be used as principals in S3 bucket policies.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html",
      "correctAnswerExplanations": [
        {
          "answer": "Create an origin access identity (OAI), assign this OAI to the CloudFront distribution, and adjust the S3 bucket permissions so that only the OAI has read access.",
          "explanation": "When using Amazon CloudFront with an Amazon S3 bucket, it is recommended to restrict direct access to the S3 bucket and instead route all requests through CloudFront for better control and security. To achieve this, an origin access identity (OAI) can be created, which is a special CloudFront user. By assigning this OAI to the CloudFront distribution and configuring the S3 bucket permissions to grant read access only to the OAI, direct access to the S3 bucket is prevented."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Construct individual policies for every S3 bucket to grant read-only permissions to CloudFront.",
          "explanation": "This option does not prevent direct access to S3 URLs."
        },
        {
          "answer": "Generate an IAM user, provide read access to the objects in the S3 bucket to this user, and link this user to CloudFront.",
          "explanation": "IAM users are not directly associated with CloudFront distributions. CloudFront distributions use Origin Access Identities (OAIs)."
        },
        {
          "answer": "Formulate an S3 bucket policy that sets the CloudFront distribution ID as the Principal and assigns the S3 bucket as the Amazon Resource Name (ARN).",
          "explanation": "CloudFront distribution IDs cannot be used as principals in S3 bucket policies."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html"
      ]
    },
    {
      "id": 3,
      "question": "A tech startup has recently launched a new web application that is expected to have unpredictable user growth. The application is currently hosted on a small number of on-demand Amazon EC2 instances. The company has a limited budget and wants to optimize costs without compromising the application's performance and availability.\n\nWhich of the following strategies would be the MOST cost-effective solution for this scenario?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Auto Scaling with a mix of on-demand, spot, and reserved EC2 instances.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Switch all on-demand EC2 instances to reserved instances.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Increase the number of on-demand EC2 instances to handle potential growth.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Move the entire application to AWS Lambda.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nUse Auto Scaling with a mix of on-demand, spot, and reserved EC2 instances.\n\nThis option ensures cost optimization by taking advantage of the pricing benefits of different types of EC2 instances. Reserved instances can be used for baseline traffic, on-demand instances can handle traffic spikes, and spot instances, which offer significant discounts, can be used when availability is not a concern. Furthermore, Auto Scaling ensures that the application can handle user growth by automatically adjusting the number of EC2 instances.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSwitch all on-demand EC2 instances to reserved instances.\n\nReserved instances are cheaper than on-demand instances, but they lack flexibility. If the application's demand decreases, the company will still have to pay for the reserved capacity, leading to unnecessary costs.\n\n\n\n\nMove the entire application to AWS Lambda.\n\nWhile AWS Lambda can be cost-effective, it is not always the best choice for all types of workloads. It is more suitable for event-driven workloads, and there might be limitations and additional costs if the application requires long-running tasks or uses a lot of compute resources.\n\n\n\n\nIncrease the number of on-demand EC2 instances to handle potential growth.\n\nWhile this ensures availability, it is not a cost-effective solution. It could lead to unnecessary costs if the demand is lower than expected.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/ec2/pricing/\n\nhttps://aws.amazon.com/autoscaling/\n\nhttps://aws.amazon.com/lambda/",
      "correctAnswerExplanations": [
        {
          "answer": "Use Auto Scaling with a mix of on-demand, spot, and reserved EC2 instances.",
          "explanation": "This option ensures cost optimization by taking advantage of the pricing benefits of different types of EC2 instances. Reserved instances can be used for baseline traffic, on-demand instances can handle traffic spikes, and spot instances, which offer significant discounts, can be used when availability is not a concern. Furthermore, Auto Scaling ensures that the application can handle user growth by automatically adjusting the number of EC2 instances."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Switch all on-demand EC2 instances to reserved instances.",
          "explanation": "Reserved instances are cheaper than on-demand instances, but they lack flexibility. If the application's demand decreases, the company will still have to pay for the reserved capacity, leading to unnecessary costs."
        },
        {
          "answer": "Move the entire application to AWS Lambda.",
          "explanation": "While AWS Lambda can be cost-effective, it is not always the best choice for all types of workloads. It is more suitable for event-driven workloads, and there might be limitations and additional costs if the application requires long-running tasks or uses a lot of compute resources."
        },
        {
          "answer": "Increase the number of on-demand EC2 instances to handle potential growth.",
          "explanation": "While this ensures availability, it is not a cost-effective solution. It could lead to unnecessary costs if the demand is lower than expected."
        }
      ],
      "references": [
        "https://aws.amazon.com/ec2/pricing/",
        "https://aws.amazon.com/autoscaling/",
        "https://aws.amazon.com/lambda/"
      ]
    },
    {
      "id": 4,
      "question": "A university plans to use Amazon DynamoDB to store student information. The university is focused on cost optimization. The database will not be accessed during weekends, but on weekdays, the read and write traffic can be unpredictable and sudden traffic spikes are expected.\n\nWhat should a solutions architect recommend?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Create a DynamoDB table in provisioned capacity mode and configure it as a global table.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Establish a DynamoDB table with provisioned capacity and enable auto scaling.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Configure a DynamoDB table with on-demand capacity mode.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Set up a DynamoDB table with a global secondary index.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nConfigure a DynamoDB table with on-demand capacity mode.\n\nOn-demand capacity mode automatically adjusts to the current workload, allowing the table to handle sudden traffic spikes without any manual intervention. This mode is particularly suitable for cases where read and write traffic is unpredictable or has significant fluctuations. On-demand capacity mode also helps with cost optimization, as you only pay for the read and write capacity that you actually consume.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up a DynamoDB table with a global secondary index.\n\nA global secondary index is useful for optimizing query performance with different partition keys and sort keys, but it does not address the concern of unpredictable traffic and cost optimization.\n\n\n\n\nEstablish a DynamoDB table with provisioned capacity and enable auto scaling.\n\nAlthough auto scaling can adjust the capacity based on the workload, it may not respond as quickly as on-demand capacity mode to sudden traffic spikes, which are expected in this scenario.\n\n\n\n\nCreate a DynamoDB table in provisioned capacity mode and configure it as a global table.\n\nA global table is designed for multi-region replication and doesn't address the concerns of unpredictable traffic and cost optimization.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/dynamodb/pricing/on-demand/\n\nhttps://aws.amazon.com/dynamodb/global-tables/",
      "correctAnswerExplanations": [
        {
          "answer": "Configure a DynamoDB table with on-demand capacity mode.",
          "explanation": "On-demand capacity mode automatically adjusts to the current workload, allowing the table to handle sudden traffic spikes without any manual intervention. This mode is particularly suitable for cases where read and write traffic is unpredictable or has significant fluctuations. On-demand capacity mode also helps with cost optimization, as you only pay for the read and write capacity that you actually consume."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Set up a DynamoDB table with a global secondary index.",
          "explanation": "A global secondary index is useful for optimizing query performance with different partition keys and sort keys, but it does not address the concern of unpredictable traffic and cost optimization."
        },
        {
          "answer": "Establish a DynamoDB table with provisioned capacity and enable auto scaling.",
          "explanation": "Although auto scaling can adjust the capacity based on the workload, it may not respond as quickly as on-demand capacity mode to sudden traffic spikes, which are expected in this scenario."
        },
        {
          "answer": "Create a DynamoDB table in provisioned capacity mode and configure it as a global table.",
          "explanation": "A global table is designed for multi-region replication and doesn't address the concerns of unpredictable traffic and cost optimization."
        }
      ],
      "references": [
        "https://aws.amazon.com/dynamodb/pricing/on-demand/",
        "https://aws.amazon.com/dynamodb/global-tables/"
      ]
    },
    {
      "id": 5,
      "question": "A multinational corporation generates 15 TB of telemetry data daily from various devices deployed across multiple facilities. The data, comprised of JSON files, is stored on a storage area network (SAN) within an on-premises data center at each facility. The corporation aims to transmit this data securely to Amazon S3, where it can be utilized by several other systems for crucial near-real-time analytics. Data security is a priority, given the sensitive nature of the data.\n\nWhich solution ensures the MOST reliable data transfer?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "AWS Database Migration Service (AWS DMS) over AWS Direct Connect",
          "correct": false
        },
        {
          "id": 2,
          "answer": "AWS Database Migration Service (AWS DMS) over public internet",
          "correct": false
        },
        {
          "id": 3,
          "answer": "AWS DataSync over public internet",
          "correct": false
        },
        {
          "id": 4,
          "answer": "AWS DataSync over AWS Direct Connect",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nAWS DataSync over AWS Direct Connect\n\nUsing AWS DataSync over AWS Direct Connect is the most reliable data transfer solution for the given scenario. AWS DataSync simplifies and accelerates the process of transferring data between on-premises storage systems and Amazon S3. By leveraging AWS Direct Connect, a dedicated network connection is established between the on-premises data center and AWS, bypassing the public internet. This not only improves the transfer speed but also enhances data security by reducing exposure to potential threats. Furthermore, AWS DataSync automatically handles data validation and integrity checks, ensuring the transmitted data is accurate and consistent. Overall, this solution is efficient, secure, and reliable for transferring large volumes of sensitive data.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAWS DataSync over public internet\n\nWhile AWS DataSync can facilitate data transfer between on-premises storage and Amazon S3, using a public internet connection compromises data security. As the data is sensitive, transferring it over the public internet increases the risk of unauthorized access or interception.\n\n\n\n\nAWS Database Migration Service (AWS DMS) over public internet\n\nAWS Database Migration Service is designed for migrating databases rather than transferring JSON files or telemetry data. Additionally, using a public internet connection for data transfer poses security risks for sensitive data.\n\n\n\n\nAWS Database Migration Service (AWS DMS) over AWS Direct Connect\n\nAlthough AWS Direct Connect provides a secure and dedicated connection, AWS Database Migration Service is not the appropriate service for transferring JSON files or telemetry data, as it is primarily intended for database migration.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/datasync/\n\nhttps://aws.amazon.com/directconnect/",
      "correctAnswerExplanations": [
        {
          "answer": "AWS DataSync over AWS Direct Connect",
          "explanation": "Using AWS DataSync over AWS Direct Connect is the most reliable data transfer solution for the given scenario. AWS DataSync simplifies and accelerates the process of transferring data between on-premises storage systems and Amazon S3. By leveraging AWS Direct Connect, a dedicated network connection is established between the on-premises data center and AWS, bypassing the public internet. This not only improves the transfer speed but also enhances data security by reducing exposure to potential threats. Furthermore, AWS DataSync automatically handles data validation and integrity checks, ensuring the transmitted data is accurate and consistent. Overall, this solution is efficient, secure, and reliable for transferring large volumes of sensitive data."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "AWS DataSync over public internet",
          "explanation": "While AWS DataSync can facilitate data transfer between on-premises storage and Amazon S3, using a public internet connection compromises data security. As the data is sensitive, transferring it over the public internet increases the risk of unauthorized access or interception."
        },
        {
          "answer": "AWS Database Migration Service (AWS DMS) over public internet",
          "explanation": "AWS Database Migration Service is designed for migrating databases rather than transferring JSON files or telemetry data. Additionally, using a public internet connection for data transfer poses security risks for sensitive data."
        },
        {
          "answer": "AWS Database Migration Service (AWS DMS) over AWS Direct Connect",
          "explanation": "Although AWS Direct Connect provides a secure and dedicated connection, AWS Database Migration Service is not the appropriate service for transferring JSON files or telemetry data, as it is primarily intended for database migration."
        }
      ],
      "references": [
        "https://aws.amazon.com/datasync/",
        "https://aws.amazon.com/directconnect/"
      ]
    },
    {
      "id": 6,
      "question": "An e-commerce company is developing an application where customers submit product reviews. After a review is submitted, it needs a one-time processing to extract key insights and save them in JSON format for later analysis. Each review should be processed as soon as possible after submission. The volume of reviews will fluctuate, with some days seeing high volumes and other days having little to no activity.\n\nWhich solution meets these requirements with the LEAST operational overhead?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis Data Streams when a new review is submitted. Use an AWS Lambda function to consume the event from the stream and process the data. Store the resulting JSON data in Amazon RDS.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Configure Amazon DynamoDB to send event notifications to an Amazon Simple Queue Service (Amazon SQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting JSON data in Amazon S3.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Configure Amazon DynamoDB to trigger an AWS Lambda function for each new review. Run processing scripts to extract insights. Store the resulting JSON data in Amazon RDS.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Configure Amazon DynamoDB to send event notifications to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the resulting JSON data in Amazon S3.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nConfigure Amazon DynamoDB to send event notifications to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the resulting JSON data in Amazon S3.\n\nAmazon SQS provides a reliable and scalable messaging service for decoupling the ingestion of reviews from processing. By configuring DynamoDB to send event notifications to an SQS queue, an AWS Lambda function can be triggered to process the message and extract key insights. The use of Lambda function for processing is efficient and cost-effective, as it is only triggered when a message is added to the queue. The resulting JSON data can be stored in Amazon S3, which is a highly durable and scalable object storage service with low cost, suitable for storing and analyzing large amounts of unstructured data.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure Amazon DynamoDB to trigger an AWS Lambda function for each new review. Run processing scripts to extract insights. Store the resulting JSON data in Amazon RDS.\n\nAWS Lambda directly triggered by DynamoDB may not scale well for high volumes of reviews.\n\n\n\n\nConfigure Amazon DynamoDB to send event notifications to an Amazon Simple Queue Service (Amazon SQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting JSON data in Amazon S3.\n\nManaging EC2 instances for this task adds unnecessary operational overhead.\n\n\n\n\nConfigure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis Data Streams when a new review is submitted. Use an AWS Lambda function to consume the event from the stream and process the data. Store the resulting JSON data in Amazon RDS.\n\nIt introduces unnecessary complexity and operational overhead by including Kinesis Data Streams and Amazon RDS.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
      "correctAnswerExplanations": [
        {
          "answer": "Configure Amazon DynamoDB to send event notifications to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the resulting JSON data in Amazon S3.",
          "explanation": "Amazon SQS provides a reliable and scalable messaging service for decoupling the ingestion of reviews from processing. By configuring DynamoDB to send event notifications to an SQS queue, an AWS Lambda function can be triggered to process the message and extract key insights. The use of Lambda function for processing is efficient and cost-effective, as it is only triggered when a message is added to the queue. The resulting JSON data can be stored in Amazon S3, which is a highly durable and scalable object storage service with low cost, suitable for storing and analyzing large amounts of unstructured data."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Configure Amazon DynamoDB to trigger an AWS Lambda function for each new review. Run processing scripts to extract insights. Store the resulting JSON data in Amazon RDS.",
          "explanation": "AWS Lambda directly triggered by DynamoDB may not scale well for high volumes of reviews."
        },
        {
          "answer": "Configure Amazon DynamoDB to send event notifications to an Amazon Simple Queue Service (Amazon SQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting JSON data in Amazon S3.",
          "explanation": "Managing EC2 instances for this task adds unnecessary operational overhead."
        },
        {
          "answer": "Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis Data Streams when a new review is submitted. Use an AWS Lambda function to consume the event from the stream and process the data. Store the resulting JSON data in Amazon RDS.",
          "explanation": "It introduces unnecessary complexity and operational overhead by including Kinesis Data Streams and Amazon RDS."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"
      ]
    },
    {
      "id": 7,
      "question": "A ride-hailing company is creating a multi-tier architecture to track the real-time location of its vehicles during busy hours. The company plans to incorporate this data into its current analytics platform. A solutions architect must identify the most suitable multi-tier solution to support this architecture. The location data must be retrievable via the REST API.\n\nWhat would be the most appropriate approach for storing and retrieving location data?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon API Gateway with Amazon EC2.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use Amazon RDS with Amazon S3.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use Amazon API Gateway with AWS Lambda.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Use Amazon QuickSight with Amazon RDS.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nUse Amazon API Gateway with AWS Lambda\n\nAmazon API Gateway can be used to create, deploy, and manage a RESTful API that can capture the location data. AWS Lambda can then process this data and store it appropriately. This serverless approach provides scalability and flexibility, enabling the system to handle high amounts of location data during peak hours efficiently.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon RDS with Amazon S3\n\nIt does not provide a way to create a RESTful API for capturing location data.\n\n\n\n\nUse Amazon API Gateway with Amazon EC2\n\nThis option could work, but it would require more operational overhead to manage the EC2 instances compared to the serverless nature of AWS Lambda.\n\n\n\n\nUse Amazon QuickSight with Amazon RDS\n\nWhile Amazon QuickSight is an excellent tool for data visualization and Amazon RDS can store the location data, this option does not provide a way to create a RESTful API to capture the data.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/api-gateway/\n\nhttps://aws.amazon.com/lambda/",
      "correctAnswerExplanations": [],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use Amazon API Gateway with AWS Lambda",
          "explanation": "Amazon API Gateway can be used to create, deploy, and manage a RESTful API that can capture the location data. AWS Lambda can then process this data and store it appropriately. This serverless approach provides scalability and flexibility, enabling the system to handle high amounts of location data during peak hours efficiently."
        },
        {
          "answer": "Use Amazon RDS with Amazon S3",
          "explanation": "It does not provide a way to create a RESTful API for capturing location data."
        },
        {
          "answer": "Use Amazon API Gateway with Amazon EC2",
          "explanation": "This option could work, but it would require more operational overhead to manage the EC2 instances compared to the serverless nature of AWS Lambda."
        },
        {
          "answer": "Use Amazon QuickSight with Amazon RDS",
          "explanation": "While Amazon QuickSight is an excellent tool for data visualization and Amazon RDS can store the location data, this option does not provide a way to create a RESTful API to capture the data."
        }
      ],
      "references": [
        "https://aws.amazon.com/api-gateway/",
        "https://aws.amazon.com/lambda/"
      ]
    },
    {
      "id": 8,
      "question": "A company is using multiple Amazon EC2 instances to process data and store results in a shared Amazon RDS PostgreSQL Multi-AZ DB instance. The company requires a secure method for the EC2 instances to access the database and a way to rotate user credentials regularly.\n\nWhich solution meets these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Store the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) on the EC2 instance file system. The EC2 instances should be able to decrypt the files and access the database.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Store the database user credentials in AWS Systems Manager Parameter Store. Grant the necessary IAM permissions to allow the EC2 instances to access Parameter Store.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Store the database user credentials in AWS Secrets Manager. Grant the necessary IAM permissions to allow the EC2 instances to access AWS Secrets Manager.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Store the database user credentials in a secure Amazon S3 bucket. Grant the necessary IAM permissions to allow the EC2 instances to retrieve credentials and access the database.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nStore the database user credentials in AWS Secrets Manager. Grant the necessary IAM permissions to allow the EC2 instances to access AWS Secrets Manager.\n\nAWS Secrets Manager offers a secure and scalable method to store and manage secrets such as passwords, database credentials, and API keys. AWS Secrets Manager provides strong encryption for secrets using AWS KMS, integrates with Amazon RDS, and supports automatic rotation of secrets to improve security. AWS Secrets Manager offers more advanced features such as automatic rotation and is better suited for database credentials.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStore the database user credentials in AWS Systems Manager Parameter Store. Grant the necessary IAM permissions to allow the EC2 instances to access Parameter Store.\n\nAlthough Parameter Store can securely store parameters, it does not provide built-in rotation capabilities like AWS Secrets Manager.\n\n\n\n\nStore the database user credentials in a secure Amazon S3 bucket. Grant the necessary IAM permissions to allow the EC2 instances to retrieve credentials and access the database.\n\nStoring credentials in an Amazon S3 bucket does not provide built-in rotation capabilities and is not as secure as using AWS Secrets Manager.\n\n\n\n\nStore the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) on the EC2 instance file system. The EC2 instances should be able to decrypt the files and access the database.\n\nStoring credentials on the EC2 instance file system is not as secure as using AWS Secrets Manager, and it does not provide built-in rotation capabilities.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/secrets-manager/\n\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html",
      "correctAnswerExplanations": [
        {
          "answer": "Store the database user credentials in AWS Secrets Manager. Grant the necessary IAM permissions to allow the EC2 instances to access AWS Secrets Manager.",
          "explanation": "AWS Secrets Manager offers a secure and scalable method to store and manage secrets such as passwords, database credentials, and API keys. AWS Secrets Manager provides strong encryption for secrets using AWS KMS, integrates with Amazon RDS, and supports automatic rotation of secrets to improve security. AWS Secrets Manager offers more advanced features such as automatic rotation and is better suited for database credentials."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Store the database user credentials in AWS Systems Manager Parameter Store. Grant the necessary IAM permissions to allow the EC2 instances to access Parameter Store.",
          "explanation": "Although Parameter Store can securely store parameters, it does not provide built-in rotation capabilities like AWS Secrets Manager."
        },
        {
          "answer": "Store the database user credentials in a secure Amazon S3 bucket. Grant the necessary IAM permissions to allow the EC2 instances to retrieve credentials and access the database.",
          "explanation": "Storing credentials in an Amazon S3 bucket does not provide built-in rotation capabilities and is not as secure as using AWS Secrets Manager."
        },
        {
          "answer": "Store the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) on the EC2 instance file system. The EC2 instances should be able to decrypt the files and access the database.",
          "explanation": "Storing credentials on the EC2 instance file system is not as secure as using AWS Secrets Manager, and it does not provide built-in rotation capabilities."
        }
      ],
      "references": [
        "https://aws.amazon.com/secrets-manager/",
        "https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html"
      ]
    },
    {
      "id": 9,
      "question": "A data analytics company runs an AWS Glue extract, transform, and load (ETL) operation daily at a specific time. The ETL job processes CSV data that resides in an Amazon S3 bucket, where new data is added every day. The company's solutions architect discovers that the ETL job is reprocessing all data during each operation, not just the new data.\n\nWhat should the solutions architect do to avoid reprocessing the old data?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Modify the ETL job to set the MaximumCapacity field to 1.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Modify the ETL job to enable job bookmarks.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Modify the ETL job to erase data post-processing.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use a FindMatches machine learning (ML) transform.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nModify the ETL job to enable job bookmarks.\n\nAWS Glue job bookmarks help the ETL job remember the state of the data processed in the previous run of an ETL job. When enabled, the job processes only new or changed data since the last ETL job run. This feature is highly useful in scenarios like the one described, where new data is added daily and there's a need to avoid reprocessing old data.\n\n\n\n\n\n\n\nIncorrect Options:\n\nModify the ETL job to erase data post-processing.\n\nDeleting data after it has been processed isn't a good practice, as it would make the data unavailable for any future requirement or analysis.\n\n\n\n\nModify the ETL job to set the MaximumCapacity field to 1.\n\nSetting the MaximumCapacity field to 1 limits the computational resources available for the job, which can significantly slow down the ETL operation. It doesn't prevent reprocessing of old data.\n\n\n\n\nUse a FindMatches machine learning (ML) transform.\n\nFindMatches ML transform is used for deduplication and finding matching records in your dataset, not for avoiding reprocessing of old data.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html",
      "correctAnswerExplanations": [
        {
          "answer": "Modify the ETL job to enable job bookmarks.",
          "explanation": "AWS Glue job bookmarks help the ETL job remember the state of the data processed in the previous run of an ETL job. When enabled, the job processes only new or changed data since the last ETL job run. This feature is highly useful in scenarios like the one described, where new data is added daily and there's a need to avoid reprocessing old data."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Modify the ETL job to erase data post-processing.",
          "explanation": "Deleting data after it has been processed isn't a good practice, as it would make the data unavailable for any future requirement or analysis."
        },
        {
          "answer": "Modify the ETL job to set the MaximumCapacity field to 1.",
          "explanation": "Setting the MaximumCapacity field to 1 limits the computational resources available for the job, which can significantly slow down the ETL operation. It doesn't prevent reprocessing of old data."
        },
        {
          "answer": "Use a FindMatches machine learning (ML) transform.",
          "explanation": "FindMatches ML transform is used for deduplication and finding matching records in your dataset, not for avoiding reprocessing of old data."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html"
      ]
    },
    {
      "id": 10,
      "question": "A company is running a content management system that allows users to submit articles through a web portal. According to new legal regulations, once articles are submitted and stored, they cannot be modified or removed.\n\nWhat should a solutions architect implement to meet this requirement?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Save the submitted articles on an Amazon Elastic Block Store (Amazon EBS) volume. Access the data by attaching the volume in read-only mode.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Save the submitted articles in an Amazon S3 bucket with S3 Versioning enabled. Use an IAM policy to restrict all access to read-only.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Save the submitted articles in an Amazon S3 bucket. Configure an S3 Lifecycle policy to move the articles to Amazon Glacier periodically.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Save the submitted articles in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nSave the submitted articles in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled.\n\nS3 Versioning provides the ability to store multiple versions of objects, and S3 Object Lock prevents any future version of the object from being deleted or overwritten for a specified retention period. With both of these features enabled, any article submitted through the web portal will be stored in the S3 bucket, and the object versioning will track any changes made to the object over time.\n\nS3 Object Lock adds an additional layer of protection by enforcing a legal hold on objects, preventing any object versions from being deleted or overwritten for the duration of the lock. This ensures that once an article is submitted and stored in the S3 bucket, it cannot be altered or deleted until the retention period has expired, meeting the legal requirement.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSave the submitted articles in an Amazon S3 bucket. Configure an S3 Lifecycle policy to move the articles to Amazon Glacier periodically.\n\nMoving objects to Amazon Glacier does not prevent them from being modified or deleted.\n\n\n\n\nSave the submitted articles in an Amazon S3 bucket with S3 Versioning enabled. Use an IAM policy to restrict all access to read-only.\n\nIAM policies can be changed, allowing modification or deletion of the objects.\n\n\n\n\nSave the submitted articles on an Amazon Elastic Block Store (Amazon EBS) volume. Access the data by attaching the volume in read-only mode.\n\nAmazon EBS does not provide the same level of protection as S3 Object Lock to prevent modification or deletion of objects.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html",
      "correctAnswerExplanations": [
        {
          "answer": "Save the submitted articles in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled.",
          "explanation": "S3 Versioning provides the ability to store multiple versions of objects, and S3 Object Lock prevents any future version of the object from being deleted or overwritten for a specified retention period. With both of these features enabled, any article submitted through the web portal will be stored in the S3 bucket, and the object versioning will track any changes made to the object over time."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Save the submitted articles in an Amazon S3 bucket. Configure an S3 Lifecycle policy to move the articles to Amazon Glacier periodically.",
          "explanation": "Moving objects to Amazon Glacier does not prevent them from being modified or deleted."
        },
        {
          "answer": "Save the submitted articles in an Amazon S3 bucket with S3 Versioning enabled. Use an IAM policy to restrict all access to read-only.",
          "explanation": "IAM policies can be changed, allowing modification or deletion of the objects."
        },
        {
          "answer": "Save the submitted articles on an Amazon Elastic Block Store (Amazon EBS) volume. Access the data by attaching the volume in read-only mode.",
          "explanation": "Amazon EBS does not provide the same level of protection as S3 Object Lock to prevent modification or deletion of objects."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html",
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html"
      ]
    },
    {
      "id": 11,
      "question": "A retail company collects transaction data and stores it in Amazon DynamoDB. The company's new policy requires all transaction data to be stored and analyzed in real-time using Amazon OpenSearch Service (formerly known as Amazon Elasticsearch Service).\n\nWhich solution will meet this requirement with the LEAST operational overhead?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Setup an Amazon Kinesis Data Firehose delivery stream with DynamoDB as the source and Amazon OpenSearch Service as the destination.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Configure an Amazon DynamoDB stream to stream the transaction data to Amazon OpenSearch Service.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Develop an AWS Lambda function triggered by DynamoDB streams to write the transaction data to Amazon OpenSearch Service.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Install and configure Amazon Kinesis Agent on each application server to deliver the transaction data to Amazon Kinesis Data Streams and configure Kinesis Data Streams to deliver the data to Amazon OpenSearch Service.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nConfigure an Amazon DynamoDB stream to stream the transaction data to Amazon OpenSearch Service.\n\nBy configuring an Amazon DynamoDB stream to stream the transaction data to Amazon OpenSearch Service (formerly known as Amazon Elasticsearch Service), the data can be stored and analyzed in real-time without the need for additional components or services.\n\nDynamoDB streams provide an ordered sequence of item-level modifications in a DynamoDB table. By enabling the stream on the DynamoDB table and configuring it to stream to Amazon OpenSearch Service, the transaction data can be automatically and efficiently indexed and analyzed in real-time within the OpenSearch cluster. This allows for seamless integration between DynamoDB and OpenSearch, ensuring that the transaction data is readily available for analysis without additional manual processing.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDevelop an AWS Lambda function triggered by DynamoDB streams to write the transaction data to Amazon OpenSearch Service.\n\nThis option would require developing, deploying, and maintaining a custom Lambda function, which would increase operational overhead.\n\n\n\n\nSetup an Amazon Kinesis Data Firehose delivery stream with DynamoDB as the source and Amazon OpenSearch Service as the destination.\n\nAmazon Kinesis Data Firehose doesn't support DynamoDB as a direct source. Therefore, this solution is not feasible.\n\n\n\n\nInstall and configure Amazon Kinesis Agent on each application server to deliver the transaction data to Amazon Kinesis Data Streams and configure Kinesis Data Streams to deliver the data to Amazon OpenSearch Service.\n\nThis option adds additional complexity and operational overhead as it involves managing Kinesis Agents and Kinesis Data Streams.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\n\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html",
      "correctAnswerExplanations": [
        {
          "answer": "Configure an Amazon DynamoDB stream to stream the transaction data to Amazon OpenSearch Service.",
          "explanation": "By configuring an Amazon DynamoDB stream to stream the transaction data to Amazon OpenSearch Service (formerly known as Amazon Elasticsearch Service), the data can be stored and analyzed in real-time without the need for additional components or services."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Develop an AWS Lambda function triggered by DynamoDB streams to write the transaction data to Amazon OpenSearch Service.",
          "explanation": "This option would require developing, deploying, and maintaining a custom Lambda function, which would increase operational overhead."
        },
        {
          "answer": "Setup an Amazon Kinesis Data Firehose delivery stream with DynamoDB as the source and Amazon OpenSearch Service as the destination.",
          "explanation": "Amazon Kinesis Data Firehose doesn't support DynamoDB as a direct source. Therefore, this solution is not feasible."
        },
        {
          "answer": "Install and configure Amazon Kinesis Agent on each application server to deliver the transaction data to Amazon Kinesis Data Streams and configure Kinesis Data Streams to deliver the data to Amazon OpenSearch Service.",
          "explanation": "This option adds additional complexity and operational overhead as it involves managing Kinesis Agents and Kinesis Data Streams."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html",
        "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html"
      ]
    },
    {
      "id": 12,
      "question": "An organization is planning to move its on-premises MySQL database to Amazon RDS MySQL. The on-premises database needs to be fully functional and reachable during the migration. Moreover, the RDS database has to stay synchronized with the on-premises database.\n\nWhat combination of steps must a solutions architect execute to meet these requirements? (Select TWO.)",
      "corrects": [
        2,
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to supervise the database synchronization.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an ongoing replication task.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Create an AWS Database Migration Service (AWS DMS) replication instance.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Use the AWS Schema Conversion Tool (AWS SCT) to alter the database schema.",
          "correct": false
        },
        {
          "id": 5,
          "answer": "Generate a backup of the on-premises database.",
          "correct": false
        }
      ],
      "multiple": true,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nCreate an ongoing replication task.\n\nConfigure an AWS DMS ongoing replication task to continuously replicate the data from the on-premises MySQL database to the Amazon RDS MySQL database. This ensures that any changes made to the on-premises database are replicated to the RDS database in near real-time, keeping them fully synchronized.\n\n\n\n\nCreate an AWS Database Migration Service (AWS DMS) replication instance.\n\nAWS DMS provides a managed service for database migration and replication. By setting up an AWS DMS replication instance, you can replicate data from the on-premises MySQL database to the Amazon RDS MySQL database. This ensures that the RDS database stays synchronized with the on-premises database.\n\n\n\n\n\n\n\nIncorrect Options:\n\nGenerate a backup of the on-premises database.\n\nWhile creating a backup is a good practice, it does not address the requirement of keeping the on-premises database operational and synchronized with the RDS instance during the migration.\n\n\n\n\nUse the AWS Schema Conversion Tool (AWS SCT) to alter the database schema.\n\nThis tool is usually used when migrating between different database engines, not for the same database engines like MySQL to MySQL.\n\n\n\n\nCreate an Amazon EventBridge (Amazon CloudWatch Events) rule to supervise the database synchronization.\n\nWhile EventBridge could be used for monitoring, it doesn't directly help in database synchronization.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/dms/\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.html\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.html",
      "correctAnswerExplanations": [
        {
          "answer": "Create an ongoing replication task.",
          "explanation": "Configure an AWS DMS ongoing replication task to continuously replicate the data from the on-premises MySQL database to the Amazon RDS MySQL database. This ensures that any changes made to the on-premises database are replicated to the RDS database in near real-time, keeping them fully synchronized."
        },
        {
          "answer": "Create an AWS Database Migration Service (AWS DMS) replication instance.",
          "explanation": "AWS DMS provides a managed service for database migration and replication. By setting up an AWS DMS replication instance, you can replicate data from the on-premises MySQL database to the Amazon RDS MySQL database. This ensures that the RDS database stays synchronized with the on-premises database."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Generate a backup of the on-premises database.",
          "explanation": "While creating a backup is a good practice, it does not address the requirement of keeping the on-premises database operational and synchronized with the RDS instance during the migration."
        },
        {
          "answer": "Use the AWS Schema Conversion Tool (AWS SCT) to alter the database schema.",
          "explanation": "This tool is usually used when migrating between different database engines, not for the same database engines like MySQL to MySQL."
        },
        {
          "answer": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to supervise the database synchronization.",
          "explanation": "While EventBridge could be used for monitoring, it doesn't directly help in database synchronization."
        }
      ],
      "references": [
        "https://aws.amazon.com/dms/",
        "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.html",
        "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.html"
      ]
    },
    {
      "id": 13,
      "question": "A company is running its analytics application on Amazon EC2 instances in a VPC. The application needs to interact with AWS DynamoDB to read and write data. The company's security policy mandates that no application traffic is allowed to travel across the internet.\n\nWhich solution will meet these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Configure a NAT gateway in the same subnet as the EC2 instances.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create a DynamoDB table in the same AWS Region as the EC2 instances.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Configure a DynamoDB VPC endpoint.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Create a DynamoDB table in a private subnet.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nConfigure a DynamoDB VPC endpoint.\n\nA VPC endpoint is a service that enables you to privately connect your VPC to supported AWS services without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Using a VPC endpoint for DynamoDB, traffic between your VPC and DynamoDB is routed through an internal network connection rather than over the internet. This meets the company's security policy of not allowing any application traffic to travel across the internet.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate a DynamoDB table in a private subnet.\n\nDynamoDB is a managed service and does not reside within your VPC or its subnets. You cannot place a DynamoDB table in a private subnet.\n\n\n\n\nCreate a DynamoDB table in the same AWS Region as the EC2 instances.\n\nWhile it's a good practice to have your DynamoDB tables in the same region as your EC2 instances for latency purposes, it doesn't address the security policy that no traffic should travel across the internet.\n\n\n\n\nConfigure a NAT gateway in the same subnet as the EC2 instances.\n\nA NAT gateway is used to enable instances in a private subnet to connect to the internet or other AWS services, but it doesn't prevent traffic from traveling across the internet.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html",
      "correctAnswerExplanations": [
        {
          "answer": "Configure a DynamoDB VPC endpoint.",
          "explanation": "A VPC endpoint is a service that enables you to privately connect your VPC to supported AWS services without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Using a VPC endpoint for DynamoDB, traffic between your VPC and DynamoDB is routed through an internal network connection rather than over the internet. This meets the company's security policy of not allowing any application traffic to travel across the internet."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Create a DynamoDB table in a private subnet.",
          "explanation": "DynamoDB is a managed service and does not reside within your VPC or its subnets. You cannot place a DynamoDB table in a private subnet."
        },
        {
          "answer": "Create a DynamoDB table in the same AWS Region as the EC2 instances.",
          "explanation": "While it's a good practice to have your DynamoDB tables in the same region as your EC2 instances for latency purposes, it doesn't address the security policy that no traffic should travel across the internet."
        },
        {
          "answer": "Configure a NAT gateway in the same subnet as the EC2 instances.",
          "explanation": "A NAT gateway is used to enable instances in a private subnet to connect to the internet or other AWS services, but it doesn't prevent traffic from traveling across the internet."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html",
        "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html"
      ]
    },
    {
      "id": 14,
      "question": "A business is required to maintain data in Amazon S3 such that it cannot be altered. The data, once uploaded to Amazon S3, should remain in its original state for an undefined duration until the business decides to update it. Only designated users within the company's AWS account should be allowed to remove the data.\n\nWhat should a solutions architect recommend to fulfill these needs?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Establish an S3 Glacier vault. Implement a write-once, read-many (WORM) vault lock policy for the data.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Establish an S3 bucket with S3 Object Lock activated. Enable versioning. Apply a legal hold to the data. Include the s3:PutObjectLegalHold permission in the IAM policies of users who need to remove the data.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Establish an S3 bucket with S3 Object Lock activated. Enable versioning. Specify a retention period of 100 years. Choose governance mode as the default retention mode for new data in the S3 bucket.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Establish an S3 bucket. Employ AWS CloudTrail to monitor any S3 API events that modify the data. Once alerted, retrieve the modified data from any saved versions that the business possesses.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nEstablish an S3 bucket with S3 Object Lock activated. Enable versioning. Apply a legal hold to the data. Include the s3:PutObjectLegalHold permission in the IAM policies of users who need to remove the data.\n\nIt recommends establishing an S3 bucket with S3 Object Lock activated, enabling versioning, and applying a legal hold to the data. The legal hold ensures that the data is not modified or deleted by any user, except the designated users who have been granted the s3:PutObjectLegalHold permission in their IAM policies. Enabling versioning helps in keeping track of any changes made to the data, allowing the business to retrieve previous versions of the data in case of any modification.\n\n\n\n\n\n\n\nIncorrect Options:\n\nEstablish an S3 Glacier vault. Implement a write-once, read-many (WORM) vault lock policy for the data.\n\nWhile Glacier vaults support WORM policies, they are not designed for immediate access or updates to the data, which the question requires.\n\n\n\n\nEstablish an S3 bucket with S3 Object Lock activated. Enable versioning. Specify a retention period of 100 years. Choose governance mode as the default retention mode for new data in the S3 bucket.\n\nThe governance mode of S3 Object Lock only supports retention periods of up to 10 years, which also does not fulfill the business's requirement of an undefined duration.\n\n\n\n\nEstablish an S3 bucket. Employ AWS CloudTrail to monitor any S3 API events that modify the data. Once alerted, retrieve the modified data from any saved versions that the business possesses.\n\nMonitoring S3 API events with AWS CloudTrail does not prevent users from modifying or deleting the data. It only helps in detecting any changes made to the data.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-legal-hold.html",
      "correctAnswerExplanations": [
        {
          "answer": "Establish an S3 bucket with S3 Object Lock activated. Enable versioning. Apply a legal hold to the data. Include the s3:PutObjectLegalHold permission in the IAM policies of users who need to remove the data.",
          "explanation": "It recommends establishing an S3 bucket with S3 Object Lock activated, enabling versioning, and applying a legal hold to the data. The legal hold ensures that the data is not modified or deleted by any user, except the designated users who have been granted the s3:PutObjectLegalHold permission in their IAM policies. Enabling versioning helps in keeping track of any changes made to the data, allowing the business to retrieve previous versions of the data in case of any modification."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Establish an S3 Glacier vault. Implement a write-once, read-many (WORM) vault lock policy for the data.",
          "explanation": "While Glacier vaults support WORM policies, they are not designed for immediate access or updates to the data, which the question requires."
        },
        {
          "answer": "Establish an S3 bucket with S3 Object Lock activated. Enable versioning. Specify a retention period of 100 years. Choose governance mode as the default retention mode for new data in the S3 bucket.",
          "explanation": "The governance mode of S3 Object Lock only supports retention periods of up to 10 years, which also does not fulfill the business's requirement of an undefined duration."
        },
        {
          "answer": "Establish an S3 bucket. Employ AWS CloudTrail to monitor any S3 API events that modify the data. Once alerted, retrieve the modified data from any saved versions that the business possesses.",
          "explanation": "Monitoring S3 API events with AWS CloudTrail does not prevent users from modifying or deleting the data. It only helps in detecting any changes made to the data."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html",
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html",
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-legal-hold.html"
      ]
    },
    {
      "id": 15,
      "question": "A multinational corporation is building a complex data analytics platform on AWS. They are expecting variable workloads with potentially high spikes in data processing requirements. The company wants to achieve a balance between cost optimization and performance. The platform involves data storage, data processing, and data visualization.\n\nWhich solutions recommend to meet these requirements?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon S3 for data storage, AWS Lambda for data processing, and Amazon QuickSight for data visualization.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Use Amazon DynamoDB for data storage, AWS Fargate for data processing, and Amazon Athena for data visualization.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use Amazon EFS for data storage, Amazon EC2 On-Demand instances for data processing, and AWS Glue for data visualization.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use Amazon Glacier for data storage, Amazon EC2 Spot instances for data processing, and Amazon Kinesis for data visualization.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nUse Amazon S3 for data storage, AWS Lambda for data processing, and Amazon QuickSight for data visualization.\n\nAmazon S3 provides cost-effective and scalable storage. AWS Lambda is a serverless computing service that runs your code in response to events and automatically manages the underlying compute resources for you, allowing you to pay only for the compute time consumed. Amazon QuickSight is a fast, cloud-powered business intelligence service that makes it easy to visualize insights and integrates seamlessly with your data, thus optimizing costs.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon EFS for data storage, Amazon EC2 On-Demand instances for data processing, and AWS Glue for data visualization.\n\nWhile this option might work, it's less cost-effective. Amazon EFS and EC2 On-Demand instances can be more expensive than S3 and Lambda for storage and compute, respectively. AWS Glue is primarily a fully managed extract, transform, and load (ETL) service, not a data visualization tool.\n\n\n\n\nUse Amazon Glacier for data storage, Amazon EC2 Spot instances for data processing, and Amazon Kinesis for data visualization.\n\nAmazon Glacier is used for long-term archival storage and not suitable for analytics platform where data is frequently accessed. Amazon Kinesis is primarily used for streaming data, not for visualization.\n\n\n\n\nUse Amazon DynamoDB for data storage, AWS Fargate for data processing, and Amazon Athena for data visualization.\n\nAmazon DynamoDB might be more expensive for large-scale data storage compared to S3. AWS Fargate, while a good option for containerized applications, might not be the most cost-effective choice for variable data processing workloads. Amazon Athena is a query service and not a visualization tool.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/\n\nhttps://aws.amazon.com/lambda/\n\nhttps://aws.amazon.com/quicksight/",
      "correctAnswerExplanations": [
        {
          "answer": "Use Amazon S3 for data storage, AWS Lambda for data processing, and Amazon QuickSight for data visualization.",
          "explanation": "Amazon S3 provides cost-effective and scalable storage. AWS Lambda is a serverless computing service that runs your code in response to events and automatically manages the underlying compute resources for you, allowing you to pay only for the compute time consumed. Amazon QuickSight is a fast, cloud-powered business intelligence service that makes it easy to visualize insights and integrates seamlessly with your data, thus optimizing costs."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use Amazon EFS for data storage, Amazon EC2 On-Demand instances for data processing, and AWS Glue for data visualization.",
          "explanation": "While this option might work, it's less cost-effective. Amazon EFS and EC2 On-Demand instances can be more expensive than S3 and Lambda for storage and compute, respectively. AWS Glue is primarily a fully managed extract, transform, and load (ETL) service, not a data visualization tool."
        },
        {
          "answer": "Use Amazon Glacier for data storage, Amazon EC2 Spot instances for data processing, and Amazon Kinesis for data visualization.",
          "explanation": "Amazon Glacier is used for long-term archival storage and not suitable for analytics platform where data is frequently accessed. Amazon Kinesis is primarily used for streaming data, not for visualization."
        },
        {
          "answer": "Use Amazon DynamoDB for data storage, AWS Fargate for data processing, and Amazon Athena for data visualization.",
          "explanation": "Amazon DynamoDB might be more expensive for large-scale data storage compared to S3. AWS Fargate, while a good option for containerized applications, might not be the most cost-effective choice for variable data processing workloads. Amazon Athena is a query service and not a visualization tool."
        }
      ],
      "references": [
        "https://aws.amazon.com/s3/",
        "https://aws.amazon.com/lambda/",
        "https://aws.amazon.com/quicksight/"
      ]
    },
    {
      "id": 16,
      "question": "A media streaming company's online platform is running on multiple Amazon EC2 instances within an Auto Scaling group, which is served through a Network Load Balancer (NLB). The company has been experiencing issues with the platform returning HTTP errors, and manual intervention is required to restart the affected EC2 instances. The company wants to enhance the platform's availability without using custom scripts or code.\n\nWhat should a solutions architect recommend to achieve this goal?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Configure a cron job on the EC2 instances to examine the platform's logs every minute and restart the platform if HTTP errors are detected.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Enable HTTP health checks on the NLB by providing the URL of the media streaming platform.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Set up an Application Load Balancer instead of the NLB, enabling HTTP health checks with the platform's URL, and configure an Auto Scaling action to replace unhealthy instances.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Implement an Amazon CloudWatch alarm to monitor the platform's HTTP errors and configure an Auto Scaling action to replace unhealthy instances.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nSet up an Application Load Balancer instead of the NLB, enabling HTTP health checks with the platform's URL, and configure an Auto Scaling action to replace unhealthy instances.\n\nApplication Load Balancers (ALBs) are specifically designed to handle application-level traffic and provide advanced routing and health check features, such as HTTP health checks. By replacing the NLB with an ALB, the company can enable HTTP health checks to monitor the instances' health based on application-level responses. This ensures that unhealthy instances are identified and replaced in a timely manner, improving the platform's availability. The Auto Scaling group can be configured to replace instances that fail the health checks, further enhancing the system's resilience.\n\n\n\n\n\n\n\nIncorrect Options:\n\nImplement an Amazon CloudWatch alarm to monitor the platform's HTTP errors and configure an Auto Scaling action to replace unhealthy instances.\n\nIt does not address the root cause of the problem, which is the inability of the NLB to detect HTTP errors.\n\n\n\n\nConfigure a cron job on the EC2 instances to examine the platform's logs every minute and restart the platform if HTTP errors are detected.\n\nIt relies on custom scripting and does not provide a fully automated, scalable solution for improving the platform's availability.\n\n\n\n\nEnable HTTP health checks on the NLB by providing the URL of the media streaming platform.\n\nNLBs do not support HTTP health checks, as they operate at the TCP/UDP level, not the application level.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/elasticloadbalancing/features/#compare",
      "correctAnswerExplanations": [
        {
          "answer": "Set up an Application Load Balancer instead of the NLB, enabling HTTP health checks with the platform's URL, and configure an Auto Scaling action to replace unhealthy instances.",
          "explanation": "Application Load Balancers (ALBs) are specifically designed to handle application-level traffic and provide advanced routing and health check features, such as HTTP health checks. By replacing the NLB with an ALB, the company can enable HTTP health checks to monitor the instances' health based on application-level responses. This ensures that unhealthy instances are identified and replaced in a timely manner, improving the platform's availability. The Auto Scaling group can be configured to replace instances that fail the health checks, further enhancing the system's resilience."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Implement an Amazon CloudWatch alarm to monitor the platform's HTTP errors and configure an Auto Scaling action to replace unhealthy instances.",
          "explanation": "It does not address the root cause of the problem, which is the inability of the NLB to detect HTTP errors."
        },
        {
          "answer": "Configure a cron job on the EC2 instances to examine the platform's logs every minute and restart the platform if HTTP errors are detected.",
          "explanation": "It relies on custom scripting and does not provide a fully automated, scalable solution for improving the platform's availability."
        },
        {
          "answer": "Enable HTTP health checks on the NLB by providing the URL of the media streaming platform.",
          "explanation": "NLBs do not support HTTP health checks, as they operate at the TCP/UDP level, not the application level."
        }
      ],
      "references": [
        "https://aws.amazon.com/elasticloadbalancing/features/#compare"
      ]
    },
    {
      "id": 17,
      "question": "A data processing application operates on Amazon EC2 instances in multiple Availability Zones. The instances are managed by an Amazon EC2 Auto Scaling group behind a Network Load Balancer. The application's optimal performance is achieved when the memory utilization of the EC2 instances is approximately 60%.\n\nWhat should a solutions architect do to ensure the consistent performance across all instances in the group?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use scheduled scaling actions to increase and decrease the Auto Scaling group.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use a simple scaling policy to dynamically adjust the Auto Scaling group.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use an AWS Lambda function to modify the desired Auto Scaling group capacity.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use a target tracking policy to dynamically adjust the Auto Scaling group.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nUse a target tracking policy to dynamically adjust the Auto Scaling group.\n\nTarget tracking scaling policies for Amazon EC2 Auto Scaling let you define a target value for a specific CloudWatch metric. The Auto Scaling group then automatically adjusts the number of instances in the group to maintain that target value for the specified metric, in this case, memory utilization. This ensures the application performs optimally by maintaining the desired memory utilization across all instances.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse a simple scaling policy to dynamically adjust the Auto Scaling group.\n\nSimple scaling policies are not as effective for maintaining a specific metric value, like memory utilization, because they react to alarm thresholds rather than tracking a target metric value.\n\n\n\n\nUse an AWS Lambda function to modify the desired Auto Scaling group capacity.\n\nWhile AWS Lambda could technically be used to adjust Auto Scaling, it would require additional development and maintenance effort and would not be as efficient or reliable as using built-in Auto Scaling policies.\n\n\n\n\nUse scheduled scaling actions to increase and decrease the Auto Scaling group.\n\nScheduled scaling is useful for predictable load changes, but it doesn't adjust based on actual metrics like memory utilization, hence not an optimal solution in this context.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use a target tracking policy to dynamically adjust the Auto Scaling group.",
          "explanation": "Target tracking scaling policies for Amazon EC2 Auto Scaling let you define a target value for a specific CloudWatch metric. The Auto Scaling group then automatically adjusts the number of instances in the group to maintain that target value for the specified metric, in this case, memory utilization. This ensures the application performs optimally by maintaining the desired memory utilization across all instances."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use a simple scaling policy to dynamically adjust the Auto Scaling group.",
          "explanation": "Simple scaling policies are not as effective for maintaining a specific metric value, like memory utilization, because they react to alarm thresholds rather than tracking a target metric value."
        },
        {
          "answer": "Use an AWS Lambda function to modify the desired Auto Scaling group capacity.",
          "explanation": "While AWS Lambda could technically be used to adjust Auto Scaling, it would require additional development and maintenance effort and would not be as efficient or reliable as using built-in Auto Scaling policies."
        },
        {
          "answer": "Use scheduled scaling actions to increase and decrease the Auto Scaling group.",
          "explanation": "Scheduled scaling is useful for predictable load changes, but it doesn't adjust based on actual metrics like memory utilization, hence not an optimal solution in this context."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html"
      ]
    },
    {
      "id": 18,
      "question": "A research organization is setting up a shared storage solution for a high-performance computing (HPC) application running in their own data center. The application requires Lustre clients to access data and the solution must be fully managed.\n\nWhich solution will satisfy these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Create an AWS Storage Gateway file gateway, create a file share using the necessary client protocol, and connect the application server to the file share.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an Amazon FSx for Lustre file system, attach the file system to the origin server, and connect the application server to the file system.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Create an Amazon EC2 Windows instance, install and configure a Windows file share role on the instance, and connect the application server to the file share.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Create an Amazon Elastic File System (Amazon EFS) file system, configure it to support Lustre, attach the file system to the origin server, and connect the application server to the file system.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nCreate an Amazon FSx for Lustre file system, attach the file system to the origin server, and connect the application server to the file system.\n\nAmazon FSx for Lustre is a fully managed file system that is optimized for compute-intensive workloads, such as high-performance computing, machine learning, and media data processing workflows. It provides sub-millisecond latencies, up to hundreds of GBps of throughput, and millions of IOPS. Since Lustre is a widely adopted and high-performing file system, it's the best fit for the described scenario.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an AWS Storage Gateway file gateway, create a file share using the necessary client protocol, and connect the application server to the file share.\n\nAWS Storage Gateway does not support the Lustre file system. It provides a hybrid cloud storage service but it won't meet the specific requirement in this scenario.\n\n\n\n\nCreate an Amazon EC2 Windows instance, install and configure a Windows file share role on the instance, and connect the application server to the file share.\n\nThis option does not support the Lustre file system and also it's not a fully managed solution, which is one of the requirements.\n\n\n\n\nCreate an Amazon Elastic File System (Amazon EFS) file system, configure it to support Lustre, attach the file system to the origin server, and connect the application server to the file system.\n\nAmazon EFS does not support the Lustre file system. EFS is a fully managed NFS file system for Linux-based workloads.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/fsx/lustre/",
      "correctAnswerExplanations": [
        {
          "answer": "Create an Amazon FSx for Lustre file system, attach the file system to the origin server, and connect the application server to the file system.",
          "explanation": "Amazon FSx for Lustre is a fully managed file system that is optimized for compute-intensive workloads, such as high-performance computing, machine learning, and media data processing workflows. It provides sub-millisecond latencies, up to hundreds of GBps of throughput, and millions of IOPS. Since Lustre is a widely adopted and high-performing file system, it's the best fit for the described scenario."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Create an AWS Storage Gateway file gateway, create a file share using the necessary client protocol, and connect the application server to the file share.",
          "explanation": "AWS Storage Gateway does not support the Lustre file system. It provides a hybrid cloud storage service but it won't meet the specific requirement in this scenario."
        },
        {
          "answer": "Create an Amazon EC2 Windows instance, install and configure a Windows file share role on the instance, and connect the application server to the file share.",
          "explanation": "This option does not support the Lustre file system and also it's not a fully managed solution, which is one of the requirements."
        },
        {
          "answer": "Create an Amazon Elastic File System (Amazon EFS) file system, configure it to support Lustre, attach the file system to the origin server, and connect the application server to the file system.",
          "explanation": "Amazon EFS does not support the Lustre file system. EFS is a fully managed NFS file system for Linux-based workloads."
        }
      ],
      "references": [
        "https://aws.amazon.com/fsx/lustre/"
      ]
    },
    {
      "id": 19,
      "question": "A company is using a SQL database to store product inventory data that is frequently accessed by an internal web application. The database runs on an Amazon RDS Single-AZ DB instance. A script runs periodically throughout the day to generate reports on inventory levels. The company's development team notices that the database performance is insufficient for their tasks when the script is running. A solutions architect must recommend a solution to resolve this issue.\n\nWhich solution will meet this requirement with the LEAST operational overhead?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Instruct the development team to manually export the inventory data at the end of each day.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use Amazon ElastiCache to cache the common queries that the script runs against the database.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Modify the DB instance to be a Multi-AZ deployment.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Create a read replica of the database. Configure the script to query only the read replica.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nCreate a read replica of the database. Configure the script to query only the read replica.\n\nCreating a read replica of the database and configuring the script to query only the read replica is the most effective solution. This approach offloads the read queries generated by the script from the primary database to the read replica, thereby reducing the load on the primary database and improving its performance for other tasks, such as development activities.\n\n\n\n\n\n\n\nIncorrect Options:\n\nModify the DB instance to be a Multi-AZ deployment.\n\nConverting the DB instance to a Multi-AZ deployment only provides high availability and failover support, rather than improving read performance during the script's operation.\n\n\n\n\nInstruct the development team to manually export the inventory data at the end of each day.\n\nManually exporting the inventory data at the end of each day does not address the performance issue when the script is running during the day.\n\n\n\n\nUse Amazon ElastiCache to cache the common queries that the script runs against the database.\n\nWhile Amazon ElastiCache can help cache common queries, it does not directly address the issue of offloading read queries from the primary database. A read replica is a more appropriate solution for this scenario.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/read-replicas/\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
      "correctAnswerExplanations": [
        {
          "answer": "Create a read replica of the database. Configure the script to query only the read replica.",
          "explanation": "Creating a read replica of the database and configuring the script to query only the read replica is the most effective solution. This approach offloads the read queries generated by the script from the primary database to the read replica, thereby reducing the load on the primary database and improving its performance for other tasks, such as development activities."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Modify the DB instance to be a Multi-AZ deployment.",
          "explanation": "Converting the DB instance to a Multi-AZ deployment only provides high availability and failover support, rather than improving read performance during the script's operation."
        },
        {
          "answer": "Instruct the development team to manually export the inventory data at the end of each day.",
          "explanation": "Manually exporting the inventory data at the end of each day does not address the performance issue when the script is running during the day."
        },
        {
          "answer": "Use Amazon ElastiCache to cache the common queries that the script runs against the database.",
          "explanation": "While Amazon ElastiCache can help cache common queries, it does not directly address the issue of offloading read queries from the primary database. A read replica is a more appropriate solution for this scenario."
        }
      ],
      "references": [
        "https://aws.amazon.com/rds/features/read-replicas/",
        "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"
      ]
    },
    {
      "id": 20,
      "question": "A financial institution is planning to host sensitive customer data on Amazon DynamoDB. The data must be encrypted while at rest and the use of encryption keys should be logged for auditing. Furthermore, keys must undergo rotation every year.\n\nWhich solution satisfies these requirements with the least operational overhead?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use DynamoDB encryption at rest with AWS KMS keys and perform manual rotation.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use DynamoDB encryption at rest with AWS managed keys.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use DynamoDB encryption at rest with customer-managed keys.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use DynamoDB encryption at rest with AWS KMS keys and enable automatic rotation.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nUse DynamoDB encryption at rest with AWS KMS keys and enable automatic rotation.\n\nAWS Key Management Service (AWS KMS) is a fully managed service that makes it easy for you to create and control the encryption keys used to encrypt data. It is designed to simplify key management, with features like automatic key rotation and audit logging that help meet regulatory and compliance requirements. DynamoDB encryption at rest with AWS KMS keys is a secure way to encrypt sensitive data, while also providing the ability to rotate keys automatically, helping to reduce operational overhead.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse DynamoDB encryption at rest with customer-managed keys.\n\nManaging your own keys will add unnecessary operational overhead and it does not provide an integrated mechanism for key usage logging and rotation.\n\n\n\n\nUse DynamoDB encryption at rest with AWS managed keys.\n\nAWS managed keys do not offer the capability to audit key usage or the ability to rotate keys annually.\n\n\n\n\nUse DynamoDB encryption at rest with AWS KMS keys and perform manual rotation.\n\nManual rotation adds unnecessary operational overhead, whereas automatic rotation would be more efficient.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/encryption.tutorial.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use DynamoDB encryption at rest with AWS KMS keys and enable automatic rotation.",
          "explanation": "AWS Key Management Service (AWS KMS) is a fully managed service that makes it easy for you to create and control the encryption keys used to encrypt data. It is designed to simplify key management, with features like automatic key rotation and audit logging that help meet regulatory and compliance requirements. DynamoDB encryption at rest with AWS KMS keys is a secure way to encrypt sensitive data, while also providing the ability to rotate keys automatically, helping to reduce operational overhead."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use DynamoDB encryption at rest with customer-managed keys.",
          "explanation": "Managing your own keys will add unnecessary operational overhead and it does not provide an integrated mechanism for key usage logging and rotation."
        },
        {
          "answer": "Use DynamoDB encryption at rest with AWS managed keys.",
          "explanation": "AWS managed keys do not offer the capability to audit key usage or the ability to rotate keys annually."
        },
        {
          "answer": "Use DynamoDB encryption at rest with AWS KMS keys and perform manual rotation.",
          "explanation": "Manual rotation adds unnecessary operational overhead, whereas automatic rotation would be more efficient."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html",
        "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/encryption.tutorial.html"
      ]
    },
    {
      "id": 21,
      "question": "A solutions architect is tasked with creating a strategy to minimize the data storage expenses of a company. The company's data is currently stored in Amazon S3 Standard storage class. The company is obliged to retain all data for a minimum period of 50 years. Data from the most recent 3 years should be readily accessible and highly available.\n\nWhich solution will fulfill these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Configure an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately and to S3 Glacier Deep Archive after 3 years.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Configure an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 3 years.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Use S3 Intelligent-Tiering and activate the archiving option to ensure that data is archived in S3 Glacier Deep Archive.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Configure an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nConfigure an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 3 years.\n\nBy Configuring the S3 Lifecycle policy, the data that is more than 3 years old will be automatically transitioned to the S3 Glacier Deep Archive storage class. This allows the company to take advantage of the cost savings provided by the Glacier Deep Archive storage class, which offers the lowest storage costs among the Amazon S3 storage classes.\n\nAt the same time, the data from the most recent 3 years will remain in the S3 Standard storage class, ensuring that it is readily accessible and highly available as required. This ensures that the most frequently accessed data is stored in the appropriate storage class for optimal performance and availability.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.\n\nThis option doesn't meet the requirement that data from the most recent 3 years should be readily accessible and highly available.\n\n\n\n\nUse S3 Intelligent-Tiering and activate the archiving option to ensure that data is archived in S3 Glacier Deep Archive.\n\nWhile S3 Intelligent-Tiering can help reduce costs, it doesn't specifically target the 3-year high availability requirement.\n\n\n\n\nConfigure an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately and to S3 Glacier Deep Archive after 3 years.\n\nThis option doesn't meet the requirement that data from the most recent 3 years should be readily accessible and highly available.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\n\nhttps://aws.amazon.com/s3/storage-classes/",
      "correctAnswerExplanations": [
        {
          "answer": "Configure an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 3 years.",
          "explanation": "By Configuring the S3 Lifecycle policy, the data that is more than 3 years old will be automatically transitioned to the S3 Glacier Deep Archive storage class. This allows the company to take advantage of the cost savings provided by the Glacier Deep Archive storage class, which offers the lowest storage costs among the Amazon S3 storage classes."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Configure an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.",
          "explanation": "This option doesn't meet the requirement that data from the most recent 3 years should be readily accessible and highly available."
        },
        {
          "answer": "Use S3 Intelligent-Tiering and activate the archiving option to ensure that data is archived in S3 Glacier Deep Archive.",
          "explanation": "While S3 Intelligent-Tiering can help reduce costs, it doesn't specifically target the 3-year high availability requirement."
        },
        {
          "answer": "Configure an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately and to S3 Glacier Deep Archive after 3 years.",
          "explanation": "This option doesn't meet the requirement that data from the most recent 3 years should be readily accessible and highly available."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html",
        "https://aws.amazon.com/s3/storage-classes/"
      ]
    },
    {
      "id": 22,
      "question": "A tech firm intends to run stateless, disruption-tolerant applications in the AWS Cloud using serverless architecture. The firm seeks a solution that minimizes both cost and operational overhead.\n\nWhich approach should a solutions architect recommend to fulfill these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use AWS Lambda to run the application functions and use On-Demand Instances with AWS Fargate for cost optimization.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use AWS Lambda to run the application functions and use Spot Instances with AWS Fargate for cost optimization.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Use AWS Lambda to run the application functions and use On-Demand Instances for cost optimization.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use AWS Lambda to run the application functions and use Spot Instances for cost optimization.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nUse AWS Lambda to run the application functions and use Spot Instances with AWS Fargate for cost optimization.\n\nUsing AWS Lambda for serverless computing reduces operational overhead as it eliminates the need to manage servers. Spot Instances in combination with AWS Fargate offer an optimal cost-saving solution for running stateless and disruption-tolerant applications. Fargate removes the need to provision and manage servers, and Spot Instances offer significant cost savings compared to On-Demand Instances.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS Lambda to run the application functions and use Spot Instances for cost optimization.\n\nWhile AWS Lambda reduces operational overhead, merely using Spot Instances without AWS Fargate still requires the management of servers, thus increasing operational overhead.\n\n\n\n\nUse AWS Lambda to run the application functions and use On-Demand Instances for cost optimization.\n\nUsing On-Demand Instances instead of Spot Instances would lead to higher costs, making this option less cost-effective.\n\n\n\n\nUse AWS Lambda to run the application functions and use On-Demand Instances with AWS Fargate for cost optimization.\n\nThis option while reducing operational overhead, would incur higher costs due to the use of On-Demand Instances.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/aws-fargate-spot-now-generally-available/",
      "correctAnswerExplanations": [],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use AWS Lambda to run the application functions and </strong>use <strong>Spot Instances with AWS Fargate for cost optimization.",
          "explanation": "Using AWS Lambda for serverless computing reduces operational overhead as it eliminates the need to manage servers. Spot Instances in combination with AWS Fargate offer an optimal cost-saving solution for running stateless and disruption-tolerant applications. Fargate removes the need to provision and manage servers, and Spot Instances offer significant cost savings compared to On-Demand Instances."
        },
        {
          "answer": "Use AWS Lambda to run the application functions and use Spot Instances for cost optimization.",
          "explanation": "While AWS Lambda reduces operational overhead, merely using Spot Instances without AWS Fargate still requires the management of servers, thus increasing operational overhead."
        },
        {
          "answer": "Use AWS Lambda to run the application functions and use On-Demand Instances for cost optimization.",
          "explanation": "Using On-Demand Instances instead of Spot Instances would lead to higher costs, making this option less cost-effective."
        },
        {
          "answer": "Use AWS Lambda to run the application functions and use On-Demand Instances with AWS Fargate for cost optimization.",
          "explanation": "This option while reducing operational overhead, would incur higher costs due to the use of On-Demand Instances."
        }
      ],
      "references": [
        "https://aws.amazon.com/blogs/aws/aws-fargate-spot-now-generally-available/"
      ]
    },
    {
      "id": 23,
      "question": "A financial institution is moving its transaction processing system to AWS from an on-premises environment. The system uses a PostgreSQL database and experiences heavy read activity during normal operations. Every night, the IT team takes a full backup of the production database to set up a testing environment. During this backup period, users experience significant system slowdowns. The testing environment is also unavailable until the backup and restore process completes.\n\nThe solutions architect needs to propose a new architecture that eliminates the system slowdowns during backup and allows the IT team to use the testing environment without interruptions.\n\nWhich solution meets these requirements?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon Aurora PostgreSQL with Multi-AZ Aurora Replicas for the transaction processing system. Use database cloning to establish the testing environment on-demand.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Use Amazon RDS for PostgreSQL with a Multi-AZ deployment and read replicas for the transaction processing system. Use the standby instance for the testing environment.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use Amazon RDS for PostgreSQL with a Multi-AZ deployment and read replicas for the transaction processing system. Set up the testing environment by using a backup and restore process with the pg_dump utility.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use Amazon Aurora PostgreSQL with Multi-AZ Aurora Replicas for the transaction processing system. Set up the testing environment by using a backup and restore process with the pg_dump utility.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nUse Amazon Aurora PostgreSQL with Multi-AZ Aurora Replicas for the transaction processing system. Use database cloning to establish the testing environment on-demand.\n\nUsing Amazon Aurora PostgreSQL with Multi-AZ Aurora Replicas is an excellent option for the transaction processing system because it provides high availability, durability, and scalability. With Aurora database cloning, the IT team can create multiple clones of the production database without incurring any overhead. These clones are stored on the same cluster as the original database, which makes it easier to manage them. The testing environment can be created on-demand from a clone of the production database, which eliminates the need for a full backup and restore process. This allows the testing environment to be available at all times, without interruptions.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon Aurora PostgreSQL with Multi-AZ Aurora Replicas for the transaction processing system. Set up the testing environment by using a backup and restore process with the pg_dump utility.\n\nThe backup and restore process can cause significant system slowdowns and make the testing environment unavailable until the process completes.\n\n\n\n\nUse Amazon RDS for PostgreSQL with a Multi-AZ deployment and read replicas for the transaction processing system. Use the standby instance for the testing environment.\n\nThe standby instance in a Multi-AZ deployment is not directly accessible for read or write operations. It is used for automatic failover support in case of primary instance failure.\n\n\n\n\nUse Amazon RDS for PostgreSQL with a Multi-AZ deployment and read replicas for the transaction processing system. Set up the testing environment by using a backup and restore process with the pg_dump utility.\n\nThe backup and restore process can cause significant system slowdowns and make the testing environment unavailable until the process completes.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use Amazon Aurora PostgreSQL with Multi-AZ Aurora Replicas for the transaction processing system. Use database cloning to establish the testing environment on-demand.",
          "explanation": "Using Amazon Aurora PostgreSQL with Multi-AZ Aurora Replicas is an excellent option for the transaction processing system because it provides high availability, durability, and scalability. With Aurora database cloning, the IT team can create multiple clones of the production database without incurring any overhead. These clones are stored on the same cluster as the original database, which makes it easier to manage them. The testing environment can be created on-demand from a clone of the production database, which eliminates the need for a full backup and restore process. This allows the testing environment to be available at all times, without interruptions."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use Amazon Aurora PostgreSQL with Multi-AZ Aurora Replicas for the transaction processing system. Set up the testing environment by using a backup and restore process with the pg_dump utility.",
          "explanation": "The backup and restore process can cause significant system slowdowns and make the testing environment unavailable until the process completes."
        },
        {
          "answer": "Use Amazon RDS for PostgreSQL with a Multi-AZ deployment and read replicas for the transaction processing system. Use the standby instance for the testing environment.",
          "explanation": "The standby instance in a Multi-AZ deployment is not directly accessible for read or write operations. It is used for automatic failover support in case of primary instance failure."
        },
        {
          "answer": "Use Amazon RDS for PostgreSQL with a Multi-AZ deployment and read replicas for the transaction processing system. Set up the testing environment by using a backup and restore process with the pg_dump utility.",
          "explanation": "The backup and restore process can cause significant system slowdowns and make the testing environment unavailable until the process completes."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html",
        "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html"
      ]
    },
    {
      "id": 24,
      "question": "An organization has deployed a custom web server on three Amazon EC2 instances, situated behind a Network Load Balancer (NLB) in the ap-south-1 region. The majority of the organization's users are based in Asia and Africa. To enhance the performance and reliability of the solution, the organization has set up three EC2 instances in the af-south-1 region and incorporated these instances as targets for a new NLB.\n\nWhat solution can the organization utilize to divert traffic to all the EC2 instances?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Assign Elastic IP addresses to the six EC2 instances. Implement an Amazon Route 53 geolocation routing policy to direct requests to one of the six EC2 instances. Establish an Amazon CloudFront distribution and use the Route 53 record as the distribution's origin.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Replace the two NLBs with two Application Load Balancers (ALBs). Implement an Amazon Route 53 latency routing policy to direct requests to one of the two ALBs. Establish an Amazon CloudFront distribution and use the Route 53 record as the distribution’s origin.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Implement an Amazon Route 53 geolocation routing policy to direct requests to one of the two NLBs. Establish an Amazon CloudFront distribution and use the Route 53 record as the distribution’s origin.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Set up a standard accelerator in AWS Global Accelerator. Formulate endpoint groups in ap-south-1 and af-south-1. Include the two NLBs as endpoints for the endpoint groups.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nSet up a standard accelerator in AWS Global Accelerator. Formulate endpoint groups in ap-south-1 and af-south-1. Include the two NLBs as endpoints for the endpoint groups.\n\nAWS Global Accelerator provides global anycast IP addresses that direct client traffic to the nearest AWS edge location, improving performance by minimizing latency and reducing jitter. By configuring endpoint groups in multiple regions, the traffic can be distributed to the EC2 instances in both the ap-south-1 and af-south-1 regions, serving users in Asia and Africa respectively. This setup ensures better regional availability and load balancing.\n\n\n\n\n\n\n\nIncorrect Options:\n\nImplement an Amazon Route 53 geolocation routing policy to direct requests to one of the two NLBs. Establish an Amazon CloudFront distribution and use the Route 53 record as the distribution’s origin.\n\nThis is not the best solution as it does not provide optimal performance and high availability which AWS Global Accelerator does. Using an Amazon Route 53 geolocation routing policy with an Amazon CloudFront distribution as the origin does not directly distribute traffic to all EC2 instances behind the NLBs. It primarily focuses on directing traffic based on the geographic location of the user.\n\n\n\n\nAssign Elastic IP addresses to the six EC2 instances. Implement an Amazon Route 53 geolocation routing policy to direct requests to one of the six EC2 instances. Establish an Amazon CloudFront distribution and use the Route 53 record as the distribution's origin.\n\nThis solution is not feasible as it doesn't provide a centralized way to manage traffic and it would be complex to manage.\n\n\n\n\nReplace the two NLBs with two Application Load Balancers (ALBs). Implement an Amazon Route 53 latency routing policy to direct requests to one of the two ALBs. Establish an Amazon CloudFront distribution and use the Route 53 record as the distribution’s origin.\n\nReplacing NLBs with ALBs is not necessary and won't provide the desired performance improvement.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html",
      "correctAnswerExplanations": [
        {
          "answer": "Set up a standard accelerator in AWS Global Accelerator. Formulate endpoint groups in ap-south-1 and af-south-1. Include the two NLBs as endpoints for the endpoint groups.",
          "explanation": "AWS Global Accelerator provides global anycast IP addresses that direct client traffic to the nearest AWS edge location, improving performance by minimizing latency and reducing jitter. By configuring endpoint groups in multiple regions, the traffic can be distributed to the EC2 instances in both the ap-south-1 and af-south-1 regions, serving users in Asia and Africa respectively. This setup ensures better regional availability and load balancing."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Implement an Amazon Route 53 geolocation routing policy to direct requests to one of the two NLBs. Establish an Amazon CloudFront distribution and use the Route 53 record as the distribution’s origin.",
          "explanation": "This is not the best solution as it does not provide optimal performance and high availability which AWS Global Accelerator does. Using an Amazon Route 53 geolocation routing policy with an Amazon CloudFront distribution as the origin does not directly distribute traffic to all EC2 instances behind the NLBs. It primarily focuses on directing traffic based on the geographic location of the user."
        },
        {
          "answer": "Assign Elastic IP addresses to the six EC2 instances. Implement an Amazon Route 53 geolocation routing policy to direct requests to one of the six EC2 instances. Establish an Amazon CloudFront distribution and use the Route 53 record as the distribution's origin.",
          "explanation": "This solution is not feasible as it doesn't provide a centralized way to manage traffic and it would be complex to manage."
        },
        {
          "answer": "Replace the two NLBs with two Application Load Balancers (ALBs). Implement an Amazon Route 53 latency routing policy to direct requests to one of the two ALBs. Establish an Amazon CloudFront distribution and use the Route 53 record as the distribution’s origin.",
          "explanation": "Replacing NLBs with ALBs is not necessary and won't provide the desired performance improvement."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html"
      ]
    },
    {
      "id": 25,
      "question": "A solutions architect is designing a cloud architecture for a new image processing application on AWS. The application should process tasks in parallel, while dynamically adding or removing processing nodes based on the number of jobs in the queue. The processing application is stateless. The solutions architect must ensure that the application is loosely coupled and the task items are durably stored.\n\nWhich design should the solutions architect use?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Create an Amazon SNS topic to send the image processing tasks. Create a container image with the processing application. Create an Amazon ECS service with an Auto Scaling group. Set the scaling policy for the ECS service to add and remove tasks based on the number of messages published to the SNS topic.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an Amazon SQS queue to hold the image processing tasks. Create a container image with the processing application. Create an Amazon ECS service with an Auto Scaling group. Set the scaling policy for the ECS service to add and remove tasks based on network usage.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Create an Amazon SQS queue to hold the image processing tasks. Create a container image with the processing application. Create an Amazon ECS service with an Auto Scaling group. Set the scaling policy for the ECS service to add and remove tasks based on the number of items in the SQS queue.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Create an Amazon SNS topic to send the image processing tasks. Create a container image with the processing application. Create an Amazon ECS service with an Auto Scaling group. Set the scaling policy for the ECS service to add and remove tasks based on network usage.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nCreate an Amazon SQS queue to hold the image processing tasks. Create a container image with the processing application. Create an Amazon ECS service with an Auto Scaling group. Set the scaling policy for the ECS service to add and remove tasks based on the number of items in the SQS queue.\n\nIt uses Amazon SQS to durably store the tasks, ensuring a loosely coupled architecture. The Amazon ECS service, along with an Auto Scaling group, allows the application to scale dynamically by adding or removing processing nodes based on the number of items in the SQS queue, which is directly related to the workload.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an Amazon SQS queue to hold the image processing tasks. Create a container image with the processing application. Create an Amazon ECS service with an Auto Scaling group. Set the scaling policy for the ECS service to add and remove tasks based on network usage.\n\nIt scales the processing nodes based on CPU usage rather than the number of items in the SQS queue, which is not reflects the workload.\n\n\n\n\nCreate an Amazon SNS topic to send the image processing tasks. Create a container image with the processing application. Create an Amazon ECS service with an Auto Scaling group. Set the scaling policy for the ECS service to add and remove tasks based on network usage.\n\nIt uses Amazon SNS instead of SQS, which does not provide durable storage for tasks and is not suitable for ensuring a loosely coupled architecture.\n\n\n\n\nCreate an Amazon SNS topic to send the image processing tasks. Create a container image with the processing application. Create an Amazon ECS service with an Auto Scaling group. Set the scaling policy for the ECS service to add and remove tasks based on the number of messages published to the SNS topic.\n\nIt uses Amazon SNS instead of SQS, which does not provide durable storage for tasks and is not suitable for ensuring a loosely coupled architecture.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/sqs/\n\nhttps://aws.amazon.com/ecs/\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html",
      "correctAnswerExplanations": [
        {
          "answer": "Create an Amazon SQS queue to hold the image processing tasks. Create a container image with the processing application. Create an Amazon ECS service with an Auto Scaling group. Set the scaling policy for the ECS service to add and remove tasks based on the number of items in the SQS queue.",
          "explanation": "It uses Amazon SQS to durably store the tasks, ensuring a loosely coupled architecture. The Amazon ECS service, along with an Auto Scaling group, allows the application to scale dynamically by adding or removing processing nodes based on the number of items in the SQS queue, which is directly related to the workload."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Create an Amazon SQS queue to hold the image processing tasks. Create a container image with the processing application. Create an Amazon ECS service with an Auto Scaling group. Set the scaling policy for the ECS service to add and remove tasks based on network usage.",
          "explanation": "It scales the processing nodes based on CPU usage rather than the number of items in the SQS queue, which is not reflects the workload."
        },
        {
          "answer": "Create an Amazon SNS topic to send the image processing tasks. Create a container image with the processing application. Create an Amazon ECS service with an Auto Scaling group. Set the scaling policy for the ECS service to add and remove tasks based on network usage.",
          "explanation": "It uses Amazon SNS instead of SQS, which does not provide durable storage for tasks and is not suitable for ensuring a loosely coupled architecture."
        },
        {
          "answer": "Create an Amazon SNS topic to send the image processing tasks. Create a container image with the processing application. Create an Amazon ECS service with an Auto Scaling group. Set the scaling policy for the ECS service to add and remove tasks based on the number of messages published to the SNS topic.",
          "explanation": "It uses Amazon SNS instead of SQS, which does not provide durable storage for tasks and is not suitable for ensuring a loosely coupled architecture."
        }
      ],
      "references": [
        "https://aws.amazon.com/sqs/",
        "https://aws.amazon.com/ecs/",
        "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html"
      ]
    },
    {
      "id": 26,
      "question": "A data analytics company is considering migrating its infrastructure to AWS Cloud. The firm needs a minimum of 10 TB of storage with the highest possible I/O performance for real-time data processing, 300 TB of highly durable storage for maintaining processed data, and 900 TB of storage for archiving historical data that is no longer in active use.\n\nWhich combination of services should a solutions architect recommend to meet these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Amazon EBS for high-performance storage, Amazon EFS for durable data storage, and Amazon S3 Glacier for archival storage.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Amazon EBS for high-performance storage, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Amazon EC2 instance store for high-performance storage, Amazon EFS for durable data storage, and Amazon S3 for archival storage.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Amazon EC2 instance store for high-performance storage, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nAmazon EBS for high-performance storage, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.\n\nAmazon EBS offers high-performance storage that can be attached to EC2 instances and is ideal for workloads that require low latency, such as real-time data processing. Amazon S3 provides highly durable, scalable, and secure storage for processed data. Amazon S3 Glacier is a cost-effective storage solution for long-term archiving of historical data.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAmazon EBS for high-performance storage, Amazon EFS for durable data storage, and Amazon S3 Glacier for archival storage.\n\nWhile Amazon EFS can be used for durable data storage, it is typically more expensive than S3 and may not be necessary for this use case.\n\n\n\n\nAmazon EC2 instance store for high-performance storage, Amazon EFS for durable data storage, and Amazon S3 for archival storage.\n\nThe use of EC2 Instance Store is not suitable as it has storage limitations. We require a minimum of 10 TB of storage which it does not support.\n\n\n\n\nAmazon EC2 instance store for high-performance storage, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.\n\nThe use of EC2 Instance Store is not suitable as it has storage limitations. We require a minimum of 10 TB of storage which it does not support.\n\n\n\n\n\n\n\nReferences:\n\nhttps://www.amazonaws.cn/en/ebs/features/\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\n\nhttps://aws.amazon.com/s3/storage-classes/",
      "correctAnswerExplanations": [
        {
          "answer": "Amazon EBS for high-performance storage, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.",
          "explanation": "Amazon EBS offers high-performance storage that can be attached to EC2 instances and is ideal for workloads that require low latency, such as real-time data processing. Amazon S3 provides highly durable, scalable, and secure storage for processed data. Amazon S3 Glacier is a cost-effective storage solution for long-term archiving of historical data."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Amazon EBS for high-performance storage, Amazon EFS for durable data storage, and Amazon S3 Glacier for archival storage.",
          "explanation": "While Amazon EFS can be used for durable data storage, it is typically more expensive than S3 and may not be necessary for this use case."
        },
        {
          "answer": "Amazon EC2 instance store for high-performance storage, Amazon EFS for durable data storage, and Amazon S3 for archival storage.",
          "explanation": "The use of EC2 Instance Store is not suitable as it has storage limitations. We require a minimum of 10 TB of storage which it does not support."
        },
        {
          "answer": "Amazon EC2 instance store for high-performance storage, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.",
          "explanation": "The use of EC2 Instance Store is not suitable as it has storage limitations. We require a minimum of 10 TB of storage which it does not support."
        }
      ],
      "references": [
        "https://www.amazonaws.cn/en/ebs/features/",
        "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
        "https://aws.amazon.com/s3/storage-classes/"
      ]
    },
    {
      "id": 27,
      "question": "A company recently deployed a serverless application using AWS Lambda functions, with data stored in Amazon DynamoDB. The Lambda functions are connected to a VPC with a private subnet. The company's solutions architect needs to access the Lambda functions and DynamoDB tables securely from the on-premises network through the company's internet connection.\n\nWhich combination of steps should the solutions architect take to meet these requirements? (Select TWO.)",
      "corrects": [
        2,
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Configure a VPC peering connection between the on-premises network and the VPC containing the Lambda functions.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Configure an AWS Direct Connect connection between the on-premises network and the VPC containing the Lambda functions.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Configure an Amazon API Gateway with a custom domain and a client certificate for secure access to the Lambda functions.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Configure the Lambda functions to only allow inbound access from the external IP range for the company.",
          "correct": false
        },
        {
          "id": 5,
          "answer": "Configure the Lambda functions to only allow inbound access from the internal IP range for the company.",
          "correct": false
        }
      ],
      "multiple": true,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nConfigure an AWS Direct Connect connection between the on-premises network and the VPC containing the Lambda functions.\n\nConfigure an Amazon API Gateway with a custom domain and a client certificate for secure access to the Lambda functions.\n\nTo securely access Lambda functions and DynamoDB tables from the on-premises network, the solutions architect should configure an AWS Direct Connect connection and an Amazon API Gateway with a custom domain and a client certificate. AWS Direct Connect provides a dedicated, private network connection between the on-premises network and the VPC containing the Lambda functions, ensuring secure and high-performance connectivity. The Amazon API Gateway allows the solutions architect to create, publish, and manage APIs for the Lambda functions while securing the API with a custom domain and a client certificate for authentication and encryption.\n\n\n\n\nIncorrect Options:\n\nConfigure a VPC peering connection between the on-premises network and the VPC containing the Lambda functions.\n\nConfiguring a VPC peering connection is not suitable in this scenario, as it is used to connect two VPCs and not an on-premises network to a VPC. Hence, this option is incorrect.\n\n\n\n\nConfigure the Lambda functions to only allow inbound access from the internal IP range for the company.\n\nConfiguring the Lambda functions to only allow inbound access from the internal IP range for the company is not applicable, as Lambda functions do not have the concept of security groups. Access to Lambda functions is managed through IAM policies, and the secure access is achieved using API Gateway. This option is incorrect.\n\n\n\n\nConfigure the Lambda functions to only allow inbound access from the external IP range for the company.\n\nConfiguring the Lambda functions to only allow inbound access from the external IP range for the company is not applicable, as Lambda functions do not have the concept of security groups. Secure access to Lambda functions is achieved using API Gateway with a custom domain and a client certificate. This option is incorrect.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html",
      "correctAnswerExplanations": [
        {
          "answer": "Configure an AWS Direct Connect connection between the on-premises network and the VPC containing the Lambda functions.",
          "explanation": "<strong>Configure an Amazon API Gateway with a custom domain and a client certificate for secure access to the Lambda functions.</strong>"
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Configure a VPC peering connection between the on-premises network and the VPC containing the Lambda functions.",
          "explanation": "Configuring a VPC peering connection is not suitable in this scenario, as it is used to connect two VPCs and not an on-premises network to a VPC. Hence, this option is incorrect."
        },
        {
          "answer": "Configure the Lambda functions to only allow inbound access from the internal IP range for the company.",
          "explanation": "Configuring the Lambda functions to only allow inbound access from the internal IP range for the company is not applicable, as Lambda functions do not have the concept of security groups. Access to Lambda functions is managed through IAM policies, and the secure access is achieved using API Gateway. This option is incorrect."
        },
        {
          "answer": "Configure the Lambda functions to only allow inbound access from the external IP range for the company.",
          "explanation": "Configuring the Lambda functions to only allow inbound access from the external IP range for the company is not applicable, as Lambda functions do not have the concept of security groups. Secure access to Lambda functions is achieved using API Gateway with a custom domain and a client certificate. This option is incorrect."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html",
        "https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html"
      ]
    },
    {
      "id": 28,
      "question": "An international organization uses Amazon CloudFront to distribute content for its global membership base across regions us-east-1 and ap-southeast-2. A solutions architect is tasked with designing a solution to safeguard this CloudFront distributed content across various accounts from SQL injection and cross-site scripting attacks.\n\nWhich solution will fulfill these requirements with the minimum administrative effort?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Implement AWS Shield in one of the regions and associate regional web ACLs with a CloudFront distribution.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Implement AWS WAF in both regions and associate regional web ACLs with a CloudFront distribution.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Implement AWS Firewall Manager in both regions and centrally configure AWS WAF rules.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Implement AWS Shield in both regions and associate regional web ACLs with a CloudFront distribution.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nImplement AWS Firewall Manager in both regions and centrally configure AWS WAF rules.\n\nAWS Firewall Manager allows centralized control over AWS WAF rules across multiple accounts and resources, making it the best choice for minimal administrative effort. Firewall Manager also allows enforcement of a common set of security rules, which is beneficial for protecting against threats like SQL injection and cross-site scripting attacks.\n\n\n\n\n\n\n\nIncorrect Options:\n\nImplement AWS WAF in both regions and associate regional web ACLs with a CloudFront distribution.\n\nWhile AWS WAF can effectively protect against the mentioned attacks, managing it individually for each region and account can be administratively intensive.\n\n\n\n\nImplement AWS Shield in both regions and associate regional web ACLs with a CloudFront distribution.\n\nAWS Shield primarily provides protection against DDoS attacks, not specifically against SQL injection and cross-site scripting attacks.\n\n\n\n\nImplement AWS Shield in one of the regions and associate regional web ACLs with a CloudFront distribution.\n\nAWS Shield is not specifically designed to protect against SQL injection and cross-site scripting attacks.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/firewall-manager/",
      "correctAnswerExplanations": [
        {
          "answer": "Implement AWS Firewall Manager in both regions and centrally configure AWS WAF rules.",
          "explanation": "AWS Firewall Manager allows centralized control over AWS WAF rules across multiple accounts and resources, making it the best choice for minimal administrative effort. Firewall Manager also allows enforcement of a common set of security rules, which is beneficial for protecting against threats like SQL injection and cross-site scripting attacks."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Implement AWS WAF in both regions and associate regional web ACLs with a CloudFront distribution.",
          "explanation": "While AWS WAF can effectively protect against the mentioned attacks, managing it individually for each region and account can be administratively intensive."
        },
        {
          "answer": "Implement AWS Shield in both regions and associate regional web ACLs with a CloudFront distribution.",
          "explanation": "AWS Shield primarily provides protection against DDoS attacks, not specifically against SQL injection and cross-site scripting attacks."
        },
        {
          "answer": "Implement AWS Shield in one of the regions and associate regional web ACLs with a CloudFront distribution.",
          "explanation": "AWS Shield is not specifically designed to protect against SQL injection and cross-site scripting attacks."
        }
      ],
      "references": [
        "https://aws.amazon.com/firewall-manager/"
      ]
    },
    {
      "id": 29,
      "question": "A rapidly expanding company is using AWS for its operations. The company is spread across multiple regions and has different departments like Marketing, HR, Engineering, etc., each having separate AWS accounts. To optimize costs, the company's CFO wants a granular view of the costs incurred by each department and also wants the ability to apportion shared costs (like VPN, Direct Connect, etc.) among the departments.\n\nWhich set of actions should be taken to accomplish this?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use AWS Cost and Usage Reports (CUR) for detailed cost information, AWS Organizations for cost allocation, and AWS Budgets for consolidated billing.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use AWS Management Console for cost allocation, AWS Control Tower for account management, and AWS Cost Categories for detailed cost views.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use AWS Organizations for consolidated billing, apply AWS Cost Categories to allocate costs, and use AWS Cost Explorer for detailed views.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Use AWS Control Tower for account management, AWS Trusted Advisor for cost recommendations, and AWS Budgets for cost allocation.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nUse AWS Organizations for consolidated billing, apply AWS Cost Categories to allocate costs, and use AWS Cost Explorer for detailed views.\n\nAWS Organizations allows the company to consolidate billing across multiple AWS accounts, providing a single payment method and billing statement. This simplifies cost management and provides an overall view of costs across the organization.\n\nAWS Cost Categories is a feature that enables the company to categorize costs based on their own organizational structure. It allows the company to define custom cost categories such as departments (Marketing, HR, Engineering) and allocate costs accordingly.\n\nAWS Cost Explorer provides powerful visualization and analysis tools to explore cost and usage data. It allows the company to drill down into specific accounts, services, and time periods to gain detailed insights into costs and identify areas for optimization.\n\nBy combining these three steps, the company can have a consolidated view of costs, allocate costs to specific departments using custom cost categories, and utilize AWS Cost Explorer for detailed analysis and reporting.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS Control Tower for account management, AWS Trusted Advisor for cost recommendations, and AWS Budgets for cost allocation.\n\nWhile these services are valuable, AWS Trusted Advisor does not offer cost allocation and AWS Budgets is more about setting cost and usage thresholds, not allocating costs.\n\n\n\n\nUse AWS Cost and Usage Reports (CUR) for detailed cost information, AWS Organizations for cost allocation, and AWS Budgets for consolidated billing.\n\nAWS Organizations does not provide cost allocation functionality and AWS Budgets does not handle consolidated billing.\n\n\n\n\nUse AWS Management Console for cost allocation, AWS Control Tower for account management, and AWS Cost Categories for detailed cost views.\n\nAWS Management Console does not provide a specific functionality for cost allocation, and AWS Control Tower is not primarily for cost management.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html\n\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-cost-categories.html\n\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/ce-what-is.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use AWS Organizations for consolidated billing, apply AWS Cost Categories to allocate costs, and use AWS Cost Explorer for detailed views.",
          "explanation": "AWS Organizations allows the company to consolidate billing across multiple AWS accounts, providing a single payment method and billing statement. This simplifies cost management and provides an overall view of costs across the organization."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use AWS Control Tower for account management, AWS Trusted Advisor for cost recommendations, and AWS Budgets for cost allocation.",
          "explanation": "While these services are valuable, AWS Trusted Advisor does not offer cost allocation and AWS Budgets is more about setting cost and usage thresholds, not allocating costs."
        },
        {
          "answer": "Use AWS Cost and Usage Reports (CUR) for detailed cost information, AWS Organizations for cost allocation, and AWS Budgets for consolidated billing.",
          "explanation": "AWS Organizations does not provide cost allocation functionality and AWS Budgets does not handle consolidated billing."
        },
        {
          "answer": "Use AWS Management Console for cost allocation, AWS Control Tower for account management, and AWS Cost Categories for detailed cost views.",
          "explanation": "AWS Management Console does not provide a specific functionality for cost allocation, and AWS Control Tower is not primarily for cost management."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html",
        "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-cost-categories.html",
        "https://docs.aws.amazon.com/cost-management/latest/userguide/ce-what-is.html"
      ]
    },
    {
      "id": 30,
      "question": "A financial firm uses an application to handle stock market data. The data is stored in an Amazon RDS PostgreSQL DB instance. The team has detected a significant slowdown in the application's performance during peak trading hours and has decided to separate read and write traffic to improve the application's response times. The solutions architect is tasked with enhancing the application's performance swiftly.\n\nWhat should the solutions architect recommend?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Convert the current database to a Multi-AZ deployment. Direct the read requests to the standby Availability Zone.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Convert the current database to a Multi-AZ deployment. Direct the read requests to the primary Availability Zone.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Set up read replicas for the database. Configure the read replicas with half of the compute and storage resources as the master database.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Set up read replicas for the database. Configure the read replicas with the same compute and storage resources as the master database.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nSet up read replicas for the database. Configure the read replicas with the same compute and storage resources as the master database.\n\nCreating read replicas of the existing database allows you to offload read traffic from the master database, thus improving performance. By ensuring that the read replicas have the same compute and storage resources as the master database, you can maintain the same performance levels for read operations as for write operations.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConvert the current database to a Multi-AZ deployment. Direct the read requests to the primary Availability Zone.\n\nIn a Multi-AZ deployment, the standby database in the secondary AZ is not accessible for read operations.\n\n\n\n\nConvert the current database to a Multi-AZ deployment. Direct the read requests to the standby Availability Zone.\n\nIn a Multi-AZ deployment, the standby database in the secondary AZ is not accessible for read operations.\n\n\n\n\nSet up read replicas for the database. Configure the read replicas with half of the compute and storage resources as the master database.\n\nHaving less resources on the read replicas than on the master database could potentially lead to performance issues, especially during peak usage times.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/read-replicas/\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PostgreSQL.Replication.ReadReplicas.html",
      "correctAnswerExplanations": [
        {
          "answer": "Set up read replicas for the database. Configure the read replicas with the same compute and storage resources as the master database.",
          "explanation": "Creating read replicas of the existing database allows you to offload read traffic from the master database, thus improving performance. By ensuring that the read replicas have the same compute and storage resources as the master database, you can maintain the same performance levels for read operations as for write operations."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Convert the current database to a Multi-AZ deployment. Direct the read requests to the primary Availability Zone.",
          "explanation": "In a Multi-AZ deployment, the standby database in the secondary AZ is not accessible for read operations."
        },
        {
          "answer": "Convert the current database to a Multi-AZ deployment. Direct the read requests to the standby Availability Zone.",
          "explanation": "In a Multi-AZ deployment, the standby database in the secondary AZ is not accessible for read operations."
        },
        {
          "answer": "Set up read replicas for the database. Configure the read replicas with half of the compute and storage resources as the master database.",
          "explanation": "Having less resources on the read replicas than on the master database could potentially lead to performance issues, especially during peak usage times."
        }
      ],
      "references": [
        "https://aws.amazon.com/rds/features/read-replicas/",
        "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PostgreSQL.Replication.ReadReplicas.html"
      ]
    },
    {
      "id": 31,
      "question": "A hospital needs to store electronic health records (EHRs) in an Amazon RDS instance for a minimum of 7 years, as per regulatory requirements.\n\nWhat is the MOST operationally efficient solution that fulfills these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use RDS point-in-time recovery to continually back up the RDS instance.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Configure AWS Backup to create backup schedules and retention policies for the RDS instance.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Create a manual snapshot of the RDS instance through the AWS Management Console. Store the snapshot in an Amazon S3 bucket and set an S3 Lifecycle configuration.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to trigger an AWS Lambda function. Configure the Lambda function to create a snapshot of the RDS instance and store it in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nConfigure AWS Backup to create backup schedules and retention policies for the RDS instance.\n\nAWS Backup allows you to define a centralized backup schedule and retention policy for multiple AWS resources, including Amazon RDS instances. By using AWS Backup, the hospital can ensure that backups are created automatically according to the defined schedule, and the retention policies will take care of retaining the backups for the required 7-year period. This approach minimizes the operational overhead and ensures compliance with the regulatory requirements.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse RDS point-in-time recovery to continually back up the RDS instance.\n\nThis option is incorrect because RDS point-in-time recovery only retains backups for a maximum of 35 days, which doesn't fulfill the 7-year retention requirement.\n\n\n\n\nCreate a manual snapshot of the RDS instance through the AWS Management Console. Store the snapshot in an Amazon S3 bucket and set an S3 Lifecycle configuration.\n\nThis option is incorrect because it suggests a manual process for creating snapshots, which increases operational overhead and is not as efficient as using an automated solution like AWS Backup.\n\n\n\n\nSet up an Amazon EventBridge (Amazon CloudWatch Events) rule to trigger an AWS Lambda function. Configure the Lambda function to create a snapshot of the RDS instance and store it in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket.\n\nThis option is incorrect because, although it automates the backup process, it requires additional configuration and management compared to using AWS Backup, which is specifically designed for creating and managing backups in AWS.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/backup/\n\nhttps://aws.amazon.com/rds/",
      "correctAnswerExplanations": [
        {
          "answer": "Configure AWS Backup to create backup schedules and retention policies for the RDS instance.",
          "explanation": "AWS Backup allows you to define a centralized backup schedule and retention policy for multiple AWS resources, including Amazon RDS instances. By using AWS Backup, the hospital can ensure that backups are created automatically according to the defined schedule, and the retention policies will take care of retaining the backups for the required 7-year period. This approach minimizes the operational overhead and ensures compliance with the regulatory requirements."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use RDS point-in-time recovery to continually back up the RDS instance.",
          "explanation": "This option is incorrect because RDS point-in-time recovery only retains backups for a maximum of 35 days, which doesn't fulfill the 7-year retention requirement."
        },
        {
          "answer": "Create a manual snapshot of the RDS instance through the AWS Management Console. Store the snapshot in an Amazon S3 bucket and set an S3 Lifecycle configuration.",
          "explanation": "This option is incorrect because it suggests a manual process for creating snapshots, which increases operational overhead and is not as efficient as using an automated solution like AWS Backup."
        },
        {
          "answer": "Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to trigger an AWS Lambda function. Configure the Lambda function to create a snapshot of the RDS instance and store it in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket.",
          "explanation": "This option is incorrect because, although it automates the backup process, it requires additional configuration and management compared to using AWS Backup, which is specifically designed for creating and managing backups in AWS."
        }
      ],
      "references": [
        "https://aws.amazon.com/backup/",
        "https://aws.amazon.com/rds/"
      ]
    },
    {
      "id": 32,
      "question": "A solutions architect is designing a VPC for a multi-tier web application. The VPC includes both public and private subnets using IPv4 CIDR blocks. Each of the three Availability Zones (AZs) has one public subnet hosting the web servers and one private subnet hosting the database servers for high availability. The web servers in the public subnets can access the internet via an internet gateway, but the database servers in the private subnets need internet access to receive regular software patches.\n\nWhat should the solutions architect do to enable Internet access for the private subnets?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Deploy three NAT instances, one for each private subnet in each AZ. Establish a private route table for each AZ that directs non-VPC traffic to the NAT instance in its AZ.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Set up three NAT gateways, one for each public subnet in each AZ. Establish a private route table for each AZ that directs non-VPC traffic to the NAT gateway in its AZ.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Install an egress-only internet gateway on one of the public subnets. Modify the route table for the private subnets to direct non-VPC traffic to the egress-only internet gateway.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Install a second internet gateway on one of the private subnets. Modify the route table for the private subnets to direct non-VPC traffic to the private internet gateway.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nSet up three NAT gateways, one for each public subnet in each AZ. Establish a private route table for each AZ that directs non-VPC traffic to the NAT gateway in its AZ.\n\nNAT gateways provide internet access for instances in private subnets while preventing inbound traffic initiated by external hosts, making them ideal for this scenario. They are fully managed, highly available, and automatically scale up to 45 Gbps of bandwidth. Each NAT gateway is associated with a specific Availability Zone and subnet. Thus, to maintain high availability, the architect should create a NAT gateway in each AZ.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDeploy three NAT instances, one for each private subnet in each AZ. Establish a private route table for each AZ that directs non-VPC traffic to the NAT instance in its AZ.\n\nWhile NAT instances can provide similar functionality to NAT gateways, they need to be manually managed and don't scale as well, making them less suitable for production environments.\n\n\n\n\nInstall a second internet gateway on one of the private subnets. Modify the route table for the private subnets to direct non-VPC traffic to the private internet gateway.\n\nYou can't attach more than one internet gateway to a VPC, and directly attaching one to a private subnet would expose it to the internet, compromising security.\n\n\n\n\nInstall an egress-only internet gateway on one of the public subnets. Modify the route table for the private subnets to direct non-VPC traffic to the egress-only internet gateway.\n\nEgress-only internet gateways are for IPv6 traffic only and do not support IPv4, which is used in this scenario.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-example-private-subnets-nat.html",
      "correctAnswerExplanations": [
        {
          "answer": "Set up three NAT gateways, one for each public subnet in each AZ. Establish a private route table for each AZ that directs non-VPC traffic to the NAT gateway in its AZ.",
          "explanation": "NAT gateways provide internet access for instances in private subnets while preventing inbound traffic initiated by external hosts, making them ideal for this scenario. They are fully managed, highly available, and automatically scale up to 45 Gbps of bandwidth. Each NAT gateway is associated with a specific Availability Zone and subnet. Thus, to maintain high availability, the architect should create a NAT gateway in each AZ."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Deploy three NAT instances, one for each private subnet in each AZ. Establish a private route table for each AZ that directs non-VPC traffic to the NAT instance in its AZ.",
          "explanation": "While NAT instances can provide similar functionality to NAT gateways, they need to be manually managed and don't scale as well, making them less suitable for production environments."
        },
        {
          "answer": "Install a second internet gateway on one of the private subnets. Modify the route table for the private subnets to direct non-VPC traffic to the private internet gateway.",
          "explanation": "You can't attach more than one internet gateway to a VPC, and directly attaching one to a private subnet would expose it to the internet, compromising security."
        },
        {
          "answer": "Install an egress-only internet gateway on one of the public subnets. Modify the route table for the private subnets to direct non-VPC traffic to the egress-only internet gateway.",
          "explanation": "Egress-only internet gateways are for IPv6 traffic only and do not support IPv4, which is used in this scenario."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
        "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-example-private-subnets-nat.html"
      ]
    },
    {
      "id": 33,
      "question": "A large e-commerce company is running its mission-critical web application on AWS, which uses a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The company operates in a multi-region setup to ensure high availability and disaster recovery. It has recently started exploring various cost-optimization strategies without compromising the application's performance and uptime.\n\nWhich of the following combinations of actions would provide the MOST cost-effective solution without impacting the application's availability and performance?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use EC2 On-Demand Instances exclusively, implement Amazon RDS Multi-AZ deployments for the database, and use Amazon CloudFront for content delivery.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use EC2 Reserved Instances for baseline traffic in each region, implement Auto Scaling groups with Spot Instances for unexpected traffic surges, and utilize AWS Savings Plans for consistent workloads.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Move the entire application to AWS Lambda, utilize AWS Savings Plans, and host static content on Amazon S3.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use EC2 Spot Instances for all the workloads, store session state data in Amazon DynamoDB, and leverage Amazon CloudFront for caching.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nUse EC2 Reserved Instances for baseline traffic in each region, implement Auto Scaling groups with Spot Instances for unexpected traffic surges, and utilize AWS Savings Plans for consistent workloads.\n\nUsing a combination of EC2 Reserved Instances, Spot Instances, and AWS Savings Plans provides the most cost-effective solution. Reserved Instances provide cost savings for predictable workloads, Spot Instances can be used for handling unexpected traffic surges at a significantly lower cost, and Savings Plans offer flexible pricing for consistent workloads.\n\n\n\n\n\n\n\nIncorrect Options:\n\nMove the entire application to AWS Lambda, utilize AWS Savings Plans, and host static content on Amazon S3.\n\nWhile this option might be cost-effective for some workloads, moving an entire mission-critical application to AWS Lambda might not be suitable, especially for long-running tasks and may introduce new complexities.\n\n\n\n\nUse EC2 Spot Instances for all the workloads, store session state data in Amazon DynamoDB, and leverage Amazon CloudFront for caching.\n\nWhile Spot Instances are cost-effective, using them for all workloads can be risky, especially for critical applications, as they can be interrupted with short notice when AWS needs the capacity back.\n\n\n\n\nUse EC2 On-Demand Instances exclusively, implement Amazon RDS Multi-AZ deployments for the database, and use Amazon CloudFront for content delivery.\n\nThis option is the least cost-optimized. Although it ensures high availability, using On-Demand Instances exclusively is not cost-effective for predictable workloads.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/ec2/pricing/\n\nhttps://aws.amazon.com/savingsplans/\n\nhttps://aws.amazon.com/autoscaling/",
      "correctAnswerExplanations": [
        {
          "answer": "Use EC2 Reserved Instances for baseline traffic in each region, implement Auto Scaling groups with Spot Instances for unexpected traffic surges, and utilize AWS Savings Plans for consistent workloads.",
          "explanation": "Using a combination of EC2 Reserved Instances, Spot Instances, and AWS Savings Plans provides the most cost-effective solution. Reserved Instances provide cost savings for predictable workloads, Spot Instances can be used for handling unexpected traffic surges at a significantly lower cost, and Savings Plans offer flexible pricing for consistent workloads."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Move the entire application to AWS Lambda, utilize AWS Savings Plans, and host static content on Amazon S3.",
          "explanation": "While this option might be cost-effective for some workloads, moving an entire mission-critical application to AWS Lambda might not be suitable, especially for long-running tasks and may introduce new complexities."
        },
        {
          "answer": "Use EC2 Spot Instances for all the workloads, store session state data in Amazon DynamoDB, and leverage Amazon CloudFront for caching.",
          "explanation": "While Spot Instances are cost-effective, using them for all workloads can be risky, especially for critical applications, as they can be interrupted with short notice when AWS needs the capacity back."
        },
        {
          "answer": "Use EC2 On-Demand Instances exclusively, implement Amazon RDS Multi-AZ deployments for the database, and use Amazon CloudFront for content delivery.",
          "explanation": "This option is the least cost-optimized. Although it ensures high availability, using On-Demand Instances exclusively is not cost-effective for predictable workloads."
        }
      ],
      "references": [
        "https://aws.amazon.com/ec2/pricing/",
        "https://aws.amazon.com/savingsplans/",
        "https://aws.amazon.com/autoscaling/"
      ]
    },
    {
      "id": 34,
      "question": "A business is running a highly flexible data analysis task that deploys multiple Amazon EC2 instances. The task doesn't maintain state, can be initiated and halted at any moment without negative consequences, and typically requires over an hour to finish. The business has requested a solutions architect to design a scalable and cost-effective solution that fulfills the task's needs.\n\nWhich solution should the solutions architect recommend?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use EC2 Reserved Instances.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use EC2 Spot Instances.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Use EC2 On-Demand Instances.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Execute the analysis on AWS Lambda.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nUse EC2 Spot Instances.\n\nSpot Instances allow you to bid on spare Amazon EC2 computing capacity. Because Spot Instances can be interrupted with two minutes of notification when EC2 needs the capacity back, they are a good choice for applications that have flexible start and end times. The described task is stateless and can be started or stopped at any time, which makes it a good fit for Spot Instances, offering potential cost savings.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse EC2 Reserved Instances.\n\nReserved Instances provide a capacity reservation and offer significant discounts on the hourly charge for an instance. However, they are suited for predictable workloads or capacity reservations, which does not align with the dynamic nature of the task described.\n\n\n\n\nUse EC2 On-Demand Instances.\n\nOn-Demand Instances let you pay for compute capacity by the hour with no long-term commitments, but they are more expensive than Spot Instances. Given the nature of the task, Spot Instances would be more cost-effective.\n\n\n\n\nExecute the analysis on AWS Lambda.\n\nAWS Lambda allows you to run code without provisioning or managing servers. However, Lambda has a maximum execution time of 15 minutes, which is not suitable for a task that takes over an hour to complete.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use EC2 Spot Instances.",
          "explanation": "Spot Instances allow you to bid on spare Amazon EC2 computing capacity. Because Spot Instances can be interrupted with two minutes of notification when EC2 needs the capacity back, they are a good choice for applications that have flexible start and end times. The described task is stateless and can be started or stopped at any time, which makes it a good fit for Spot Instances, offering potential cost savings."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use EC2 Reserved Instances.",
          "explanation": "Reserved Instances provide a capacity reservation and offer significant discounts on the hourly charge for an instance. However, they are suited for predictable workloads or capacity reservations, which does not align with the dynamic nature of the task described."
        },
        {
          "answer": "Use EC2 On-Demand Instances.",
          "explanation": "On-Demand Instances let you pay for compute capacity by the hour with no long-term commitments, but they are more expensive than Spot Instances. Given the nature of the task, Spot Instances would be more cost-effective."
        },
        {
          "answer": "Execute the analysis on AWS Lambda.",
          "explanation": "AWS Lambda allows you to run code without provisioning or managing servers. However, Lambda has a maximum execution time of 15 minutes, which is not suitable for a task that takes over an hour to complete."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html"
      ]
    },
    {
      "id": 35,
      "question": "A multinational corporation uses AWS Organizations to establish separate AWS accounts for each department, allowing each to independently manage their account as required. A critical alert sent to the root user email of one account was overlooked by the recipient. The company wishes to ensure all future alerts are promptly addressed, while restricting notifications to only account administrators.\n\nWhat approach will fulfill these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Configure all AWS account root user email alerts to be sent to a single administrator responsible for monitoring and forwarding alerts to the relevant teams.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Set up all AWS account root user emails as mailing lists that are directed to a select group of administrators capable of handling alerts. Also, set up AWS account alternative contacts either via the AWS Organizations console or programmatically.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Set up all existing and new AWS accounts to use a common root user email address. Also, configure AWS account alternative contacts either via the AWS Organizations console or programmatically.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Configure the corporate email server to forward alert email messages received on the root user email address of each AWS account to all organization members.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nSet up all AWS account root user emails as mailing lists that are directed to a select group of administrators capable of handling alerts. Also, set up AWS account alternative contacts either via the AWS Organizations console or programmatically.\n\nSetting up the root user emails as mailing lists allows the alerts to be distributed to a group of administrators who are responsible for handling and responding to the alerts. This ensures that the critical alerts are not overlooked or missed by a single recipient.\n\nConfiguring AWS account alternative contacts provides an additional layer of notification by specifying additional email addresses or phone numbers that can receive alerts. This allows for redundancy and ensures that alerts reach the designated administrators even if there are issues with the root user email communication.\n\nThis approach helps to improve the responsiveness and reliability of alert handling within the organization while ensuring that only authorized administrators receive the notifications.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure the corporate email server to forward alert email messages received on the root user email address of each AWS account to all organization members.\n\nThis option could result in important alerts being lost in the volume of emails, and not all organization members may be equipped to handle these alerts.\n\n\n\n\nConfigure all AWS account root user email alerts to be sent to a single administrator responsible for monitoring and forwarding alerts to the relevant teams.\n\nThis puts too much responsibility on a single individual and if they are unavailable, alerts could be missed.\n\n\n\n\nSet up all existing and new AWS accounts to use a common root user email address. Also, configure AWS account alternative contacts either via the AWS Organizations console or programmatically.\n\nWhile this provides a common point of receipt for alerts, it could lead to confusion and delay in handling alerts specific to each account.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices_mgmt-acct.html",
      "correctAnswerExplanations": [
        {
          "answer": "Set up all AWS account root user emails as mailing lists that are directed to a select group of administrators capable of handling alerts. Also, set up AWS account alternative contacts either via the AWS Organizations console or programmatically.",
          "explanation": "Setting up the root user emails as mailing lists allows the alerts to be distributed to a group of administrators who are responsible for handling and responding to the alerts. This ensures that the critical alerts are not overlooked or missed by a single recipient."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Configure the corporate email server to forward alert email messages received on the root user email address of each AWS account to all organization members.",
          "explanation": "This option could result in important alerts being lost in the volume of emails, and not all organization members may be equipped to handle these alerts."
        },
        {
          "answer": "Configure all AWS account root user email alerts to be sent to a single administrator responsible for monitoring and forwarding alerts to the relevant teams.",
          "explanation": "This puts too much responsibility on a single individual and if they are unavailable, alerts could be missed."
        },
        {
          "answer": "Set up all existing and new AWS accounts to use a common root user email address. Also, configure AWS account alternative contacts either via the AWS Organizations console or programmatically.",
          "explanation": "While this provides a common point of receipt for alerts, it could lead to confusion and delay in handling alerts specific to each account."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices_mgmt-acct.html"
      ]
    },
    {
      "id": 36,
      "question": "A company operates a multi-tiered web application on several Amazon EC2 instances. The application receives data from an Amazon SQS queue, processes it, stores it in an Amazon RDS table, and then removes the message from the queue. The RDS table occasionally has duplicate entries, even though the SQS queue does not contain any duplicate messages.\n\nHow can a solutions architect ensure that each message is processed only once?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use the ReceiveMessage API call with a suitable MaxNumberOfMessages value.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use the ChangeMessageVisibility API call to increase the visibility timeout.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Use the PurgeQueue API call to remove all messages from the queue.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use the SendMessage API call with the appropriate DelaySeconds parameter.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nUse the ChangeMessageVisibility API call to increase the visibility timeout.\n\nThis option ensures that messages are processed only once by increasing the visibility timeout, which prevents other consumers from processing the same message before the current consumer has finished processing it. By extending the visibility timeout, the solutions architect can avoid duplicate processing that leads to duplicate entries in the RDS table.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse the PurgeQueue API call to remove all messages from the queue.\n\nThis option will delete all messages from the queue, including unprocessed messages, which does not solve the issue of duplicate processing.\n\n\n\n\nUse the SendMessage API call with the appropriate DelaySeconds parameter.\n\nThis option adds a delay before a message becomes visible to consumers, but it does not address the issue of duplicate processing once the message is visible.\n\n\n\n\nUse the ReceiveMessage API call with a suitable MaxNumberOfMessages value.\n\nThis option adjusts the maximum number of messages returned by a single call, but it does not prevent duplicate processing of messages by different consumers.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use the ChangeMessageVisibility API call to increase the visibility timeout.",
          "explanation": "This option ensures that messages are processed only once by increasing the visibility timeout, which prevents other consumers from processing the same message before the current consumer has finished processing it. By extending the visibility timeout, the solutions architect can avoid duplicate processing that leads to duplicate entries in the RDS table."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use the PurgeQueue API call to remove all messages from the queue.",
          "explanation": "This option will delete all messages from the queue, including unprocessed messages, which does not solve the issue of duplicate processing."
        },
        {
          "answer": "Use the SendMessage API call with the appropriate DelaySeconds parameter.",
          "explanation": "This option adds a delay before a message becomes visible to consumers, but it does not address the issue of duplicate processing once the message is visible."
        },
        {
          "answer": "Use the ReceiveMessage API call with a suitable MaxNumberOfMessages value.",
          "explanation": "This option adjusts the maximum number of messages returned by a single call, but it does not prevent duplicate processing of messages by different consumers."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html"
      ]
    },
    {
      "id": 37,
      "question": "A startup is launching a three-tier web application on AWS. The application consists of a web frontend, an application layer, and a database layer. The frontend should be publicly accessible, but the application and database layers should be isolated from the public internet. The application layer needs internet access to interact with an external REST API. The application needs to be designed for high availability.\n\nWhich combination of configuration options will satisfy these requirements? (Select TWO.)",
      "corrects": [
        1,
        5
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Set up a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnets.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Set up a VPC with two private subnets and two NAT gateways across two Availability Zones. Deploy a Network Load Balancer in the private subnets.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Set up a VPC with one public subnet, one private subnet, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnet.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use an Auto Scaling group to launch the application layer on EC2 instances in public subnets across two Availability Zones. Use an RDS Multi-AZ DB instance in private subnets for the database layer.",
          "correct": false
        },
        {
          "id": 5,
          "answer": "Deploy the application layer on EC2 instances within private subnets using an Auto Scaling group. Use an RDS Multi-AZ DB instance in private subnets for the database layer.",
          "correct": true
        }
      ],
      "multiple": true,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nDeploy the application layer on EC2 instances within private subnets using an Auto Scaling group. Use an RDS Multi-AZ DB instance in private subnets for the database layer.\n\nDeploying the application layer on EC2 instances within private subnets using an Auto Scaling group ensures that the application layer is isolated from the public internet. By placing the RDS Multi-AZ DB instance in private subnets, the database layer is also isolated. This configuration provides security and isolation for the sensitive database layer.\n\n\n\n\nSet up a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnets.\n\nSetting up a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones allows for a high availability configuration. Deploying an Application Load Balancer (ALB) in the public subnets enables public access to the frontend, satisfying the requirement for public accessibility. The private subnets and NAT gateways provide internet access for the application layer to interact with the external REST API while maintaining isolation from the public internet.\n\nBy combining these solutions, the web application achieves the desired architecture with a publicly accessible frontend and isolated application and database layers, ensuring high availability and security.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up a VPC with two private subnets and two NAT gateways across two Availability Zones. Deploy a Network Load Balancer in the private subnets.\n\nWhile this setup provides internet access to the backend tiers and high availability, it does not provide public access to the web frontend, which is a requirement.\n\n\n\n\nUse an Auto Scaling group to launch the application layer on EC2 instances in public subnets across two Availability Zones. Use an RDS Multi-AZ DB instance in private subnets for the database layer.\n\nThis setup exposes the application layer to the public internet, which violates the requirement for isolation from the public internet.\n\n\n\n\nSet up a VPC with one public subnet, one private subnet, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnet.\n\nThis setup lacks sufficient redundancy to ensure high availability as it only involves a single private subnet.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/configure-subnets.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html",
      "correctAnswerExplanations": [
        {
          "answer": "Deploy the application layer on EC2 instances within private subnets using an Auto Scaling group. Use an RDS Multi-AZ DB instance in private subnets for the database layer.",
          "explanation": "Deploying the application layer on EC2 instances within private subnets using an Auto Scaling group ensures that the application layer is isolated from the public internet. By placing the RDS Multi-AZ DB instance in private subnets, the database layer is also isolated. This configuration provides security and isolation for the sensitive database layer."
        },
        {
          "answer": "Set up a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnets.",
          "explanation": "Setting up a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones allows for a high availability configuration. Deploying an Application Load Balancer (ALB) in the public subnets enables public access to the frontend, satisfying the requirement for public accessibility. The private subnets and NAT gateways provide internet access for the application layer to interact with the external REST API while maintaining isolation from the public internet."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Set up a VPC with two private subnets and two NAT gateways across two Availability Zones. Deploy a Network Load Balancer in the private subnets.",
          "explanation": "While this setup provides internet access to the backend tiers and high availability, it does not provide public access to the web frontend, which is a requirement."
        },
        {
          "answer": "Use an Auto Scaling group to launch the application layer on EC2 instances in public subnets across two Availability Zones. Use an RDS Multi-AZ DB instance in private subnets for the database layer.",
          "explanation": "This setup exposes the application layer to the public internet, which violates the requirement for isolation from the public internet."
        },
        {
          "answer": "Set up a VPC with one public subnet, one private subnet, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnet.",
          "explanation": "This setup lacks sufficient redundancy to ensure high availability as it only involves a single private subnet."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/vpc/latest/userguide/configure-subnets.html",
        "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
        "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html"
      ]
    },
    {
      "id": 38,
      "question": "A company's web application is hosted on on-premises servers in Asia. The company plans to expand its customer base in North America and wants to improve the application's performance for the new users. The application's backend must remain in Asia. The expansion is happening soon, and a quick solution is required.\n\nWhat should the solutions architect recommend?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Launch an Amazon EC2 instance in the ap-northeast-1 region and migrate the application to it.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Migrate the web application to Amazon S3. Use Cross-Region Replication between regions.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use an Amazon Route 53 geolocation routing policy pointing to on-premises servers.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nUse Amazon CloudFront with a custom origin pointing to the on-premises servers.\n\nAmazon CloudFront is a global Content Delivery Network (CDN) that accelerates the delivery of web content to users by caching and serving content from edge locations closer to the users. By using CloudFront with a custom origin pointing to the on-premises servers, the company can optimize site loading times for users in North America without moving the application's backend from Asia.\n\n\n\n\n\n\n\nIncorrect Options:\n\nLaunch an Amazon EC2 instance in the ap-northeast-1 region and migrate the application to it.\n\nLaunching an EC2 instance in the same region as the on-premises servers will not improve the application's performance for users in North America.\n\n\n\n\nMigrate the web application to Amazon S3. Use Cross-Region Replication between regions.\n\nWhile Cross-Region Replication can help distribute static content, it doesn't address the need to maintain the application's backend in Asia. Additionally, it does not provide the caching and edge location benefits that CloudFront offers.\n\n\n\n\nUse an Amazon Route 53 geolocation routing policy pointing to on-premises servers.\n\nAlthough Route 53 geolocation routing can help direct traffic based on the user's location, it does not provide the caching and edge location benefits that CloudFront offers to optimize site loading times.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers.",
          "explanation": "Amazon CloudFront is a global Content Delivery Network (CDN) that accelerates the delivery of web content to users by caching and serving content from edge locations closer to the users. By using CloudFront with a custom origin pointing to the on-premises servers, the company can optimize site loading times for users in North America without moving the application's backend from Asia."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Launch an Amazon EC2 instance in the ap-northeast-1 region and migrate the application to it.",
          "explanation": "Launching an EC2 instance in the same region as the on-premises servers will not improve the application's performance for users in North America."
        },
        {
          "answer": "Migrate the web application to Amazon S3. Use Cross-Region Replication between regions.",
          "explanation": "While Cross-Region Replication can help distribute static content, it doesn't address the need to maintain the application's backend in Asia. Additionally, it does not provide the caching and edge location benefits that CloudFront offers."
        },
        {
          "answer": "Use an Amazon Route 53 geolocation routing policy pointing to on-premises servers.",
          "explanation": "Although Route 53 geolocation routing can help direct traffic based on the user's location, it does not provide the caching and edge location benefits that CloudFront offers to optimize site loading times."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html"
      ]
    },
    {
      "id": 39,
      "question": "A solutions architect is tasked with designing an infrastructure for a mission-critical web application running on Amazon EC2 Linux servers. The application must remain available even during large-scale DDoS attacks originating from numerous IP addresses.\n\nWhich steps should the solutions architect take to protect the application from these potential attacks? (Select TWO.)",
      "corrects": [
        4,
        5
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon Inspector to automatically block the attackers.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an AWS Lambda function to automatically add attacker IP addresses to a Security Group deny list.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy set to 80% CPU utilization.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use Amazon CloudFront to deliver both static and dynamic content of the web application.",
          "correct": true
        },
        {
          "id": 5,
          "answer": "Use AWS Shield Advanced to mitigate the DDoS attack.",
          "correct": true
        }
      ],
      "multiple": true,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Options:\n\nUse AWS Shield Advanced to mitigate the DDoS attack.\n\nAWS Shield Advanced provides comprehensive DDoS protection for mission-critical applications. It protects applications against large and sophisticated DDoS attacks which could otherwise lead to application downtime.\n\n\n\n\nUse Amazon CloudFront to deliver both static and dynamic content of the web application.\n\nCloudFront is a content delivery network (CDN) service that can deliver static and dynamic web content. It protects applications by providing a shield against DDoS attacks and by spreading traffic across multiple edge locations, thereby reducing the risk of a single point of failure.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon Inspector to automatically block the attackers.\n\nAmazon Inspector is a security assessment service that helps improve the security and compliance of applications deployed on AWS. It does not have features to automatically block attackers.\n\n\n\n\nCreate an AWS Lambda function to automatically add attacker IP addresses to a Security Group deny list.\n\nWhile it might be possible to build a system that uses Lambda to add IP addresses to a deny list, this approach is not scalable for large-scale DDoS attacks, which often involve thousands or even millions of IP addresses.\n\n\n\n\nUse EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy set to 80% CPU utilization.\n\nWhile Auto Scaling and Spot Instances can help ensure sufficient capacity and cost-effectiveness, they do not directly protect against DDoS attacks.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/ddos-advanced-summary.html\n\nhttps://aws.amazon.com/cloudfront/security/",
      "correctAnswerExplanations": [
        {
          "answer": "Use AWS Shield Advanced to mitigate the DDoS attack.",
          "explanation": "AWS Shield Advanced provides comprehensive DDoS protection for mission-critical applications. It protects applications against large and sophisticated DDoS attacks which could otherwise lead to application downtime."
        },
        {
          "answer": "Use Amazon CloudFront to deliver both static and dynamic content of the web application.",
          "explanation": "CloudFront is a content delivery network (CDN) service that can deliver static and dynamic web content. It protects applications by providing a shield against DDoS attacks and by spreading traffic across multiple edge locations, thereby reducing the risk of a single point of failure."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use Amazon Inspector to automatically block the attackers.",
          "explanation": "Amazon Inspector is a security assessment service that helps improve the security and compliance of applications deployed on AWS. It does not have features to automatically block attackers."
        },
        {
          "answer": "Create an AWS Lambda function to automatically add attacker IP addresses to a Security Group deny list.",
          "explanation": "While it might be possible to build a system that uses Lambda to add IP addresses to a deny list, this approach is not scalable for large-scale DDoS attacks, which often involve thousands or even millions of IP addresses."
        },
        {
          "answer": "Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy set to 80% CPU utilization.",
          "explanation": "While Auto Scaling and Spot Instances can help ensure sufficient capacity and cost-effectiveness, they do not directly protect against DDoS attacks."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/waf/latest/developerguide/ddos-advanced-summary.html",
        "https://aws.amazon.com/cloudfront/security/"
      ]
    },
    {
      "id": 40,
      "question": "A company is running a large analytics workload on AWS, using an unencrypted Amazon Redshift cluster in a Multi-AZ deployment. Regular snapshots are taken from this cluster. The solutions architect is tasked with ensuring that the cluster and snapshots are encrypted from now on.\n\nWhat should the solutions architect do to achieve this?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the snapshots to it. Enable encryption on the Redshift cluster.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS). Restore the encrypted snapshot to the existing cluster.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with AWS Key Management Service (AWS KMS) managed keys (SSE-KMS).",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Create an encrypted copy of the latest cluster snapshot. Replace the existing cluster by restoring from the encrypted snapshot.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nCreate an encrypted copy of the latest cluster snapshot. Replace the existing cluster by restoring from the encrypted snapshot.\n\nWhen creating an encrypted copy of the latest cluster snapshot, the copy will be encrypted using an AWS Key Management Service (AWS KMS). This ensures that the data is protected with strong encryption. By replacing the existing cluster with the restored encrypted snapshot, the new cluster will inherit the encryption properties of the snapshot.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the snapshots to it. Enable encryption on the Redshift cluster.\n\nAmazon EBS volumes are not used with Amazon Redshift. Redshift has its own storage and does not interact with EBS.\n\n\n\n\nCopy the snapshots and enable encryption using AWS Key Management Service (AWS KMS). Restore the encrypted snapshot to the existing cluster.\n\nThis is not possible because you cannot restore an encrypted snapshot to an existing, unencrypted cluster.\n\n\n\n\nCopy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with AWS Key Management Service (AWS KMS) managed keys (SSE-KMS).\n\nThis approach is not applicable because Redshift snapshots cannot be copied to an S3 bucket for the purpose of encryption.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html",
      "correctAnswerExplanations": [
        {
          "answer": "Create an encrypted copy of the latest cluster snapshot. Replace the existing cluster by restoring from the encrypted snapshot.",
          "explanation": "When creating an encrypted copy of the latest cluster snapshot, the copy will be encrypted using an AWS Key Management Service (AWS KMS). This ensures that the data is protected with strong encryption. By replacing the existing cluster with the restored encrypted snapshot, the new cluster will inherit the encryption properties of the snapshot."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the snapshots to it. Enable encryption on the Redshift cluster.",
          "explanation": "Amazon EBS volumes are not used with Amazon Redshift. Redshift has its own storage and does not interact with EBS."
        },
        {
          "answer": "Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS). Restore the encrypted snapshot to the existing cluster.",
          "explanation": "This is not possible because you cannot restore an encrypted snapshot to an existing, unencrypted cluster."
        },
        {
          "answer": "Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with AWS Key Management Service (AWS KMS) managed keys (SSE-KMS).",
          "explanation": "This approach is not applicable because Redshift snapshots cannot be copied to an S3 bucket for the purpose of encryption."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html"
      ]
    },
    {
      "id": 41,
      "question": "A software development company is building a new serverless application. The application will utilize an AWS Lambda function that will be invoked by an Amazon Simple Notification Service (SNS) topic. The solutions architect is required to implement the principle of least privilege when setting up permissions for the function.\n\nWhich approach fulfills these requirements?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Attach a resource-based policy to the function with lambda:InvokeFunction as the action and Service: sns.amazonaws.com as the principal.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Attach a resource-based policy to the function with lambda:* as the action and Service: sns.amazonaws.com as the principal.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Assign an execution role to the function with lambda:InvokeFunction as the action and * as the principal.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Assign an execution role to the function with lambda:InvokeFunction as the action and Service: lambda.amazonaws.com as the principal.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nAttach a resource-based policy to the function with lambda:InvokeFunction as the action and Service: sns.amazonaws.com as the principal.\n\nBy attaching a resource-based policy to the function, we can restrict access to the function only for the authorized principals. In this case, we want to restrict access to the function only to the Amazon SNS topic, so we need to set the Service principal to sns.amazonaws.com. The lambda:InvokeFunction action is the only required permission for the Lambda function to be invoked by the SNS topic, so we should set this as the action in the policy.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAssign an execution role to the function with lambda:InvokeFunction as the action and * as the principal.\n\nAn execution role with a wildcard principal would allow any AWS service to invoke the function, violating the principle of least privilege.\n\n\n\n\nAssign an execution role to the function with lambda:InvokeFunction as the action and Service: lambda.amazonaws.com as the principal.\n\nThe principal should be the service that is invoking the Lambda function (SNS in this case), not the Lambda service itself.\n\n\n\n\nAttach a resource-based policy to the function with lambda:* as the action and Service: sns.amazonaws.com as the principal.\n\nProviding lambda:* as the action would grant SNS all Lambda permissions, which is broader than necessary and violates the principle of least privilege.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/access-control-resource-based.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html",
      "correctAnswerExplanations": [
        {
          "answer": "Attach a resource-based policy to the function with lambda:InvokeFunction as the action and Service: sns.amazonaws.com as the principal.",
          "explanation": "By attaching a resource-based policy to the function, we can restrict access to the function only for the authorized principals. In this case, we want to restrict access to the function only to the Amazon SNS topic, so we need to set the Service principal to sns.amazonaws.com. The lambda:InvokeFunction action is the only required permission for the Lambda function to be invoked by the SNS topic, so we should set this as the action in the policy."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Assign an execution role to the function with lambda:InvokeFunction as the action and * as the principal.",
          "explanation": "An execution role with a wildcard principal would allow any AWS service to invoke the function, violating the principle of least privilege."
        },
        {
          "answer": "Assign an execution role to the function with lambda:InvokeFunction as the action and Service: lambda.amazonaws.com as the principal.",
          "explanation": "The principal should be the service that is invoking the Lambda function (SNS in this case), not the Lambda service itself."
        },
        {
          "answer": "Attach a resource-based policy to the function with lambda:* as the action and Service: sns.amazonaws.com as the principal.",
          "explanation": "Providing lambda:* as the action would grant SNS all Lambda permissions, which is broader than necessary and violates the principle of least privilege."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/lambda/latest/dg/access-control-resource-based.html",
        "https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html"
      ]
    },
    {
      "id": 42,
      "question": "A retail company maintains its inventory data in a database hosted on Amazon RDS. When a product is sold out, the listing must be removed from the online store, and the data should be propagated to multiple internal systems for record-keeping.\n\nWhat architecture should a solutions architect propose for this requirement?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Subscribe to an RDS event notification and send it to an Amazon Simple Queue Service (Amazon SQS) queue, which is fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics. Use AWS Lambda functions to update the internal systems.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Set up an AWS Lambda function that is triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) queue, which the internal systems can consume.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Create an AWS Lambda function that is triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) FIFO queue, which the internal systems can consume.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Subscribe to an RDS event notification and send it to an Amazon Simple Notification Service (Amazon SNS) topic, which is fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the internal systems.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nSubscribe to an RDS event notification and send it to an Amazon Simple Notification Service (Amazon SNS) topic, which is fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the internal systems.\n\nWhen a product is sold out, a database update will be triggered. The RDS event notification will detect this update and send the information to the SNS topic. This topic will then fan out the notification to multiple SQS queues, where multiple Lambda functions can consume the information and update the internal systems.\n\nUsing Amazon SNS and Amazon SQS ensures that the data is distributed to multiple internal systems in a reliable and scalable manner. AWS Lambda can be used to perform any necessary data transformations before sending it to the appropriate internal system.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up an AWS Lambda function that is triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) queue, which the internal systems can consume.\n\nIt doesn't provide the necessary decoupling for handling multiple target systems.\n\n\n\n\nCreate an AWS Lambda function that is triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) FIFO queue, which the internal systems can consume.\n\nFIFO queues might not be necessary and could potentially slow down the system.\n\n\n\n\nSubscribe to an RDS event notification and send it to an Amazon Simple Queue Service (Amazon SQS) queue, which is fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics. Use AWS Lambda functions to update the internal systems.\n\nSQS doesn't support fanning out to multiple SNS topics.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html\n\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html",
      "correctAnswerExplanations": [
        {
          "answer": "Subscribe to an RDS event notification and send it to an Amazon Simple Notification Service (Amazon SNS) topic, which is fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the internal systems.",
          "explanation": "When a product is sold out, a database update will be triggered. The RDS event notification will detect this update and send the information to the SNS topic. This topic will then fan out the notification to multiple SQS queues, where multiple Lambda functions can consume the information and update the internal systems."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Set up an AWS Lambda function that is triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) queue, which the internal systems can consume.",
          "explanation": "It doesn't provide the necessary decoupling for handling multiple target systems."
        },
        {
          "answer": "Create an AWS Lambda function that is triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) FIFO queue, which the internal systems can consume.",
          "explanation": "FIFO queues might not be necessary and could potentially slow down the system."
        },
        {
          "answer": "Subscribe to an RDS event notification and send it to an Amazon Simple Queue Service (Amazon SQS) queue, which is fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics. Use AWS Lambda functions to update the internal systems.",
          "explanation": "SQS doesn't support fanning out to multiple SNS topics."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html",
        "https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html"
      ]
    },
    {
      "id": 43,
      "question": "A university wants to develop a real-time attendance tracking system for its online classes. The system requires an API, real-time data processing to recognize students' names, and a storage solution to store the attendance records.\n\nWhich option provides the LEAST operational overhead for this scenario?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Configure an Amazon API Gateway API to send attendance data to AWS Glue. Use AWS Lambda functions for data processing and AWS Glue to store the processed data in Amazon S3.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Disable source/destination checking on the EC2 instance. Utilize AWS Glue to process the data and store it in Amazon S3.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Set up an Amazon API Gateway API to transmit attendance data to an Amazon Kinesis data stream. Configure an Amazon Kinesis Data Firehose delivery stream using the Kinesis data stream as the source. Employ AWS Lambda functions to process the data and deliver it to Amazon S3 via the Kinesis Data Firehose delivery stream.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Use an Amazon EC2 instance to create an API that sends attendance data to an Amazon Kinesis data stream. Set up an Amazon Kinesis Data Firehose delivery stream using the Kinesis data stream as the source. Use AWS Lambda functions to process the data and send the processed data to Amazon S3 through the Kinesis Data Firehose delivery stream.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nSet up an Amazon API Gateway API to transmit attendance data to an Amazon Kinesis data stream. Configure an Amazon Kinesis Data Firehose delivery stream using the Kinesis data stream as the source. Employ AWS Lambda functions to process the data and deliver it to Amazon S3 via the Kinesis Data Firehose delivery stream.\n\nIt provides the least operational overhead by utilizing managed services like Amazon API Gateway, Amazon Kinesis, and AWS Lambda. The API Gateway handles the API requests, Kinesis data stream manages the real-time data ingestion, Lambda functions process the data, and Kinesis Data Firehose delivery stream sends the processed data to Amazon S3. This architecture leverages serverless components, reducing the management and operational overhead.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse an Amazon EC2 instance to create an API that sends attendance data to an Amazon Kinesis data stream. Set up an Amazon Kinesis Data Firehose delivery stream using the Kinesis data stream as the source. Use AWS Lambda functions to process the data and send the processed data to Amazon S3 through the Kinesis Data Firehose delivery stream.\n\nUsing an EC2 instance to create the API adds unnecessary operational overhead compared to using API Gateway.\n\n\n\n\nDeploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Disable source/destination checking on the EC2 instance. Utilize AWS Glue to process the data and store it in Amazon S3.\n\nIt uses an EC2 instance for the API and disables source/destination checking, increasing the operational overhead and security risks.\n\n\n\n\nConfigure an Amazon API Gateway API to send attendance data to AWS Glue. Use AWS Lambda functions for data processing and AWS Glue to store the processed data in Amazon S3.\n\nIt doesn't provide real-time data processing capabilities, as AWS Glue is not meant for real-time data processing scenarios.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/kinesis/data-firehose/\n\nhttps://aws.amazon.com/lambda/\n\nhttps://aws.amazon.com/api-gateway/",
      "correctAnswerExplanations": [
        {
          "answer": "Set up an Amazon API Gateway API to transmit attendance data to an Amazon Kinesis data stream. Configure an Amazon Kinesis Data Firehose delivery stream using the Kinesis data stream as the source. Employ AWS Lambda functions to process the data and deliver it to Amazon S3 via the Kinesis Data Firehose delivery stream.",
          "explanation": "It provides the least operational overhead by utilizing managed services like Amazon API Gateway, Amazon Kinesis, and AWS Lambda. The API Gateway handles the API requests, Kinesis data stream manages the real-time data ingestion, Lambda functions process the data, and Kinesis Data Firehose delivery stream sends the processed data to Amazon S3. This architecture leverages serverless components, reducing the management and operational overhead."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use an Amazon EC2 instance to create an API that sends attendance data to an Amazon Kinesis data stream. Set up an Amazon Kinesis Data Firehose delivery stream using the Kinesis data stream as the source. Use AWS Lambda functions to process the data and send the processed data to Amazon S3 through the Kinesis Data Firehose delivery stream.",
          "explanation": "Using an EC2 instance to create the API adds unnecessary operational overhead compared to using API Gateway."
        },
        {
          "answer": "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Disable source/destination checking on the EC2 instance. Utilize AWS Glue to process the data and store it in Amazon S3.",
          "explanation": "It uses an EC2 instance for the API and disables source/destination checking, increasing the operational overhead and security risks."
        },
        {
          "answer": "Configure an Amazon API Gateway API to send attendance data to AWS Glue. Use AWS Lambda functions for data processing and AWS Glue to store the processed data in Amazon S3.",
          "explanation": "It doesn't provide real-time data processing capabilities, as AWS Glue is not meant for real-time data processing scenarios."
        }
      ],
      "references": [
        "https://aws.amazon.com/kinesis/data-firehose/",
        "https://aws.amazon.com/lambda/",
        "https://aws.amazon.com/api-gateway/"
      ]
    },
    {
      "id": 44,
      "question": "A video sharing platform allows users to upload video clips to its application. The application, hosted on Amazon EC2 instances, compresses the video files during upload and stores the compressed files in Amazon S3. Users are experiencing slow upload times to the application.\n\nThe company wants to improve the application's performance and reduce tight coupling in the architecture. A solutions architect is required to design the most operationally effective method for video uploads.\n\nWhich combination of actions should the solutions architect suggest to achieve these goals? (Select TWO.)",
      "corrects": [
        3,
        5
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Adjust the application to upload video files directly from each user's browser to Amazon S3 using a presigned URL.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers an AWS Lambda function on a schedule to compress uploaded videos.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Set up S3 Event Notifications to trigger an AWS Lambda function when a video is uploaded. Use the function to compress the video.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Adjust the application to upload videos to S3 Glacier.",
          "correct": false
        },
        {
          "id": 5,
          "answer": "Configure the web server to directly upload the original video files to Amazon S3.",
          "correct": true
        }
      ],
      "multiple": true,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nConfigure the web server to directly upload the original video files to Amazon S3\n\nSet up S3 Event Notifications to trigger an AWS Lambda function when a video is uploaded. Use the function to compress the video.\n\nBy configuring the web server to upload the original video files directly to Amazon S3, the company can eliminate the processing load of compressing the video files from the EC2 instances, thereby improving the performance of the application. Additionally, by using S3 Event Notifications to trigger an AWS Lambda function to compress the video upon upload, the company can effectively decouple the compression process from the upload process, further improving performance and scalability.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAdjust the application to upload videos to S3 Glacier.\n\nS3 Glacier is a storage class for long-term archiving and does not offer immediate accessibility, making it unsuitable for this use case.\n\n\n\n\nAdjust the application to upload video files directly from each user's browser to Amazon S3 using a presigned URL.\n\nWhile this option can help improve upload speed, it does not address the requirement to reduce coupling and improve the operational efficiency of video compression.\n\n\n\n\nCreate an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers an AWS Lambda function on a schedule to compress uploaded videos.\n\nWhile this could potentially work, it is less efficient than triggering the Lambda function immediately upon file upload via S3 Event Notifications.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3.html",
      "correctAnswerExplanations": [],
      "incorrectAnswerExplanations": [
        {
          "answer": "Configure the web server to directly upload the original video files to Amazon S3",
          "explanation": "<strong>Set up S3 Event Notifications to trigger an AWS Lambda function when a video is uploaded. Use the function to compress the video.</strong>"
        },
        {
          "answer": "Adjust the application to upload videos to S3 Glacier.",
          "explanation": "S3 Glacier is a storage class for long-term archiving and does not offer immediate accessibility, making it unsuitable for this use case."
        },
        {
          "answer": "Adjust the application to upload video files directly from each user's browser to Amazon S3 using a presigned URL.",
          "explanation": "While this option can help improve upload speed, it does not address the requirement to reduce coupling and improve the operational efficiency of video compression."
        },
        {
          "answer": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers an AWS Lambda function on a schedule to compress uploaded videos.",
          "explanation": "While this could potentially work, it is less efficient than triggering the Lambda function immediately upon file upload via S3 Event Notifications."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html",
        "https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html"
      ]
    },
    {
      "id": 45,
      "question": "A multimedia company is developing an application on Amazon EC2 instances across multiple Availability Zones. The application will allow users to access a library of video content, estimated to be about 1 PB in size. The company expects that the application will face high traffic during peak hours. A solutions architect needs to ensure that the storage solution for the video content can scale to meet the application's demand at any time. The company is also concerned about managing the costs of the solution efficiently.\n\nWhich storage solution fulfills these requirements most cost-effectively?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Amazon Elastic File System (Amazon EFS)",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Amazon FSx for Lustre",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Amazon Elastic Block Store (Amazon EBS)",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Amazon S3",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nAmazon S3\n\nAmazon S3 is the most cost-effective solution for this scenario. S3 is designed for 99.999999999% (11 9's) of durability, and it is ideal for storing and retrieving any amount of data, at any time, from anywhere. It is perfect for large static files such as multimedia content. Also, S3 can scale automatically to handle high traffic demands, thus fulfilling the requirement for scalability during high demand periods.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAmazon Elastic Block Store (Amazon EBS)\n\nEBS provides block-level storage volumes for use with EC2 instances but is not designed for storing large amounts of static content like videos. It also has size limitations, and the cost per GB is typically higher than that of Amazon S3.\n\n\n\n\nAmazon Elastic File System (Amazon EFS)\n\nAlthough EFS is scalable and can handle large amounts of data, it is more expensive than S3 and is better suited for workloads that require a shared file system, not for serving static content like videos.\n\n\n\n\nAmazon FSx for Lustre\n\nFSx for Lustre is designed for fast processing of workloads, rather than for serving static content like videos. It's also more expensive than S3.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html",
      "correctAnswerExplanations": [
        {
          "answer": "Amazon S3",
          "explanation": "Amazon S3 is the most cost-effective solution for this scenario. S3 is designed for 99.999999999% (11 9's) of durability, and it is ideal for storing and retrieving any amount of data, at any time, from anywhere. It is perfect for large static files such as multimedia content. Also, S3 can scale automatically to handle high traffic demands, thus fulfilling the requirement for scalability during high demand periods."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Amazon Elastic Block Store (Amazon EBS)",
          "explanation": "EBS provides block-level storage volumes for use with EC2 instances but is not designed for storing large amounts of static content like videos. It also has size limitations, and the cost per GB is typically higher than that of Amazon S3."
        },
        {
          "answer": "Amazon Elastic File System (Amazon EFS)",
          "explanation": "Although EFS is scalable and can handle large amounts of data, it is more expensive than S3 and is better suited for workloads that require a shared file system, not for serving static content like videos."
        },
        {
          "answer": "Amazon FSx for Lustre",
          "explanation": "FSx for Lustre is designed for fast processing of workloads, rather than for serving static content like videos. It's also more expensive than S3."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html"
      ]
    },
    {
      "id": 46,
      "question": "A software company operates a multi-tiered application with a MySQL database storing user data. The application is containerized and runs on several Linux servers. Currently, the company is struggling with the operational burden of managing the infrastructure and capacity planning, which hampers their growth. A solutions architect is tasked to improve the application's infrastructure.\n\nWhich combination of actions should the solutions architect take to accomplish this? (Select TWO.)",
      "corrects": [
        1,
        5
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Migrate the MySQL database to Amazon RDS.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Use Amazon ElastiCache to create a caching layer between the application and the MySQL database.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Host the application on AWS Elastic Beanstalk.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Implement Amazon CloudFront for the application content distribution.",
          "correct": false
        },
        {
          "id": 5,
          "answer": "Host the application on AWS Fargate with Amazon Elastic Kubernetes Service (EKS).",
          "correct": true
        }
      ],
      "multiple": true,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nMigrate the MySQL database to Amazon RDS.\n\nBy migrating the MySQL database to Amazon RDS, the software company can offload the operational burden of managing the database infrastructure. Amazon RDS provides a managed database service that handles tasks such as backups, patching, and automated software updates, allowing the company to focus on application development rather than database management.\n\nHost the application on AWS Fargate with Amazon Elastic Kubernetes Service (EKS).\n\nAWS Fargate eliminates the need to manage the underlying infrastructure, such as EC2 instances, and automatically scales resources based on application demand. Amazon EKS provides a managed Kubernetes service, simplifying the deployment and management of containerized applications.\n\n\n\n\n\n\n\nIncorrect Options:\n\nHost the application on AWS Elastic Beanstalk.\n\nWhile Elastic Beanstalk could simplify application deployment and management, it doesn't offer the specific benefits of containerized infrastructure that the company already uses, as offered by Fargate with EKS.\n\n\n\n\nImplement Amazon CloudFront for the application content distribution.\n\nCloudFront is a content delivery network (CDN) service, which may enhance the application's performance but doesn't address the main issue of operational overhead.\n\n\n\n\nUse Amazon ElastiCache to create a caching layer between the application and the MySQL database.\n\nWhile ElastiCache could improve application performance, it doesn't specifically address the issues of infrastructure management and capacity planning.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rds/mysql/\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/fargate.html",
      "correctAnswerExplanations": [
        {
          "answer": "Migrate the MySQL database to Amazon RDS.",
          "explanation": "By migrating the MySQL database to Amazon RDS, the software company can offload the operational burden of managing the database infrastructure. Amazon RDS provides a managed database service that handles tasks such as backups, patching, and automated software updates, allowing the company to focus on application development rather than database management."
        },
        {
          "answer": "Host the application on AWS Fargate with Amazon Elastic Kubernetes Service (EKS).",
          "explanation": "AWS Fargate eliminates the need to manage the underlying infrastructure, such as EC2 instances, and automatically scales resources based on application demand. Amazon EKS provides a managed Kubernetes service, simplifying the deployment and management of containerized applications."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Host the application on AWS Elastic Beanstalk.",
          "explanation": "While Elastic Beanstalk could simplify application deployment and management, it doesn't offer the specific benefits of containerized infrastructure that the company already uses, as offered by Fargate with EKS."
        },
        {
          "answer": "Implement Amazon CloudFront for the application content distribution.",
          "explanation": "CloudFront is a content delivery network (CDN) service, which may enhance the application's performance but doesn't address the main issue of operational overhead."
        },
        {
          "answer": "Use Amazon ElastiCache to create a caching layer between the application and the MySQL database.",
          "explanation": "While ElastiCache could improve application performance, it doesn't specifically address the issues of infrastructure management and capacity planning."
        }
      ],
      "references": [
        "https://aws.amazon.com/rds/mysql/",
        "https://docs.aws.amazon.com/eks/latest/userguide/fargate.html"
      ]
    },
    {
      "id": 47,
      "question": "A startup needs to automate the process of rotating their API keys for an Amazon Elasticsearch Service domain across multiple AWS Regions every month.\n\nWhich solution offers the LEAST operational overhead to accomplish this task?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Store the API keys in an Amazon S3 bucket with server-side encryption enabled. Use Amazon EventBridge (Amazon CloudWatch Events) to trigger an AWS Lambda function for rotating the keys.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Save the API keys as secure string parameters in AWS Systems Manager Parameter Store. Enable multi-Region secret replication and schedule automatic rotation using Parameter Store.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Encrypt the API keys using AWS Key Management Service (AWS KMS) multi-Region customer managed keys. Save the keys in an Amazon DynamoDB global table and create an AWS Lambda function to fetch the keys from DynamoDB. Rotate the keys using the Elasticsearch API.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Store the API keys in AWS Secrets Manager with multi-Region secret replication enabled. Schedule automatic rotation for the secrets.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nStore the API keys in AWS Secrets Manager with multi-Region secret replication enabled. Schedule automatic rotation for the secrets.\n\nAWS Secrets Manager is designed to store and manage sensitive information like API keys. By enabling multi-Region secret replication, the API keys are automatically replicated to the required Regions. Secrets Manager also provides built-in support for rotating secrets on a schedule, which reduces operational overhead. This solution fulfills the requirement of automating API key rotation for the Elasticsearch domain across multiple AWS Regions.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSave the API keys as secure string parameters in AWS Systems Manager Parameter Store. Enable multi-Region secret replication and schedule automatic rotation using Parameter Store.\n\nAlthough AWS Systems Manager Parameter Store can store secure string parameters, it does not support automatic rotation of secrets. This makes it an unsuitable choice for the given requirement.\n\n\n\n\nStore the API keys in an Amazon S3 bucket with server-side encryption enabled. Use Amazon EventBridge (Amazon CloudWatch Events) to trigger an AWS Lambda function for rotating the keys.\n\nWhile Amazon S3 provides encryption, it is not designed for storing and managing secrets like API keys. Additionally, this approach requires custom Lambda functions for rotating the keys, leading to higher operational overhead.\n\n\n\n\nEncrypt the API keys using AWS Key Management Service (AWS KMS) multi-Region customer managed keys. Save the keys in an Amazon DynamoDB global table and create an AWS Lambda function to fetch the keys from DynamoDB. Rotate the keys using the Elasticsearch API.\n\nAWS KMS is used for encryption and decryption, but it does not manage secrets. Storing the keys in a DynamoDB global table also increases the operational complexity, as custom Lambda functions are required for key rotation.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html",
      "correctAnswerExplanations": [
        {
          "answer": "Store the API keys in AWS Secrets Manager with multi-Region secret replication enabled. Schedule automatic rotation for the secrets.",
          "explanation": "AWS Secrets Manager is designed to store and manage sensitive information like API keys. By enabling multi-Region secret replication, the API keys are automatically replicated to the required Regions. Secrets Manager also provides built-in support for rotating secrets on a schedule, which reduces operational overhead. This solution fulfills the requirement of automating API key rotation for the Elasticsearch domain across multiple AWS Regions."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Save the API keys as secure string parameters in AWS Systems Manager Parameter Store. Enable multi-Region secret replication and schedule automatic rotation using Parameter Store.",
          "explanation": "Although AWS Systems Manager Parameter Store can store secure string parameters, it does not support automatic rotation of secrets. This makes it an unsuitable choice for the given requirement."
        },
        {
          "answer": "Store the API keys in an Amazon S3 bucket with server-side encryption enabled. Use Amazon EventBridge (Amazon CloudWatch Events) to trigger an AWS Lambda function for rotating the keys.",
          "explanation": "While Amazon S3 provides encryption, it is not designed for storing and managing secrets like API keys. Additionally, this approach requires custom Lambda functions for rotating the keys, leading to higher operational overhead."
        },
        {
          "answer": "Encrypt the API keys using AWS Key Management Service (AWS KMS) multi-Region customer managed keys. Save the keys in an Amazon DynamoDB global table and create an AWS Lambda function to fetch the keys from DynamoDB. Rotate the keys using the Elasticsearch API.",
          "explanation": "AWS KMS is used for encryption and decryption, but it does not manage secrets. Storing the keys in a DynamoDB global table also increases the operational complexity, as custom Lambda functions are required for key rotation."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html"
      ]
    },
    {
      "id": 48,
      "question": "A streaming service company runs its video content on two Amazon EC2 instances. The company uses its own SSL certificate on each instance for SSL termination. Recently, due to a surge in subscribers, SSL encryption and decryption have been maxing out the compute capacity of the servers.\n\nWhat could a solutions architect do to boost the application's performance?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Import the SSL certificate into AWS Certificate Manager (ACM). Set up a Network Load Balancer with an HTTPS listener that uses the SSL certificate from ACM.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Set up an additional EC2 instance as a reverse proxy. Move the SSL certificate to the new instance and configure it to route connections to the existing EC2 instances.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Transfer the SSL certificate to an Amazon S3 bucket. Configure the EC2 instances to use the S3 bucket for SSL termination.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Create a new SSL certificate using AWS Certificate Manager (ACM) and install the ACM certificate on each instance.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nImport the SSL certificate into AWS Certificate Manager (ACM). Set up a Network Load Balancer with an HTTPS listener that uses the SSL certificate from ACM.\n\nBy importing the SSL certificate into ACM, you can take advantage of the integrated certificate management and automated renewal provided by ACM. This reduces the operational overhead of managing SSL certificates manually.\n\nSetting up an NLB with an HTTPS listener allows for SSL termination at the load balancer level, offloading the SSL encryption and decryption from the EC2 instances. The NLB can handle the compute-intensive SSL operations, distributing the workload across multiple instances, and improving the overall performance and scalability of the application.\n\nThe NLB acts as a reverse proxy, receiving incoming requests and forwarding them to the EC2 instances. With SSL termination happening at the NLB, the EC2 instances can focus on processing the application logic and serving the video content, leading to improved performance and increased capacity.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate a new SSL certificate using AWS Certificate Manager (ACM) and install the ACM certificate on each instance.\n\nEven though ACM manages SSL certificates effectively, this option doesn't solve the issue of SSL termination overwhelming the EC2 instances' compute capacity.\n\n\n\n\nTransfer the SSL certificate to an Amazon S3 bucket. Configure the EC2 instances to use the S3 bucket for SSL termination.\n\nWhile it's possible to store SSL certificates in S3, this option doesn't address the core issue, which is the SSL termination consuming the compute capacity of the EC2 instances.\n\n\n\n\nSet up an additional EC2 instance as a reverse proxy. Move the SSL certificate to the new instance and configure it to route connections to the existing EC2 instances.\n\nWhile this might seem like a viable option, it doesn't effectively address the issue. It simply moves the SSL termination workload to another EC2 instance rather than offloading it entirely.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/elastic-load-balancer-support-for-ssl-termination/",
      "correctAnswerExplanations": [
        {
          "answer": "Import the SSL certificate into AWS Certificate Manager (ACM). Set up a Network Load Balancer with an HTTPS listener that uses the SSL certificate from ACM.",
          "explanation": "By importing the SSL certificate into ACM, you can take advantage of the integrated certificate management and automated renewal provided by ACM. This reduces the operational overhead of managing SSL certificates manually."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Create a new SSL certificate using AWS Certificate Manager (ACM) and install the ACM certificate on each instance.",
          "explanation": "Even though ACM manages SSL certificates effectively, this option doesn't solve the issue of SSL termination overwhelming the EC2 instances' compute capacity."
        },
        {
          "answer": "Transfer the SSL certificate to an Amazon S3 bucket. Configure the EC2 instances to use the S3 bucket for SSL termination.",
          "explanation": "While it's possible to store SSL certificates in S3, this option doesn't address the core issue, which is the SSL termination consuming the compute capacity of the EC2 instances."
        },
        {
          "answer": "Set up an additional EC2 instance as a reverse proxy. Move the SSL certificate to the new instance and configure it to route connections to the existing EC2 instances.",
          "explanation": "While this might seem like a viable option, it doesn't effectively address the issue. It simply moves the SSL termination workload to another EC2 instance rather than offloading it entirely."
        }
      ],
      "references": [
        "https://aws.amazon.com/blogs/aws/elastic-load-balancer-support-for-ssl-termination/"
      ]
    },
    {
      "id": 49,
      "question": "A software firm's containerized application is hosted on an Amazon EC2 instance. The application needs to retrieve private keys before communicating securely with other components of the system. The firm wants a highly secure solution that can encrypt and decrypt the keys in near real-time. The solution also needs to store the encrypted data in a highly available storage solution.\n\nWhich solution will satisfy these requirements with the LEAST operational overhead?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Develop an AWS Lambda function that uses the Python cryptography library to receive and perform encryption operations. Store the function in an Amazon S3 bucket.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use AWS Secrets Manager secrets for encrypted keys. Update the keys manually as needed. Control access to the data by using granular IAM access.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon Elastic Block Store (Amazon EBS) volumes.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon S3.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nCreate an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon S3.\n\nAWS KMS is a fully managed service that allows you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. Storing the encrypted data on Amazon S3 is an efficient and highly available storage solution, which is suitable for this requirement. This option provides a highly secure solution that can encrypt and decrypt the keys in near real-time, while also meeting the requirement of storing encrypted data in a highly available storage solution.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS Secrets Manager secrets for encrypted keys. Update the keys manually as needed. Control access to the data by using granular IAM access.\n\nWhile AWS Secrets Manager is a good service for managing secrets, it's not designed for real-time encryption and decryption operations which are required in this scenario.\n\n\n\n\nDevelop an AWS Lambda function that uses the Python cryptography library to receive and perform encryption operations. Store the function in an Amazon S3 bucket.\n\nThis solution would require considerable operational overhead because a custom solution would need to be built and maintained.\n\n\n\n\nCreate an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon Elastic Block Store (Amazon EBS) volumes.\n\nStoring encrypted data on Amazon EBS volumes is not highly available since EBS volumes can only be attached to one EC2 instance at a time. Additionally, it would not be as efficient as using Amazon S3 for storing encrypted data and Amazon S3 for storage is a better choice for data durability and availability.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/kms/\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#key-mgmt\n\nhttps://aws.amazon.com/blogs/storage/architecting-for-high-availability-on-amazon-s3/",
      "correctAnswerExplanations": [
        {
          "answer": "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon S3.",
          "explanation": "AWS KMS is a fully managed service that allows you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. Storing the encrypted data on Amazon S3 is an efficient and highly available storage solution, which is suitable for this requirement. This option provides a highly secure solution that can encrypt and decrypt the keys in near real-time, while also meeting the requirement of storing encrypted data in a highly available storage solution."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use AWS Secrets Manager secrets for encrypted keys. Update the keys manually as needed. Control access to the data by using granular IAM access.",
          "explanation": "While AWS Secrets Manager is a good service for managing secrets, it's not designed for real-time encryption and decryption operations which are required in this scenario."
        },
        {
          "answer": "Develop an AWS Lambda function that uses the Python cryptography library to receive and perform encryption operations. Store the function in an Amazon S3 bucket.",
          "explanation": "This solution would require considerable operational overhead because a custom solution would need to be built and maintained."
        },
        {
          "answer": "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon Elastic Block Store (Amazon EBS) volumes.",
          "explanation": "Storing encrypted data on Amazon EBS volumes is not highly available since EBS volumes can only be attached to one EC2 instance at a time. Additionally, it would not be as efficient as using Amazon S3 for storing encrypted data and Amazon S3 for storage is a better choice for data durability and availability."
        }
      ],
      "references": [
        "https://aws.amazon.com/kms/",
        "https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#key-mgmt",
        "https://aws.amazon.com/blogs/storage/architecting-for-high-availability-on-amazon-s3/"
      ]
    },
    {
      "id": 50,
      "question": "A multinational corporation runs a complex data analytics workload using a variety of AWS services, including Amazon Redshift for data warehousing, Amazon EMR for big data processing, and AWS Glue for ETL jobs. The data processed is highly variable, with peak loads occurring at the end of each quarter. The company wants to optimize its costs without compromising the performance and completion times of these critical workloads.\n\nWhich combination of strategies would MOST effectively optimize costs while maintaining the desired performance?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Purchase Reserved Instances for Amazon Redshift and EMR clusters to handle peak loads, and use On-Demand Instances for off-peak workloads. Enable auto-pause for the Redshift clusters to save costs when not in use.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use On-Demand Instances for Amazon EMR and Redshift clusters during peak loads, and switch to Spot Instances during off-peak times. Utilize AWS Glue job bookmarks to handle job failures due to Spot Interruptions.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use Spot Instances exclusively for Amazon EMR and Redshift clusters, and leverage AWS Glue job recovery options to handle Spot Interruptions.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use Savings Plans for Amazon EMR and AWS Glue, use Redshift Concurrency Scaling for peak loads, and enable automatic workload management in Redshift to balance query loads.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nUse Savings Plans for Amazon EMR and AWS Glue, use Redshift Concurrency Scaling for peak loads, and enable automatic workload management in Redshift to balance query loads.\n\nBy using Savings Plans for Amazon EMR and AWS Glue, the multinational corporation can benefit from cost savings based on their committed usage. Savings Plans provide flexibility and cost optimization for EC2 instances used by EMR and Glue. This helps reduce costs without compromising performance during regular workloads.\n\nAdditionally, using Redshift Concurrency Scaling for peak loads allows the company to dynamically add or remove clusters based on demand. This ensures optimal performance during high-demand periods while reducing costs during off-peak times.\n\nEnabling automatic workload management in Redshift helps balance query loads across the available resources, further optimizing performance and cost efficiency. This feature intelligently manages query queues and resource allocation to ensure smooth operation and efficient utilization of resources.\n\nBy implementing these strategies, the company can effectively optimize costs while maintaining the desired performance levels for their data analytics workload.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse On-Demand Instances for Amazon EMR and Redshift clusters during peak loads, and switch to Spot Instances during off-peak times. Utilize AWS Glue job bookmarks to handle job failures due to Spot Interruptions.\n\nWhile this option may save some costs, the overhead of managing the transition between On-Demand and Spot Instances and handling job failures can be substantial and may impact workload completion times.\n\n\n\n\nPurchase Reserved Instances for Amazon Redshift and EMR clusters to handle peak loads, and use On-Demand Instances for off-peak workloads. Enable auto-pause for the Redshift clusters to save costs when not in use.\n\nThis option may not be cost-optimized for variable workloads, as Reserved Instances are more suited for steady-state workloads.\n\n\n\n\nUse Spot Instances exclusively for Amazon EMR and Redshift clusters, and leverage AWS Glue job recovery options to handle Spot Interruptions.\n\nWhile Spot Instances can offer significant savings, using them exclusively for critical data analytics workloads can risk job completion times due to potential Spot Interruptions.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/emr/pricing/\n\nhttps://aws.amazon.com/redshift/pricing/\n\nhttps://aws.amazon.com/glue/pricing/",
      "correctAnswerExplanations": [
        {
          "answer": "Use Savings Plans for Amazon EMR and AWS Glue, use Redshift Concurrency Scaling for peak loads, and enable automatic workload management in Redshift to balance query loads.",
          "explanation": "By using Savings Plans for Amazon EMR and AWS Glue, the multinational corporation can benefit from cost savings based on their committed usage. Savings Plans provide flexibility and cost optimization for EC2 instances used by EMR and Glue. This helps reduce costs without compromising performance during regular workloads."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use On-Demand Instances for Amazon EMR and Redshift clusters during peak loads, and switch to Spot Instances during off-peak times. Utilize AWS Glue job bookmarks to handle job failures due to Spot Interruptions.",
          "explanation": "While this option may save some costs, the overhead of managing the transition between On-Demand and Spot Instances and handling job failures can be substantial and may impact workload completion times."
        },
        {
          "answer": "Purchase Reserved Instances for Amazon Redshift and EMR clusters to handle peak loads, and use On-Demand Instances for off-peak workloads. Enable auto-pause for the Redshift clusters to save costs when not in use.",
          "explanation": "This option may not be cost-optimized for variable workloads, as Reserved Instances are more suited for steady-state workloads."
        },
        {
          "answer": "Use Spot Instances exclusively for Amazon EMR and Redshift clusters, and leverage AWS Glue job recovery options to handle Spot Interruptions.",
          "explanation": "While Spot Instances can offer significant savings, using them exclusively for critical data analytics workloads can risk job completion times due to potential Spot Interruptions."
        }
      ],
      "references": [
        "https://aws.amazon.com/emr/pricing/",
        "https://aws.amazon.com/redshift/pricing/",
        "https://aws.amazon.com/glue/pricing/"
      ]
    },
    {
      "id": 51,
      "question": "A software development company is running an on-premises Jira application that uses Microsoft Windows shared file storage. The company aims to move this application to AWS and is exploring various storage options. The chosen storage solution must ensure high availability and should be integrated with Active Directory for access control.\n\nWhich solution will fulfill these requirements?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory domain for authentication.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Configure Amazon EFS storage and configure the Active Directory domain for authentication.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Create an Amazon S3 bucket and configure the Microsoft Windows Server to mount it as a volume.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Create an SMB file share on an AWS Storage Gateway file gateway across two Availability Zones.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nCreate an Amazon FSx for Windows File Server file system on AWS and set the Active Directory domain for authentication.\n\nAmazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system which is highly available, scalable, and secure. It supports the Server Message Block (SMB) protocol, which is the standard for Windows file sharing. FSx can be easily integrated with AWS Managed Microsoft AD or on-premises Microsoft Active Directory to provide seamless domain-joined access, simplifying user authentication.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure Amazon EFS storage and configure the Active Directory domain for authentication.\n\nAmazon EFS does not natively support the SMB protocol used by Windows systems for file sharing. Therefore, this option is not suitable for a Windows-based application.\n\n\n\n\nCreate an SMB file share on an AWS Storage Gateway file gateway across two Availability Zones.\n\nWhile AWS Storage Gateway supports SMB protocol, it is not designed to provide a highly available, multi-AZ solution directly. This makes it less ideal for a critical, high-availability requirement.\n\n\n\n\nCreate an Amazon S3 bucket and configure the Microsoft Windows Server to mount it as a volume.\n\nAmazon S3 is an object storage service and does not natively support the SMB protocol used by Windows systems for file sharing. Hence, it is not the right solution for the given scenario.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/aws-ad-integration-fsxW.html",
      "correctAnswerExplanations": [
        {
          "answer": "Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory domain for authentication.",
          "explanation": "Amazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system which is highly available, scalable, and secure. It supports the Server Message Block (SMB) protocol, which is the standard for Windows file sharing. FSx can be easily integrated with AWS Managed Microsoft AD or on-premises Microsoft Active Directory to provide seamless domain-joined access, simplifying user authentication."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Configure Amazon EFS storage and configure the Active Directory domain for authentication.",
          "explanation": "Amazon EFS does not natively support the SMB protocol used by Windows systems for file sharing. Therefore, this option is not suitable for a Windows-based application."
        },
        {
          "answer": "Create an SMB file share on an AWS Storage Gateway file gateway across two Availability Zones.",
          "explanation": "While AWS Storage Gateway supports SMB protocol, it is not designed to provide a highly available, multi-AZ solution directly. This makes it less ideal for a critical, high-availability requirement."
        },
        {
          "answer": "Create an Amazon S3 bucket and configure the Microsoft Windows Server to mount it as a volume.",
          "explanation": "Amazon S3 is an object storage service and does not natively support the SMB protocol used by Windows systems for file sharing. Hence, it is not the right solution for the given scenario."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/aws-ad-integration-fsxW.html"
      ]
    },
    {
      "id": 52,
      "question": "A multinational company is operating multiple AWS accounts across various regions. The company has been struggling with managing costs and needs a solution to gain detailed insights into its AWS usage and costs. The solution should provide the ability to visualize, understand, and manage AWS costs and usage over time.\n\nWhat combination of AWS services and features should the company implement to enhance cost management and reporting?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use AWS Cost Explorer for creating custom cost and usage budgets, AWS Budgets for cost optimization recommendations, and AWS Organizations for multi-account management.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use AWS Trusted Advisor for creating custom cost and usage budgets, AWS Cost and Usage Report (CUR) for detailed cost reports, and AWS Control Tower for multi-account management.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use AWS Budgets for creating custom cost and usage budgets, AWS Cost Explorer to visualize and analyze costs, and AWS Organizations for consolidated billing.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Use AWS Trusted Advisor for cost optimization recommendations, AWS Cost and Usage Report (CUR) for detailed cost reports, and AWS Control Tower for multi-account management.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nUse AWS Budgets for creating custom cost and usage budgets, AWS Cost Explorer to visualize and analyze costs, and AWS Organizations for consolidated billing.\n\nAWS Budgets allows the company to create custom cost and usage budgets, enabling them to set specific spending limits and receive alerts when the budget thresholds are exceeded. This helps in proactive cost management and controlling expenses.\n\nAWS Cost Explorer provides powerful visualization and analysis capabilities for AWS costs. It allows the company to view and understand cost trends over time, identify cost drivers, and make informed decisions based on the cost data.\n\nAWS Organizations enables consolidated billing, allowing the company to centralize payment and manage costs across multiple AWS accounts. It simplifies cost allocation and provides a unified view of costs across the organization.\n\nBy implementing this combination of services, the company can effectively manage and optimize its AWS costs, gain insights into usage patterns, and make data-driven decisions to improve cost efficiency.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS Trusted Advisor for cost optimization recommendations, AWS Cost and Usage Report (CUR) for detailed cost reports, and AWS Control Tower for multi-account management.\n\nWhile this combination includes useful services, AWS Trusted Advisor is not primarily for creating budgets, and AWS Control Tower focuses more on governance, not specifically on cost management.\n\n\n\n\nUse AWS Cost Explorer for creating custom cost and usage budgets, AWS Budgets for cost optimization recommendations, and AWS Organizations for multi-account management.\n\nThis option misplaces the roles of AWS Cost Explorer and AWS Budgets.\n\n\n\n\nUse AWS Trusted Advisor for creating custom cost and usage budgets, AWS Cost and Usage Report (CUR) for detailed cost reports, and AWS Control Tower for multi-account management.\n\nThis option incorrectly assigns budget creation to AWS Trusted Advisor, which primarily provides best practice recommendations.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/aws-cost-management/",
      "correctAnswerExplanations": [
        {
          "answer": "Use AWS Budgets for creating custom cost and usage budgets, AWS Cost Explorer to visualize and analyze costs, and AWS Organizations for consolidated billing.",
          "explanation": "AWS Budgets allows the company to create custom cost and usage budgets, enabling them to set specific spending limits and receive alerts when the budget thresholds are exceeded. This helps in proactive cost management and controlling expenses."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use AWS Trusted Advisor for cost optimization recommendations, AWS Cost and Usage Report (CUR) for detailed cost reports, and AWS Control Tower for multi-account management.",
          "explanation": "While this combination includes useful services, AWS Trusted Advisor is not primarily for creating budgets, and AWS Control Tower focuses more on governance, not specifically on cost management."
        },
        {
          "answer": "Use AWS Cost Explorer for creating custom cost and usage budgets, AWS Budgets for cost optimization recommendations, and AWS Organizations for multi-account management.",
          "explanation": "This option misplaces the roles of AWS Cost Explorer and AWS Budgets."
        },
        {
          "answer": "Use AWS Trusted Advisor for creating custom cost and usage budgets, AWS Cost and Usage Report (CUR) for detailed cost reports, and AWS Control Tower for multi-account management.",
          "explanation": "This option incorrectly assigns budget creation to AWS Trusted Advisor, which primarily provides best practice recommendations."
        }
      ],
      "references": [
        "https://aws.amazon.com/aws-cost-management/"
      ]
    },
    {
      "id": 53,
      "question": "A healthcare company is storing confidential patient records in an Amazon S3 bucket. The company wants to ensure secure access to this bucket from their analytics application running on Amazon EC2 instances inside a VPC.\n\nWhich combination of steps should a solutions architect take to accomplish this? (Select TWO.)",
      "corrects": [
        1,
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Establish a VPC gateway endpoint for Amazon S3 within the VPC.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Generate an IAM user with an S3 access policy and transfer the IAM credentials to the EC2 instance.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Configure a bucket policy that restricts access to only the analytics application running in the VPC.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Amend a bucket policy to make the objects in the S3 bucket publicly accessible.",
          "correct": false
        },
        {
          "id": 5,
          "answer": "Construct a NAT gateway and instruct the EC2 instances to use the NAT gateway to access the S3 bucket.",
          "correct": false
        }
      ],
      "multiple": true,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Options:\n\nEstablish a VPC gateway endpoint for Amazon S3 within the VPC.\n\nCreating a VPC gateway endpoint for Amazon S3 provides a secure and efficient route to access S3 buckets directly from the VPC, without requiring internet access, NAT device, VPN, or AWS Direct Connect. By creating a VPC endpoint for S3, the company can route all S3 bucket traffic through the VPC endpoint, which will protect data in transit by keeping it within the AWS network.\n\n\n\n\nConfigure a bucket policy that restricts access to only the analytics application running in the VPC.\n\nThe company should implement a bucket policy to restrict access to the S3 bucket only for authorized applications, in this case, the analytics application running in the VPC. The policy should deny all access to anyone who tries to access the bucket outside the VPC endpoint.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAmend a bucket policy to make the objects in the S3 bucket publicly accessible.\n\nAs making the objects in the S3 bucket publicly accessible is a security risk, especially considering the sensitive nature of the data.\n\n\n\n\nGenerate an IAM user with an S3 access policy and transfer the IAM credentials to the EC2 instance.\n\nIt involves transferring IAM credentials to the EC2 instance, which could potentially be insecure.\n\n\n\n\nConstruct a NAT gateway and instruct the EC2 instances to use the NAT gateway to access the S3 bucket.\n\nAs NAT gateways allow instances in a private subnet to connect to the internet or other AWS services, so the option is incorrect.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html",
      "correctAnswerExplanations": [
        {
          "answer": "Establish a VPC gateway endpoint for Amazon S3 within the VPC.",
          "explanation": "Creating a VPC gateway endpoint for Amazon S3 provides a secure and efficient route to access S3 buckets directly from the VPC, without requiring internet access, NAT device, VPN, or AWS Direct Connect. By creating a VPC endpoint for S3, the company can route all S3 bucket traffic through the VPC endpoint, which will protect data in transit by keeping it within the AWS network."
        },
        {
          "answer": "Configure a bucket policy that restricts access to only the analytics application running in the VPC.",
          "explanation": "The company should implement a bucket policy to restrict access to the S3 bucket only for authorized applications, in this case, the analytics application running in the VPC. The policy should deny all access to anyone who tries to access the bucket outside the VPC endpoint."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Amend a bucket policy to make the objects in the S3 bucket publicly accessible.",
          "explanation": "As making the objects in the S3 bucket publicly accessible is a security risk, especially considering the sensitive nature of the data."
        },
        {
          "answer": "Generate an IAM user with an S3 access policy and transfer the IAM credentials to the EC2 instance.",
          "explanation": "It involves transferring IAM credentials to the EC2 instance, which could potentially be insecure."
        },
        {
          "answer": "Construct a NAT gateway and instruct the EC2 instances to use the NAT gateway to access the S3 bucket.",
          "explanation": "As NAT gateways allow instances in a private subnet to connect to the internet or other AWS services, so the option is incorrect."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html",
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html"
      ]
    },
    {
      "id": 54,
      "question": "A solutions architect is designing a three-tier web application. The application includes a public-facing web tier hosted on Amazon EC2 in public subnets, an application tier hosted on Amazon EC2 in private subnets, and a database tier consisting of Amazon RDS in private subnets. Security is a high priority for the company.\n\nHow should security groups be configured in this situation? (Select TWO.)",
      "corrects": [
        4,
        5
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Configure the security group for the application tier to allow outbound traffic on ports 80 & 443 from 0.0.0.0/0.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Configure the security group for the database tier to allow outbound traffic on a specific port to the security group for the application tier.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Configure the security group for the application tier to allow inbound traffic on a specific port from the security group for the web tier.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Configure the security group for the web tier to allow inbound traffic on ports 80 & 443 from 0.0.0.0/0.",
          "correct": true
        },
        {
          "id": 5,
          "answer": "Configure the security group for the database tier to allow inbound traffic on a specific port from the security group for the application tier.",
          "correct": true
        }
      ],
      "multiple": true,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nConfigure the security group for the web tier to allow inbound traffic on ports 80 & 443 from 0.0.0.0/0.\n\nThis configuration allows the public-facing web tier to accept incoming traffic on ports 80 & 443 from any IP address, enabling users to access the web application.\n\n\n\n\nConfigure the security group for the database tier to allow inbound traffic on a specific port from the security group for the application tier.\n\nThis configuration ensures that the database tier accepts traffic only from the application tier, enhancing the security of the database and minimizing exposure to potential attacks.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure the security group for the application tier to allow outbound traffic on ports 80 & 443 from 0.0.0.0/0.\n\nIt does not restrict outbound traffic to a specific destination, increasing potential security risks.\n\n\n\n\nConfigure the security group for the application tier to allow inbound traffic on a specific port from the security group for the web tier.\n\nIt does not specify how the application tier should communicate with the database tier, which is crucial for a secure, functional setup.\n\n\n\n\nConfigure the security group for the database tier to allow outbound traffic on a specific port to the security group for the application tier.\n\nIt focuses on outbound traffic from the database tier rather than the necessary inbound traffic.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html",
      "correctAnswerExplanations": [
        {
          "answer": "Configure the security group for the database tier to allow inbound traffic on a specific port from the security group for the application tier.",
          "explanation": "This configuration ensures that the database tier accepts traffic only from the application tier, enhancing the security of the database and minimizing exposure to potential attacks."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Configure the security group for the web tier to allow inbound traffic on ports 80 &amp; 443 from 0.0.0.0/0.",
          "explanation": "This configuration allows the public-facing web tier to accept incoming traffic on ports 80 &amp; 443 from any IP address, enabling users to access the web application."
        },
        {
          "answer": "Configure the security group for the application tier to allow outbound traffic on ports 80 &amp; 443 from 0.0.0.0/0.",
          "explanation": "It does not restrict outbound traffic to a specific destination, increasing potential security risks."
        },
        {
          "answer": "Configure the security group for the application tier to allow inbound traffic on a specific port from the security group for the web tier.",
          "explanation": "It does not specify how the application tier should communicate with the database tier, which is crucial for a secure, functional setup."
        },
        {
          "answer": "Configure the security group for the database tier to allow outbound traffic on a specific port to the security group for the application tier.",
          "explanation": "It focuses on outbound traffic from the database tier rather than the necessary inbound traffic."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html"
      ]
    },
    {
      "id": 55,
      "question": "A global organization requires a robust key management system to support its IT team who needs to manage SSL/TLS certificates for their applications.\n\nWhat should a solutions architect recommend to lessen the operational overhead?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Implement an IAM policy to restrict the scope of users who have access permissions to manage SSL/TLS certificates.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use AWS Key Management Service (AWS KMS) to create, store, and assign the SSL/TLS certificates.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Implement multi-factor authentication (MFA) to protect the SSL/TLS certificates.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use AWS Certificate Manager (ACM) to manage the SSL/TLS certificates.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nUse AWS Certificate Manager (ACM) to manage the SSL/TLS certificates.\n\nAWS Certificate Manager (ACM) is designed to protect and manage SSL/TLS certificates, reducing the operational burden associated with manual certificate generation, storage, and renewal. ACM can generate a certificate, deploy it to AWS resources, and renew it automatically before it expires, making it a suitable choice for this scenario.\n\n\n\n\n\n\n\nIncorrect Options:\n\nImplement multi-factor authentication (MFA) to protect the SSL/TLS certificates.\n\nWhile MFA adds a layer of security, it doesn't specifically address the operational burden associated with managing SSL/TLS certificates.\n\n\n\n\nUse AWS Key Management Service (AWS KMS) to create, store, and assign the SSL/TLS certificates.\n\nAWS KMS is designed for creating and managing encryption keys, not SSL/TLS certificates, which are needed for secure web communication.\n\n\n\n\nImplement an IAM policy to restrict the scope of users who have access permissions to manage SSL/TLS certificates.\n\nWhile it's good practice to restrict access, this option doesn't address the operational burden related to the management of SSL/TLS certificates.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use AWS Certificate Manager (ACM) to manage the SSL/TLS certificates.",
          "explanation": "AWS Certificate Manager (ACM) is designed to protect and manage SSL/TLS certificates, reducing the operational burden associated with manual certificate generation, storage, and renewal. ACM can generate a certificate, deploy it to AWS resources, and renew it automatically before it expires, making it a suitable choice for this scenario."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Implement multi-factor authentication (MFA) to protect the SSL/TLS certificates.",
          "explanation": "While MFA adds a layer of security, it doesn't specifically address the operational burden associated with managing SSL/TLS certificates."
        },
        {
          "answer": "Use AWS Key Management Service (AWS KMS) to create, store, and assign the SSL/TLS certificates.",
          "explanation": "AWS KMS is designed for creating and managing encryption keys, not SSL/TLS certificates, which are needed for secure web communication."
        },
        {
          "answer": "Implement an IAM policy to restrict the scope of users who have access permissions to manage SSL/TLS certificates.",
          "explanation": "While it's good practice to restrict access, this option doesn't address the operational burden related to the management of SSL/TLS certificates."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html"
      ]
    },
    {
      "id": 56,
      "question": "A business runs a containerized API service on a cluster of local servers. The API service's demand is quickly escalating, and the local servers can no longer manage the growing number of requests. The company plans to migrate the service to AWS with minimal modifications to the existing code and minimal development effort.\n\nWhich solution would satisfy these requirements with the LOWEST operational overhead?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use AWS Fargate on Amazon Elastic Kubernetes Service (EKS) to host the containerized API service, with Kubernetes Horizontal Pod Autoscaler to manage load. Use an Application Load Balancer to distribute incoming requests.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Use a couple of Amazon EC2 instances to host the containerized API service. Use an Application Load Balancer to distribute the incoming requests.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Rewrite the service for AWS Lambda using one of the supported languages. Create multiple Lambda functions to handle the load. Use Amazon API Gateway as the entry point to the Lambda functions.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use a high-performance computing (HPC) solution like AWS Batch to establish a processing pipeline that can manage incoming requests at the appropriate scale.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nUse AWS Fargate on Amazon Elastic Kubernetes Service (EKS) to host the containerized API service, with Kubernetes Horizontal Pod Autoscaler to manage load. Use an Application Load Balancer to distribute incoming requests.\n\nThis option uses the power of AWS Fargate and EKS, providing a serverless environment for running the containerized applications. It reduces the operational overhead as there is no need to manage the underlying infrastructure. Kubernetes Horizontal Pod Autoscaler automatically scales the number of pods based on observed CPU utilization, ensuring service availability during demand spikes. The Application Load Balancer efficiently distributes incoming traffic to the service.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse a couple of Amazon EC2 instances to host the containerized API service. Use an Application Load Balancer to distribute the incoming requests.\n\nThis option might not scale efficiently to handle the increasing load, leading to potential service disruptions. Also, managing EC2 instances introduces additional operational overhead.\n\n\n\n\nRewrite the service for AWS Lambda using one of the supported languages. Create multiple Lambda functions to handle the load. Use Amazon API Gateway as the entry point to the Lambda functions.\n\nThis option requires substantial code changes, which contradicts the requirement for minimum code changes and development effort.\n\n\n\n\nUse a high-performance computing (HPC) solution like AWS Batch to establish a processing pipeline that can manage incoming requests at the appropriate scale.\n\nThis option is not suitable for a web API service. AWS Batch is designed for batch processing, not for handling real-time, scalable web API requests.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/fargate.html\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use AWS Fargate on Amazon Elastic Kubernetes Service (EKS) to host the containerized API service, with Kubernetes Horizontal Pod Autoscaler to manage load. Use an Application Load Balancer to distribute incoming requests.",
          "explanation": "This option uses the power of AWS Fargate and EKS, providing a serverless environment for running the containerized applications. It reduces the operational overhead as there is no need to manage the underlying infrastructure. Kubernetes Horizontal Pod Autoscaler automatically scales the number of pods based on observed CPU utilization, ensuring service availability during demand spikes. The Application Load Balancer efficiently distributes incoming traffic to the service."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use a couple of Amazon EC2 instances to host the containerized API service. Use an Application Load Balancer to distribute the incoming requests.",
          "explanation": "This option might not scale efficiently to handle the increasing load, leading to potential service disruptions. Also, managing EC2 instances introduces additional operational overhead."
        },
        {
          "answer": "Rewrite the service for AWS Lambda using one of the supported languages. Create multiple Lambda functions to handle the load. Use Amazon API Gateway as the entry point to the Lambda functions.",
          "explanation": "This option requires substantial code changes, which contradicts the requirement for minimum code changes and development effort."
        },
        {
          "answer": "Use a high-performance computing (HPC) solution like AWS Batch to establish a processing pipeline that can manage incoming requests at the appropriate scale.",
          "explanation": "This option is not suitable for a web API service. AWS Batch is designed for batch processing, not for handling real-time, scalable web API requests."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/eks/latest/userguide/fargate.html",
        "https://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html",
        "https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html"
      ]
    },
    {
      "id": 57,
      "question": "A company is collaborating with an AWS Managed Service Provider (MSP) Partner on a project requiring a shared Amazon RDS snapshot. The RDS snapshot is encrypted using an AWS Key Management Service (AWS KMS) customer managed key.\n\nWhat is the MOST secure method for the solutions architect to share the encrypted RDS snapshot with the MSP Partner's AWS account?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Modify the snapshot permissions to share the RDS snapshot with the MSP Partner's AWS account only. Adjust the key policy to trust a new KMS key that is owned by the MSP Partner for encryption.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Make the encrypted RDS snapshot publicly available. Modify the key policy to allow the MSP Partner's AWS account to use the key.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Modify the snapshot permissions to share the RDS snapshot with the MSP Partner's AWS account only. Adjust the key policy to allow the MSP Partner's AWS account to use the key.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Export the RDS snapshot to an Amazon S3 bucket in the MSP Partner's AWS account. Encrypt the S3 bucket with a new KMS key owned by the MSP Partner. Import and restore the snapshot in the MSP Partner's AWS account.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nModify the snapshot permissions to share the RDS snapshot with the MSP Partner's AWS account only. Adjust the key policy to allow the MSP Partner's AWS account to use the key.\n\nBy modifying the snapshot permissions, the solutions architect can control who has access to the snapshot. By sharing the RDS snapshot with the MSP Partner's AWS account only, the solutions architect ensures that the snapshot is only accessible to the partner.\n\nAdjusting the key policy to allow the MSP Partner's AWS account to use the key enables the MSP Partner's AWS account to decrypt the encrypted RDS snapshot. The solutions architect can specify the necessary permissions in the key policy to ensure that the MSP Partner's AWS account can use the key for decryption purposes only.\n\nThis approach is the most secure because it ensures that only the MSP Partner's AWS account can access the RDS snapshot, and the encryption key is not shared with unauthorized users.\n\n\n\n\n\n\n\nIncorrect Options:\n\nMake the encrypted RDS snapshot publicly available. Modify the key policy to allow the MSP Partner's AWS account to use the key.\n\nMaking the RDS snapshot publicly available poses a security risk, as anyone with the link can access the snapshot.\n\n\n\n\nModify the snapshot permissions to share the RDS snapshot with the MSP Partner's AWS account only. Adjust the key policy to trust a new KMS key that is owned by the MSP Partner for encryption.\n\nIt doesn't grant the MSP Partner's AWS account the necessary permissions to use the original customer managed key for decryption, which is required to access the encrypted RDS snapshot.\n\n\n\n\nExport the RDS snapshot to an Amazon S3 bucket in the MSP Partner's AWS account. Encrypt the S3 bucket with a new KMS key owned by the MSP Partner. Import and restore the snapshot in the MSP Partner's AWS account.\n\nExporting and importing RDS snapshots through S3 is not a supported method in AWS, making this approach infeasible.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html",
      "correctAnswerExplanations": [
        {
          "answer": "Modify the snapshot permissions to share the RDS snapshot with the MSP Partner's AWS account only. Adjust the key policy to allow the MSP Partner's AWS account to use the key.",
          "explanation": "By modifying the snapshot permissions, the solutions architect can control who has access to the snapshot. By sharing the RDS snapshot with the MSP Partner's AWS account only, the solutions architect ensures that the snapshot is only accessible to the partner."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Make the encrypted RDS snapshot publicly available. Modify the key policy to allow the MSP Partner's AWS account to use the key.",
          "explanation": "Making the RDS snapshot publicly available poses a security risk, as anyone with the link can access the snapshot."
        },
        {
          "answer": "Modify the snapshot permissions to share the RDS snapshot with the MSP Partner's AWS account only. Adjust the key policy to trust a new KMS key that is owned by the MSP Partner for encryption.",
          "explanation": "It doesn't grant the MSP Partner's AWS account the necessary permissions to use the original customer managed key for decryption, which is required to access the encrypted RDS snapshot."
        },
        {
          "answer": "Export the RDS snapshot to an Amazon S3 bucket in the MSP Partner's AWS account. Encrypt the S3 bucket with a new KMS key owned by the MSP Partner. Import and restore the snapshot in the MSP Partner's AWS account.",
          "explanation": "Exporting and importing RDS snapshots through S3 is not a supported method in AWS, making this approach infeasible."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html",
        "https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html"
      ]
    },
    {
      "id": 58,
      "question": "A media company uses AWS to manage its video streaming services. Every new video request is passed as a message in a RabbitMQ queue that runs on an Amazon EC2 instance in a single Availability Zone. These messages are then processed by a different application running on a separate EC2 instance. This application stores the details in a MySQL database on another EC2 instance. All the EC2 instances are in the same Availability Zone.\n\nThe company is aiming to redesign its architecture to ensure the highest availability with minimal operational overhead.\n\nWhat should a solutions architect recommend to meet these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Use another Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for MySQL.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Use another Multi-AZ Auto Scaling group for EC2 instances that host the application. Use a third Multi-AZ Auto Scaling group for EC2 instances that host the MySQL database.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Use a Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for MySQL.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Use a Multi-AZ Auto Scaling group for EC2 instances that host the application. Use another Multi-AZ Auto Scaling group for EC2 instances that host the MySQL database.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nMigrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Use a Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for MySQL.\n\nWith Amazon MQ, a managed message broker service, a redundant pair of RabbitMQ instances can be set up to provide high availability for the queue. This ensures that even if one instance fails, the other can continue processing messages.\n\nUsing a Multi-AZ Auto Scaling group for the EC2 instances hosting the application allows for automatic scaling and redundancy across multiple Availability Zones, ensuring that the application remains available even in the event of an instance failure or AZ outage.\n\nMigrating the database to a Multi-AZ deployment of Amazon RDS for MySQL provides automated backups, software patching, and high availability. With Multi-AZ, synchronous replication is used to maintain a standby replica in a different Availability Zone, allowing for failover in case of a primary database failure.\n\nBy combining these solutions, the architecture achieves high availability for both the messaging queue and the application, while also reducing operational overhead through managed services.\n\n\n\n\n\n\n\nIncorrect Options:\n\nMigrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Use a Multi-AZ Auto Scaling group for EC2 instances that host the application. Use another Multi-AZ Auto Scaling group for EC2 instances that host the MySQL database.\n\nThis option doesn't provide the same level of high availability, automated backups, and failover support for the database as Amazon RDS does.\n\n\n\n\nUse a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Use another Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for MySQL.\n\nThis option involves more operational overhead with managing RabbitMQ on EC2 instances, even with Auto Scaling, compared to using Amazon MQ.\n\n\n\n\nUse a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Use another Multi-AZ Auto Scaling group for EC2 instances that host the application. Use a third Multi-AZ Auto Scaling group for EC2 instances that host the MySQL database.\n\nThis option doesn't provide the same level of high availability, automated backups, and failover support for the database as Amazon RDS does, and it also involves more operational overhead for managing RabbitMQ.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazon-mq/\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\n\nhttps://aws.amazon.com/rds/features/multi-az/\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-mysql-database-to-amazon-rds-for-mysql.html",
      "correctAnswerExplanations": [
        {
          "answer": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Use a Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for MySQL.",
          "explanation": "With Amazon MQ, a managed message broker service, a redundant pair of RabbitMQ instances can be set up to provide high availability for the queue. This ensures that even if one instance fails, the other can continue processing messages."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Use a Multi-AZ Auto Scaling group for EC2 instances that host the application. Use another Multi-AZ Auto Scaling group for EC2 instances that host the MySQL database.",
          "explanation": "This option doesn't provide the same level of high availability, automated backups, and failover support for the database as Amazon RDS does."
        },
        {
          "answer": "Use a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Use another Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for MySQL.",
          "explanation": "This option involves more operational overhead with managing RabbitMQ on EC2 instances, even with Auto Scaling, compared to using Amazon MQ."
        },
        {
          "answer": "Use a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Use another Multi-AZ Auto Scaling group for EC2 instances that host the application. Use a third Multi-AZ Auto Scaling group for EC2 instances that host the MySQL database.",
          "explanation": "This option doesn't provide the same level of high availability, automated backups, and failover support for the database as Amazon RDS does, and it also involves more operational overhead for managing RabbitMQ."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/amazon-mq/",
        "https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html",
        "https://aws.amazon.com/rds/features/multi-az/",
        "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-mysql-database-to-amazon-rds-for-mysql.html"
      ]
    },
    {
      "id": 59,
      "question": "A healthcare organization wants to migrate their multi-layered telemedicine application from their on-premises infrastructure to the AWS Cloud to enhance the application's efficiency. The application comprises multiple layers that interact through RESTful services. When one layer becomes overwhelmed, transactions are dropped. A solutions architect is tasked with designing a solution that addresses these issues while modernizing the application.\n\nWhich solution is the MOST operationally efficient in meeting these requirements?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon API Gateway to route transactions to AWS Lambda functions serving as the application layer, and use Amazon Simple Queue Service (Amazon SQS) for communication between application services.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SQS queue length and scale up when communication failures are detected.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SNS queue length and scale up and down as required.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use Amazon CloudWatch metrics to analyze the application performance history to determine the servers' peak utilization during the performance failures. Increase the size of the application server's Amazon EC2 instances to meet the peak requirements.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nUse Amazon API Gateway to route transactions to AWS Lambda functions serving as the application layer, and use Amazon Simple Queue Service (Amazon SQS) for communication between application services.\n\nUsing Amazon API Gateway in conjunction with AWS Lambda functions enables serverless architecture, which allows the application to scale automatically without the need for manual intervention. This approach eliminates the need to manage servers and is highly cost-effective due to the pay-per-use pricing model. Moreover, Amazon SQS is an ideal choice for communication between application services, as it provides a reliable and scalable messaging service, allowing messages to be stored and processed at varying rates. By decoupling components in the application and ensuring efficient communication, this solution effectively resolves the issues related to dropped transactions and improves the overall performance of the application.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon CloudWatch metrics to analyze the application performance history to determine the servers' peak utilization during the performance failures. Increase the size of the application server's Amazon EC2 instances to meet the peak requirements.\n\nThis option is not the most operationally efficient as it involves manual scaling and does not address the core issue of dropped transactions due to an overloaded tier. Additionally, increasing the size of Amazon EC2 instances can be costlier than adopting a serverless architecture.\n\n\n\n\nUse Amazon Simple Notification Service (Amazon SNS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SNS queue length and scale up and down as required.\n\nWhile Amazon SNS can handle messaging between application servers, it is not designed for the same level of reliability and scaling as Amazon SQS. Furthermore, this option still relies on EC2 instances, which may be less operationally efficient than a serverless solution like AWS Lambda.\n\n\n\n\nUse Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SQS queue length and scale up when communication failures are detected.\n\nThis option is not the most operationally efficient because it still relies on Amazon EC2 instances, which require manual scaling and management compared to a serverless solution like AWS Lambda.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/api-gateway/\n\nhttps://aws.amazon.com/lambda/\n\nhttps://aws.amazon.com/sqs/",
      "correctAnswerExplanations": [
        {
          "answer": "Use Amazon API Gateway to route transactions to AWS Lambda functions serving as the application layer, and use Amazon Simple Queue Service (Amazon SQS) for communication between application services.",
          "explanation": "Using Amazon API Gateway in conjunction with AWS Lambda functions enables serverless architecture, which allows the application to scale automatically without the need for manual intervention. This approach eliminates the need to manage servers and is highly cost-effective due to the pay-per-use pricing model. Moreover, Amazon SQS is an ideal choice for communication between application services, as it provides a reliable and scalable messaging service, allowing messages to be stored and processed at varying rates. By decoupling components in the application and ensuring efficient communication, this solution effectively resolves the issues related to dropped transactions and improves the overall performance of the application."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use Amazon CloudWatch metrics to analyze the application performance history to determine the servers' peak utilization during the performance failures. Increase the size of the application server's Amazon EC2 instances to meet the peak requirements.",
          "explanation": "This option is not the most operationally efficient as it involves manual scaling and does not address the core issue of dropped transactions due to an overloaded tier. Additionally, increasing the size of Amazon EC2 instances can be costlier than adopting a serverless architecture."
        },
        {
          "answer": "Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SNS queue length and scale up and down as required.",
          "explanation": "While Amazon SNS can handle messaging between application servers, it is not designed for the same level of reliability and scaling as Amazon SQS. Furthermore, this option still relies on EC2 instances, which may be less operationally efficient than a serverless solution like AWS Lambda."
        },
        {
          "answer": "Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SQS queue length and scale up when communication failures are detected.",
          "explanation": "This option is not the most operationally efficient because it still relies on Amazon EC2 instances, which require manual scaling and management compared to a serverless solution like AWS Lambda."
        }
      ],
      "references": [
        "https://aws.amazon.com/api-gateway/",
        "https://aws.amazon.com/lambda/",
        "https://aws.amazon.com/sqs/"
      ]
    },
    {
      "id": 60,
      "question": "A company has developed an audio processing application where users can upload audio files and apply various filters to modify their sound. The users upload audio files and metadata to specify which filters they want to apply. The application runs on a single Amazon EC2 instance and uses Amazon DynamoDB to store the metadata.\n\nThe application's popularity is increasing, leading to a growing number of users. The company anticipates significant fluctuations in the number of concurrent users based on the time of day and day of the week. The company must ensure that the application can scale to meet the growing user base's needs.\n\nWhich solution meets these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon Kinesis Data Firehose to process the audio files and to store the audio files and metadata.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use AWS Lambda to process the audio files. Store the audio files and metadata in DynamoDB.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use AWS Lambda to process the audio files. Store the audio files in Amazon S3. Continue using DynamoDB to store the metadata.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volumes to store the audio files and metadata.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nUse AWS Lambda to process the audio files. Store the audio files in Amazon S3. Continue using DynamoDB to store the metadata.\n\nAWS Lambda is a serverless compute service that runs your code in response to events and automatically manages the underlying compute resources for you. Amazon S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance. Keeping the metadata in DynamoDB, a key-value and document database that delivers single-digit millisecond performance at any scale, allows for quick retrieval and association with the audio files. Thereby the solution is the most suitable as it allows the application to scale dynamically based on the workload.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS Lambda to process the audio files. Store the audio files and metadata in DynamoDB.\n\nDynamoDB is not designed for storing large binary files like audio files. It's best for storing structured data in the form of metadata.\n\n\n\n\nUse Amazon Kinesis Data Firehose to process the audio files and to store the audio files and metadata.\n\nAmazon Kinesis Data Firehose is primarily used for capturing, transforming, and loading streaming data into other data stores. It's not an ideal solution for this scenario.\n\n\n\n\nIncrease the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volumes to store the audio files and metadata.\n\nThis option would not efficiently handle fluctuations in user traffic as it doesn't provide automatic scaling.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/lambda/\n\nhttps://aws.amazon.com/s3/\n\nhttps://aws.amazon.com/dynamodb/",
      "correctAnswerExplanations": [
        {
          "answer": "Use AWS Lambda to process the audio files. Store the audio files in Amazon S3. Continue using DynamoDB to store the metadata.",
          "explanation": "AWS Lambda is a serverless compute service that runs your code in response to events and automatically manages the underlying compute resources for you. Amazon S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance. Keeping the metadata in DynamoDB, a key-value and document database that delivers single-digit millisecond performance at any scale, allows for quick retrieval and association with the audio files. Thereby the solution is the most suitable as it allows the application to scale dynamically based on the workload."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use AWS Lambda to process the audio files. Store the audio files and metadata in DynamoDB.",
          "explanation": "DynamoDB is not designed for storing large binary files like audio files. It's best for storing structured data in the form of metadata."
        },
        {
          "answer": "Use Amazon Kinesis Data Firehose to process the audio files and to store the audio files and metadata.",
          "explanation": "Amazon Kinesis Data Firehose is primarily used for capturing, transforming, and loading streaming data into other data stores. It's not an ideal solution for this scenario."
        },
        {
          "answer": "Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volumes to store the audio files and metadata.",
          "explanation": "This option would not efficiently handle fluctuations in user traffic as it doesn't provide automatic scaling."
        }
      ],
      "references": [
        "https://aws.amazon.com/lambda/",
        "https://aws.amazon.com/s3/",
        "https://aws.amazon.com/dynamodb/"
      ]
    },
    {
      "id": 61,
      "question": "A financial services firm is running a data analysis application on Amazon EC2 instances. The application processes customer transaction data stored on Amazon S3. The EC2 instances are located in public subnets. These instances access Amazon S3 over the internet and don't require any other network access.\n\nRecently, a new regulation requires that the network traffic for data transfers should follow a private route, and not traverse the public internet.\n\nWhat change should a solutions architect recommend to meet this new requirement?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Modify the security group for the EC2 instances to limit outbound traffic, only permitting traffic to the S3 prefix list.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Establish a NAT gateway. Adjust the route table for the public subnets to direct traffic to Amazon S3 through the NAT gateway.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Remove the internet gateway from the VPC. Implement an AWS Direct Connect connection, and route traffic to Amazon S3 via the Direct Connect connection.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Relocate the EC2 instances to private subnets. Establish a VPC endpoint for Amazon S3, and associate the endpoint with the route table for the private subnets.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nRelocate the EC2 instances to private subnets. Establish a VPC endpoint for Amazon S3, and associate the endpoint with the route table for the private subnets.\n\nBy relocating the EC2 instances to private subnets and establishing a VPC endpoint for Amazon S3, the network traffic for data transfers between the EC2 instances and Amazon S3 will follow a private route and not traverse the public internet.\n\nA VPC endpoint for Amazon S3 allows direct connectivity between the VPC and S3 without requiring internet gateway or NAT gateway involvement. This ensures that the traffic remains within the AWS network, providing a secure and private connection. By associating the VPC endpoint with the route table for the private subnets, the EC2 instances in those subnets can access S3 securely without going through the public internet.\n\n\n\n\n\n\n\nIncorrect Options:\n\nEstablish a NAT gateway. Adjust the route table for the public subnets to direct traffic to Amazon S3 through the NAT gateway.\n\nNAT gateways are used to provide internet access to EC2 instances in private subnets. However, they don't offer private connectivity to S3.\n\n\n\n\nModify the security group for the EC2 instances to limit outbound traffic, only permitting traffic to the S3 prefix list.\n\nThis doesn't ensure that the traffic will take a private route. It only restricts the traffic to S3.\n\n\n\n\nRemove the internet gateway from the VPC. Implement an AWS Direct Connect connection, and route traffic to Amazon S3 via the Direct Connect connection.\n\nAWS Direct Connect is typically used for dedicated, private connections between on-premises data centers and AWS, not for private connectivity within AWS itself.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html",
      "correctAnswerExplanations": [
        {
          "answer": "Relocate the EC2 instances to private subnets. Establish a VPC endpoint for Amazon S3, and associate the endpoint with the route table for the private subnets.",
          "explanation": "By relocating the EC2 instances to private subnets and establishing a VPC endpoint for Amazon S3, the network traffic for data transfers between the EC2 instances and Amazon S3 will follow a private route and not traverse the public internet."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Establish a NAT gateway. Adjust the route table for the public subnets to direct traffic to Amazon S3 through the NAT gateway.",
          "explanation": "NAT gateways are used to provide internet access to EC2 instances in private subnets. However, they don't offer private connectivity to S3."
        },
        {
          "answer": "Modify the security group for the EC2 instances to limit outbound traffic, only permitting traffic to the S3 prefix list.",
          "explanation": "This doesn't ensure that the traffic will take a private route. It only restricts the traffic to S3."
        },
        {
          "answer": "Remove the internet gateway from the VPC. Implement an AWS Direct Connect connection, and route traffic to Amazon S3 via the Direct Connect connection.",
          "explanation": "AWS Direct Connect is typically used for dedicated, private connections between on-premises data centers and AWS, not for private connectivity within AWS itself."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"
      ]
    },
    {
      "id": 62,
      "question": "A financial company receives daily transaction data files in an Amazon S3 bucket. Currently, analysts manually transfer these files from this initial S3 bucket to another S3 bucket for data analysis. With the company's growth, more departments are beginning to submit larger files to the initial S3 bucket.\n\nThe analysts want the files to be automatically moved to the analysis S3 bucket as soon as they are uploaded to the initial bucket. They also plan to use AWS Lambda to run anomaly detection code on the transferred data. Furthermore, they wish to feed the data files into a machine learning model pipeline in Amazon SageMaker Pipelines.\n\nWhich solution should a solutions architect recommend to fulfill these requirements with the LEAST operational overhead?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Develop a Lambda function to shift the files to the analysis S3 bucket. Configure an S3 event notification for the analysis S3 bucket. Set Lambda and SageMaker Pipelines as event notification destinations. Choose s3:ObjectCreated:Put as the event type.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Configure S3 replication between the S3 buckets. Set up an S3 event notification for the analysis S3 bucket. Set Lambda and SageMaker Pipelines as destinations of the event notification. Choose s3:ObjectCreated:Put as the event type.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Configure S3 replication between the S3 buckets. Set the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Set Lambda and SageMaker Pipelines as targets for the rule.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Develop a Lambda function to shift the files to the analysis S3 bucket. Set the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Set Lambda and SageMaker Pipelines as targets for the rule.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nConfigure S3 replication between the S3 buckets. Set the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Set Lambda and SageMaker Pipelines as targets for the rule.\n\nBy configuring S3 replication between the initial S3 bucket and the analysis S3 bucket, any new files uploaded to the initial bucket will automatically be replicated to the analysis bucket. This eliminates the need for manual file transfers.\n\nAdditionally, by setting up an event notification on the analysis S3 bucket using Amazon EventBridge (CloudWatch Events), you can trigger actions whenever a new file is created. In this case, you can configure an ObjectCreated rule in EventBridge to trigger the Lambda function and feed the data into SageMaker Pipelines for further processing. This ensures that the anomaly detection code and the machine learning model pipeline are automatically executed whenever new files are uploaded to the analysis S3 bucket.\n\nThis solution eliminates manual intervention, streamlines the data transfer process, and enables real-time data analysis with Lambda and SageMaker Pipelines.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDevelop a Lambda function to shift the files to the analysis S3 bucket. Configure an S3 event notification for the analysis S3 bucket. Set Lambda and SageMaker Pipelines as event notification destinations. Choose s3:ObjectCreated:Put as the event type.\n\nThis option does not automate the file transfer process, which was a requirement. Manual invocation of Lambda for file transfer increases operational overhead.\n\n\n\n\nDevelop a Lambda function to shift the files to the analysis S3 bucket. Set the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Set Lambda and SageMaker Pipelines as targets for the rule.\n\nThis does not automate the file transfer process, leading to higher operational overhead.\n\n\n\n\nConfigure S3 replication between the S3 buckets. Set up an S3 event notification for the analysis S3 bucket. Set Lambda and SageMaker Pipelines as destinations of the event notification. Choose s3:ObjectCreated:Put as the event type.\n\nWhile this option provides automatic file transfer, it uses S3 event notifications which are tied to a specific S3 bucket, increasing complexity when compared to the flexibility of Amazon EventBridge.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html",
      "correctAnswerExplanations": [
        {
          "answer": "Configure S3 replication between the S3 buckets. Set the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Set Lambda and SageMaker Pipelines as targets for the rule.",
          "explanation": "By configuring S3 replication between the initial S3 bucket and the analysis S3 bucket, any new files uploaded to the initial bucket will automatically be replicated to the analysis bucket. This eliminates the need for manual file transfers."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Develop a Lambda function to shift the files to the analysis S3 bucket. Configure an S3 event notification for the analysis S3 bucket. Set Lambda and SageMaker Pipelines as event notification destinations. Choose s3:ObjectCreated:Put as the event type.",
          "explanation": "This option does not automate the file transfer process, which was a requirement. Manual invocation of Lambda for file transfer increases operational overhead."
        },
        {
          "answer": "Develop a Lambda function to shift the files to the analysis S3 bucket. Set the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Set Lambda and SageMaker Pipelines as targets for the rule.",
          "explanation": "This does not automate the file transfer process, leading to higher operational overhead."
        },
        {
          "answer": "Configure S3 replication between the S3 buckets. Set up an S3 event notification for the analysis S3 bucket. Set Lambda and SageMaker Pipelines as destinations of the event notification. Choose s3:ObjectCreated:Put as the event type.",
          "explanation": "While this option provides automatic file transfer, it uses S3 event notifications which are tied to a specific S3 bucket, increasing complexity when compared to the flexibility of Amazon EventBridge."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html",
        "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html",
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html"
      ]
    },
    {
      "id": 63,
      "question": "A media production company has been storing its video files on Amazon S3. The files include both raw footage and final productions. The raw footage is accessed frequently for a month but is then rarely accessed. The final productions are accessed intermittently but must be readily available. The company also maintains a significant number of outdated video files that must be preserved for compliance reasons. These outdated videos are almost never accessed. The company is looking for a cost-optimization strategy for its storage.\n\nWhich combination of S3 storage classes and features should the company use to optimize storage costs?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Store raw footage in S3 Standard, final productions in S3 Standard-Infrequent Access, and outdated videos in S3 Glacier Deep Archive.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Store raw footage in S3 Standard-Infrequent Access, final productions in S3 Intelligent-Tiering, and outdated videos in S3 Glacier.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Store raw footage in S3 Intelligent-Tiering, final productions in S3 Standard, and outdated videos in S3 Glacier Deep Archive.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Store raw footage in S3 Standard, final productions in S3 One Zone-Infrequent Access, and outdated videos in S3 Glacier.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nStore raw footage in S3 Intelligent-Tiering, final productions in S3 Standard, and outdated videos in S3 Glacier Deep Archive.\n\nS3 Intelligent-Tiering optimizes costs for data with changing or unknown access patterns like raw footage. S3 Standard provides low latency and high throughput performance for frequently accessed data like final productions. S3 Glacier Deep Archive is the lowest-cost storage class and supports long-term archiving and digital preservation, suitable for outdated videos.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStore raw footage in S3 Standard, final productions in S3 One Zone-Infrequent Access, and outdated videos in S3 Glacier.\n\nThis is not the most cost-optimized solution. Raw footage will not benefit from S3 Standard if it's rarely accessed after a month, and S3 One Zone-IA may risk loss of data due to its single-zone nature.\n\n\n\n\nStore raw footage in S3 Standard, final productions in S3 Standard-Infrequent Access, and outdated videos in S3 Glacier.\n\nStoring raw footage in S3 Standard may not be cost-effective as it's rarely accessed after a month. S3 Standard-IA may not provide the low latency needed for final productions.\n\n\n\n\nStore raw footage in S3 Standard-Infrequent Access, final productions in S3 Intelligent-Tiering, and outdated videos in S3 Glacier.\n\nStoring raw footage in S3 Standard-IA is not cost-effective for the first month of frequent access. Using Intelligent-Tiering for final productions might not provide the optimal performance needed.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes/",
      "correctAnswerExplanations": [
        {
          "answer": "Store raw footage in S3 Intelligent-Tiering, final productions in S3 Standard, and outdated videos in S3 Glacier Deep Archive.",
          "explanation": "S3 Intelligent-Tiering optimizes costs for data with changing or unknown access patterns like raw footage. S3 Standard provides low latency and high throughput performance for frequently accessed data like final productions. S3 Glacier Deep Archive is the lowest-cost storage class and supports long-term archiving and digital preservation, suitable for outdated videos."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Store raw footage in S3 Standard, final productions in S3 One Zone-Infrequent Access, and outdated videos in S3 Glacier.",
          "explanation": "This is not the most cost-optimized solution. Raw footage will not benefit from S3 Standard if it's rarely accessed after a month, and S3 One Zone-IA may risk loss of data due to its single-zone nature."
        },
        {
          "answer": "Store raw footage in S3 Standard, final productions in S3 Standard-Infrequent Access, and outdated videos in S3 Glacier.",
          "explanation": "Storing raw footage in S3 Standard may not be cost-effective as it's rarely accessed after a month. S3 Standard-IA may not provide the low latency needed for final productions."
        },
        {
          "answer": "Store raw footage in S3 Standard-Infrequent Access, final productions in S3 Intelligent-Tiering, and outdated videos in S3 Glacier.",
          "explanation": "Storing raw footage in S3 Standard-IA is not cost-effective for the first month of frequent access. Using Intelligent-Tiering for final productions might not provide the optimal performance needed."
        }
      ],
      "references": [
        "https://aws.amazon.com/s3/storage-classes/"
      ]
    },
    {
      "id": 64,
      "question": "An organization runs a corporate website using widely used content management software. The ongoing patching and maintenance of this software have become cumbersome. The company is planning to redesign the website and wants a new solution. The website will be updated quarterly, and it doesn't require any dynamic content. The solution should be highly scalable and offer superior security.\n\nWhich combination of changes will meet these requirements with the LEAST operational overhead? (Select TWO.)",
      "corrects": [
        1,
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Setup Amazon CloudFront for the website to utilize HTTPS functionality.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Develop the new website. Deploy the website using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Deploy AWS WAF web ACL for the website to provide HTTPS functionality.",
          "correct": false
        },
        {
          "id": 5,
          "answer": "Develop and deploy an AWS Lambda function to manage and deliver the website content.",
          "correct": false
        }
      ],
      "multiple": true,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Options:\n\nSetup Amazon CloudFront for the website to utilize HTTPS functionality.\nSetting up Amazon CloudFront for the website to utilize HTTPS functionality is a suitable solution. CloudFront provides a scalable and highly available content delivery network (CDN) that can distribute the blog content globally with low latency. Enabling HTTPS ensures secure communication between the end users and the website, enhancing security.\n\n\n\n\nCreate the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.\n\nDeploying the website on an Amazon S3 bucket with static website hosting enabled is another appropriate choice. Amazon S3 provides highly scalable and durable object storage. By using S3 for hosting the static blog content, the operational overhead is reduced as there is no need to manage and maintain traditional web servers. Additionally, S3's static website hosting functionality simplifies the deployment process.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDeploy AWS WAF web ACL for the website to provide HTTPS functionality.\n\nAWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits. Although it enhances security, it does not provide HTTPS functionality.\n\n\n\n\nDevelop and deploy an AWS Lambda function to manage and deliver the website content.\n\nAWS Lambda lets you run your code without provisioning or managing servers. However, managing and serving static blog content through Lambda is not a standard practice and could add unnecessary complexity.\n\n\n\n\nDevelop the new website. Deploy the website using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer.\n\nThis is more suitable for dynamic content and requires more operational overhead compared to hosting a static website on S3 and using CloudFront.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
      "correctAnswerExplanations": [],
      "incorrectAnswerExplanations": [
        {
          "answer": "Setup Amazon CloudFront for the website to utilize HTTPS functionality.<br></strong>Setting up Amazon CloudFront for the website to utilize HTTPS functionality is a suitable solution. CloudFront provides a scalable and highly available content delivery network (CDN) that can distribute the blog content globally with low latency. Enabling HTTPS ensures secure communication between the end users and the website, enhancing security.</p><p><br></p><p><strong>Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.",
          "explanation": "Deploying the website on an Amazon S3 bucket with static website hosting enabled is another appropriate choice. Amazon S3 provides highly scalable and durable object storage. By using S3 for hosting the static blog content, the operational overhead is reduced as there is no need to manage and maintain traditional web servers. Additionally, S3's static website hosting functionality simplifies the deployment process."
        },
        {
          "answer": "Deploy AWS WAF web ACL for the website to provide HTTPS functionality.",
          "explanation": "AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits. Although it enhances security, it does not provide HTTPS functionality."
        },
        {
          "answer": "Develop and deploy an AWS Lambda function to manage and deliver the website content.",
          "explanation": "AWS Lambda lets you run your code without provisioning or managing servers. However, managing and serving static blog content through Lambda is not a standard practice and could add unnecessary complexity."
        },
        {
          "answer": "Develop the new website. Deploy the website using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer.",
          "explanation": "This is more suitable for dynamic content and requires more operational overhead compared to hosting a static website on S3 and using CloudFront."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html",
        "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html"
      ]
    },
    {
      "id": 65,
      "question": "A media company stores 60 TB of video data on-premises for archival purposes and now wants to migrate this data to AWS. A proprietary application located in the company's data center performs monthly encoding jobs on the video data. The company intends to halt the application until the data migration is completed and plans to start the migration process immediately.\n\nThe data center lacks spare network capacity for additional tasks. A solutions architect must migrate the data and set up the encoding job to continue running in the AWS Cloud.\n\nWhich solution will satisfy these requirements with the LOWEST operational overhead?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Order an AWS Snowcone device for data migration. Deploy the encoding application on the device.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Order an AWS Snowball Edge Storage Optimized device with integrated Amazon EC2 compute for data migration. Transfer the data to the device. Create a new EC2 instance on AWS to run the encoding application.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Order an AWS Snowball Edge Storage Optimized device for data migration. Transfer the data to the device. Create a custom encoding job using AWS Glue.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Use AWS DataSync for data migration. Create a custom encoding job using AWS Glue.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nOrder an AWS Snowball Edge Storage Optimized device for data migration. Transfer the data to the device. Create a custom encoding job using AWS Glue.\n\nThis solution involves the use of AWS Snowball Edge Storage Optimized device, which is designed to move large volumes of data into and out of the AWS Cloud. It is ideal for situations where network bandwidth is constrained. After transferring the data to AWS, AWS Glue, a fully managed extract, transform, and load (ETL) service can be used to create the custom encoding job. This approach minimizes operational overhead by using fully managed services.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS DataSync for data migration. Create a custom encoding job using AWS Glue.\n\nAWS DataSync requires network bandwidth, which is not available according to the scenario.\n\n\n\n\nOrder an AWS Snowcone device for data migration. Deploy the encoding application on the device.\n\nAWS Snowcone devices are not suitable for large data transfers (60 TB) because they are designed for edge computing, storage, and data transfer applications that require portable, rugged, and secure devices.\n\n\n\n\nOrder an AWS Snowball Edge Storage Optimized device with integrated Amazon EC2 compute for data migration. Transfer the data to the device. Create a new EC2 instance on AWS to run the encoding application.\n\nThis option requires managing EC2 instances, which would increase operational overhead compared to using AWS Glue.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/snowball/latest/developer-guide/sbe-specifications.html\n\nhttps://aws.amazon.com/glue/",
      "correctAnswerExplanations": [
        {
          "answer": "Order an AWS Snowball Edge Storage Optimized device for data migration. Transfer the data to the device. Create a custom encoding job using AWS Glue.",
          "explanation": "This solution involves the use of AWS Snowball Edge Storage Optimized device, which is designed to move large volumes of data into and out of the AWS Cloud. It is ideal for situations where network bandwidth is constrained. After transferring the data to AWS, AWS Glue, a fully managed extract, transform, and load (ETL) service can be used to create the custom encoding job. This approach minimizes operational overhead by using fully managed services."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use AWS DataSync for data migration. Create a custom encoding job using AWS Glue.",
          "explanation": "AWS DataSync requires network bandwidth, which is not available according to the scenario."
        },
        {
          "answer": "Order an AWS Snowcone device for data migration. Deploy the encoding application on the device.",
          "explanation": "AWS Snowcone devices are not suitable for large data transfers (60 TB) because they are designed for edge computing, storage, and data transfer applications that require portable, rugged, and secure devices."
        },
        {
          "answer": "Order an AWS Snowball Edge Storage Optimized device with integrated Amazon EC2 compute for data migration. Transfer the data to the device. Create a new EC2 instance on AWS to run the encoding application.",
          "explanation": "This option requires managing EC2 instances, which would increase operational overhead compared to using AWS Glue."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/snowball/latest/developer-guide/sbe-specifications.html",
        "https://aws.amazon.com/glue/"
      ]
    }
  ]