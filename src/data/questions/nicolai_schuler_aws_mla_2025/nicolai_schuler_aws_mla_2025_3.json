[
  {
    "id": 1,
    "question": "<p>A data science team is using SageMaker Ground Truth to perform text classification labeling. They are interested in reducing their labeling costs by automating some tasks while ensuring sensitive data is handled by trusted individuals. <br><br>Which combination of strategies should the team use to achieve this?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a third-party vendor for sensitive data while relying on fully automated models for all labeling tasks.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up Mechanical Turk workers for sensitive data and automate labeling for complex tasks.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use a private workforce for sensitive data, and leverage active learning to automate simple tasks. </p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Rely entirely on private workforce labelers to handle both simple and complex tasks. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 2,
    "question": "<p>A machine learning team needs to regularly transform and load large datasets from Amazon S3 into a data warehouse for model training. They are considering using AWS Glue for this ETL process but want to ensure the process is scalable, cost-efficient, and able to handle incremental updates. Which features of AWS Glue should they leverage to meet these requirements? (Choose Two)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Glue’s built-in scheduling to trigger ETL jobs at regular intervals and process the entire dataset each time for consistency.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Utilize AWS Glue’s serverless architecture and auto-scaling capabilities to manage Spark clusters automatically, ensuring scalability.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Glue to configure incremental ETL jobs that only process new or modified data.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Glue crawlers to infer the schema and classify the data in real-time, ensuring all new files are processed immediately.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Pre-configure fixed Spark cluster resources to avoid unexpected scaling charges.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 3,
    "question": "<p>A company is building a machine learning pipeline using SageMaker. They want to automatically set up an environment where multiple data scientists can collaborate, manage various machine learning tasks, and store their work in a persistent and scalable environment. Which feature of SageMaker would best support these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Studio</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon EMR</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Glue</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Notebook Instances</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 4,
    "question": "<p>A company is building a generative AI application using Amazon Bedrock. The company needs to quickly integrate a foundation model for generating product recommendations based on customer interactions. They also want to ensure the infrastructure is scalable and secure without managing it directly. <br>Which features of Amazon Bedrock make it the best choice for this use case? (Choose Two)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Data encryption in transit and at rest</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Serverless environment with automatic scaling</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Custom instances for high-complexity models</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>On-premises infrastructure for enhanced security</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Direct access to GPUs for model training </p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 5,
    "question": "<p>A data scientist is running multiple machine learning experiments using Amazon SageMaker to determine the best hyperparameter configurations for their model. They want to track each experiment's metadata, compare the results, and ensure that the process is fully reproducible. Which SageMaker feature would BEST meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Experiments</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Pipelines</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker Autopilot</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Debugger</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 6,
    "question": "<p>Your team has deployed a machine learning model for batch inference on Amazon SageMaker. You want to monitor the model's performance and be notified if the model's accuracy declines over time, indicating a potential model quality drift. <br><br>Which steps should you take to set up model quality monitoring for this batch inference model using SageMaker Model Monitor? (Choose Two)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable CloudTrail logs to track the data drift in batch jobs.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Manually track the model’s prediction results on a daily basis.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Merge the ground truth labels with the captured predictions to assess the model’s accuracy.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Schedule monitoring jobs to capture the incoming data and baseline comparison results.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Deploy a new version of the model every time drift is detected.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 7,
    "question": "<p>A company is developing an AI solution that needs to reduce both underfitting and overfitting to ensure a balanced model. Which combination of strategies can the data scientist apply to mitigate bias and variance in the model? (Choose Two)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Ignore high variance to focus on reducing bias.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use early stopping to prevent the model from overfitting during training.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Increase the model complexity by adding more parameters.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Limit the dataset size to reduce noise in the model.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use cross-validation to assess model performance on multiple data subsets.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 8,
    "question": "<p>Which of the following is an example of unsupervised learning in machine learning?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Predicting the price of a stock based on historical data</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Predicting whether an email is spam or not</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Forecasting sales for the next quarter</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Grouping customers based on purchasing behavior without labeled categories</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 9,
    "question": "<p>A manufacturing company is using machine learning to predict equipment failures. The company has data on equipment usage, temperature, and maintenance records stored in an Amazon S3 bucket. They need to clean and analyze this data, engineer features such as average machine temperature over time, and store these features for both batch and real-time predictions. The company also wants to utilize a pre-built machine learning model to accelerate the deployment process. <br><br>Which combination of services would BEST meet the company's needs?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Data Wrangler to clean and preprocess the data, store the features in Amazon Redshift, and use SageMaker JumpStart to deploy a pre-built model.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Data Wrangler to clean and transform the data, store the features in SageMaker Feature Store, and use SageMaker JumpStart to deploy a pre-built predictive maintenance model.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon EMR to preprocess the data, store the features in Amazon DynamoDB, and use SageMaker to train and deploy the model.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Notebooks to preprocess the data, store the features in Amazon S3, and use SageMaker Autopilot to train and deploy a predictive model.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 10,
    "question": "<p>An organization needs to train a machine learning model with a custom algorithm that is not natively supported by SageMaker's built-in algorithms. The organization prefers to use SageMaker’s pre-built containers for framework support, but with custom training and inference code. <br>Which approach should the organization take?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker's built-in algorithms and modify them to include custom training code.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Write custom training code directly in the SageMaker notebook instance and run it using the notebook’s execution kernel.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilize SageMaker’s Script Mode, write the custom training logic in a Python script, and specify the entry point in a SageMaker estimator.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Build a custom Docker container from scratch, package all dependencies, and upload it to Amazon ECR for use in SageMaker.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 11,
    "question": "<p>A pharmaceutical company is using machine learning to analyze clinical trial data. They need to ingest raw data from multiple sources, such as CSV files and relational databases, and clean and join this data. Afterward, the data should be used to create a set of features, including patient age, treatment history, and genetic markers. The company also wants to maintain a version-controlled feature repository that can be reused across different machine learning models. <br><br>Which AWS services should the company use to preprocess the data, create features, and maintain a version-controlled feature repository?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon QuickSight for data preprocessing, SageMaker Feature Store to store features, and SageMaker JumpStart to deploy a pre-built model.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Glue for data cleaning, store features in Amazon DynamoDB, and use SageMaker to train models.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Athena to preprocess the data, store the features in Amazon S3, and use SageMaker Autopilot to automatically train models.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Data Wrangler for data preprocessing, SageMaker Feature Store to store and manage the features, and SageMaker Notebooks to train the models.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 12,
    "question": "<p>A data scientist is setting up a new SageMaker pipeline for automating an ML workflow. They need to define the steps in the pipeline, which include training a model, registering the model in the model registry, and deploying it. What structure should they follow to create a functional SageMaker pipeline? </p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Only define the steps, without specifying any parameters or names, since SageMaker Pipelines automatically handles these.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a notebook in SageMaker Studio, then manually execute each step in the pipeline.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS CloudFormation to define and automate all steps of the pipeline without utilizing SageMaker's pipeline features.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Define the name of the pipeline, the steps for each action, and parameters for step configuration.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 13,
    "question": "<p>A healthcare company wants to transcribe doctor-patient conversations into text for further analysis and store the results in Amazon S3. They need the solution to automatically remove any personally identifiable information (PII) to ensure compliance with HIPAA regulations. Which combination of AWS services should be used to achieve this goal?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Transcribe Medical to convert speech to text, utilize AWS Lambda to trigger Amazon Comprehend Medical for PII detection and redaction, and store the processed text in Amazon S3.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Transcribe Medical to convert speech to text, store the transcriptions in Amazon S3, and enable Amazon Macie to scan and redact PII.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Transcribe to convert speech to text, store the transcriptions in Amazon DynamoDB, and use AWS Glue to detect and redact PII from the data.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Transcribe to convert speech to text, store the transcriptions in Amazon RDS, and manually review the data for PII.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 14,
    "question": "<p>A data scientist is tasked with ensuring that their machine learning model is trustworthy, transparent, and avoids potential risks, including bias and poor generalization. They want to use a technique that assesses the model's fairness and explainability, both before and after training, while also meeting regulatory requirements. <br>Which AWS service can help them achieve this? </p><p><br></p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon SageMaker Clarify</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon Personalize</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Glue</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon SageMaker Ground Truth</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 15,
    "question": "<p>What is the main difference between identity-based policies and resource-based policies in AWS IAM?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Identity-based policies are applied to IAM users, groups, or roles, whereas resource-based policies are directly attached to AWS resources like S3 buckets.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Identity-based policies can only be applied to groups, while resource-based policies can only be applied to individual users.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Resource-based policies must always be AWS managed policies, while identity-based policies must be inline policies.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Identity-based policies are applied directly to AWS resources, while resource-based policies are attached to users, groups, or roles.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 16,
    "question": "<p>You are tasked with training a deep neural network model with billions of parameters using Amazon SageMaker. The model is too large to fit into a single GPU, and the dataset is also substantial. Which SageMaker technique should you use to ensure efficient training?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use only a single machine with larger memory to handle both the data and the model.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Data parallelism, where the dataset is split across multiple machines but the model remains on one GPU. </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Model parallelism, where the model itself is split across multiple GPUs, and each GPU handles a portion of the model.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use pre-built SageMaker containers without any adjustments for distributed training.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 17,
    "question": "<p>An organization is working on a text classification problem and decides to use Amazon SageMaker's built-in algorithms. The team is concerned about optimizing model performance without manually tuning hyperparameters. What features of SageMaker can help address this requirement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker's automatic hyperparameter tuning with the built-in linear learner algorithm.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Manually adjust hyperparameters for the built-in algorithm by trial and error.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilize custom algorithms to bypass hyperparameter tuning altogether.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker JumpStart to automatically deploy pre-trained text generation models without any customization.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 18,
    "question": "<p>You are managing a machine learning project and want to reuse a data preprocessing step from a previous pipeline for a new model. What feature of SageMaker Pipelines allows you to achieve this?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Pipelines must be redefined for each new model.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Only parameter values can be reused in different pipelines.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Pipeline steps are not reusable and must be manually duplicated for each new model.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Steps in a pipeline can be reused across different projects, saving time and effort. </p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 19,
    "question": "<p>A company is training a large image classification model on SageMaker using PyTorch, but the training process is taking much longer than expected. The machine learning engineer suspects there is a resource bottleneck. They decide to use SageMaker Profiler to diagnose and fix the issue. <br><br>Which of the following actions should the engineer take to identify and resolve the bottleneck? (Choose two.)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Profiler to monitor GPU utilization in real-time and check for underutilization.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Adjust the learning rate dynamically during training based on real-time profiler metrics.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Profiler to monitor disk I/O and reduce the dataset size if slow disk reads are detected.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up custom metrics to monitor memory allocation, and adjust the number of epochs if memory usage is too high.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 20,
    "question": "<p>A security officer at a financial institution wants to ensure that all S3 buckets in the account are configured according to the company’s security standards, such as enabling server-side encryption and blocking public access. The institution also requires continuous monitoring and real-time alerts if any bucket becomes non-compliant. <br><br>Which AWS services should be used to meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure AWS Config to evaluate S3 bucket settings against security rules and set up CloudWatch Events for real-time alerts.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up CloudWatch Alarms for changes in S3 bucket configurations, including encryption and access control policies.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use CloudWatch Logs to monitor S3 bucket changes and deploy Lambda functions to automatically correct misconfigurations.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS CloudTrail to track S3 API calls and alert if encryption or public access settings are changed.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 21,
    "question": "<p>A machine learning team is building an end-to-end workflow using Amazon SageMaker Pipelines to automate their ML tasks. They want to ensure that their workflow is reusable, scalable, and can handle concurrent execution of multiple machine learning jobs. What key features of SageMaker Pipelines make this possible?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The ability to manually trigger each step in the pipeline for better control. </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Directed Acyclic Graph (DAG) representation with automatic step execution and scalability to tens of thousands of concurrent workflows.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The integration with SageMaker Studio, which only supports small-scale experimental workflows.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Pre-defined templates for building models with a user-friendly UI but limited scalability.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 22,
    "question": "<p>A company wants to ensure efficient ETL operations on a large dataset stored in Amazon S3 using AWS Glue. They decide to partition the dataset by the \"year\" and \"month\" fields to optimize query performance and reduce costs. Which of the following is the MOST significant advantage of partitioning data in this manner?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>It allows Glue ETL jobs to process partitions independently, improving performance and reducing query costs.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>It ensures that the entire dataset is always scanned during each query.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>It eliminates the need for Glue crawlers to detect new data.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Partitioning ensures that data integrity is maintained across different S3 locations.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 23,
    "question": "<p>A media company wants to build an automated video transcription and translation pipeline. They need to transcribe speech from videos, translate the text into multiple languages, and create audio versions of the translated text. Which AWS services would fulfill these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Rekognition for transcription, AWS Lambda for text translation, and Amazon Polly for speech synthesis.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Transcribe to convert speech to text, Amazon Translate for text translation, and Amazon Polly to convert translated text to speech.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Transcribe to convert speech to text, Amazon SageMaker for text translation, and Amazon Rekognition for speech synthesis.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Glue for transcription, Amazon Translate for text translation, and AWS Lambda to convert the translated text to speech.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 24,
    "question": "<p>A company has deployed a machine learning model into production using Amazon SageMaker. Over time, the model's performance starts degrading due to changes in customer behavior. The team decides to monitor the model and retrain it periodically. <br><br>Which combination of AWS services should the team use to monitor and maintain the model’s performance?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon CloudWatch for monitoring model performance and SageMaker Data Wrangler to retrain the model.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Feature Store to track changes in data distribution, and SageMaker Clarify to handle data bias in producti</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker Model Monitor for monitoring data drift, and SageMaker Pipelines to automate model retraining.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Data Wrangler for data monitoring, and AWS Glue to retrain the model automatically.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 25,
    "question": "<p>An organization uses AWS Secrets Manager to store sensitive data such as API keys and database credentials. They want to share a secret with another AWS account without replicating it. How can they achieve this securely while ensuring the secret remains encrypted?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the Secrets Manager API to export the secret and share it with the other account via email.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Share the secret through an encrypted S3 bucket that the other account has access to, and send the KMS decryption key separately.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Attach a resource policy to the secret that grants read access to an IAM role in the other account, and ensure that the KMS key policy allows decryption by the same role.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Systems Manager to create a shared parameter that references the secret across accounts.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 26,
    "question": "<p>A financial services company needs to monitor its application infrastructure hosted on AWS. The company wants to set up an alarm that triggers an action if the average CPU utilization of their EC2 instances exceeds 80% over a 5-minute period. Additionally, they want to automatically stop the instance if the alarm triggers. <br><br>Which steps should the machine learning engineer take to meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a CloudWatch alarm to monitor the average CPU utilization of the EC2 instances. Use the alarm action to trigger an Auto Scaling policy that stops the instances.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Create a CloudWatch alarm to monitor the average CPU utilization of the EC2 instances. Configure an alarm action to send an SNS notification when the alarm state is triggered.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a CloudWatch alarm to monitor the average CPU utilization of the EC2 instances. Configure the alarm action to stop the instances directly.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a CloudWatch alarm to monitor the average CPU utilization of the EC2 instances. Use the alarm action to trigger an AWS Lambda function that stops the instances.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 27,
    "question": "<p>A retail company is using Amazon Kinesis Data Firehose to stream clickstream data to Amazon S3. Occasionally, the data fails transformation due to malformed records. The company wants to ensure that these failed records are captured for further analysis. What is the best approach to handle this scenario?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable data transformation with AWS Lambda and configure a Dead Letter Queue (DLQ) to capture malformed records that cannot be processed.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure Amazon Kinesis Data Firehose to use a retry policy to reprocess failed records indefinitely until successful.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable error logging in Amazon CloudWatch to identify and fix the records manually.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure Amazon Kinesis Data Firehose to send failed records to a separate Amazon S3 bucket for analysis using the backup configuration.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 28,
    "question": "<p>A retail company wants to process large volumes of real-time customer interaction data to personalize user experience on their website. They plan to use Amazon Kinesis to ingest, analyze, and store the streaming data. They need to perform complex analytics on the data in real-time and store it for future analysis. Which combination of services should they use to achieve these objectives?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Kinesis Firehose for data ingestion, Kinesis Data Analytics for real-time analysis, and AWS Glue for data storage in Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Kinesis Data Streams for data ingestion, AWS Lambda for real-time analysis, and Kinesis Firehose to deliver the processed data to Amazon Redshift.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Kinesis Data Streams for data ingestion, Kinesis Firehose for real-time analysis, and Kinesis Data Analytics to store the processed data in Amazon RDS.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Kinesis Data Streams for data ingestion, Kinesis Data Analytics for real-time analysis, and Kinesis Firehose to deliver the processed data to Amazon S3.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 29,
    "question": "<p>A machine learning engineer is training a deep learning model on SageMaker using TensorFlow. During training, the model's performance stops improving, and the engineer suspects that the model may be experiencing vanishing gradients. They want to use SageMaker Debugger to monitor the training and resolve this issue. Which steps should the engineer take to identify and address the vanishing gradients using SageMaker Debugger?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Utilize SageMaker Profiler to collect GPU utilization data and implement early stopping when vanishing gradients are detected.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable SageMaker Debugger to monitor resource utilization and set up alarms for underutilized GPUs.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Debugger’s built-in rules to monitor the gradient values during training and adjust the learning rate if vanishing gradients are detected.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Run a custom SageMaker Debugger rule to visualize CPU and memory usage, then adjust the batch size to optimize resource utilization.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 30,
    "question": "<p>A data scientist wants to optimize a machine learning model for deployment across multiple edge devices using SageMaker Neo. After training the model using TensorFlow in SageMaker, what are the next steps the data scientist should take to optimize the model for edge deployment while maintaining performance?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Neo to compile the model for the target hardware platform and deploy it using AWS IoT Greengrass.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up SageMaker Debugger to capture real-time metrics during inference on edge devices.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Debugger to profile the edge device resource usage and optimize the model based on profiling results.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Manually rewrite the model’s code to ensure it is compatible with the target edge hardware.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 31,
    "question": "<p>A machine learning engineer is tasked with building a predictive model for an e-commerce platform using Amazon SageMaker. The team is considering whether to use a SageMaker Notebook Instance or SageMaker Studio. The engineer needs the following features: <br>- The ability to run multiple notebooks in parallel within the same environment. <br>- A centralized space for persistent storage of datasets, notebooks, and logs. <br>- Seamless integration with existing AWS services like S3 and IAM. <br><br>Which SageMaker environment should the engineer choose, and why?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Studio because it is specifically designed for large-scale deep learning workloads, offering GPU-based instances for parallel notebook execution.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Notebook Instance because it allows running multiple notebooks in isolated environments, ensuring that tasks remain independent.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker Notebook Instance because it provides access to the full AWS CLI, enabling tighter control over resources and configurations.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Studio because it offers a persistent Jupyter Lab space where multiple notebooks can be managed, allowing centralized data access and integration with AWS services.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 32,
    "question": "<p>Which of the following is the primary advantage of using the Hyperband algorithm for hyperparameter tuning over other methods?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>It guarantees that all hyperparameter combinations are tested</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>It uses prior training results to suggest the next set of hyperparameters to test</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>It allocates more resources to promising configurations while stopping underperforming configurations early</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>It randomly selects hyperparameters to reduce training time</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 33,
    "question": "<p>A retail company is developing a machine learning model to forecast product demand using Amazon SageMaker. The dataset includes sales history, customer demographics, and seasonal trends. The data is stored in S3 and updated frequently. The company wants to automate the workflow for data preprocessing, feature engineering, and model training. They also need to ensure that the model is continuously retrained as new data arrives, and performance degradation is monitored. Additionally, the company wants to track different versions of the model and hyperparameter configurations during experimentation. <br>Which solution should the company implement?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Feature Store for real-time feature updates, SageMaker Clarify to monitor data bias, and SageMaker Pipelines to retrain the model based on new data.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Pipelines to automate data preprocessing, model training, and retraining, SageMaker Model Monitor to detect performance degradation, and SageMaker Experiments to track different model versions and hyperparameter settings.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Data Wrangler for data preprocessing, SageMaker Feature Store for feature engineering, and SageMaker Autopilot to automate the model training and hyperparameter tuning process.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Ground Truth to label data, SageMaker Model Monitor to automate model retraining, and SageMaker Autopilot for hyperparameter optimization.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 34,
    "question": "<p>A media company wants to automate the content moderation process for user-submitted images and accompanying text comments. They need to identify inappropriate content such as violent imagery in photos and offensive language in the text. The solution should automatically flag inappropriate submissions and store the flagged data for further review. <br><br>Which combination of services should the company use to implement this automated system?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Rekognition for content moderation of images, Amazon Comprehend for identifying offensive language in the text, and Amazon S3 for storing flagged data.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon SageMaker to build custom models for detecting inappropriate images, Amazon Comprehend for text analysis, and Amazon RDS for storing flagged data.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Rekognition to detect inappropriate content in images, Amazon Comprehend for sentiment analysis of the text, and Amazon DynamoDB for storing flagged data.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Comprehend to detect sentiment and entity extraction from text, Amazon Lex for conversational moderation, and Amazon Elasticsearch for storing flagged data.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 35,
    "question": "<p>A machine learning engineer is building a model to predict rare diseases based on a dataset where the majority of patients do not have the disease. To handle the imbalance in the dataset, the engineer is considering using SageMaker Data Wrangler to preprocess the data. <br><br>Which technique provided by SageMaker Data Wrangler is the BEST option to address class imbalance without increasing the risk of overfitting?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Simple oversampling to duplicate samples from the minority class.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Random undersampling to reduce the number of samples in the majority class.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Synthetic Minority Oversampling Technique (SMOTE) to generate new synthetic samples in the minority class.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Feature scaling to normalize the feature values in the dataset.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 36,
    "question": "<p>A machine learning engineer is working on a binary classification problem for detecting fraudulent transactions. The model's performance is being evaluated using multiple metrics. The model has a precision of 90% and a recall of 60%. The engineer is concerned about missing too many fraudulent transactions. <br><br>Which evaluation metric should the engineer focus on to improve the model’s ability to identify more fraud cases?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>F1 Score</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Accuracy</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Recall</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Precision</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 37,
    "question": "<p>A machine learning engineer is using SageMaker Data Wrangler to preprocess data from multiple sources. They want to analyze the relationships between different features in the dataset before selecting which features to include in model training. The engineer also needs to understand which features might have the greatest impact on the target variable. Which two functionalities of Data Wrangler will help the engineer achieve this? (Choose TWO)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Feature selection through automatic feature scaling.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Feature importance visualization using the Quick Model.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Exporting data directly to Amazon SageMaker for model deployment.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Recursive feature elimination.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Data visualization tools such as histograms and bar charts.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 38,
    "question": "<p>A machine learning engineer wants to ensure that their deployed model remains unbiased over time. They are concerned that the model may begin favoring a particular group in the predictions, which did not exist during training. <br><br>What specific type of drift should the engineer monitor using Amazon SageMaker Model Monitor to detect this issue?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Feature attribution drift</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Model quality drift</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Data drift</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Bias drift</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 39,
    "question": "<p>An organization is using SageMaker to train and deploy a reinforcement learning (RL) model that optimizes a portfolio of financial assets. Which of the following features of SageMaker RL would benefit their use case?</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The RL model must always be deployed on real-world environments; simulated environments are not supported.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>SageMaker supports pre-built RL toolkits like Intel Coach and Ray RLlib for state-of-the-art algorithms.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Training jobs can only run on external instances and not locally on notebook instances.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker allows monitoring and adjusting the training through Python SDK integration.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 40,
    "question": "<p>A company is using AWS Glue to run ETL jobs on large datasets stored in Amazon S3. The data is organized into partitions based on the year the data was collected, with each year having its own folder structure. The team frequently runs queries that filter data by year, such as retrieving records only from 2023. How does this partitioning strategy benefit their data processing and querying?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>It increases the IOPS operations required, slowing down query response times.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>It allows AWS Glue to skip irrelevant partitions, reducing data scanned and improving query performance.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>It increases the storage costs due to more files being created in S3.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>It causes AWS Glue to process all partitions equally, regardless of the query conditions.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 41,
    "question": "<p>A machine learning engineer is tasked with training a custom PyTorch model using Amazon SageMaker. They want to take advantage of SageMaker's pre-built PyTorch container while maintaining control over the training logic. The engineer also needs to include several custom Python libraries to handle specific pre-processing tasks. <br><br>What steps should the engineer follow to set up and train the model using SageMaker's Script Mode?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Create a custom Docker container for the model, push it to Amazon ECR, and pull it into SageMaker for training.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Lambda to trigger a training job, passing the model and training script from the notebook instance.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Write a Python script with the training logic, specify it as an entry point in the SageMaker estimator, and include a requirements.txt file to install custom libraries during training.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Install the required Python packages directly on the SageMaker notebook instance and run the training using the SageMaker built-in algorithms. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 42,
    "question": "<p>Team of data scientists is working on a computer vision project and wants to leverage AWS SageMaker’s built-in algorithms. They plan to use SageMaker JumpStart to streamline the process. What are the key advantages of using SageMaker JumpStart for this project?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker JumpStart only supports text-based models, so it will not be useful for image classification tasks.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>JumpStart forces data scientists to only use SageMaker Studio for deployment, limiting flexibility. </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker JumpStart allows the use of pre-built solutions that come with end-to-end pipelines, reducing time to deployment.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Using JumpStart will eliminate the need for any hyperparameter tuning and data preparation.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 43,
    "question": "<p>A healthcare company needs to ensure compliance with data privacy regulations like HIPAA and GDPR for their sensitive information stored in S3 buckets. They plan to use AWS Macie to monitor and secure this data. Which of the following are the key benefits of using AWS Macie in this scenario? (Choose TWO)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Automatically classifies sensitive data such as PII, financial data, and health records stored in S3.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Monitors the performance of S3 buckets to ensure optimal read/write operations.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Automatically rotates sensitive data keys in S3 buckets for enhanced security.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Detects anomalous access patterns to identify potential unauthorized access to sensitive data. </p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Provides encryption for sensitive data using AWS KMS without additional configuration.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 44,
    "question": "<p>A company is managing multiple AWS accounts and wants to implement a centralized user management system. They need to ensure that users can seamlessly access resources across all accounts without managing multiple credentials. The company also requires the ability to control access through policies and track user activities. <br><br>Which solution will meet these requirements MOST effectively?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up AWS Organizations and enable Single Sign-On (SSO) for centralized access management across all accounts.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configure IAM roles in each account and allow users to switch roles as needed.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create IAM users in each AWS account and assign the necessary permissions.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Directory Service to create a single directory and link it to all AWS accounts.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 45,
    "question": "<p>A data engineering team is tasked with setting up an automated ETL pipeline using AWS Glue. They have a semi-structured data file stored in Amazon S3 and need to extract, transform, and load this data into Amazon Redshift for further analysis. Additionally, they want to catalog the data schema and make it queryable via SQL. How should the team implement this solution efficiently using AWS Glue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Manually create the schema in Redshift before loading the data to avoid unnecessary processing by AWS Glue crawlers and run AWS Glue ETL jobs to load the data.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Glue crawlers to automatically infer the schema from the data in S3 and store the schema in the Glue Data Catalog. Then, run AWS Glue ETL jobs to extract, transform, and load the data into Amazon Redshift.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Lambda to manually trigger AWS Glue jobs and control costs, avoiding automatic cataloging and schema inference.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Glue ETL jobs to extract the data from S3, transform it using pre-built transformations, and load it directly into Amazon Redshift without any further processing.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 46,
    "question": "<p>Which of the following is the most appropriate problem type for regression models?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Estimating the future sales revenue based on advertising spend and historical sales data.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Predicting whether a customer will churn or not based on past behavior.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Classifying images into different categories such as cats, dogs, and birds.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Grouping users into segments based on browsing patterns without knowing specific user behaviors in advance.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 47,
    "question": "<p>Which of the following are valid sources for importing data into Amazon SageMaker Data Wrangler? (Choose TWO)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon S3</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon Kinesis</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker Feature Store</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>AWS Glue DataBrew </p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Amazon RDS</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 48,
    "question": "<p>A Machine Learning engineer is building an e-commerce chatbot using Amazon Bedrock. The chatbot must handle real-time customer queries, generate personalized recommendations, and operate at scale without infrastructure management. The engineer also needs to fine-tune the foundational model with historical customer data and ensure the security of this data. <br>Which combination of services should the engineer use to achieve these requirements? (Choose TWO)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Bedrock for deploying foundation models with serverless infrastructure and fine-tuning capabilities</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Lambda for model deployment, which enables the scaling of real-time inference for customer queries</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon S3 for storing historical customer data and generated model outputs</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon SageMaker to deploy and fine-tune foundation models, ensuring customization of machine learning models</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use AWS CloudTrail to monitor model inference processes and ensure fine-grained data access control</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 49,
    "question": "<p>A machine learning team wants to track the history of model versions, including training data, hyperparameters, and performance metrics over time. Which feature of SageMaker should they use?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Model Registry</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Notebook Instances</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker Training Jobs</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Pipelines</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 50,
    "question": "<p>A financial institution is using Amazon Athena to run SQL queries on its transaction logs stored in Amazon S3. They need a scalable way to keep track of schema changes in these logs, as the structure of the logs may change over time. Additionally, they want to ensure the queries are fast and cost-effective. <br><br>Which of the following is the most effective approach to ensure efficient schema management and optimized query performance?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Glue Crawlers to automatically detect schema changes and store the metadata in the Glue Data Catalog for Amazon Athena to query.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable S3 Select to query only the necessary parts of the logs, ensuring that schema changes are handled automatically.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create an Amazon Athena table with a hardcoded schema and manually update it whenever the log structure changes.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the transaction logs in Amazon RDS and create a pipeline to replicate them to Amazon Athena for better querying capabilities.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 51,
    "question": "<p>A machine learning team is using Amazon SageMaker Experiments to evaluate different model configurations. They want to track and compare the performance of several training runs with varying hyperparameters, data features, and algorithms to identify the best performing model. What components in SageMaker Experiments should they use to organize and track these training runs?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Pipelines, Trials, Stages, Logs</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Experiments, Trials, Trial Components, Trackers</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Hyperparameters, Experiments, Trackers, Notebooks</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Trackers, Algorithms, Metrics, Pipelines</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 52,
    "question": "<p>Which of the following features of SageMaker Jupyter Notebooks helps ensure that data is not lost during the machine learning workflow?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Persistent storage</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>On-demand instance provisioning</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Real-time inference endpoints</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Pre-installed libraries like TensorFlow and PyTorch</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 53,
    "question": "<p>A data scientist is working with Amazon SageMaker Studio and needs to set up a JupyterLab environment for collaboration with their team. Which of the following best describes how SageMaker Studio notebooks are different from standalone SageMaker notebook instances?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Studio notebooks are integrated within SageMaker Studio, allowing persistent storage and multiple notebooks per project, while standalone notebook instances operate independently.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Studio notebooks are pre-configured only for data preparation tasks and cannot be used for model training or deployment.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker Studio notebooks are for lightweight tasks, whereas standalone SageMaker notebook instances are used for heavy machine learning workloads.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Standalone SageMaker notebook instances have built-in collaboration tools, whereas SageMaker Studio notebooks do not support collaboration.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 54,
    "question": "<p>A financial institution's machine learning model in production shows a significant drop in prediction accuracy over the last month. Upon investigation, the data science team suspects that the real-world data used for inference has shifted from the training dataset. They want to continuously monitor the quality of the input data to ensure it meets the expected constraints. <br><br>Which steps should the team take to monitor the data quality using Amazon SageMaker Model Monitor? (Select TWO)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Manually review each data point for drift after the model has made predictions.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS CloudTrail to log and evaluate prediction errors.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Define a baseline of the training data to set expected statistical properties.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Enable data capture on the SageMaker endpoint to store incoming requests and predictions.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Set up a batch transform job to retrain the model when data drift is detected.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 55,
    "question": "<p>You are a data scientist at a healthcare company tasked with building a predictive model using SageMaker. Your team wants to avoid the complexity of managing infrastructure and prefers a solution that minimizes setup time. <br><br>Which of the following SageMaker features would be MOST appropriate for your use case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker’s built-in algorithms, which come pre-packaged in containers, requiring only data input and hyperparameter setup.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Ground Truth to automatically handle infrastructure setup and dataset labeling.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Build a custom container with proprietary algorithms and manage the infrastructure manually for optimized performance.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Build your own deep learning model using TensorFlow or PyTorch and manually deploy it on an Amazon EC2 instance.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 56,
    "question": "<p>A machine learning team is using Amazon SageMaker Ground Truth to label a dataset for an object detection task. They want to reduce labeling costs without sacrificing quality. What strategy provided by Ground Truth would BEST help the team achieve this?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a completely automated machine learning model to handle all labeling tasks.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Utilize a third-party vendor for all labeling tasks to ensure the highest accuracy.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Implement active learning, where a model labels simple cases, and human labelers handle complex cases.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Fully manual labeling by a private workforce within the organization.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 57,
    "question": "<p>A global retail company collects customer feedback in various languages through email and web forms. The company wants to automatically detect customer sentiment, categorize the feedback into relevant product categories, and translate the feedback into English for further analysis by the product team. The solution should support scalability and automate the entire process. <br><br>Which combination of services would BEST meet the company’s needs?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Comprehend for sentiment analysis, Amazon Translate for translating feedback to English, and Amazon SageMaker for classifying feedback into product categories.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon Rekognition for analyzing the text within images, Amazon Comprehend for categorizing feedback, and Amazon Translate for language translation.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Translate for translating feedback, Amazon Comprehend for sentiment analysis and entity recognition, and AWS Lambda for automating the process.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Amazon Comprehend for both sentiment analysis and categorization, Amazon Translate for translation, and Amazon Kendra for providing search capabilities across the feedback.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 58,
    "question": "<p>A company needs to create a machine learning model to detect fraudulent transactions in real-time. They plan to use Amazon SageMaker for model training and deployment. The data for training is stored in Amazon S3, and they require the ability to preprocess the data, perform hyperparameter tuning, and deploy the model to an endpoint for real-time inference. The company also wants to automate the entire workflow from data preprocessing to model deployment. <br><br>Which solution will meet these requirements MOST effectively?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon SageMaker Processing for data preprocessing, SageMaker built-in algorithms for training, SageMaker Automatic Model Tuning for hyperparameter tuning, and SageMaker Endpoint for deployment.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Glue for data preprocessing, train the model on a custom EC2 instance, use Amazon SageMaker for hyperparameter tuning, and deploy it using Amazon API Gateway.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon EMR for data preprocessing, SageMaker Script Mode for custom training scripts, perform hyperparameter tuning manually, and deploy the model using Amazon SageMaker Hosting Services.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon SageMaker Data Wrangler for data preprocessing, SageMaker built-in algorithms for training, SageMaker Experiments for tracking, and deploy the model using AWS Lambda.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 59,
    "question": "<p>Which of the following correctly describes the relationship between trials and trial components within SageMaker Experiments?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A trial consists of multiple experiments, and each experiment captures metadata about data processing.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>A trial component tracks the hardware resources used during training, while a trial compares different algorithms.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>A trial captures different models trained in parallel, and trial components represent different datasets used.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>A trial is a single training run with specific configurations, and trial components capture different stages of the workflow such as data processing or model training.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 60,
    "question": "<p>A media company is ingesting high-velocity log data using Amazon Kinesis Data Firehose. They need to analyze this data in near real-time using SQL queries. The data is semi-structured and needs to be loaded into a data warehouse for fast querying. Which combination of services should the company use to accomplish this?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Kinesis Data Firehose to load the data into Amazon Redshift using the native Firehose to Redshift integration for fast querying.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Stream data into Amazon Kinesis Data Streams, then use a Lambda function to transform and store the data in Amazon DynamoDB</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Kinesis Data Firehose to stream the data into Amazon RDS with a PostgreSQL database for querying.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Kinesis Data Firehose to stream the data into an Amazon S3 bucket, then use Amazon Athena to run SQL queries on the data stored in S3.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 61,
    "question": "<p>What does data drift refer to in the context of machine learning model monitoring, and why is it important to detect?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The sudden failure of a model due to an error in the underlying algorithm.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>A situation where the model's predictions deviate from expected outcomes in a systematic way.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>A change in the input data distribution over time, making it different from the data used for model training. </p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>An issue that arises when the system running the model experiences infrastructure downtime.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 62,
    "question": "<p>A data engineering team needs to analyze large datasets stored in an Amazon S3 data lake using SQL queries. The data is in semi-structured JSON format, and the team wants to improve query performance by converting it into a columnar format. They also want to ensure the schema is automatically inferred and stored, so that the data can be queried directly from Amazon S3 without loading it into a relational database. Which of the following steps should the team implement using AWS Glue and Amazon Athena? (Choose Two)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up an AWS Glue Crawler to automatically infer the schema from the JSON files and store the metadata in the Glue Data Catalog.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Athena to run a query that converts JSON data to Apache ORC format and saves it directly into Amazon Redshift.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Manually create an Amazon Athena schema for each dataset and register it in the Glue Data Catalog.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Glue ETL jobs to convert the JSON data into Apache Parquet format and store it back in S3.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Enable partitioning in Athena to automatically split large datasets by date, optimizing query performance.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 63,
    "question": "<p>An e-commerce platform ingests high volumes of real-time customer behavior data and has multiple applications consuming this data. Due to high throughput needs, the company wants to avoid bottlenecks when scaling the number of consumers. Which Kinesis Data Streams feature should they configure?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enhanced Fan-Out</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>On-Demand Sharding</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Provisioned Throughput</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Partition Key</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 64,
    "question": "<p>A data scientist is working on a binary classification problem to detect fraudulent transactions. The model has a high precision but a relatively low recall. What does this indicate about the model’s performance?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The model is poorly identifying true negatives.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The model is accurately identifying most of the fraudulent transactions.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The model has a high rate of correctly predicted frauds, but misses many actual frauds.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>The model's predictions are consistent, but it makes many false positive predictions.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 65,
    "question": "<p>A data scientist at a retail company needs to quickly build a machine learning model to predict customer churn. The data scientist has limited time and prefers to avoid the complexity of setting up custom infrastructure or algorithms. Which approach would be most suitable for this scenario using Amazon SageMaker?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a custom model using a proprietary algorithm on Amazon EC2, ensuring complete control over the infrastructure.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Jumpstart to deploy a pre-built solution for customer churn prediction without customization.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker built-in algorithms, such as XGBoost, with the provided dataset, specifying only the hyperparameters and instance type.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Build a custom algorithm using TensorFlow on a SageMaker notebook instance, with manual setup for data processing and infrastructure management.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  }
]