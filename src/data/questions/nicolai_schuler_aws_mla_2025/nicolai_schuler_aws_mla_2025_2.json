[
  {
    "id": 1,
    "question": "<p>\"A company needs to automatically resize images uploaded to an Amazon S3 bucket and store the resized images in another S3 bucket. How can this be accomplished using AWS services? (Choose Two)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Batch to process the images and store the results in another S3 bucket. </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon EC2 instances to constantly monitor the S3 bucket and resize images.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure AWS Glue to transform and resize the images as they are uploaded.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create a Lambda function to resize images and configure it to be triggered by S3 'ObjectCreated' events.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Set up an S3 Event Notification to trigger the Lambda function when a new object is created.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 2,
    "question": "<p>An organization using Amazon SageMaker has deployed a machine learning model into production. To adhere to responsible AI practices, they want to continuously monitor the model for fairness and transparency over time. <br>Which SageMaker feature should they use to ensure that biases do not emerge in the model as it processes new data?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Model Monitor to track data distribution and performance.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Autopilot for automatic tuning and optimization of the model’s hyperparameters.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker Pipelines to automate model retraining when bias is detected.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Data Wrangler for pre-training data processing.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 3,
    "question": "<p>A company stores a large dataset in S3 that consists of multiple CSV files. They want to run queries on the data and visualize it using QuickSight. However, the dataset lacks a predefined schema. <br><br>How should the company set up the data infrastructure to meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Glue crawlers to infer the schema, store the schema in Glue Data Catalog, and use Athena to run queries directly on the CSV files in S3.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Redshift Spectrum to read the CSV files from S3 and manually create a schema for Redshift to query the data. </p><p><br></p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Glue ETL to convert CSV files into JSON format, create a schema in Glue Data Catalog, and then import data into Redshift for querying.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Glue ETL to transform the CSV data into a Parquet format, store the schema in the Glue Data Catalog, and use Athena to query the data.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 4,
    "question": "<p>You are using SageMaker Data Wrangler to preprocess data from Amazon S3 for a machine learning model. After performing one-hot encoding on a categorical feature, you notice that some features are dominating the learning process due to large numerical ranges. What technique should you apply to ensure all features are treated equally by the model?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Drop the dominating features to balance the dataset</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Label encoding for categorical features</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Feature normalization using Min-Max scaling</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use recursive feature elimination to remove unimportant features</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 5,
    "question": "<p>A company needs to ensure that its machine learning models are ethically developed by reducing bias and detecting potential vulnerabilities in the output. How can Amazon SageMaker Ground Truth help in this process? </p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>By leveraging Ground Truth’s human-in-the-loop feedback mechanism to review and correct model outputs for bias and vulnerabilities.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>By using Ground Truth only for labeling data, leaving bias detection to other AWS services like SageMaker Clarify.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>By exclusively relying on third-party vendors to detect bias and vulnerabilities in model outputs.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>By using Ground Truth's fully automated labeling system to eliminate all bias from the training data.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 6,
    "question": "<p>Which of the following best describes a trust policy in AWS IAM?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A policy attached to a user that allows them to assume roles in different AWS accounts.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>A policy that defines permissions to access AWS resources from external services but only for AWS-managed resources like Lambda functions.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>A policy that defines all actions permitted for an AWS account’s root user.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>A policy that specifies which identities (users, services, or accounts) are allowed to assume a role, commonly used for cross-account access.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 7,
    "question": "<p>A company uses Amazon SageMaker Pipelines to orchestrate its machine learning workflows. They need to reuse the same preprocessing step across multiple different models. How can SageMaker Pipelines help with reusability?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>By enabling the same pipeline step to be reused across different pipelines without redefinition.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>By storing steps in Amazon S3, which can be manually imported into each pipeline.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>By requiring each pipeline to be redefined from scratch for different models.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>By allowing steps to be manually copied and pasted across different pipeline definitions.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 8,
    "question": "<p>Which of the following best describes the \"human-in-the-loop\" (HITL) approach in Amazon SageMaker Ground Truth?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A process where human laborers label all the data to ensure the highest accuracy.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>A combination of machine learning and human feedback where human workers only intervene in complex tasks, improving labeling efficiency.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>A method of labeling data that eliminates the need for human feedback entirely through automation.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>A system where human labelers work in parallel with machines to label data simultaneously.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 9,
    "question": "<p>A data scientist at a healthcare company is tasked with building a machine learning model to predict patient outcomes based on clinical data. The team decides to test different algorithms, hyperparameters, and feature sets to find the most accurate model. To manage and compare these variations, the data scientist decides to use SageMaker Experiments. <br><br>Which of the following strategies will BEST allow the team to track and compare their experiments, ensuring reproducibility and organized management of model configurations?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Experiments to organize model runs into trials under a single experiment, where each trial logs configurations and metrics, enabling easy comparison of different models and hyperparameters.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Log every hyperparameter and configuration change manually to a spreadsheet and compare the metrics offline after each run.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a single trial with one configuration and manually adjust hyperparameters each time for new tests.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Ground Truth to label datasets and organize each training run as a separate trial component under different experiments.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 10,
    "question": "<p>A fintech company is building a machine learning pipeline to predict credit card fraud. They plan to use SageMaker Pipelines to automate the entire workflow, including data preprocessing, model training, hyperparameter tuning, and deployment. To ensure that the pipeline produces the most accurate model, they want to test multiple configurations of the model, including variations in the algorithm and feature sets. They also need to track and compare each configuration to determine the best performing model. <br><br>Which of the following solutions BEST fits their needs?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon S3 to store training data, manually track hyperparameter configurations, and SageMaker Pipelines to handle training and deployment.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Pipelines to automate the workflow and SageMaker Experiments to track and compare multiple configurations.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Feature Store to manage model versions and SageMaker Autopilot to automatically deploy the best model based on accuracy.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Ground Truth to create a labeled dataset, SageMaker Autopilot to automate model training, and SageMaker Feature Store to track hyperparameters and metrics.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 11,
    "question": "<p>A company is training a deep learning model with billions of parameters, which is too large to fit on a single machine. They decide to use model parallelism on Amazon SageMaker. <br>What key feature of SageMaker makes model parallelism easier to implement, and how does it work?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Model Monitor automatically manages and optimizes the model splitting process across multiple GPUs.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>SageMaker’s Model Parallelism Library automatically splits the model across multiple GPUs or machines, based on the model architecture and available memory.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>SageMaker’s built-in algorithms dynamically adjust the batch size to ensure the model can be trained on multiple machines.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker handles model parallelism by dividing the dataset across multiple machines and averaging the model gradients.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 12,
    "question": "<p>A company is using Amazon Bedrock to develop a generative AI application that creates personalized email content based on customer preferences. They need to store customer interaction data, securely fine-tune the foundation models with their own dataset, and monitor model performance. <br><br>Which combination of AWS services should the company use to meet these requirements? (Choose THREE.)</p>",
    "corrects": [
      1,
      5,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon SageMaker for model fine-tuning</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>AWS CloudWatch for storing and querying model logs</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Kinesis for streaming real-time customer interaction data to the model </p><p><br></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS CloudTrail for monitoring and auditing access to the Bedrock models</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>AWS Identity and Access Management (IAM) for securing access to Bedrock APIs</p>",
        "correct": true
      },
      {
        "id": 6,
        "answer": "<p>Amazon S3 for storing customer interaction data</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 13,
    "question": "<p>You are developing an AI model for image classification that shows poor performance on both training and new data. The model exhibits high bias, meaning it is unable to capture the complexity of the data. According to responsible AI practices, which TWO strategies should you implement to reduce bias and improve model performance? (Choose TWO)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use cross-validation to ensure the model is generalizing well across different subsets of data.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Apply early stopping to prevent the model from learning noise in the training data.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Reduce the number of features by applying dimensionality reduction techniques like PCA.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Implement oversampling to duplicate samples from the majority class in the dataset.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Add more training data to increase the model's ability to learn patterns from diverse examples.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 14,
    "question": "<p>A company needs to perform regular ETL operations on large semi-structured datasets stored in Amazon S3 and then load the processed data into Amazon Redshift for analytics. They also want to automatically extract metadata from the S3 files and query the data using SQL without managing the infrastructure. <br><br>Which combination of AWS Glue services should the company use to meet their requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Glue Data Catalog for metadata extraction, Glue crawlers for processing data into Redshift, and Glue ETL jobs to transform the data.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Glue Data Catalog for metadata extraction, Glue ETL jobs for data transformation, and Athena for querying the data directly from S3.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS Glue Data Catalog to manage S3 data, manual scripts to process the data, and Redshift for analytics.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Glue Data Catalog for metadata extraction, Glue ETL jobs to process the data, and Glue crawlers to schedule the jobs.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 15,
    "question": "<p>Your organization wants to perform image classification using Amazon SageMaker’s built-in algorithms. <br>Which steps are required to train a model using one of SageMaker’s pre-packaged algorithms for this task? (Choose Two) </p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Ground Truth to automatically label the data before training.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Write custom code to build and deploy a container with a pre-trained model for image classification.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Select an appropriate algorithm such as XGBoost, which is optimized for image classification.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Provide a labeled dataset for training the model and define the necessary hyperparameters.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Define the compute resources (e.g., instance type) and run a training job by referencing the pre-built container.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 16,
    "question": "<p>What is the key challenge that results from a high variance model in the context of responsible AI, and what technique is most effective to combat it?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Overfitting, and the solution is early stopping</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Underfitting, and the solution is reducing the complexity of the model</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Overfitting, and the solution is adding more training data</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Underfitting, and the solution is cross-validation</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 17,
    "question": "<p>A company is developing a machine learning model using a proprietary machine learning framework that is not supported by SageMaker. They decide to use SageMaker's \"bring your own container\" approach to handle both training and inference. What advantages does this approach provide, and which tasks are still the company’s responsibility?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker will fully manage the environment, and the company only needs to provide their training code.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>SageMaker will automatically handle container creation and distribution, but the company must manage data preprocessing steps.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The company gains full control over the training and inference environment but must manage the Docker image creation, container configuration, and deployment.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>The company can still use SageMaker’s built-in training algorithms, and only needs to customize the inference process with the custom container.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 18,
    "question": "<p>A data scientist needs to set up monitoring for a deployed machine learning model using Amazon SageMaker. They are primarily concerned with detecting when the incoming data used for predictions becomes significantly different from the training data. What type of drift should they monitor for in this case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Bias Drift</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Data Drift</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Model Quality Drift</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Feature Attribution Drift</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 19,
    "question": "<p>Which of the following is a benefit of using SageMaker Pipelines for machine learning workflows?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>It automates and organizes machine learning workflows, making them reusable and scalable.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>It allows users to run SQL queries on large datasets stored in Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>It provides a platform for building deep learning models with low-latency inference.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>It enables real-time analytics without batch processing.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 20,
    "question": "<p>A company is using Amazon SageMaker for hyperparameter tuning. They want a technique that stops poorly performing models early to allocate resources more efficiently to better performing models. Which tuning strategy should they use?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Random Search</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Grid Search</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Bayesian Optimization</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Hyperband</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 21,
    "question": "<p>A company is training a reinforcement learning model in SageMaker to optimize energy usage in a smart grid. After training the model, they need to evaluate its performance before deployment. Which SageMaker feature is most appropriate for this evaluation?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Athena to run SQL queries on the training results to evaluate performance.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the SageMaker model monitor to automatically track and evaluate the model's performance on new data.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy the model to an endpoint and evaluate performance using Amazon QuickSight.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker's built-in evaluation tools and plot training metrics using SageMaker SDK. </p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 22,
    "question": "<p>A machine learning engineer is training a deep learning model on SageMaker and notices that the model performs well on the training data but poorly on the validation set, indicating overfitting. The engineer wants to use SageMaker Debugger to automatically detect overfitting and take corrective actions. <br><br>Which of the following SageMaker Debugger features can help identify and resolve overfitting issues during training?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up SageMaker Profiler to track GPU memory utilization and adjust the batch size to prevent overfitting.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use built-in SageMaker Debugger rules to monitor the loss curve and alert when overfitting is detected.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Monitor I/O operations using SageMaker Profiler to detect overfitting and dynamically adjust the learning rate.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable real-time alerts in SageMaker Debugger when CPU utilization exceeds a threshold to detect overfitting.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 23,
    "question": "<p>A machine learning team is running several trials using Amazon SageMaker Experiments to identify the best model configuration for their project. After multiple trials, they need to visualize and compare the accuracy and precision of each trial to select the optimal model. <br>What feature of SageMaker Experiments will allow them to easily perform this comparison?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Studio's graphical interface</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Pipelines</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Trackers in SageMaker CLI</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>CloudWatch Metrics Dashboard</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 24,
    "question": "<p>A retail company has deployed a demand forecasting model using Amazon SageMaker. They want to ensure the model remains accurate over time by monitoring both data quality and model performance. They also need to automate the retraining of the model if significant data drift or performance degradation is detected. Which combination of AWS services and features would best help the company achieve this?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Ground Truth for monitoring data drift, Amazon CloudWatch for logging, and AWS Step Functions for model deployment</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Data Wrangler for detecting data drift, AWS Glue for data processing, and SageMaker Debugger for retraining the model</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Neo for monitoring model performance, AWS Lambda for data drift detection, and Amazon Kinesis for real-time predictions</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Model Monitor for detecting data drift, Amazon CloudWatch for setting alerts, and SageMaker Pipelines for automating model retraining</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 25,
    "question": "<p>An e-commerce company has deployed a web application on AWS using multiple EC2 instances behind an Elastic Load Balancer (ELB). The company wants to monitor real-time performance metrics of its application, such as CPU utilization, memory usage, and request latency, and receive alerts if any thresholds are exceeded. Which AWS services and features should be used to meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up AWS Lambda functions to periodically pull CPU and memory metrics from EC2 and push them to CloudWatch for monitoring and alerting.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS CloudTrail to capture EC2 instance activity and automatically alert on high CPU or memory usage.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS CloudWatch to monitor CPU utilization and set CloudWatch Alarms for threshold breaches. Install the CloudWatch Agent on EC2 instances to monitor memory usage and other custom metrics.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Enable AWS Config to track the CPU and memory metrics, and set up SNS notifications for threshold alerts.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 26,
    "question": "<p>A machine learning engineer is setting up a Jupyter notebook environment to streamline an end-to-end machine learning workflow, including model training and deployment. The engineer wants to minimize manual infrastructure management and use a fully integrated environment within Amazon SageMaker. <br><br>Which option should the engineer choose to achieve this?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a SageMaker notebook instance from the AWS Management Console and attach an IAM role.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Manually provision an EC2 instance with Jupyter notebooks installed and manage the environment through AWS CLI.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Install SageMaker libraries on a local machine and run Jupyter notebooks locally.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Studio to set up a domain and configure multiple Jupyter Lab notebooks within the same workspace.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 27,
    "question": "<p>Which of the following types of drift is best described as a systematic favoring of certain groups in the model's predictions over time?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Data drift</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Model quality drift</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Feature attribution drift</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Bias drift</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 28,
    "question": "<p>A company is preparing a machine learning pipeline using Amazon SageMaker Data Wrangler. The data engineer needs to integrate multiple data sources, clean and transform the data, and output it to an Amazon S3 bucket for training. What steps should the engineer take to ensure the data is properly prepared for training in SageMaker?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Export the transformed dataset from Data Wrangler directly to SageMaker Feature Store for training.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Data Wrangler to create a pipeline, automatically export the transformed data to S3 in Apache Parquet format, and then use SageMaker training jobs to access the data.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Clean and transform the data using SageMaker Processing jobs, and then manually upload it to an Amazon S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Data Wrangler to import, clean, and transform the data, and then export the final dataset to Amazon S3 in CSV format.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 29,
    "question": "<p>An organization needs to develop and deploy a machine learning model for real-time customer behavior prediction using SageMaker. They want a solution that provides persistent storage, allows collaborative development, and makes it easy to track and manage multiple experiments in an integrated interface. <br><br>Which of the following best meets the organization's needs?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Studio</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Neo</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker Autopilot</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Ground Truth</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 30,
    "question": "<p>A customer support system uses both Amazon Kendra for retrieving relevant support documents and Amazon Lex for handling voice and text interactions. The company wants to ensure that their Kendra index is always up to date with new documents from an S3 bucket, and that Lex handles fallback intents when the user's question is unclear. <br>What solution ensures that both services are working together optimally?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Glue to continuously update the Kendra index and rely on Lex's speech recognition for fallback intent handling. </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon EventBridge to trigger Lambda functions that update the Kendra index and implement Lex's fallback intent for handling ambiguous queries.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Implement AWS CloudTrail to monitor the Kendra index and integrate it with Lex’s natural language processing for ambiguous questions.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon S3 Event Notifications to directly update the Kendra index and set up manual intents in Lex for ambiguous queries.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 31,
    "question": "<p>A customer service company is looking to enhance its chatbot by providing real-time voice interaction with users. The bot needs to handle user queries in both text and speech formats. It must convert incoming speech to text, process the text using a chatbot engine, and then convert the bot’s text responses back to speech for the user. <br><br>Which combination of services should the company implement?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Transcribe to convert user speech to text, Amazon Lex to handle the chatbot processing, and Amazon Polly to convert text responses to speech.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Polly to convert user speech to text, Amazon Lex to process the text queries, and Amazon Transcribe to convert text responses to speech.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Transcribe to convert user speech to text, AWS Lambda to process the queries, and Amazon Polly to convert text responses to speech.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Rekognition for speech-to-text conversion, Amazon Lex for chatbot processing, and Amazon Polly for text-to-speech conversion. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 32,
    "question": "<p>A machine learning team is using SageMaker Model Registry to manage the lifecycle of their models. After training a new version of a model, they want to automate the registration of the model in the registry and deploy it to a real-time endpoint. What features of SageMaker Model Registry and Pipelines will help achieve this?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Model versions in the registry cannot be automatically deployed; manual steps are always required.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Pipelines can automate the registration of the model in the registry and then trigger the deployment to a real-time endpoint based on predefined steps. </p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The team must manually register the model in the Model Registry and deploy it using the AWS Management Console.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Model Registry automatically deploys the model to a real-time endpoint without any further configuration.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 33,
    "question": "<p>You are tasked with building a reinforcement learning model in Amazon SageMaker to optimize warehouse robot navigation. You need to define the reinforcement learning (RL) environment, the agent's actions, and the reward function. How should you formulate the environment and rewards to ensure the robot reaches its destination efficiently?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The environment should simulate a fixed path for the robot, with a reward given only at the destination.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The environment should simulate real-world warehouse conditions with dynamic obstacles, and rewards should be given when the robot makes progress toward the destination, with penalties for collisions or delays.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The environment should simulate fixed navigation tasks, and the reward should be given for maintaining a steady speed throughout the navigation.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The environment should simulate dynamic obstacles, and rewards should be given only if the robot avoids all obstacles.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 34,
    "question": "<p>A retail company is using Amazon Personalize to provide personalized product recommendations based on real-time customer behavior on their e-commerce platform. They want to integrate Amazon DynamoDB to store customer session data, use Amazon Kinesis for real-time event streaming, and leverage Amazon S3 for historical data storage. Additionally, they need the solution to scale during peak shopping periods, such as holidays. Which of the following strategies should they implement? (Choose TWO.)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Personalize to directly process real-time streaming data from Amazon Kinesis and Amazon DynamoDB.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon EC2 Auto Scaling to manage the increasing number of personalized recommendation requests during peak traffic.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Lambda to trigger real-time updates to Amazon Personalize when new customer events are detected in Amazon DynamoDB.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Store session data in Amazon RDS for high availability and scalability during peak traffic.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Store historical customer interaction data in Amazon S3 and periodically feed it into Amazon Personalize for batch training.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 35,
    "question": "<p>A data engineer is tasked with building a real-time streaming data pipeline where data from IoT devices is continuously ingested and needs to be distributed to multiple consumers for further processing and analysis. However, the engineer is concerned about potential bottlenecks due to multiple consumers sharing the same read throughput. What feature of Kinesis Data Streams can address this concern?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Shard Splitting</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enhanced Fan-out</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Partition Key</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Kinesis Producer Library</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 36,
    "question": "<p>A data scientist is using Amazon SageMaker for a machine learning project. They are deciding between two options: using SageMaker notebook instances or SageMaker Studio notebooks. The project involves running multiple notebooks and requires easy collaboration with other team members. <br><br>Which of the following should the data scientist choose?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Launch Jupyter notebooks on EC2 instances for better customization and control.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use local Jupyter notebooks and store results in an Amazon S3 bucket for collaboration.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker notebook instances, as they operate as standalone environments with flexible configurations.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Studio notebooks, as they provide a collaborative environment with persistent storage and integrated tools.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 37,
    "question": "<p>A company wants to automate the processing of onboarding documents submitted by new employees. These documents include scanned images of identification cards and PDF files containing contracts and employee information. The company needs to extract relevant details such as employee names, addresses, and job roles from both images and text documents, and categorize the documents based on content (e.g., ID card, contract, tax form). The system should also enable easy searching of these documents using natural language queries. <br><br>Which combination of AWS services would BEST meet the company’s needs?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Textract for text extraction from images and documents, Amazon Comprehend for entity recognition and document categorization, and Amazon Kendra for search functionality. </p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon Comprehend for extracting text from images, Amazon Kendra for categorizing documents, and Amazon Elasticsearch for indexing and searching the documents. </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Rekognition for image analysis, Amazon SageMaker for building a custom entity recognition model, and Amazon DynamoDB for storing the document metadata. </p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Rekognition for text and image analysis, Amazon Comprehend for entity extraction, and Amazon S3 for storing the categorized documents.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 38,
    "question": "<p>A machine learning engineer is tasked with automating the machine learning workflow, from data ingestion to model deployment. Which AWS services should the engineer use to automate the end-to-end ML lifecycle efficiently? </p><p><br></p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Manually write Python scripts to manage data ingestion, model development, and deployment on EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Feature Store for all tasks, including data collection, model training, and deployment.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Glue for model development and SageMaker Pipelines for monitoring.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Data Wrangler for data preparation, SageMaker Model Registry for model versioning, and SageMaker Pipelines to orchestrate the workflow.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 39,
    "question": "<p>A machine learning team is training a deep learning model using Amazon SageMaker and wants to monitor the training process in real time to identify issues like vanishing gradients and overfitting. They also need to automatically track and compare different training experiments, including hyperparameter configurations and model performance, to optimize the model. <br><br>Which combination of SageMaker features should the team use to achieve these goals?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Debugger for real-time training monitoring and SageMaker Experiments to track and compare training runs</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Neo to optimize model performance and SageMaker Feature Store to manage training data features</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Ground Truth for data labeling and SageMaker Autopilot for model optimization </p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Studio to manually track training metrics and SageMaker Data Wrangler to handle training data</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 40,
    "question": "<p>Which type of machine learning is most suitable for grouping products into categories based on customer reviews without knowing the product categories beforehand?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Reinforcement learning</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Supervised learning</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Unsupervised learning</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Regression</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 41,
    "question": "<p>You are tasked with building an AI application using Amazon Bedrock. The application will experience variable traffic, with periods of very high usage followed by long idle times. Which pricing option is the MOST cost-effective for this use case?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Reserved instance pricing</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>On-demand pricing</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Spot instance pricing</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Provisioned throughput mode</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 42,
    "question": "<p>A company is using SageMaker built-in algorithms for text analysis. They need to optimize the model’s performance on their dataset by adjusting parameters such as learning rate and batch size. Which of the following approaches would be MOST effective for tuning these parameters?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker’s built-in support for automatic hyperparameter tuning to efficiently explore different configurations.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Perform feature engineering on the dataset before training to eliminate the need for hyperparameter tuning.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilize pre-trained models in SageMaker Jumpstart without any further tuning, as they are optimized out-of-the-box for all datasets. </p><p><br></p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Write custom Python code to perform a manual grid search for the best hyperparameters.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 43,
    "question": "<p>A company uses a robotic arm to pick and place objects in a factory. The robot continuously learns through trial and error by interacting with its environment and receiving feedback based on its actions. Which machine learning paradigm is the robot using?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Supervised Learning</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Unsupervised Learning</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Reinforcement Learning</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Semi-supervised Learning</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 44,
    "question": "<p>A telecommunications company is building a machine learning model to predict customer churn. The model requires features such as call duration, number of support tickets, and customer feedback scores. The company wants to store these features in a centralized repository for reuse across multiple models. The system also needs to provide low-latency access to features for real-time predictions while supporting batch updates for periodic model training. <br><br>Which of the following options provides the MOST effective solution for their requirements? </p><p><br></p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store features in SageMaker Feature Store’s offline store for real-time predictions and in Amazon S3 for batch updates.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Feature Store’s online store for real-time predictions and offline store for batch model training.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Store all features in Amazon RDS for batch updates and in SageMaker Feature Store’s online store for real-time predictions.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Ground Truth to create labeled features and store them in SageMaker Feature Store’s offline store for real-time predictions.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 45,
    "question": "<p>A machine learning team has deployed several models using Amazon SageMaker. To ensure the models perform optimally in production, they need to monitor the models for data drift and performance degradation. Additionally, they want to track which version of the dataset and hyperparameters were used in the experiments that produced these models. Which combination of SageMaker services will help them achieve this?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Feature Store and SageMaker Model Monitor</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Clarify and SageMaker Studio</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker Experiments and SageMaker Neo</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Model Monitor and SageMaker Experiments</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 46,
    "question": "<p>When using Amazon SageMaker's automatic hyperparameter tuning, which of the following must be specified to start the tuning job? (Choose Two)</p>",
    "corrects": [
      1,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Performance metric</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Batch size for model training</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>A fixed set of hyperparameter values</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Training algorithm</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Number of EC2 spot instances</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 47,
    "question": "<p>A data scientist is using Amazon SageMaker Data Wrangler to preprocess data for a machine learning project. The data is stored in Amazon Redshift and contains missing values. The scientist needs to handle these missing values before training a model. <br>Which of the following options are valid methods for handling missing values in Data Wrangler? (Choose TWO)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Impute missing values using the mean or median of the column</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use interpolation to estimate missing values based on related features</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Automatically replace missing values with zeros</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Data Wrangler’s feature store to store missing values for later use</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Drop all rows that contain missing values</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 48,
    "question": "<p>A fintech company is using Amazon Kinesis Data Firehose to collect sensitive financial data, which is then stored in Amazon S3 for compliance reporting. The company is required to meet strict encryption and data integrity requirements. Which of the following steps should the company take to secure the data stream?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the data in Amazon S3 without any additional encryption since S3 already provides security.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Kinesis Data Streams instead of Firehose to ensure stronger encryption of the data.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Shield to secure the streaming data from Distributed Denial of Service (DDoS) attacks.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Enable encryption at rest using AWS KMS for S3 and encryption in transit using HTTPS. </p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 49,
    "question": "<p>A company wants to collaborate on an ongoing machine learning project. They need an environment where multiple team members can work on different Jupyter notebooks in a persistent workspace with shared access to project resources. Which of the following options should the company choose to meet this requirement?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon EC2</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon SageMaker Ground Truth</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon SageMaker Studio</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Amazon SageMaker Notebook Instances</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 50,
    "question": "<p>A data scientist needs to implement a custom machine learning algorithm that is not natively supported by Amazon SageMaker. They want to modularize their code to separate the model definition, training, and inference logic into distinct components, and they need to include additional Python libraries for the project. <br><br>Which of the following strategies should they follow?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Write the entire code in one Python script, and SageMaker will automatically handle dependencies without any need to modularize the code.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Script Mode to modularize the code across multiple Python scripts, specifying the entry point script and including additional dependencies via a requirements.txt file.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker only for built-in algorithms, as it does not support custom libraries or complex modularization in Script Mode.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Build a custom Docker container from scratch, package the code, libraries, and dependencies manually, and deploy it in SageMaker.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 51,
    "question": "<p>A team of data scientists is building a recommendation system to suggest movies to users based on their previous viewing behavior and ratings. They are considering using the factorization machines algorithm in Amazon SageMaker. Why is the factorization machines algorithm particularly suited for this task?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>It creates clusters of similar users and movies based on behavior.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>It models interactions between features and is effective for sparse datasets, such as recommendation systems.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>It identifies outliers in user behavior that may indicate anomalies in preferences.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>It assigns a label to each user and movie, allowing personalized recommendations.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 52,
    "question": "<p>An organization wants to visually preprocess and clean its data before using it in machine learning models. They need a tool that allows non-coding users to create data transformation workflows and schedule them for periodic execution. Which AWS service should they use to meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Glue Databrew</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon SageMaker Data Wrangler</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Glue ETL jobs</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Lambda</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 53,
    "question": "<p>An e-commerce company is training a recommendation model using SageMaker, but they observe that the training process is slow, and resource utilization is uneven. The ML engineer decides to use SageMaker Profiler to investigate the issue. <br><br>What are two potential actions the engineer can take after analyzing the profiler’s metrics to improve resource utilization? (Choose two.)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Optimize the I/O operations based on disk usage metrics to prevent bottlenecks in data loading.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up custom metrics to monitor CPU utilization and dynamically increase the batch size if CPU usage is low.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Monitor GPU utilization and adjust the data pipeline if GPU underutilization is detected.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Profiler to detect overfitting and stop the training early to save resources.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 54,
    "question": "<p>Which of the following strategies is best suited to address both bias and variance in the context of responsible AI? (Choose two)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Early stopping</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Cross-validation</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Data Augmentation</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Principal Component Analysis (PCA)</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Using a more complex model with more features</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 55,
    "question": "<p>In SageMaker Pipelines, which component is used to define the sequence of operations such as data pre-processing, model training, and model registration?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Pipeline Definition</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Model Group</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Model Package</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Pipeline SDK</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 56,
    "question": "<p>A data scientist is working with a large dataset that cannot fit into the memory of a single machine and decides to use distributed training in Amazon SageMaker. The model is small enough to fit in memory on each worker, but the data is too large for one machine. Which distributed training approach should they use, and how will SageMaker manage the process?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Model parallelism, where each worker trains the same data on a subset of the model layers.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Model parallelism, where the model is split across different workers, and each worker trains a portion of the model.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Data parallelism, where the dataset is split into chunks across multiple workers, and each worker trains the same model on its chunk of data.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Data parallelism, where the model is divided into smaller components, and each component is trained on a different machine. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 57,
    "question": "<p>You are designing a reinforcement learning workflow in Amazon SageMaker for an AI agent to play a game. Which of the following components are critical to defining the reinforcement learning problem?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The preset files, policy gradient, and reward function.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The reward function, policy gradient, and training job metrics.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The action space, state space, and training job parameters.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The state, actions, environment, and reward function.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 58,
    "question": "<p>A legal firm wants to build an intelligent document retrieval system where users can search for case files using natural language queries. The system should be able to extract key legal terms like case names, judges, and dates from the documents, and allow users to find relevant cases based on specific legal topics. The documents are stored in various formats, including PDFs and Word documents. <br><br>Which combination of AWS services would BEST suit this use case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Textract for text extraction, Amazon Comprehend for identifying legal entities, and Amazon Kendra for creating a natural language search interface.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon Textract for extracting text, Amazon Rekognition for facial recognition in legal documents, and Amazon Kendra for providing search functionality.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Rekognition for identifying objects in the legal documents, Amazon Kendra for search, and Amazon Lex for creating a conversational interface.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Comprehend for both text extraction and entity recognition, Amazon S3 for storing the documents, and Amazon Elasticsearch for search functionality. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 59,
    "question": "<p>A data scientist is tasked with monitoring feature attribution drift for a deployed model in SageMaker. What does feature attribution drift indicate?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The model's predictions differ from actual outcomes, affecting model quality.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The input data no longer follows the distribution used during model training.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The model shows a preference for specific categories or groups in a biased way.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Certain features in the input data have a varying level of importance in the model’s predictions over time.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 60,
    "question": "<p>A financial services company is managing sensitive data such as database credentials, API keys, and access tokens in AWS Secrets Manager. They need to ensure that their secrets are accessible across multiple regions for high availability and also want to enable automatic secret rotation for enhanced security. <br><br>Which of the following steps should they take to meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable cross-region replication for secrets and set up customer-managed KMS keys to manage encryption policies across regions.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Manually rotate secrets by scheduling Lambda functions that update the secret values across regions.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Store secrets only in one region and rely on AWS Global Accelerator for multi-region access.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create separate secrets for each region manually and use AWS KMS default keys for encryption.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 61,
    "question": "<p>A financial services firm is building a fraud detection system and is considering whether to use custom models or built-in algorithms in Amazon SageMaker. In which scenario would a custom algorithm be MORE appropriate than a built-in algorithm?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>When the task requires basic machine learning models like Linear Learner or XGBoost.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>When your goal is to minimize infrastructure management and focus only on tuning hyperparameters.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>When built-in algorithms in SageMaker are optimized for the task at hand, such as fraud detection. </p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>When you need to use proprietary machine learning models that are not available as built-in algorithms in SageMaker.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 62,
    "question": "<p>A machine learning engineer is evaluating a binary classification model that predicts whether a transaction is fraudulent or not. The model has the following evaluation metrics:<br>Precision: 85% <br>Recall: 60% <br><br>Which of the following scenarios BEST describes what this model is achieving?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The model often predicts non-fraudulent transactions as fraudulent, leading to a high number of false positives.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The model is good at correctly identifying most fraudulent transactions but occasionally misclassifies non-fraudulent transactions.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The model correctly identifies a high proportion of fraudulent transactions but often misses actual fraudulent cases.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The model is good at identifying fraudulent transactions, but it misses a significant portion of actual fraudulent transactions.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 63,
    "question": "<p>Your team is tasked with building a recommendation engine using Amazon SageMaker’s built-in algorithms. After training the model, you want to deploy it for real-time inference. Which of the following SageMaker services would be MOST appropriate for model deployment?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Endpoints, which allow for real-time model deployment and scaling.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>AWS Lambda, which should be used to directly host the trained model and serve predictions.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker Model Monitor to automatically deploy the model in a real-time environment.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon EC2 instances to host and serve the trained model.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 64,
    "question": "<p>An e-learning platform wants to generate audio versions of its course materials in multiple languages. The platform stores text-based course content in Amazon S3 and wants to automate the process of translating the text, generating audio files, and storing the audio back in Amazon S3 for easy access. Which combination of AWS services will BEST achieve this?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Translate to translate the text, Amazon Polly to generate the speech in the desired languages, and an AWS Lambda function to store the audio files back in Amazon S3. </p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Comprehend to detect the language, Amazon Polly to generate the speech, and AWS Glue to store the audio files in Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Lambda to translate the text, Amazon Transcribe to convert the text to speech, and Amazon SageMaker to store the audio files in Amazon S3. </p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Polly to translate the text, Amazon Lex to generate the speech, and Amazon DynamoDB to store the audio files.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 65,
    "question": "<p>A media company uses AWS Glue for processing and analyzing streaming video metadata stored in Amazon S3, with a requirement to make the processed data available in a SQL-like queryable format for quick insights. Given the need to regularly update the metadata catalog and the integration with analytics services, which TWO actions should be implemented to optimize the processing workflow in AWS Glue? (Choose Two)</p>",
    "corrects": [
      3,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Schedule AWS Glue ETL jobs to perform transformations only during off-peak hours to reduce impact on operational costs and resource utilization.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Rely on Amazon Redshift Spectrum to handle the ETL processes directly from the S3 data lake, eliminating the need for AWS Glue.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilize AWS Glue crawlers to automatically infer schema from new data and update the Glue Data Catalog, ensuring data remains queryable with updated schema.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure incremental data loads in Glue ETL jobs to ensure only newly added data is processed, thus minimizing compute resources and processing time.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Enable AWS Glue Studio to manage ETL jobs and transformations visually, thereby simplifying the modification and deployment of data pipelines.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  }
]