[
  {
    "id": 1,
    "question": "<p>A machine learning engineer is using Amazon SageMaker to develop a model. They want to use a managed Jupyter notebook environment that will allow them to write and execute code, visualize data, and train models without needing to configure or maintain the underlying infrastructure. <br>Which of the following options provides the most suitable solution?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Lambda</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon EMR</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker Notebook Instances</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>AWS Glue</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 2,
    "question": "<p>A machine learning team wants to identify the most important features in their dataset using Amazon SageMaker Data Wrangler's \"Quick Model\" feature. What is the primary purpose of using this feature?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>To apply advanced deep learning techniques for feature engineering.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>To automatically split the dataset, train a simple model, and evaluate feature importance.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>To visualize the correlation matrix of the dataset to identify highly correlated features.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>To perform automated hyperparameter tuning for the best model performance.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 3,
    "question": "<p>You are working with SageMaker built-in algorithms and need to perform image classification. Which of the following statements correctly describes the process of using a SageMaker built-in algorithm for training and deployment?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker built-in algorithms require the use of custom containers where you manually install the necessary libraries.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>You need to bring your dataset and specify hyperparameters, but the algorithm is pre-packaged and managed by AWS, including the necessary infrastructure setup. </p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The built-in algorithms come fully trained and do not require further training on your specific dataset, ensuring immediate deployment.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker built-in algorithms cannot be used for image classification tasks, and custom algorithms must be used for these purposes. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 4,
    "question": "<p>A data scientist at a financial institution wants to train a machine learning model using Amazon SageMaker's built-in algorithms. They need to optimize for ease of use and infrastructure management, while still having the flexibility to define their hyperparameters and perform automatic hyperparameter tuning. The data scientist is considering the built-in \"XGBoost\" algorithm for a classification task. Which of the following steps are necessary to properly set up and train the model using the built-in algorithm?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Customize the XGBoost algorithm container and manage infrastructure manually before training.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the pre-packaged XGBoost algorithm container, specify the data location, define the hyperparameters, and select an appropriate compute instance.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Develop a custom XGBoost container to allow for proprietary data processing pipelines.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Install the XGBoost framework locally, develop the model, and upload the trained model to SageMaker for inference. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 5,
    "question": "<p>When using Amazon SageMaker's automatic hyperparameter tuning, which of the following must be specified to start the tuning job? (Select TWO)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Performance metric</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Training algorithm</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Batch size for model training</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Number of EC2 spot instances</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>A fixed set of hyperparameter values</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 6,
    "question": "<p>Which of the following scenarios would be most appropriate for using Amazon SageMaker Script Mode over creating a custom Docker container?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The machine learning team needs to use a custom version of TensorFlow that has specialized libraries not supported by SageMaker’s built-in containers.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>A data scientist wants to focus on model development using a pre-configured PyTorch environment while avoiding the complexity of managing container infrastructure.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>A developer requires full control over the operating system, libraries, and dependencies for an advanced C++ model.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The model needs to be trained and deployed in a completely different programming language, such as Rust, which SageMaker does not support natively.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 7,
    "question": "<p>A machine learning team is using Amazon SageMaker Experiments to run multiple training jobs with different hyperparameter configurations and feature sets. They want to organize, track, and compare the results to identify the best-performing model. Which of the following components in SageMaker Experiments allows the team to track and compare each individual training run within the experiment?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Pipelines</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Trials</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>SageMaker Autopilot</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Trial Components</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 8,
    "question": "<p>A startup working on autonomous driving technology needs to label a large dataset of road images for training their object detection model. The team wants to minimize the cost of labeling while maintaining high-quality results. They aim to use a combination of human labelers and automation to label straightforward objects while having human reviewers handle more complex cases. <br><br>Which of the following strategies BEST fits their needs?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Ground Truth with active learning to automate labeling of simple cases while sending complex cases to human labelers for review.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Feature Store to preprocess and label the images before passing them to the machine learning model for training.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use a fully automated labeling approach without human intervention to reduce labeling costs.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Experiments to run multiple trials of different labeling approaches to determine which strategy reduces costs the most.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 9,
    "question": "<p>During a model training session, you notice that your training job is taking significantly longer than expected. To identify the root cause, you decide to use the SageMaker Profiler to analyze resource bottlenecks, such as CPU and GPU underutilization. Which of the following actions should you take to correctly set up SageMaker Profiler for this task?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add SageMaker Profiler start and stop commands in the training script to collect system resource metrics like CPU and GPU utilization. </p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Define custom debug rules to monitor vanishing gradients and early stopping criteria during training. </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Model Monitor to track and visualize the training job's input data distribution changes.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Automatically correct detected bottlenecks by reducing the training batch size using SageMaker Debugger’s real-time insights.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 10,
    "question": "<p>A machine learning team has deployed a model to production using Amazon SageMaker. They set up SageMaker Model Monitor to detect any data drift. After several weeks, they notice that the predictions of the model are becoming less accurate compared to the actual outcomes, and SageMaker Model Monitor has flagged significant differences in the input data distribution over time. <br><br>What is the MOST likely cause of the declining model performance, and what is the recommended action?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Data drift has occurred, causing the model to receive input data that is different from the training data. The team should update the model's hyperparameters to resolve the issue.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Data drift has occurred, causing the input data distribution to deviate from the training data. The team should retrain the model with updated training data to resolve the issue.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Model quality drift has occurred because the model’s predictions are becoming less accurate. The team should switch to a different algorithm to improve performance.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Bias drift has occurred, leading the model to favor certain outcomes. The team should retrain the model with a new algorithm and set stricter constraints in SageMaker Model Monitor.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 11,
    "question": "<p>A data science team deployed a model using Amazon SageMaker real-time endpoints. They are concerned that the distribution of data in production may change over time, potentially leading to data drift. To mitigate this risk, they want to set up continuous monitoring of the model in production. Which of the following does Amazon SageMaker Model Monitor allow the team to track to address their concerns? (Select TWO)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Model Latency Drift</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Data Drift</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Feature Attribution Drift</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>AWS CloudFormation Drift</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 12,
    "question": "<p>A machine learning engineer is tasked with automating the registration, versioning, and deployment of models as part of a workflow using SageMaker Pipelines. The goal is to track model development, including data preprocessing, training runs, and version history. The engineer needs to ensure that the pipeline can log the necessary details for auditing and compliance. <br>Which SageMaker feature should the engineer use to track the history of the model’s development?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Model Group</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Approval Status</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Model Lineage </p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Model Metrics</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 13,
    "question": "<p>An organization uses SageMaker Pipelines to automate its ML workflow. After training a model, they need to register it with the SageMaker Model Registry, which tracks different versions of the model. They want to ensure that only approved models are deployed to production. <br><br>Which feature of SageMaker Model Registry should be used to control the deployment of models?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Model Group</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Approval Status</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Model Package</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Model Lineage</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 14,
    "question": "<p>A retail company wants to predict if a customer will purchase a product based on historical data, including past purchases, browsing behavior, and demographic details. Which type of machine learning model is best suited for this task?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>K-Means Clustering</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Regression</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Binary Classification</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Reinforcement Learning</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 15,
    "question": "<p>A data scientist is training a machine learning model to classify emails as either spam or not spam. They want to evaluate the model using various metrics. If the model correctly classifies 85% of the emails as spam or not spam, which metric is being used in this scenario?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Precision</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Recall</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Accuracy</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>F1 Score</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 16,
    "question": "<p>Which of the following are key features of Amazon Bedrock that simplify the management and scaling of generative AI applications? (Choose TWO) </p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Provisioned throughput mode for handling large, steady workloads.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Ability to directly modify the foundation models' source code for deeper customization.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Serverless infrastructure with automatic scaling and built-in data encryption.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Requirement to configure and manage underlying EC2 instances for model hosting.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Fine-tuning models with custom data to improve performance for specific tasks. </p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 17,
    "question": "<p>A retail company plans to use Amazon Bedrock to build a product recommendation system that generates personalized suggestions for users. The company wants to enhance the accuracy of the recommendations using real-time data and integrate additional knowledge sources to improve model responses. <br>Which features of Amazon Bedrock should the company use to meet these needs? (Choose TWO)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use agents in Amazon Bedrock to break down tasks and connect to real-time data via APIs. </p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Leverage AWS Lambda to trigger model updates after each user interaction. </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Bedrock's knowledge bases to incorporate private data into the recommendation model.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Configure EC2 instances to run additional AI models for better performance.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Enable cross-region data sharing for better model accuracy.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 18,
    "question": "<p>A company is setting up an AI-powered assistant using Amazon Q Business to streamline internal operations. They want to generate summaries of meetings and ensure that the assistant can retrieve relevant information from a knowledge base to provide accurate responses. <br>What key feature of Amazon Business should be implemented to meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the Retrieval Augmented Generation (RAG) system to pull information from the knowledge base.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Set up a custom plugin to generate responses based on Jira tickets.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Deploy the assistant with access to only Amazon S3 as the primary data source.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Disable the knowledge base integration to ensure faster response times.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 19,
    "question": "<p>An organization has large volumes of log data stored in Amazon S3. They plan to process this data using AWS Glue ETL jobs, which will run daily to extract, transform, and load the data into Amazon Redshift. The organization wants to ensure that only the newly added log files are processed in each ETL run to reduce cost and optimize performance. <br><br>Which AWS Glue feature can be leveraged to achieve this goal most cost-effectively?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Glue Crawler with automatic schema detection</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Incremental loading using partitioning</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Using AWS Lambda to trigger ETL jobs on every new file upload</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Defining custom transformations in AWS Glue to identify new data </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 20,
    "question": "<p>A machine learning team is using AWS Glue to preprocess large datasets for training models. They need to ensure that the Glue ETL jobs are optimized for cost without sacrificing performance. The team wants to apply custom transformations and only pay for the compute resources they use. <br><br>What are the best practices the team should follow to manage AWS Glue costs? (Choose TWO)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the AWS Glue visual interface to build ETL jobs instead of custom scripts to save on costs.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure the Glue job to run with a minimal number of Data Processing Units (DPUs) that meet performance requirements.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Glue Crawlers to run on a schedule and detect changes in the data schema, reducing the need for constant job execution.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Implement manual ETL processes using EC2 Spot Instances to lower the compute costs. </p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use AWS Glue's built-in auto-scaling feature to automatically scale up DPUs based on workload.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 21,
    "question": "<p>A data engineer is tasked with building an ETL pipeline using AWS Glue to process semi-structured data stored in Amazon S3. The pipeline should automatically infer the schema of the input data, perform transformations, and store the output back to S3 in a different format. The process must be efficient and scalable, without the need for infrastructure management. Which of the following AWS Glue features best fulfill this requirement? (Choose Two)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Glue Crawler</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>AWS Glue Data Catalog</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS Glue Auto Scaling</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon EC2-based Spark Cluster</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>AWS Glue Triggers</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 22,
    "question": "<p>A machine learning is preparing a large dataset for machine learning model training using AWS Glue and AWS Glue Databrew. The goal is to clean and preprocess the data, including removing duplicates and converting text columns to uppercase. What is the BEST way to achieve this using AWS Glue Databrew?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Databrew’s visual interface to apply transformation steps, such as removing duplicates and converting text to uppercase, then save these transformations as a recipe.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Manually write code to perform the transformations, and run it on an Amazon EC2 instance.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Lambda functions to apply text transformations and remove duplicates before importing the data into Databrew.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Store the dataset in Amazon RDS and use SQL queries to preprocess the data. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 23,
    "question": "<p>A data scientist is using AWS Lambda to preprocess incoming JSON data from an IoT device stream. The Lambda function needs to validate the data, enrich it by calling an external API, and forward the processed data to an Amazon Kinesis stream for further machine learning processing. The solution should be cost-effective, minimize latency, and ensure the Lambda function does not exceed its timeout limit during the API call. <br><br>Which of the following approaches should the data scientist take?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Lambda with asynchronous invocation to call the external API, validate the data, and forward the processed data to Kinesis.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Lambda with the built-in AWS SDK to call the external API synchronously, validate the data, and send the result to Kinesis.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Step Functions to manage the workflow, including the API call and data validation, and then use a separate Lambda function to forward data to Kinesis.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Lambda to invoke an AWS Fargate container for handling the external API call, and forward the enriched data to Kinesis from the Lambda function after validation.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 24,
    "question": "<p>A healthcare company needs to ingest and store real-time health monitoring data from wearable devices. The data must be compressed and transformed into a format that supports fast querying in Amazon Athena. They also need to ensure the processed data is securely stored in Amazon S3. <br><br>Which of the following solutions would meet the company's requirements using Amazon Kinesis Data Firehose?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Ingest data using Amazon Kinesis Data Streams, transform the data using AWS Lambda, compress it with Gzip, and store it in Amazon S3 in CSV format.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Ingest data using Amazon Kinesis Data Firehose, compress the data using Gzip, convert it to Parquet format using the built-in data transformation feature, and store it in Amazon S3 with server-side encryption.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Ingest data using Amazon Kinesis Data Firehose, use a Lambda function for data transformation, compress the data with Snappy, and store it in Amazon Redshift. </p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Ingest data using Amazon Kinesis Data Streams, compress the data with Zlib, transform it to JSON format, and store it in Amazon S3.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 25,
    "question": "<p>A Machine Learning Engineer is preparing to train a deep learning model on Amazon SageMaker using a custom algorithm. The dataset is very large and stored across multiple S3 buckets. Due to the data size, it needs to be streamed directly into SageMaker for training rather than loaded all at once.</p><p>What is the recommended approach to efficiently manage and stream this data during training in Amazon SageMaker?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Pipe mode to stream the data directly from S3 to the training instances.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Glue to join the data into a single file before loading it into SageMaker.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Manually download and preprocess the data on an Amazon EC2 instance before uploading it to SageMaker.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the SageMaker batch transform feature to prepare the data before training.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 26,
    "question": "<p>A data scientist is using Amazon SageMaker to develop a machine learning model for customer behavior prediction. The scientist is working in a SageMaker Notebook Instance and wants to collaborate with a team of data analysts on this project. Additionally, they need to ensure that their work, including data visualizations and intermediate results, is saved and can be accessed even after the notebook instance is stopped. <br><br>Which configuration should the data scientist consider to achieve these goals? (Select TWO)</p>",
    "corrects": [
      1,
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Studio to enable collaboration, as it allows multiple users to access shared resources within a Jupyter Lab environment.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable persistent storage in the SageMaker Notebook Instance to automatically save data and code, ensuring that all work is retained even after stopping the instance.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Install all necessary libraries manually in the SageMaker Notebook Instance to ensure the team has access to the correct dependencies for their machine learning project.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker notebook instances to share code directly, which allows for real-time collaboration between multiple users.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Configure a custom EC2 instance with an EBS volume for persistent storage, ensuring data is saved when the instance is stopped.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 27,
    "question": "<p>Machine learning team is tasked with preparing a dataset for a model that will predict customer churn. They decide to use SageMaker Data Wrangler to streamline the data preparation. The dataset contains time-based information, and they want to engineer new features such as \"day of the week\" from timestamps and assess the importance of these features. <br>What TWO features of SageMaker Data Wrangler will assist the team in this process? (Choose TWO)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Time series analysis and forecasting.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Feature engineering options to create new features like \"day of the week\" from timestamps.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The Quick Model feature to automatically compute feature importance.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Integrating data directly from Amazon RDS for real-time predictions.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Data export to Amazon Athena for advanced SQL-based transformations.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 28,
    "question": "<p>You are tasked with training a machine learning model for image classification using Amazon SageMaker's built-in algorithms. <br>Which of the following steps accurately describe the process of using SageMaker's built-in algorithms? </p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up a custom container, upload the algorithm, and configure infrastructure.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Select the algorithm from the SageMaker console, upload your dataset, configure hyperparameters, and launch the training job.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Create a custom script for model training, manually configure hyperparameter tuning, and deploy the model on an EC2 instance.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker built-in image generation algorithms and skip hyperparameter configuration to deploy a pre-trained model.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 29,
    "question": "<p>A machine learning engineer is training a decision tree model and is tasked with tuning the model’s hyperparameters. Which of the following is an example of a hyperparameter that the engineer can configure before training begins?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The weights assigned to each node in the decision tree</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The depth of the tree</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The data points used for training</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The split criteria for each node learned from the data</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 30,
    "question": "<p>A machine learning engineer is working with Amazon SageMaker to train a decision tree model and needs to optimize the hyperparameters. The model's performance needs to be optimized by tuning the depth of the tree and the minimum samples required for a split. <br><br>Which of the following describes the difference between hyperparameters and model parameters?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Hyperparameters are learned from the training data, while model parameters are set before training.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Hyperparameters are external configurations set before training begins, while model parameters are learned during training.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Hyperparameters control the size of the data input, while model parameters control the training speed.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Model parameters influence how quickly the model learns, while hyperparameters control the training algorithm.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 31,
    "question": "<p>A machine learning team needs to train a model on a large dataset that can fit into memory, but the training would be too slow on a single machine. Which distributed training method should they use in Amazon SageMaker to speed up the process?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Model parallelism, where the model is split across multiple machines.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use data parallelism to split the dataset across several machines, with each machine training a copy of the model.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Manually manage data distribution and gradient synchronization across multiple machines.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Train the model on a single large instance with more compute power.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 32,
    "question": "<p>A media company is developing a machine learning model to categorize a large library of video content. Since manually labeling thousands of videos would be costly, the team wants to reduce labeling costs by automating part of the labeling process while ensuring that complex cases are reviewed by human annotators. They also want the labeling process to improve over time based on the labeled data. <br><br>What is the BEST strategy for optimizing labeling costs while ensuring high-quality results?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Ground Truth’s active learning to automate simple labeling tasks and send more complex labeling tasks to human annotators.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Rely solely on human workers to manually label all video content, ensuring maximum accuracy but at a higher cost. </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Ground Truth to automate all labeling tasks without human involvement to reduce costs.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Experiments to track labeling performance and automatically adjust labeling strategies based on accuracy metrics.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 33,
    "question": "<p>A data scientist is developing a machine learning model for a healthcare application. During model evaluation, they observe that the model has high bias and is underfitting the data. They decide to implement strategies to address the bias and variance issues to ensure the model generalizes well on new data. <br><br>Which combination of strategies would be MOST effective to balance bias and variance and avoid both underfitting and overfitting?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use early stopping during training to prevent overfitting and add regularization to simplify the model. </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Add more training data and implement cross-validation to reduce overfitting and ensure better generalization.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Implement early stopping and dimensionality reduction to reduce the complexity of the model and prevent underfitting.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Increase the number of model parameters and reduce the training data size to reduce variance. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 34,
    "question": "<p>A financial company is concerned about bias in its AI models and wants to assess the influence of individual features on model predictions. The company aims to build trust with stakeholders and ensure regulatory compliance by explaining how certain features impact predictions. <br>Which feature of Amazon SageMaker Clarify would best meet these needs?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Bias Detection</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>SHAP (Shapley Additive Explanations) values for feature attribution</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Data Wrangling</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Early stopping</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 35,
    "question": "<p>A company has developed a machine learning model using XGBoost and plans to deploy it on edge devices with limited compute resources. The model needs to be optimized for performance without significantly affecting accuracy. The company decides to use SageMaker Neo for this task. <br><br>Which of the following steps should the company take to optimize and deploy the model using SageMaker Neo?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Neo to automatically compile the model for the target hardware and deploy it using AWS IoT Greengrass.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Deploy the unoptimized model directly on the edge devices and monitor the performance through SageMaker Debugger.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Retrain the model with a simplified architecture to reduce compute demand before deploying on edge devices.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Profiler to analyze edge device resource utilization before optimizing the model with SageMaker Neo.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 36,
    "question": "<p>A machine learning engineer is responsible for deploying a model in Amazon SageMaker. After deployment, the engineer needs to monitor the model to detect data drift, model quality drift, and feature attribution drift over time. What are the most appropriate steps to take to monitor the deployed model effectively?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Implement a baseline job, define constraints based on training data, and schedule monitoring jobs.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Monitor only the model predictions for accuracy and retrain the model when performance declines.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon S3 to store logs manually and monitor changes in the input data via manual inspection.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Perform model re-training without any active monitoring to avoid unnecessary processing costs.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 37,
    "question": "<p>You are deploying a model in production and want to ensure that only the most trusted model versions are used. Which feature of the SageMaker Model Registry will help you achieve this?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Model lineage</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Model approval status </p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Model group</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Model metrics</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 38,
    "question": "<p>Which of the following scenarios best describes the use of supervised learning?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A financial institution segments its customers based on spending patterns without predefined categories.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>A company develops a predictive model to classify emails as either spam or not spam using historical labeled data.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>A retail business uses machine learning to automatically group products based on their similarity without providing labeled data.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>A gaming company implements a system where an AI learns to improve its strategy by playing games and receiving feedback from wins and losses.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 39,
    "question": "<p>A financial firm is using Amazon SageMaker to develop a reinforcement learning (RL) model for optimizing portfolio management. The agent is tasked with adjusting the portfolio by buying, selling, or holding assets in a fluctuating market. To ensure the agent maximizes the long-term return while minimizing risk, how should the reward function be structured?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The reward function should penalize any change in the portfolio and reward only large gains in asset value.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The reward function should reward every transaction made by the agent to encourage exploration of different strategies.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The reward function should provide small rewards for gains in asset value and larger penalties for significant losses, to encourage both risk management and profit maximization.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>The reward function should only reward short-term profit increases, ensuring the agent takes immediate action for portfolio growth.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 40,
    "question": "<p>A large e-commerce company is using Amazon Fraud Detector to prevent fraudulent account signups. The company wants to automate the detection process using machine learning models and receive fraud scores in real-time. What steps should the company take to implement this solution efficiently? (Choose TWO)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Ingest historical data such as past account signups and user details to train the Amazon Fraud Detector model</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Personalize to generate fraud scores based on user interaction data and account creation activities</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up Amazon Fraud Detector to automatically extract relevant features, such as user location and device type, for model training</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy AWS Glue to periodically retrain the fraud detection model on newly ingested data</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Use Amazon Fraud Detector to assign a fraud score in real-time, allowing the company to take immediate actions based on the score</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 41,
    "question": "<p>A retail company is planning to use Amazon Q Business to automate customer support by integrating with third-party applications like ServiceNow and Zendesk. They want to ensure smooth communication between Amazon Business and these tools. <br>What feature of Amazon Business enables this integration? </p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use IAM Identity Center to control access to ServiceNow and Zendesk.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up plugins to enable interaction with third-party applications.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure pre-built connectors to synchronize all data between Amazon S3 and the third-party tools.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Manually code the integration between Amazon Business and ServiceNow or Zendesk.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 42,
    "question": "<p>A financial firm needs to create a scheduled ETL process using AWS Glue that extracts transaction data from Amazon Redshift, transforms it, and loads it into another Redshift cluster for reporting. The process should avoid reprocessing the same data repeatedly and only process newly added transactions. <br>Which AWS Glue feature will allow the firm to achieve this?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Glue Crawler</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Glue Incremental Processing</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS Glue Triggers</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>AWS Glue Development Endpoint</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 43,
    "question": "<p>A company is preparing to automate the transformation and loading of log data from an Amazon S3 bucket using AWS Glue. The data should be transformed into a columnar format to enhance query performance and stored back into S3 for subsequent analysis using business intelligence tools. Considering cost optimization and effective data transformation, which TWO of the following strategies should the company implement? (Choose Two)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configure an AWS Glue crawler to detect and update the data schema, enabling automatic partitioning by adding new partitions as data grows.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Directly use Amazon S3 to perform ETL operations on the data, bypassing AWS Glue to minimize costs and avoid complex configurations.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilize AWS Glue's built-in capability to transform the data format to Apache Parquet for optimized analytical querying before storing it back to S3.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Implement manual ETL processes that rely on EC2 instances to periodically transform and overwrite data in S3 to ensure data is up-to-date.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Set up AWS Glue ETL jobs to run on a minimal number of Data Processing Units (DPUs) to effectively balance cost and performance.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 44,
    "question": "<p>A media company uses AWS Glue for processing and analyzing streaming video metadata stored in Amazon S3, with a requirement to make the processed data available in a SQL-like queryable format for quick insights. Given the need to regularly update the metadata catalog and the integration with analytics services, which TWO actions should be implemented to optimize the processing workflow in AWS Glue? (Choose Two)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Utilize AWS Glue crawlers to automatically infer schema from new data and update the Glue Data Catalog, ensuring data remains queryable with updated schema.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Schedule AWS Glue ETL jobs to perform transformations only during off-peak hours to reduce impact on operational costs and resource utilization.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable AWS Glue Studio to manage ETL jobs and transformations visually, thereby simplifying the modification and deployment of data pipelines.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Rely on Amazon Redshift Spectrum to handle the ETL processes directly from the S3 data lake, eliminating the need for AWS Glue.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Configure incremental data loads in Glue ETL jobs to ensure only newly added data is processed, thus minimizing compute resources and processing time.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 45,
    "question": "<p>A retail company uses AWS Lambda to process images uploaded to an Amazon S3 bucket. The company wants to enhance the Lambda function to extract metadata from the images (such as resolution and file format), store the metadata in an Amazon DynamoDB table, and trigger a machine learning model hosted on SageMaker to classify the images. Which of the following options is the most proper and scalable for described workflow?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an S3 event trigger to invoke the Lambda function, extract metadata, store it in DynamoDB, and directly invoke the SageMaker endpoint for classification within the same Lambda function.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use an S3 event trigger to invoke the Lambda function, extract metadata, and store it in DynamoDB. Then, trigger a separate Lambda function to invoke the SageMaker endpoint for classification.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Set up an Amazon S3 batch operation to invoke the Lambda function, extract metadata, and invoke the SageMaker endpoint. Use a separate process to store the metadata in DynamoDB after classification.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon EventBridge to monitor S3 uploads, triggering the Lambda function to extract metadata, store it in DynamoDB, and schedule a periodic invocation of SageMaker for classification.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 46,
    "question": "<p>A data scientist is training a machine learning model using a large dataset with various features. During the initial evaluation, the model appears to be overfitting. Which combination of techniques can the data scientist apply to reduce overfitting and improve generalization? (Choose TWO)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use cross-validation by rotating through training and validation sets.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Increase the number of parameters in the model to capture more data patterns.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Apply regularization to penalize extreme model parameters.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Reduce the amount of training data to limit the complexity of the model.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Avoid using dimensionality reduction techniques to retain all feature information.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 47,
    "question": "<p>A machine learning team is developing a model to predict customer churn. They need to address bias and ensure fairness in their model outputs. Which Amazon SageMaker feature can they use to analyze and mitigate bias during and after the training phase?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Clarify</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Data Wrangler</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker Debugger</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Ground Truth</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 48,
    "question": "<p>A machine learning engineer is using Amazon SageMaker Debugger to monitor a training job. The engineer notices that the model’s validation loss starts to increase while the training loss continues to decrease. What action should the engineer take to address this issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Increase the learning rate to improve the model's performance.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use early stopping to prevent further overfitting.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Add more features to the model to increase its complexity.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Reduce the amount of training data to prevent overfitting.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 49,
    "question": "<p>A data scientist is working to improve the generalization of their machine learning model and reduce overfitting. They have been advised to use cross-validation, add more training data, and apply regularization techniques. Which of the following best describes the purpose and impact of using regularization?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Regularization simplifies the model by penalizing extreme parameter values, reducing overfitting risk.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Regularization increases model complexity to improve performance on diverse datasets.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Regularization rotates through different subsets of data to enhance model generalization.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Regularization augments data size by adding synthetic samples from minority classes.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 50,
    "question": "<p>A machine learning specialist is using SageMaker Clarify to evaluate potential bias in a customer satisfaction prediction model. The data includes sensitive attributes like gender and age. Which of the following actions can SageMaker Clarify perform to detect and address bias in this context?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>It will generate new features from the existing data to balance all classes.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>It will run an analysis on specified sensitive attributes and generate a report with bias metrics.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>It will optimize the model by re-training it on only the sensitive features to reduce bias.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>It will apply synthetic data generation to equalize sample sizes across all categories.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 51,
    "question": "<p>A team is training a neural network model in Amazon SageMaker. During training, they notice that the model's performance has plateaued, and suspect a vanishing gradient issue. How can they leverage SageMaker Debugger to diagnose and address this problem?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up custom debug rules to capture disappearing gradients and adjust the learning rate as needed.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable SageMaker Clarify to detect vanishing gradients during training.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use cross-validation to rotate training data, preventing gradient loss.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Increase the training epochs until SageMaker Debugger resolves the issue.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 52,
    "question": "<p>A mobile application requires occasional, unpredictable access to machine learning predictions. The app’s usage fluctuates significantly throughout the day, leading to periods of high traffic and very low usage at off-peak times. Which deployment strategy in Amazon SageMaker would optimize costs while still ensuring scalability during peak periods?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Batch Transform</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Real-Time Inference Endpoint</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Serverless Inference Endpoint</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Asynchronous Inference Endpoint</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 53,
    "question": "<p>An organization is deploying a machine learning model to edge devices with limited computing resources. They want to optimize the model to reduce resource usage without compromising accuracy. Which AWS service should they use to achieve this optimization?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon SageMaker Real-Time Inference Endpoint</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon SageMaker Neo</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS IoT Greengrass</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon SageMaker Batch Transform</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 54,
    "question": "<p>A financial institution wants to deploy a machine learning model that provides fraud detection in real time. Traffic to the model is highly variable, with periods of high activity followed by periods of low or no activity. The institution needs a cost-effective and scalable solution that adjusts to these traffic patterns.</p><p>Which deployment option should they choose in Amazon SageMaker?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Real-Time Inference Endpoint</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Batch Transform</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Serverless Inference</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Asynchronous Inference</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 55,
    "question": "<p>A retail company is using Amazon SageMaker for customer recommendations. The company’s model requires low-latency responses to provide real-time product recommendations based on user interactions. They also need to monitor the model to detect data drift and quality drift over time to maintain recommendation accuracy.</p><p>Which of the following steps should they take to ensure both low-latency inference and model monitoring? (Choose TWO.)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Deploy the model using a real-time inference endpoint for low-latency responses.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Serverless Inference to handle traffic spikes and save costs.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up SageMaker Model Monitor to detect data and quality drift on the deployed endpoint.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Batch Transform to periodically update recommendations for all users.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Enable SageMaker Neo to enhance model performance for edge deployment.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 56,
    "question": "<p>A tech company wants to deploy a machine learning model for processing large video files. The model processes each video file individually and does not require real-time responses. The company wants to minimize costs by only paying for compute when processing files and scale resources based on demand.</p><p>Which SageMaker deployment option should they choose?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Real-Time Inference Endpoint</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Batch Transform</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Serverless Inference</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Asynchronous Inference</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 57,
    "question": "<p>An e-commerce company wants to integrate Amazon Bedrock to enhance its customer support system with a generative AI chatbot. The company expects usage to fluctuate significantly, with peak usage during holiday sales and lower usage during off-peak times. They also want to avoid managing infrastructure manually. <br>Which Amazon Bedrock feature will best support these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Foundation models with fine-tuning capabilities</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Provisioned throughput mode for predictable traffic</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Serverless environment with automatic scaling</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Knowledge bases to enhance model responses</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 58,
    "question": "<p>A financial institution wants to detect fraudulent activities in real time to protect against payment fraud and account takeovers. They plan to use Amazon Fraud Detector for this purpose and need to automatically assess each transaction based on its risk level. <br>Which step in Amazon Fraud Detector’s workflow directly identifies transactions that are likely to be fraudulent?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Feature engineering to prepare data features</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Real-time evaluation using fraud score</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Model deployment to a production environment</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Data ingestion of historical transaction data</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 59,
    "question": "<p>A media company wants to generate synthetic data to improve the training of its recommendation model and is considering Amazon Bedrock for this purpose. The company plans to use a foundation model for content generation and requires secure, scalable data handling. Which Amazon Bedrock feature ensures data privacy and compliance during content generation?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Integration with Amazon CloudWatch for monitoring</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>On-demand pricing based on token usage</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Region-specific data handling and encryption</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Provisioned throughput mode for steady workloads</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 60,
    "question": "<p>An e-commerce company is planning to integrate Amazon Bedrock for customer support and product recommendation. They want a solution that provides personalized product suggestions based on user interactions in real-time. Additionally, they need a secure environment where sensitive user data remains within a specific AWS region.</p><p>Which feature of Amazon Bedrock would best address their need for data security?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Knowledge bases</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Serverless environment with automated scaling</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Identity and Access Management (IAM) integration</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Data encryption in transit and at rest</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 61,
    "question": "<p>An organization has multiple AWS Lambda functions that require specific permissions to access S3 and DynamoDB resources. The team wants to avoid managing permissions for each individual Lambda function and instead use a centralized approach.</p><p>Which AWS IAM feature should the team use to manage access permissions effectively for these Lambda functions?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>IAM Users</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>IAM Groups</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>IAM Roles</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Root User Access</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 62,
    "question": "<p>A security-sensitive application requires encryption for all data stored in Amazon S3. The organization also wants complete control over encryption keys, including the ability to audit their usage and set specific rotation policies.</p><p>Which type of AWS Key Management Service (KMS) key should they choose?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Managed Key</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Owned Key</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Customer Managed Key</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Symmetric Key</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 63,
    "question": "<p>A company is concerned about potential Distributed Denial of Service (DDoS) attacks on its application, which has been targeted in the past. They require advanced protection, including automatic mitigation for complex DDoS threats and access to AWS’s DDoS Response Team (DRT) for real-time support during incidents.</p><p>Which AWS service should they use to meet these requirements?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>AWS Shield Standard</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Shield Advanced</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>AWS WAF</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon CloudWatch</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 64,
    "question": "<p>An organization needs to ensure that its AWS resources comply with regulatory standards. They decide to implement AWS Config to monitor and manage the configuration of these resources. The organization also requires automated alerts for non-compliant resources and periodic compliance checks.</p><p>Which combination of AWS Config features should they enable to meet these requirements? (Choose TWO)</p>",
    "corrects": [
      2,
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Configuration Items</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>AWS Config Rules with periodic evaluation</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configuration Stream</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Custom rules created with AWS Lambda</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Real-time proactive evaluation mode</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 65,
    "question": "<p>A company wants to use AWS Config to maintain a detailed history of configuration changes across all of its resources for auditing purposes. They need to ensure the history of these changes is available long-term for regulatory requirements.</p><p>Which approach will allow them to retain and access this history over an extended period?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable configuration items and configure periodic evaluations.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use configuration snapshots and store them in Amazon S3.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Enable AWS Config Rules and use CloudTrail to track changes.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use a configuration stream and set up an Amazon RDS database for storage.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  }
]