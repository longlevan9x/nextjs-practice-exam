[
  {
    "id": 1,
    "question": "<p>A company wants to streamline their machine learning workflow by using AWS SageMaker. They need an environment that allows them to handle every step of the machine learning lifecycle, from data preparation to model deployment and monitoring. The company prefers a fully managed environment where infrastructure concerns, such as managing servers, can be avoided. <br>Which statement BEST describes how AWS SageMaker meets these needs?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker simplifies the machine learning workflow by providing a fully managed service, handling data preparation, model training, and deployment without requiring manual server management.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>SageMaker requires manual setup of servers and other infrastructure for model training but simplifies model deployment with managed endpoints.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>SageMaker integrates with Amazon EC2, which must be manually configured for machine learning tasks, while SageMaker handles data preparation and monitoring. </p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker only assists in the model training phase, and the company must use additional AWS services for data preparation and model deployment.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 2,
    "question": "<p>A company is using Amazon SageMaker Debugger to monitor and debug their machine learning model during training. They encounter an issue where the model's performance is no longer improving, and they suspect it might be due to disappearing gradients. Which of the following steps should they take to resolve this issue?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Manually stop the training job and restart it with more training data to solve the disappearing gradient issue.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up a custom rule in SageMaker Debugger to detect vanishing gradients and adjust the learning rate or change the optimizer.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Increase the batch size in the training script to capture disappearing gradients more effectively.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Neo to optimize the model performance for deployment on edge devices to avoid disappearing gradients.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 3,
    "question": "<p>A Machine Learning Engineer is using Amazon SageMaker to build and train a binary classification model. The training dataset contains a severe class imbalance, with 95% of the data belonging to the negative class and only 5% to the positive class. After training, the model achieves high accuracy but performs poorly on the positive class.</p><p>Which two techniques should the engineer consider to address this issue? (Choose TWO.)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the area under the ROC curve (AUC) as the evaluation metric.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Increase the size of the positive class through oversampling.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Apply class weights to penalize misclassification of the minority class.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use precision as the primary evaluation metric instead of accuracy.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Decrease the size of the negative class through undersampling.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 4,
    "question": "<p>Which of the following strategies is MOST effective in reducing overfitting when training an AI model, according to responsible AI practices?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Increasing the training data without any validation steps to capture more patterns.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Reducing the size of the training dataset to speed up training. </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Utilizing a more complex model with a larger number of parameters to improve accuracy.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Using regularization techniques that add penalties to extreme model parameters.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 5,
    "question": "<p>Which of the following best describes how Amazon SageMaker handles data parallelism during distributed training?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Splits the dataset and model across multiple machines, ensuring both are distributed equally</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Splits the dataset across multiple machines, each training the same model on a portion of the data</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Splits the dataset across multiple machines, each training a different model</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Splits the model across multiple machines and synchronizes the model parameters </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 6,
    "question": "<p>A data analyst wants to extract meaningful insights from unstructured customer feedback stored in Amazon S3. The insights should include key entities like people, organizations, and locations mentioned in the feedback, as well as the overall sentiment of the feedback. Which combination of Amazon Comprehend features should the analyst use to achieve this goal?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Text Classification and Entity Recognition</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Keyphrase Extraction and Topic Modeling</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Topic Modeling and Keyphrase Extraction </p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Entity Recognition and Sentiment Analysis </p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 7,
    "question": "<p>A data scientist is building a machine learning pipeline using Amazon SageMaker Pipelines. The pipeline includes data preprocessing, model training, and model registration. The data scientist wants to ensure that parameters used for model training can be easily adjusted without modifying the pipeline itself. Which feature of SageMaker Pipelines should the data scientist use to achieve this?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Pipeline Steps</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Directed Acyclic Graph (DAG)</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Pipeline Parameters</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Model Registry</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 8,
    "question": "<p>An e-commerce company uses a Lambda function triggered by Amazon SNS to preprocess customer transaction data before sending it to an Amazon SageMaker model for fraud detection. The transaction data is then stored in Amazon RDS for long-term analytics. The company wants to ensure that only valid transactions (based on a predefined schema) are passed to the SageMaker model and that invalid transactions are logged and stored in Amazon S3 for further analysis. <br><br>Which of the following solutions BEST fits the company's requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Lambda to validate the schema of each transaction, send valid data to SageMaker for inference, and write invalid data directly to Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Kinesis Data Firehose to preprocess and validate the data, then trigger Lambda to send valid data to SageMaker, while invalid data is redirected to S3.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Set up a Step Functions workflow to manage validation and invoke Lambda for processing valid data and storing invalid data in S3.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Lambda to validate the schema, send valid data to SageMaker, and forward invalid data to an Amazon SQS queue, which will trigger another Lambda function to log and store it in S3.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 9,
    "question": "<p>A data scientist is using SageMaker Experiments to track and compare model training runs. They want to ensure that detailed metadata, such as hyperparameters, datasets, and code changes, are logged automatically for each trial. Which feature of SageMaker Experiments provides this functionality?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Trackers</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Model Monitor</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Trial Components</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Feature Store </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 10,
    "question": "<p>A company wants to optimize the data ingestion process for a machine learning pipeline in Amazon SageMaker. The data, stored in Amazon S3, consists of large JSON files, and the current ETL job takes a long time to process the data before training. To improve performance, they want to minimize read and I/O times, especially for downstream analysis using Amazon Athena.</p><p>Which format should the company use to store their data in S3 for better processing and analytics performance?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Compress the data as a ZIP file and upload it to S3.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Apache Parquet to store the data in S3.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Convert the data to CSV format and store it in S3.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Convert the data to JSON Lines format to reduce file size.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 11,
    "question": "<p>A technology company uses AWS CloudFormation to manage its infrastructure as code. The team wants to create a CloudFormation stack that includes an Amazon S3 bucket and an Amazon EC2 instance. The S3 bucket should only be created if the EC2 instance is successfully launched. If the EC2 instance creation fails, the stack creation should be rolled back, and the S3 bucket should not be created. <br><br>Which configuration will meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use the DependsOn attribute in the S3 bucket resource to specify the EC2 instance resource. </p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use a CreationPolicy on the EC2 instance resource and an UpdatePolicy on the S3 bucket resource.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use a custom resource to check the status of the EC2 instance creation and create the S3 bucket based on that status.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the Condition attribute to create a condition that checks if the EC2 instance is created and use that condition in the S3 bucket resource.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 12,
    "question": "<p>A company is using Amazon S3 to store sensitive business documents. To enhance security, they want to grant access to the S3 bucket only to certain IAM users, while denying access to others. Additionally, the company requires the S3 bucket to be accessed by an EC2 instance running in a different AWS account, but only for reading data. <br><br>Which combination of AWS IAM features should be used to meet these requirements? (Choose TWO)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use an identity-based policy to attach to IAM users that grants or denies access to the S3 bucket.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Lambda to check the IAM users’ permissions before allowing access to the S3 bucket.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Create a role in the other AWS account with a trust policy allowing the EC2 instance to assume the role and read from the S3 bucket.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use a customer-managed IAM policy attached to the S3 bucket to define granular access for the EC2 instance and the IAM users.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Apply a resource-based bucket policy in Amazon S3 to explicitly deny access from all users except the EC2 instance and required IAM users.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 13,
    "question": "<p>A machine learning engineer wants to collaborate with a team on a project in AWS SageMaker. The team needs to share notebooks, manage multiple users, and ensure that they can easily scale resources based on their deep learning model's GPU requirements. They also prefer to work in an integrated environment that allows seamless transition between data preparation, training, and model deployment. Which SageMaker tool would BEST meet these collaboration and scaling requirements? </p><p><br></p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Studio</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Notebook Instances</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Lambda with integrated Jupyter notebooks</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon EMR with SageMaker integration </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 14,
    "question": "<p>A DevOps team needs to monitor the CPU and memory utilization of their containerized applications running in Amazon ECS. They also need to monitor custom metrics such as the number of active users. Which combination of AWS services and features is the MOST appropriate for this scenario?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Lambda to regularly query ECS task performance and push CPU and memory metrics into CloudWatch for analysis.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable AWS Config to track ECS task performance and set up CloudWatch Alarms for CPU and memory metrics.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure CloudTrail to capture ECS API calls and use EventBridge to push the CPU and memory metrics into CloudWatch.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use AWS CloudWatch to monitor ECS task-level metrics such as CPU and memory utilization. Install the CloudWatch Agent to push custom metrics for active users. </p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 15,
    "question": "<p>A company uses AWS Glue to automate their ETL processes for data stored in various formats. They are planning to optimize the performance of their Glue ETL jobs which currently handle large volumes of Parquet files. What would be the most cost-effective way to optimize these jobs without compromising on performance?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Increase the number of DPUs in each ETL job to process data faster.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Implement job bookmarking to track data processed between job runs and reduce data redundancy.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Convert all Parquet files into CSV format to simplify processing.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Manually trigger ETL jobs instead of scheduling them to control when resources are utilized.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 16,
    "question": "<p>A healthcare organization needs to analyze large volumes of patient medical forms and identify important entities such as patient names, diagnoses, and medications. These forms include both text and handwritten content. The organization also needs to extract and categorize information to automate the documentation process and store the extracted metadata for further querying. Which combination of services would BEST meet the organization’s requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon SageMaker for handwriting recognition, Amazon Comprehend for sentiment analysis, and Amazon Kendra for querying the extracted metadata.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon Rekognition for detecting text and images, Amazon Comprehend for analyzing the extracted text, and Amazon RDS for storing structured data.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon Textract for extracting both printed and handwritten text, Amazon Comprehend for entity recognition, and Amazon S3 for storing the extracted metadata. </p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Amazon Comprehend for entity extraction, Amazon Textract for analyzing images and text, and Amazon Elasticsearch for indexing and querying the results. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 17,
    "question": "<p>Which of the following scenarios best describes the use of supervised learning?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>A retail business uses machine learning to automatically group products based on their similarity without providing labeled data.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>A financial institution segments its customers based on spending patterns without predefined categories.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>A company develops a predictive model to classify emails as either spam or not spam using historical labeled data.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>A gaming company implements a system where an AI learns to improve its strategy by playing games and receiving feedback from wins and losses. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 18,
    "question": "<p>A company is using Amazon Kinesis to monitor IoT sensor data for real-time security threats. The system needs to detect anomalies in the data stream and trigger alerts immediately, minimizing latency. They plan to have multiple consumers process the data streams concurrently. <br><br>What approach should the company take to ensure optimal scalability and low latency?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Kinesis Firehose to scale automatically and reduce latency for each consumer.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable enhanced fan-out for each consumer to achieve dedicated read throughput and lower latency.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Glue for ETL and real-time data analysis of the streaming data.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use standard Kinesis consumers and scale the number of shards as needed.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 19,
    "question": "<p>A machine learning engineer at a healthcare company has deployed a model on Amazon SageMaker and wants to monitor the data quality to ensure that the data used in production maintains high integrity. <br><br>Which of the following actions is MOST appropriate for monitoring data quality using Amazon SageMaker?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use a custom SageMaker algorithm to filter out low-quality data before making predictions in the production environment.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable SageMaker Feature Store to log and validate all incoming data used for predictions and correct data quality issues in real-time.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Model Monitor to evaluate incoming data for missing values, outliers, and deviations from the baseline training data.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Set up Amazon CloudWatch to automatically retrain the model whenever the incoming data has quality issues.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 20,
    "question": "<p>A retail company uses Amazon SageMaker to forecast sales by training a machine learning model on historical data. The data includes columns such as <code>product_id</code>, <code>date</code>, <code>price</code>, <code>promotion</code>, and <code>units_sold</code>. The company wants to improve its forecast accuracy by incorporating new features, such as holidays and competitor pricing.</p><p>What is the most effective way to include these new features in the model training workflow?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Add the new features to the dataset, retrain the model, and compare the new model’s performance.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable Amazon SageMaker’s AutoPilot feature to automatically detect and add holiday and competitor pricing features.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Forecast, which automatically incorporates external variables, such as holidays, into the forecasting model.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon SageMaker’s hyperparameter tuning to adjust the model based on new features.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 21,
    "question": "<p>Suppose you have a relatively small model that trains very quickly, but you want to systematically <em>guarantee</em> coverage of the hyperparameter space. Which method is <em>most suitable</em> in this case?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Grid Search</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Bayesian Optimization</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Random Search</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Hyperband</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 22,
    "question": "<p>You are a data scientist tasked with developing a model for a large e-commerce platform. The company needs a solution that minimizes development time and operational complexity. The team is considering using Amazon SageMaker's built-in algorithms instead of building custom models. <br>Which of the following advantages do built-in SageMaker algorithms offer for this use case? (Choose TWO) </p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Custom algorithms provide faster training times and more efficient resource usage compared to built-in algorithms.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Custom frameworks such as PyTorch and TensorFlow must be used with SageMaker to enable hyperparameter tuning. </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Built-in algorithms can only be used for simple tasks like linear regression and are not suitable for complex machine learning tasks.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Pre-configured environments and infrastructure management are handled by AWS, eliminating the need for custom containers.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Built-in algorithms like XGBoost and Linear Learner abstract away infrastructure setup and allow quick model deployment.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 23,
    "question": "<p>A company is building a multi-lingual customer support system that allows users to communicate both via text and voice. The system should transcribe customer support calls into text, translate the text into the user’s preferred language, and generate audio responses in the same language. Which combination of AWS services is MOST appropriate for this use case?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Polly to convert speech to text, AWS Glue to manage translations, and Amazon Transcribe to generate speech responses. </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Lambda to convert speech to text, Amazon Translate to handle text translations, and Amazon SageMaker to generate speech responses.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon Transcribe to convert speech to text, Amazon Translate to translate the text, and Amazon Polly to convert the translated text to speech.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Transcribe to convert speech to text, Amazon Lex to handle translations, and Amazon Polly to generate the voice responses.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 24,
    "question": "<p>A financial services company is using Amazon SageMaker Model Monitor to track the feature attribution drift of their credit risk prediction model. They notice that the importance of a particular feature, \"annual income,\" has shifted significantly over time, making it a more dominant factor in the model's predictions. <br><br>What is the BEST course of action to address this shift in feature importance?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Switch to a different model algorithm that automatically reduces the importance of \"annual income\" in predictions.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Fine-tune the hyperparameters of the existing model to lower the weight of the \"annual income\" feature.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Retrain the model with a larger dataset that excludes the \"annual income\" feature to prevent bias in predictions.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Model Monitor to alert the team about this shift and retrain the model with updated data to ensure feature importance remains balanced.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 25,
    "question": "<p>A transportation company is analyzing GPS data from its fleet of vehicles to optimize delivery routes. The company needs to preprocess and clean this data, create relevant features like average speed and distance traveled, and store these features in a scalable feature store. The team wants to ensure the features are available for real-time predictions on delivery times and wants to train a machine learning model using these features in an easily managed notebook environment. <br><br>Which services should the company use to preprocess the data, store the features, and train the model?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Data Wrangler for preprocessing, Amazon Redshift for feature storage, and SageMaker Notebooks to train and deploy the machine learning model. </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Notebooks for data preprocessing, Amazon DynamoDB for feature storage, and SageMaker Feature Store to train the machine learning model.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Data Wrangler for data preprocessing, SageMaker Feature Store to store features, and SageMaker Notebooks to train and deploy the machine learning model.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon QuickSight to preprocess and clean the data, SageMaker Feature Store for feature storage, and SageMaker Autopilot to train the machine learning model. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 26,
    "question": "<p>An ML engineer wants to automate the hyperparameter tuning of their machine learning model using Amazon SageMaker. To minimize costs, they also want to utilize spot instances during the tuning process. Which combination of factors must the engineer specify to initiate the automatic tuning process in SageMaker?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Dataset, algorithm, performance metric</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Algorithm, hyperparameter range, performance metric </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Hyperparameter range, dataset, algorithm</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Algorithm, performance metric, model parameters</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 27,
    "question": "<p>A financial services company is using Amazon SageMaker to develop a machine learning model for fraud detection. The dataset contains transaction records, customer profiles, and behavioral data, stored in Amazon S3. The data is updated in real-time, and new fraudulent patterns emerge frequently, requiring regular model updates. To ensure that the model remains effective, the company wants to automate the model retraining process based on new data, detect data drift in real-time, and perform hyperparameter tuning for optimal performance. Additionally, the company needs to ensure that model predictions are explainable to regulators. <br><br>Which is the most appropriate solution to meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Model Monitor to detect data drift, SageMaker Pipelines to automate the retraining process, and SageMaker Clarify to explain model predictions.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Clarify to detect data drift, SageMaker Feature Store to manage real-time feature updates, and SageMaker Autopilot to retrain the model based on new data.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Feature Store to automatically update features as new data comes in, SageMaker Model Monitor to automate model retraining, and SageMaker Autopilot to handle hyperparameter tuning.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Data Wrangler to preprocess new data, SageMaker Pipelines to automate retraining and hyperparameter tuning, and SageMaker Ground Truth to explain model predictions.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 28,
    "question": "<p>A machine learning engineer is using SageMaker Data Wrangler to understand relationships between features in the dataset. The engineer wants to visualize the distribution of numerical data and identify relationships between features. Which of the following visualization methods should be used in Data Wrangler to achieve these objectives? (Choose TWO)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Partial Dependence Plots (PDP) for feature impact</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Confusion matrix for evaluating model predictions</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>ROC curves for model evaluation</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Histograms for understanding data distribution</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Line plots for analyzing trends over time</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 29,
    "question": "<p>A company wants to ensure that all its EC2 instances are monitored for both CPU utilization and disk read/write operations. They also need to track configuration changes to these EC2 instances over time, such as changes to security groups, instance type, and AMI IDs. Which combination of AWS services will best meet these requirements?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable AWS CloudTrail to monitor CPU and disk usage and use AWS Config to capture configuration changes.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up CloudWatch Alarms for CPU and disk operations and AWS CloudTrail for tracking configuration changes on EC2 instances.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS CloudWatch to monitor CPU utilization and disk read/write operations, and AWS Config to track configuration changes to the EC2 instances.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use CloudWatch Logs to capture both CPU utilization and configuration changes and create custom log filters for disk operations.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 30,
    "question": "<p>An ML engineer is using SageMaker Debugger's profiling feature to optimize resource usage during training. They notice that the GPU utilization is very low, causing longer training times. Which of the following actions could help improve GPU utilization and optimize the training process?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use SageMaker Debugger to monitor for vanishing gradients, which will improve GPU utilization automatically. </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Adjust the batch size and data loading pipeline to ensure that the GPU is being used efficiently throughout the training.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Migrate the model to CPU-only instances to avoid GPU underutilization.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Neo to optimize the model for better GPU performance during training.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 31,
    "question": "<p>You are working on a machine learning project where your model frequently encounters issues such as vanishing gradients and overfitting during training. You decide to use Amazon SageMaker Debugger to monitor and debug the model in real-time to resolve these issues before deployment. <br><br>Which of the following actions should you take when configuring SageMaker Debugger for this task? (Select Two)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Set up built-in or custom debug rules to monitor training metrics and detect issues such as vanishing gradients or overfitting.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Enable SageMaker Model Monitor to visualize and track resource utilization, such as CPU and GPU, during training.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure a Debugger hook in the training script to capture and log tensors, which provide insights into the model's performance.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Profiler to automatically correct hyperparameters such as learning rate and batch size during training.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 32,
    "question": "<p>A law firm wants to create an intelligent search system for internal legal documents. These documents are stored in different formats (PDFs, Word documents, and scanned images), and the firm needs to extract and index key information like case names, legal terms, and dates. The system should also allow the legal team to perform natural language searches to quickly retrieve relevant documents. Which combination of AWS services should the firm use to achieve this goal?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Rekognition to analyze the scanned images, Amazon Comprehend to detect entities and extract text from documents, and Amazon Kendra to provide natural language search capabilities.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon Textract for extracting text from scanned images, Amazon Comprehend for entity recognition, and Amazon Kendra for building an intelligent search interface.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Amazon Kendra to store the documents, Amazon Rekognition for facial recognition in the scanned images, and Amazon Textract for analyzing legal terms.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Comprehend to categorize and index the documents, Amazon Lex for voice interaction with the system, and Amazon S3 for storing the document metadata.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 33,
    "question": "<p>A data scientist is using SageMaker Script Mode to train a machine learning model. They need to modularize their code by separating model definition, training logic, and inference logic into different Python files. They also need to include several external dependencies. How should the data scientist proceed to ensure SageMaker can correctly run their custom model?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Build a custom Docker container that includes all the Python files and dependencies, and then push it to Amazon ECR for SageMaker to use. </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker’s built-in algorithms and ignore the need for custom modularized code. </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Write all code into a single Python file and include dependencies directly within the script.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Split the code into multiple Python files, package them, and specify a single entry point script in the SageMaker estimator. Use a requirements.txt file to install dependencies. </p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 34,
    "question": "<p>An ML engineer is using Amazon SageMaker Data Wrangler to preprocess data for a machine learning project. They need to import customer transaction data from various sources, clean and transform the data, and engineer features for model training. Which TWO of the following steps should the engineer take to prepare the data using Data Wrangler? (Choose TWO.)</p>",
    "corrects": [
      2,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Export the transformed data directly to Amazon Athena for further analysis.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Data Wrangler to apply one-hot encoding on categorical variables.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Perform data transformations using SageMaker Studio's in-built Jupyter notebooks.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Manually write custom Python scripts to handle missing data and normalization tasks.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Import data from Amazon S3, Redshift, or SageMaker Feature Store into Data Wrangler.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 35,
    "question": "<p>A company deployed a binary classification model on a real-time Amazon SageMaker endpoint. To ensure long-term performance, they set up a schedule for SageMaker Model Monitor to evaluate model quality based on actual predictions and outcomes over time. <br>Which of the following steps is required to effectively monitor model quality using SageMaker Model Monitor?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable scheduled retraining on SageMaker to automatically improve model accuracy without monitoring metrics.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up custom metrics for feature attribution and monitor the bias drift for predictions from the real-time endpoint.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable automatic hyperparameter tuning for the endpoint and log model predictions to Amazon CloudWatch. </p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Create baseline statistics from the model’s training data and enable data capture to record requests and predictions.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 36,
    "question": "<p>A company wants to integrate generative AI capabilities into their applications using foundation models from multiple AI providers. They need a solution that offers a serverless environment, flexibility in model customization, and ensures that data remains within a specific AWS region for compliance reasons. <br>Which AWS service would best meet these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon Bedrock</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Amazon Lex</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon SageMaker</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Amazon Rekognition</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 37,
    "question": "<p>A machine learning engineer is conducting multiple model training sessions in SageMaker using different sets of hyperparameters and algorithms to find the best configuration for their model. They want to keep track of every trial and ensure the reproducibility of their experiments. Which AWS service feature would help them organize and compare these configurations effectively?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Experiments</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Autopilot</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Amazon EC2 Spot Instances</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Ground Truth </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 38,
    "question": "<p>Which of the following statements about AWS IAM roles is correct?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>IAM roles are used only by AWS services and cannot be assumed by IAM users or external identities.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>IAM roles can be assumed by AWS services or external identities and can have multiple policies attached to manage permissions.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>IAM roles do not support inline policies and can only be defined with AWS managed policies.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>IAM roles are associated directly with an IAM user and always tied to a specific user account.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 39,
    "question": "<p>You are working with large datasets stored in an Amazon S3 bucket and need to optimize the performance of querying and processing the data in AWS Glue. The data is frequently queried for specific years. Which of the following strategies will MOST effectively reduce query execution time and cost in this scenario?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store the data in an Amazon RDS database and run queries directly from there.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Store all data in a single folder and apply a custom filter during each query.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Partition the data by year and define Glue crawlers to automatically recognize these partitions.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use AWS Glue to compress the data, but do not partition it.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 40,
    "question": "<p>You are evaluating the performance of a machine learning model and want to ensure that it generalizes well on unseen data. You decide to partition your dataset into three distinct sets: training, validation, and testing. However, your model performs exceptionally well on the training set but poorly on the validation and test sets. What is the likely cause of this performance discrepancy?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>The model is underfitting the data.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>The model is overfitting the training data.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>The model has a high bias and low variance.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>The training set is too small for meaningful evaluation.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 41,
    "question": "<p>A company needs to ensure that their data storage solutions on AWS comply with industry regulations that require encryption of data at rest. Which AWS service and feature should be implemented to meet this compliance requirement across multiple storage services?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Lambda to apply encryption to data as it is stored in each service.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Enable Amazon S3 server-side encryption (SSE) with AWS KMS-managed keys (SSE-KMS).</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Configure AWS IAM policies to enforce encryption on data storage services.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Implement AWS Shield Advanced for automatic data encryption across all AWS services.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 42,
    "question": "<p>A financial firm wants to use reinforcement learning to optimize trading strategies using Amazon SageMaker. They want to train an agent that interacts with a simulated trading environment and receives feedback in the form of profits or losses. Which of the following elements must be included to formulate the reinforcement learning problem in this context?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Defining the agent, environment, actions, and rewards</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Configuring manual data labeling for the agent's learning</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Training the model using a supervised learning algorithm</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Creating static rules for decision-making</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 43,
    "question": "<p>A machine learning team is using SageMaker Pipelines to automate the training and deployment of multiple machine learning models. They want to scale the pipeline to handle thousands of workflows simultaneously while ensuring that steps, such as model training and evaluation, are executed in a specific sequence. Which feature of SageMaker Pipelines ensures that steps are executed in a defined order?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Model Package</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Pipeline Parameters</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Directed Acyclic Graph (DAG)</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Model Monitor</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 44,
    "question": "<p>A retail company wants to use machine learning to predict customer churn. The company does not have enough expertise to build and train a model from scratch, but they need a reliable model that can quickly be deployed and customized with their customer data. The company is considering Amazon SageMaker JumpStart as a solution. <br>Which of the following actions should the company take to achieve its goal using SageMaker JumpStart? (Choose TWO.)</p>",
    "corrects": [
      2,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Train a custom model on SageMaker from scratch using their own algorithms to meet the customer churn prediction requirements.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Fine-tune a pre-built customer churn model available on SageMaker JumpStart using the company's proprietary data. </p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Browse the SageMaker JumpStart model hub to find a pre-built customer churn prediction model and deploy it directly.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Manually configure the infrastructure, storage, and compute resources required to deploy the pre-trained model from SageMaker JumpStart. </p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Deploy a SageMaker endpoint without any modifications or fine-tuning to ensure model accuracy on their data.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 45,
    "question": "<p>A data engineer is tasked with creating an Extract, Transform, Load (ETL) pipeline using AWS Glue to process sales data from various sources. The data, in CSV format, is stored in Amazon S3. The processed data needs to be stored in a columnar format for faster querying using Amazon Athena. The engineer also needs to ensure the schema is automatically detected and stored for future use. <br><br>Which of the following steps should the data engineer take to meet these requirements? (Choose TWO)</p>",
    "corrects": [
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Manually create a schema in Amazon Athena and upload it to the Glue Data Catalog to enable querying.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Set up manual scripts to define the schema for each data source and store it in Amazon RDS.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Configure AWS Lambda to process and load the data into Athena for schema inference and storage.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use an AWS Glue crawler to automatically detect the schema of the CSV files and store the metadata in the AWS Glue Data Catalog.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Use AWS Glue's built-in transformation to convert the CSV data into Apache Parquet before saving it back to S3.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 46,
    "question": "<p>An ML engineer wants to build a sentiment analysis model using pre-built solutions in SageMaker JumpStart. They need a model that can quickly be deployed and then customized using their own dataset. <br><br>What features of SageMaker JumpStart would be most useful for the ML engineer's needs?</p>",
    "corrects": [
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Only the use of prompt engineering to modify the inputs without needing to retrain the model.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Pre-built solutions that automatically perform both data preprocessing and model training without user intervention.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>The ability to fine-tune foundation models with custom datasets and perform hyperparameter tuning.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>The option to use custom-built proprietary models with full infrastructure control.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 47,
    "question": "<p>You are training a machine learning model using PyTorch on Amazon SageMaker and want to ensure that the system resources, such as CPU and GPU, are being effectively utilized. You plan to use SageMaker Profiler to track these metrics and improve the efficiency of your training job. Which of the following steps should you follow to set up and use SageMaker Profiler correctly? (Select Two)</p>",
    "corrects": [
      3,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Manually enable automatic SageMaker Profiler optimization in the SageMaker console to reduce memory overhead during training. </p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Configure the SageMaker Profiler to automatically tune your hyperparameters, such as learning rate, based on the resource utilization data.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Define the profiler configuration in your SageMaker Estimator, specifying the duration of the profiling in seconds for the CPU and GPU.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Use SageMaker Neo to compile your model for optimized CPU and GPU performance after training is completed.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Import the necessary SageMaker Profiler modules and add start and stop profiling commands in your PyTorch training script.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 48,
    "question": "<p>A financial services company is building a fraud detection system to monitor transactions in real-time. They plan to use Amazon Kinesis Data Streams to ingest transaction data and want the system to scale automatically based on demand. They require low-latency for fast fraud detection responses. <br><br>Which feature of Amazon Kinesis Data Streams should the company implement to meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Manually scale shards based on throughput requirements.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Kinesis Firehose to automatically scale the stream based on traffic. </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Enable provisioned mode and dynamically add shards.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use enhanced fan-out to reduce consumer latency and ensure dedicated throughput per consumer. </p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 49,
    "question": "<p>A data scientist is setting up an environment to develop, train, and deploy machine learning models using AWS SageMaker. The scientist wants a fully managed Jupyter notebook environment that simplifies infrastructure setup and provides pre-installed libraries such as TensorFlow and PyTorch. Additionally, they want to ensure that their work is automatically saved and can be accessed later. <br>Which AWS SageMaker feature would BEST meet these requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Studio with Jupyter Lab spaces</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Amazon EC2 with Jupyter notebook manually installed</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS Glue with integrated Jupyter environment</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Notebook Instances</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 50,
    "question": "<p>A machine learning engineer is using Amazon SageMaker Data Wrangler to prepare data for training a model. The dataset has missing values, inconsistent scales across features, and categorical data. Which of the following actions can the engineer perform using Data Wrangler to resolve these issues? (Choose THREE)</p>",
    "corrects": [
      3,
      4,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Manually code all the transformations using Python and upload the final dataset to SageMaker.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon EMR to pre-process the data for missing values and scaling.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Perform imputation to fill in missing values using strategies like mean or median.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>One-hot encode categorical data to convert it into a numerical format.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Normalize feature values to ensure all features are on a similar scale.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 51,
    "question": "<p>A machine learning model deployed into production has started underperforming due to the introduction of new data, resulting in the model making less accurate predictions. The team is concerned about both data drift (changes in input data distribution) and model drift (declining model accuracy). <br>Which AWS service combination is most suitable for addressing these issues?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Amazon S3 to store new data and SageMaker Model Monitor to manage model drift.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>SageMaker Feature Store to track data distribution and Amazon Kinesis to monitor model predictions.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>AWS CloudTrail to monitor model drift and AWS Lambda to retrain the model.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>SageMaker Model Monitor to track data drift and SageMaker Pipelines to automate model retraining.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Solution Monitoring, Maintenance, and Security",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 52,
    "question": "<p>A company is training a large neural network model with billions of parameters on Amazon SageMaker. The model is too large to fit into the memory of a single GPU. What technique should the company use to distribute the training across multiple instances in this scenario?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Gradient Parallelism</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Hybrid Model and Data Parallelism</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Data Parallelism</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Model Parallelism</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 53,
    "question": "<p>A retail company uses Amazon SageMaker to train machine learning models for product recommendation. The team stores raw customer behavior data, such as clicks and purchases, in an Amazon S3 bucket. They want to preprocess this data to remove duplicates, normalize numerical features, and encode categorical variables before training their model. Additionally, the preprocessing needs to scale to handle increasing data volumes.</p><p>Which solution will most effectively meet their requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Leverage AWS Glue for ETL to preprocess the data, and then load the transformed data into Amazon SageMaker.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Athena to query the data and perform all preprocessing using SQL, then export the processed data back to Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Write a preprocessing script in Python and run it on Amazon EC2 instances with auto-scaling enabled.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon SageMaker Processing Jobs with a pre-built container for data preprocessing and store the processed data back in S3.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 54,
    "question": "<p>An e-commerce company needs to build a recommendation engine using a machine learning model trained on customer activity data, including purchase history, browsing patterns, and demographics. The team wants to streamline their model development process by creating a centralized repository of preprocessed features that can be reused across different projects. The system needs to support real-time recommendations and batch training. <br><br>What is the MOST appropriate setup for the team to ensure consistent feature reuse, while also optimizing for real-time and batch processing?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Store all features in Amazon S3 and manually preprocess them for each model training run.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use the SageMaker Feature Store to create an offline store for batch training and an online store for real-time inference.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Ground Truth to label the data and store the features in an offline store for training only. </p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use the SageMaker Feature Store to create an online store for batch processing and an offline store for real-time recommendations. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 55,
    "question": "<p>A machine learning engineer is using Amazon SageMaker Studio to perform exploratory data analysis on a large dataset stored in Amazon S3. The analyst needs to create visualizations, run data preprocessing workflows, and share the results with team members. The analyst also wants to avoid setting up infrastructure manually. Considering these requirements, which SageMaker features should the analyst leverage to achieve these tasks efficiently? (Select THREE)</p>",
    "corrects": [
      1,
      4,
      6
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Take advantage of the built-in integrations with popular data visualization libraries such as Matplotlib and Seaborn to create interactive data visualizations.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Store the dataset in the SageMaker notebook instance's local storage to avoid the complexity of using external services like Amazon S3.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use SageMaker Automatic Model Tuning to automatically handle hyperparameter optimization during the preprocessing and analysis steps.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Share SageMaker Studio notebooks directly with team members for collaboration, ensuring that they can access the same environment and results.</p>",
        "correct": true
      },
      {
        "id": 5,
        "answer": "<p>Use Amazon SageMaker's Real-Time Inference Endpoints to deploy the model and generate immediate predictions on the dataset.</p>",
        "correct": false
      },
      {
        "id": 6,
        "answer": "<p>Utilize SageMaker Studio's JupyterLab environment to interactively run code cells, visualize data, and tweak preprocessing workflows without worrying about underlying infrastructure.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 56,
    "question": "<p>A robotics company is using SageMaker to train a reinforcement learning agent to navigate through a dynamic warehouse environment. During training, the agent sometimes moves farther away from the goal instead of closer. What is the <strong>most likely cause</strong> of this behavior?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Negative rewards or penalties</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Adjustments to the policy function</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Adjustments to the action space</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Positive rewards</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 57,
    "question": "<p>What is the main difference between model parameters and hyperparameters in machine learning?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Hyperparameters are set before training, while model parameters are learned during training.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Model parameters are set before training, while hyperparameters are learned during training.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Both model parameters and hyperparameters are set before training. </p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Both model parameters and hyperparameters are learned during training.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 58,
    "question": "<p>A data scientist is designing a SageMaker pipeline to automate an ML workflow. The pipeline needs to train a model, process data, and register the model for future deployments. Which features of SageMaker Pipelines help automate and manage the workflow effectively?</p>",
    "corrects": [
      2
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>SageMaker Pipelines use a manual step-by-step execution, allowing better control over each task.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Pipelines are defined using a directed acyclic graph (DAG), specifying the sequence of steps, and include reusable parameters for customization.</p>",
        "correct": true
      },
      {
        "id": 3,
        "answer": "<p>Each step in the pipeline is manually executed, but results are stored for reuse in future workflows.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Pipelines can only handle small-scale projects due to limitations in scaling concurrent workflows. </p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 59,
    "question": "<p>A team at a media company wants to use Amazon SageMaker Jumpstart to quickly deploy a pre-trained model for text generation. They are considering whether to customize the model to better suit their specific use case. Which of the following customization methods available in Jumpstart should the team use to adapt the pre-trained model to their needs? (Choose TWO)</p>",
    "corrects": [
      1,
      5
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use prompt engineering to design and refine the input prompts so that the model output aligns better with their business requirements.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use SageMaker Autopilot to automatically generate new models from scratch based on their dataset, bypassing the need for pre-trained models.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Perform manual hyperparameter tuning using SageMaker Studio, as Jumpstart models do not support automatic hyperparameter tuning. </p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Customize the pre-trained model by changing the underlying model architecture to better suit text analysis tasks. </p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Fine-tune the pre-trained model using their own dataset to improve performance for their specific task.</p>",
        "correct": true
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 60,
    "question": "<p>A company wants to use Amazon SageMaker to deploy a real-time machine learning model for high-traffic online predictions. The model has moderate memory and CPU requirements, but scalability and low latency are critical due to unpredictable traffic spikes.</p><p>Which of the following deployment options is best suited for this scenario?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use AWS Lambda to serve predictions from the model, scaling as per incoming requests.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Deploy the model on an Amazon EC2 instance to manually manage scaling.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use Amazon SageMaker batch transform to process requests in large batches.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Deploy the model on an Amazon SageMaker endpoint with auto-scaling enabled.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 61,
    "question": "<p>A financial services company records customer service calls and needs to perform sentiment analysis on these calls to assess customer satisfaction. They also require the ability to review the transcripts of these calls for compliance purposes. Which combination of AWS services will BEST achieve these requirements?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use Amazon Transcribe to convert the call recordings into text, store the transcripts in Amazon S3, and use Amazon Comprehend for sentiment analysis on the transcripts.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon Transcribe to convert the call recordings into text, and Amazon Lex to analyze the sentiment of the calls.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use AWS Lambda to convert the call recordings into text, store the transcripts in Amazon S3, and use Amazon Rekognition for sentiment analysis.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use Amazon Polly to convert the call recordings into text, store the transcripts in Amazon DynamoDB, and use Amazon SageMaker to perform sentiment analysis.</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Deployment and Orchestration of ML Workflows",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 62,
    "question": "<p>A company is planning to utilize AWS Kinesis Data Firehose for the ingestion of large log files from various devices. Their goal is to ensure data consistency and optimize API call usage. What configuration would best meet their requirements?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Enable immediate data processing in AWS Lambda without buffering.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Use AWS Glue to handle data ingestion without buffering.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Directly send unbuffered data to Amazon S3 for cost efficiency.</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Configure Kinesis Data Firehose to buffer data based on maximum time intervals.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 63,
    "question": "<p>A data scientist is facing the challenge of high variance in a machine learning model, where the model performs well on training data but poorly on new, unseen data. Which of the following methods would MOST LIKELY improve the model's ability to generalize, adhering to responsible AI principles?</p>",
    "corrects": [
      4
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Focus on training the model using a smaller subset of the data to prevent overfitting.</p>",
        "correct": false
      },
      {
        "id": 2,
        "answer": "<p>Remove cross-validation steps to speed up model training and testing. </p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Increase the number of parameters in the model to capture more complex patterns in the data. </p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>Use early stopping to halt the training once the model's performance starts to degrade on the validation set.</p>",
        "correct": true
      }
    ],
    "multiple": false,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 64,
    "question": "<p>A data scientist is using SageMaker Data Wrangler for feature engineering and needs to determine which features contribute most to the model's predictions. Which feature of Data Wrangler helps visualize and estimate the importance of different features?</p>",
    "corrects": [
      1
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Quick Model</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Min-Max Scaling</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Recursive Feature Elimination</p>",
        "correct": false
      },
      {
        "id": 4,
        "answer": "<p>One-hot Encoding</p>",
        "correct": false
      }
    ],
    "multiple": false,
    "domain": "Data Preparation for Machine Learning",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  },
  {
    "id": 65,
    "question": "<p>A data scientist is building a fraud detection model using Amazon SageMaker. The dataset is highly imbalanced, with only 2% of the data representing fraudulent cases. The scientist wants to ensure the model can effectively detect fraud while minimizing false negatives.</p><p>Which techniques should the scientist use to handle this issue? (Choose TWO.)</p>",
    "corrects": [
      1,
      3
    ],
    "answers": [
      {
        "id": 1,
        "answer": "<p>Use oversampling techniques, such as SMOTE, to balance the dataset.</p>",
        "correct": true
      },
      {
        "id": 2,
        "answer": "<p>Use Amazon SageMaker Random Cut Forest (RCF) algorithm for anomaly detection.</p>",
        "correct": false
      },
      {
        "id": 3,
        "answer": "<p>Use weighted loss functions during training to penalize misclassification of the minority class.</p>",
        "correct": true
      },
      {
        "id": 4,
        "answer": "<p>Perform dimensionality reduction to reduce the number of features and make the dataset more manageable.</p>",
        "correct": false
      },
      {
        "id": 5,
        "answer": "<p>Train the model on the original dataset and rely on evaluation metrics for adjustments.</p>",
        "correct": false
      }
    ],
    "multiple": true,
    "domain": "ML Model Development",
    "explanation": "",
    "correctAnswerExplanations": [],
    "incorrectAnswerExplanations": [],
    "references": []
  }
]