[
    {
      "id": 1,
      "question": "A company stores large video files, ranging from 1 MB to 500 GB, on on-premises network attached storage using NFS. The total storage is 70 TB and is no longer growing. The company plans to migrate these video files to Amazon S3 as quickly as possible while minimizing network bandwidth usage.\n\nWhich solution will meet these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use the AWS CLI with an IAM role that has write permissions to an S3 bucket, then copy all files from the local storage to the S3 bucket.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Deploy an S3 File Gateway on-premises, create a public service endpoint for the S3 File Gateway, create an S3 bucket, create a new NFS file share on the S3 File Gateway pointing to the S3 bucket, and transfer data from the existing NFS file share to the S3 File Gateway.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Order an AWS Snowball Edge job, receive the Snowball Edge device on premises, transfer data to the device using the Snowball Edge client, and return the device to AWS for data import into Amazon S3.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Set up an AWS Direct Connect connection, deploy an S3 File Gateway on-premises, create a public virtual interface (VIF) for the S3 File Gateway, create an S3 bucket, create a new NFS file share on the S3 File Gateway pointing to the S3 bucket, and transfer data from the existing NFS file share to the S3 File Gateway.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nOrder an AWS Snowball Edge job, receive the Snowball Edge device on premises, transfer data to the device using the Snowball Edge client, and return the device to AWS for data import into Amazon S3.\n\nAWS Snowball Edge is a data transfer service designed to move large volumes of data to AWS quickly and securely. Using a Snowball Edge device minimizes network bandwidth usage and accelerates the migration process, meeting the company's requirements.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse the AWS CLI with an IAM role that has write permissions to an S3 bucket, then copy all files from the local storage to the S3 bucket.\n\nThis option uses the public internet for data transfer, consuming significant network bandwidth and taking longer than the AWS Snowball Edge option.\n\n\n\n\nSet up an AWS Direct Connect connection, deploy an S3 File Gateway on-premises, create a public virtual interface (VIF) for the S3 File Gateway, create an S3 bucket, create a new NFS file share on the S3 File Gateway pointing to the S3 bucket, and transfer data from the existing NFS file share to the S3 File Gateway.\n\nWhile this option reduces data transfer latency, it requires additional infrastructure setup and is less efficient than using AWS Snowball Edge for a one-time migration.\n\n\n\n\nDeploy an S3 File Gateway on-premises, create a public service endpoint for the S3 File Gateway, create an S3 bucket, create a new NFS file share on the S3 File Gateway pointing to the S3 bucket, and transfer data from the existing NFS file share to the S3 File Gateway.\n\nThis option uses the public internet for data transfer, consuming significant network bandwidth and taking longer than the AWS Snowball Edge option.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/snowball/latest/developer-guide/whatisedge.html",
      "correctAnswerExplanations": [
        {
          "answer": "Order an AWS Snowball Edge job, receive the Snowball Edge device on premises, transfer data to the device using the Snowball Edge client, and return the device to AWS for data import into Amazon S3.",
          "explanation": "AWS Snowball Edge is a data transfer service designed to move large volumes of data to AWS quickly and securely. Using a Snowball Edge device minimizes network bandwidth usage and accelerates the migration process, meeting the company's requirements."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use the AWS CLI with an IAM role that has write permissions to an S3 bucket, then copy all files from the local storage to the S3 bucket.",
          "explanation": "This option uses the public internet for data transfer, consuming significant network bandwidth and taking longer than the AWS Snowball Edge option."
        },
        {
          "answer": "Set up an AWS Direct Connect connection, deploy an S3 File Gateway on-premises, create a public virtual interface (VIF) for the S3 File Gateway, create an S3 bucket, create a new NFS file share on the S3 File Gateway pointing to the S3 bucket, and transfer data from the existing NFS file share to the S3 File Gateway.",
          "explanation": "While this option reduces data transfer latency, it requires additional infrastructure setup and is less efficient than using AWS Snowball Edge for a one-time migration."
        },
        {
          "answer": "Deploy an S3 File Gateway on-premises, create a public service endpoint for the S3 File Gateway, create an S3 bucket, create a new NFS file share on the S3 File Gateway pointing to the S3 bucket, and transfer data from the existing NFS file share to the S3 File Gateway.",
          "explanation": "This option uses the public internet for data transfer, consuming significant network bandwidth and taking longer than the AWS Snowball Edge option."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/snowball/latest/developer-guide/whatisedge.html"
      ]
    },
    {
      "id": 2,
      "question": "A company has deployed a multi-tier web application on AWS. The web servers are located in a public subnet within a VPC, while the application servers and database servers are in private subnets within the same VPC. The company also deployed a third-party virtual intrusion detection system (IDS) from the AWS Marketplace in a separate security VPC. The IDS is configured with an IP interface that can process IP packets.\n\nA solutions architect needs to incorporate the web application with the IDS to scrutinize all incoming traffic before it reaches the web server.\n\nWhich solution will satisfy these requirements with the LEAST operational overhead?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Deploy a transit gateway in the security VPC. Configure route tables to channel incoming packets through the transit gateway for inspection.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the IDS for packet inspection.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Set up a Network Load Balancer in the public subnet of the application's VPC to redirect traffic to the IDS for packet inspection.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Implement a Gateway Load Balancer in the security VPC. Create a Gateway Load Balancer endpoint to accept incoming packets and forward them to the IDS for analysis.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nImplement a Gateway Load Balancer in the security VPC. Create a Gateway Load Balancer endpoint to accept incoming packets and forward them to the IDS for analysis.\n\nThis solution provides the least operational overhead by utilizing the Gateway Load Balancer, which is specifically designed for forwarding traffic to virtual appliances like IDS for inspection. The Gateway Load Balancer endpoint ensures seamless integration with the existing VPC architecture and maintains high availability and scalability.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up a Network Load Balancer in the public subnet of the application's VPC to redirect traffic to the IDS for packet inspection.\n\nNetwork Load Balancers are not designed for inspecting traffic and forwarding it to third-party appliances like an IDS. They are primarily used for distributing traffic across multiple targets.\n\n\n\n\nDeploy a transit gateway in the security VPC. Configure route tables to channel incoming packets through the transit gateway for inspection.\n\nThis option introduces unnecessary complexity and operational overhead. Transit gateways are typically used for connecting multiple VPCs and on-premises networks but are not specifically designed for traffic inspection.\n\n\n\n\nCreate an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the IDS for packet inspection.\n\nApplication Load Balancers are designed to route application-level traffic (HTTP/HTTPS) and are not suitable for forwarding traffic to third-party appliances like an IDS for packet inspection.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/en_us/elasticloadbalancing/latest/gateway/getting-started.html\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection-using-aws-gateway-load-balancer",
      "correctAnswerExplanations": [
        {
          "answer": "Implement a Gateway Load Balancer in the security VPC. Create a Gateway Load Balancer endpoint to accept incoming packets and forward them to the IDS for analysis.",
          "explanation": "This solution provides the least operational overhead by utilizing the Gateway Load Balancer, which is specifically designed for forwarding traffic to virtual appliances like IDS for inspection. The Gateway Load Balancer endpoint ensures seamless integration with the existing VPC architecture and maintains high availability and scalability."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Set up a Network Load Balancer in the public subnet of the application's VPC to redirect traffic to the IDS for packet inspection.",
          "explanation": "Network Load Balancers are not designed for inspecting traffic and forwarding it to third-party appliances like an IDS. They are primarily used for distributing traffic across multiple targets."
        },
        {
          "answer": "Deploy a transit gateway in the security VPC. Configure route tables to channel incoming packets through the transit gateway for inspection.",
          "explanation": "This option introduces unnecessary complexity and operational overhead. Transit gateways are typically used for connecting multiple VPCs and on-premises networks but are not specifically designed for traffic inspection."
        },
        {
          "answer": "Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the IDS for packet inspection.",
          "explanation": "Application Load Balancers are designed to route application-level traffic (HTTP/HTTPS) and are not suitable for forwarding traffic to third-party appliances like an IDS for packet inspection."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/en_us/elasticloadbalancing/latest/gateway/getting-started.html",
        "https://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection-using-aws-gateway-load-balancer"
      ]
    },
    {
      "id": 3,
      "question": "A software company provides a file conversion service for its 10 million users on AWS. The service must transform doc files such as .doc, .docx, .pdf, etc, averaging 5 MB, into .png image files. Both original and converted files need to be stored. A solutions architect must design an economical solution that can scale with the growing demand.\n\nWhich solution meets these requirements MOST cost-effectively?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Save the doc files in an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .png format. Save the doc files and the .png files in the EFS store.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Store the doc files in Amazon S3. Set up an S3 PUT event to trigger an AWS Lambda function that converts the files to .png format and stores them back in Amazon S3.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Save the doc files in an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic Block Store (Amazon EBS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .png format. Save the doc files and the .png files in the EBS store.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Store the doc files in Amazon EFS. Configure an Amazon EventBridge rule to invoke an AWS Lambda function that converts the files to .png format and stores them back in Amazon EFS.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nStore the doc files in Amazon S3. Set up an S3 PUT event to trigger an AWS Lambda function that converts the files to .png format and stores them back in Amazon S3.\n\nThis solution is the most cost-effective solution for our case. Using AWS Lambda for file conversion allows for automatic scaling, so the solution is more scalable and easier to manage compared to other options. Additionally, Amazon S3 is an economical storage option, allowing for cost-effective storage of both the original and converted files. The usage of S3 PUT event ensures the triggering of the Lambda function only when there is a new file upload to the bucket, minimizing unnecessary processing and cost.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStore the doc files in Amazon EFS. Configure an Amazon EventBridge rule to invoke an AWS Lambda function that converts the files to .png format and stores them back in Amazon EFS.\n\nAmazon EFS is more expensive than Amazon S3 and not as cost-effective for the given scenario.\n\n\n\n\nSave the doc files in an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic Block Store (Amazon EBS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .png format. Save the doc files and the .png files in the EBS store.\n\nManaging EC2 instances, EBS storage, and an Auto Scaling group introduces operational overhead and increases costs. A serverless approach with AWS Lambda and Amazon S3 is more cost-effective and scalable.\n\n\n\n\nSave the doc files in an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .png format. Save the doc files and the .png files in the EFS store.\n\nIt also involves operational overhead and increased costs associated with managing EC2 instances, EFS storage, and an Auto Scaling group. The serverless approach with AWS Lambda and Amazon S3 is more cost-effective and scalable.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html",
      "correctAnswerExplanations": [
        {
          "answer": "Store the doc files in Amazon S3. Set up an S3 PUT event to trigger an AWS Lambda function that converts the files to .png format and stores them back in Amazon S3.",
          "explanation": "This solution is the most cost-effective solution for our case. Using AWS Lambda for file conversion allows for automatic scaling, so the solution is more scalable and easier to manage compared to other options. Additionally, Amazon S3 is an economical storage option, allowing for cost-effective storage of both the original and converted files. The usage of S3 PUT event ensures the triggering of the Lambda function only when there is a new file upload to the bucket, minimizing unnecessary processing and cost."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Store the doc files in Amazon EFS. Configure an Amazon EventBridge rule to invoke an AWS Lambda function that converts the files to .png format and stores them back in Amazon EFS.",
          "explanation": "Amazon EFS is more expensive than Amazon S3 and not as cost-effective for the given scenario."
        },
        {
          "answer": "Save the doc files in an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic Block Store (Amazon EBS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .png format. Save the doc files and the .png files in the EBS store.",
          "explanation": "Managing EC2 instances, EBS storage, and an Auto Scaling group introduces operational overhead and increases costs. A serverless approach with AWS Lambda and Amazon S3 is more cost-effective and scalable."
        },
        {
          "answer": "Save the doc files in an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .png format. Save the doc files and the .png files in the EFS store.",
          "explanation": "It also involves operational overhead and increased costs associated with managing EC2 instances, EFS storage, and an Auto Scaling group. The serverless approach with AWS Lambda and Amazon S3 is more cost-effective and scalable."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html",
        "https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html"
      ]
    },
    {
      "id": 4,
      "question": "A fitness app company wants to allow users to upload their workout data to their AWS infrastructure, which will be processed by a Lambda function. The company wants to ensure that the data is processed in the order that it is received.\n\nWhich solution will meet these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use an Amazon API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the user uploads their workout data. Configure the SQS standard queue to invoke an AWS Lambda function for processing.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use an Amazon API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the user uploads their workout data. Configure the SQS FIFO queue to invoke an AWS Lambda function for processing.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Use an Amazon API Gateway authorizer to block any requests while the Lambda function is processing the workout data.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use an Amazon API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the user uploads their workout data. Subscribe an AWS Lambda function to the topic to perform processing.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nUse an Amazon API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the user uploads their workout data. Configure the SQS FIFO queue to invoke an AWS Lambda function for processing.\n\nWhen a user uploads their workout data, the Lambda function is triggered. Using a FIFO (First-In-First-Out) queue ensures that the data is processed in the order it was received. With a FIFO queue, the oldest message is processed first, and any subsequent messages are processed in the order in which they were received. This guarantees that the workout data is processed in the order that it was uploaded.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse an Amazon API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the user uploads their workout data. Subscribe an AWS Lambda function to the topic to perform processing.\n\nAmazon SNS does not guarantee that messages will be processed in the order that they were received. Although multiple Lambda functions can be subscribed to an SNS topic, there is no guarantee of the order in which they will receive messages.\n\n\n\n\nUse an Amazon API Gateway authorizer to block any requests while the Lambda function is processing the workout data.\n\nUsing an API Gateway authorizer will only block requests, it will not help with ensuring that the workout data is processed in the order it was received.\n\n\n\n\nUse an Amazon API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the user uploads their workout data. Configure the SQS standard queue to invoke an AWS Lambda function for processing.\n\nUsing an SQS standard queue does not guarantee that messages will be processed in the order that they were received. The order of messages can vary because of factors such as the number of consumers and the workload of each consumer.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\n\nhttps://aws.amazon.com/lambda",
      "correctAnswerExplanations": [
        {
          "answer": "Use an Amazon API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the user uploads their workout data. Configure the SQS FIFO queue to invoke an AWS Lambda function for processing.",
          "explanation": "When a user uploads their workout data, the Lambda function is triggered. Using a FIFO (First-In-First-Out) queue ensures that the data is processed in the order it was received. With a FIFO queue, the oldest message is processed first, and any subsequent messages are processed in the order in which they were received. This guarantees that the workout data is processed in the order that it was uploaded."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use an Amazon API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the user uploads their workout data. Subscribe an AWS Lambda function to the topic to perform processing.",
          "explanation": "Amazon SNS does not guarantee that messages will be processed in the order that they were received. Although multiple Lambda functions can be subscribed to an SNS topic, there is no guarantee of the order in which they will receive messages."
        },
        {
          "answer": "Use an Amazon API Gateway authorizer to block any requests while the Lambda function is processing the workout data.",
          "explanation": "Using an API Gateway authorizer will only block requests, it will not help with ensuring that the workout data is processed in the order it was received."
        },
        {
          "answer": "Use an Amazon API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the user uploads their workout data. Configure the SQS standard queue to invoke an AWS Lambda function for processing.",
          "explanation": "Using an SQS standard queue does not guarantee that messages will be processed in the order that they were received. The order of messages can vary because of factors such as the number of consumers and the workload of each consumer."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
        "https://aws.amazon.com/lambda"
      ]
    },
    {
      "id": 5,
      "question": "A company stores over 5 TB of file data on on-premises Linux file servers. Users and applications access the data daily. The company is migrating its Linux workloads to AWS and requires file storage access with minimal latency for both AWS and on-premises resources. The solution should minimize operational overhead and avoid major changes to existing file access patterns. The company uses AWS Direct Connect for connectivity to AWS.\n\nWhat should a solutions architect do to meet these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Deploy and configure an AWS Storage Gateway (File Gateway) on-premises. Migrate the on-premises file data to Amazon EFS. Reconfigure the workloads to use either Amazon EFS directly or the File Gateway, depending on each workload's location.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Deploy and configure an AWS Storage Gateway (File Gateway) on-premises. Migrate the on-premises file data to the File Gateway. Reconfigure the on-premises workloads and the cloud workloads to use the File Gateway.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Deploy and configure Amazon EFS on AWS. Deploy and configure an AWS Storage Gateway (File Gateway) on-premises. Migrate the on-premises file data to the File Gateway. Configure the cloud workloads to use Amazon EFS on AWS. Configure the on-premises workloads to use the File Gateway.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Deploy and configure Amazon EFS on AWS. Migrate the on-premises file data to EFS. Reconfigure the workloads to use Amazon EFS on AWS.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nDeploy and configure Amazon EFS on AWS. Deploy and configure an AWS Storage Gateway (File Gateway) on-premises. Migrate the on-premises file data to the File Gateway. Configure the cloud workloads to use Amazon EFS on AWS. Configure the on-premises workloads to use the File Gateway.\n\nAmazon EFS is used to store the file data on AWS, providing a scalable and highly available file system that can be accessed by multiple EC2 instances. An AWS Storage Gateway (File Gateway) is deployed on-premises, which provides low-latency access to the file data for on-premises workloads. The File Gateway caches frequently accessed data locally, minimizing latency and maximizing performance.\n\nMigrating the on-premises file data to the File Gateway, and configuring both cloud and on-premises workloads to use either Amazon EFS directly or the File Gateway depending on their location, ensures that both sets of workloads have access to the necessary data without major changes to existing file access patterns. This solution takes advantage of the existing AWS Direct Connect connectivity to ensure that the data can be accessed with minimal latency.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDeploy and configure Amazon EFS on AWS. Migrate the on-premises file data to EFS. Reconfigure the workloads to use Amazon EFS on AWS.\n\nThis does not address the requirement for minimal latency for on-premises workloads, as they would need to access the files directly on AWS without caching.\n\n\n\n\nDeploy and configure an AWS Storage Gateway (File Gateway) on-premises. Migrate the on-premises file data to the File Gateway. Reconfigure the on-premises workloads and the cloud workloads to use the File Gateway.\n\nThis does not provide an optimal solution for cloud workloads, as they would need to access the files through the on-premises File Gateway, increasing latency.\n\n\n\n\nDeploy and configure an AWS Storage Gateway (File Gateway) on-premises. Migrate the on-premises file data to Amazon EFS. Reconfigure the workloads to use either Amazon EFS directly or the File Gateway, depending on each workload's location.\n\nThe File Gateway does not natively support Amazon EFS as a backend storage.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/efs/\n\nhttps://aws.amazon.com/storagegateway/",
      "correctAnswerExplanations": [
        {
          "answer": "Deploy and configure Amazon EFS on AWS. Deploy and configure an AWS Storage Gateway (File Gateway) on-premises. Migrate the on-premises file data to the File Gateway. Configure the cloud workloads to use Amazon EFS on AWS. Configure the on-premises workloads to use the File Gateway.",
          "explanation": "Amazon EFS is used to store the file data on AWS, providing a scalable and highly available file system that can be accessed by multiple EC2 instances. An AWS Storage Gateway (File Gateway) is deployed on-premises, which provides low-latency access to the file data for on-premises workloads. The File Gateway caches frequently accessed data locally, minimizing latency and maximizing performance."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Deploy and configure Amazon EFS on AWS. Migrate the on-premises file data to EFS. Reconfigure the workloads to use Amazon EFS on AWS.",
          "explanation": "This does not address the requirement for minimal latency for on-premises workloads, as they would need to access the files directly on AWS without caching."
        },
        {
          "answer": "Deploy and configure an AWS Storage Gateway (File Gateway) on-premises. Migrate the on-premises file data to the File Gateway. Reconfigure the on-premises workloads and the cloud workloads to use the File Gateway.",
          "explanation": "This does not provide an optimal solution for cloud workloads, as they would need to access the files through the on-premises File Gateway, increasing latency."
        },
        {
          "answer": "Deploy and configure an AWS Storage Gateway (File Gateway) on-premises. Migrate the on-premises file data to Amazon EFS. Reconfigure the workloads to use either Amazon EFS directly or the File Gateway, depending on each workload's location.",
          "explanation": "The File Gateway does not natively support Amazon EFS as a backend storage."
        }
      ],
      "references": [
        "https://aws.amazon.com/efs/",
        "https://aws.amazon.com/storagegateway/"
      ]
    },
    {
      "id": 6,
      "question": "A software development company uses AWS Organizations to manage multiple AWS accounts for various projects. The management account has an Amazon S3 bucket that stores sensitive source code files. The company wants to ensure that access to this S3 bucket is restricted only to users of accounts within the organization in AWS Organizations.\n\nWhich approach meets these requirements with the LEAST amount of operational overhead?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Create an organizational unit (OU) for each project. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Add the aws:PrincipalOrgID global condition key with a reference to the organization ID in the S3 bucket policy.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Assign a tag to each user that requires access to the S3 bucket. Include the aws:PrincipalTag global condition key in the S3 bucket policy.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use AWS CloudTrail to track the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Modify the S3 bucket policy accordingly.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nAdd the aws:PrincipalOrgID global condition key with a reference to the organization ID in the S3 bucket policy.\n\nUsing the aws:PrincipalOrgID global condition key in the S3 bucket policy is the most efficient way to restrict access to the S3 bucket to users within the AWS Organizations. This approach minimizes operational overhead by allowing the organization to control access based on the organization ID. When a user tries to access the S3 bucket, AWS checks if the user's account belongs to the organization specified in the bucket policy. If the user's account is part of the organization, access is granted, ensuring that only users from accounts within the organization can access the S3 bucket.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an organizational unit (OU) for each project. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.\n\nIt increases operational overhead by requiring the creation and management of multiple OUs.\n\n\n\n\nUse AWS CloudTrail to track the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Modify the S3 bucket policy accordingly.\n\nIt introduces additional operational overhead by requiring continuous monitoring and policy updates.\n\n\n\n\nAssign a tag to each user that requires access to the S3 bucket. Include the aws:PrincipalTag global condition key in the S3 bucket policy.\n\nIt adds operational overhead by requiring the manual tagging of individual users.\n\n\n\n\n\n\n\nReferences\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html",
      "correctAnswerExplanations": [
        {
          "answer": "Add the aws:PrincipalOrgID global condition key with a reference to the organization ID in the S3 bucket policy.",
          "explanation": "Using the aws:PrincipalOrgID global condition key in the S3 bucket policy is the most efficient way to restrict access to the S3 bucket to users within the AWS Organizations. This approach minimizes operational overhead by allowing the organization to control access based on the organization ID. When a user tries to access the S3 bucket, AWS checks if the user's account belongs to the organization specified in the bucket policy. If the user's account is part of the organization, access is granted, ensuring that only users from accounts within the organization can access the S3 bucket."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Create an organizational unit (OU) for each project. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.",
          "explanation": "It increases operational overhead by requiring the creation and management of multiple OUs."
        },
        {
          "answer": "Use AWS CloudTrail to track the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Modify the S3 bucket policy accordingly.",
          "explanation": "It introduces additional operational overhead by requiring continuous monitoring and policy updates."
        },
        {
          "answer": "Assign a tag to each user that requires access to the S3 bucket. Include the aws:PrincipalTag global condition key in the S3 bucket policy.",
          "explanation": "It adds operational overhead by requiring the manual tagging of individual users."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html"
      ]
    },
    {
      "id": 7,
      "question": "A company offers a real-time gaming service that relies on UDP connections. The service operates on Amazon EC2 instances within an Auto Scaling group, spanning multiple AWS Regions. The company requires a solution to direct users to the Region with the lowest latency while ensuring automated failover between Regions.\n\nWhich solution will fulfill these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Set up a Network Load Balancer (NLB) with an associated target group. Connect the target group to the Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB. Implement an Amazon CloudFront distribution that uses the latency record as an origin.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Set up a Network Load Balancer (NLB) with an associated target group. Connect the target group to the Auto Scaling group. Utilize the NLB as an AWS Global Accelerator endpoint in each Region.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Set up an Application Load Balancer (ALB) with an associated target group. Connect the target group to the Auto Scaling group. Utilize the ALB as an AWS Global Accelerator endpoint in each Region.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Set up an Application Load Balancer (ALB) with an associated target group. Connect the target group to the Auto Scaling group. Create an Amazon Route 53 weighted record that points to aliases for each ALB. Implement an Amazon CloudFront distribution that uses the weighted record as an origin.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nSet up a Network Load Balancer (NLB) with an associated target group. Connect the target group to the Auto Scaling group. Utilize the NLB as an AWS Global Accelerator endpoint in each Region.\n\nThis solution fulfills the requirements of directing users to the Region with the lowest latency while ensuring automated failover between Regions. A Network Load Balancer can be set up with an associated target group that connects to the Auto Scaling group. This enables the load balancer to distribute traffic to the EC2 instances within the Auto Scaling group while providing low-latency and high-throughput UDP connections.\n\nUsing the NLB as an AWS Global Accelerator endpoint in each Region enables the Global Accelerator service to route traffic to the nearest NLB, reducing latency for users. Automated failover between Regions is also ensured as Global Accelerator continually monitors the health of the endpoints and reroutes traffic to healthy endpoints.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up an Application Load Balancer (ALB) with an associated target group. Connect the target group to the Auto Scaling group. Utilize the ALB as an AWS Global Accelerator endpoint in each Region.\n\nAn Application Load Balancer does not support UDP connections, which are required for the real-time gaming service.\n\n\n\n\nSet up a Network Load Balancer (NLB) with an associated target group. Connect the target group to the Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB. Implement an Amazon CloudFront distribution that uses the latency record as an origin.\n\nIt does not provide automated failover between Regions, which is a requirement for this scenario.\n\n\n\n\nSet up an Application Load Balancer (ALB) with an associated target group. Connect the target group to the Auto Scaling group. Create an Amazon Route 53 weighted record that points to aliases for each ALB. Implement an Amazon CloudFront distribution that uses the weighted record as an origin.\n\nIt does not support UDP connections, which are required for the real-time gaming service. Additionally, it does not provide automated failover between Regions, which is a requirement for this scenario.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/elasticloadbalancing/network-load-balancer\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/improving-real-time-communication-rtc-client-experience-with-aws-global-accelerator",
      "correctAnswerExplanations": [
        {
          "answer": "Set up a Network Load Balancer (NLB) with an associated target group. Connect the target group to the Auto Scaling group. Utilize the NLB as an AWS Global Accelerator endpoint in each Region.",
          "explanation": "This solution fulfills the requirements of directing users to the Region with the lowest latency while ensuring automated failover between Regions. A Network Load Balancer can be set up with an associated target group that connects to the Auto Scaling group. This enables the load balancer to distribute traffic to the EC2 instances within the Auto Scaling group while providing low-latency and high-throughput UDP connections."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Set up an Application Load Balancer (ALB) with an associated target group. Connect the target group to the Auto Scaling group. Utilize the ALB as an AWS Global Accelerator endpoint in each Region.",
          "explanation": "An Application Load Balancer does not support UDP connections, which are required for the real-time gaming service."
        },
        {
          "answer": "Set up a Network Load Balancer (NLB) with an associated target group. Connect the target group to the Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB. Implement an Amazon CloudFront distribution that uses the latency record as an origin.",
          "explanation": "It does not provide automated failover between Regions, which is a requirement for this scenario."
        },
        {
          "answer": "Set up an Application Load Balancer (ALB) with an associated target group. Connect the target group to the Auto Scaling group. Create an Amazon Route 53 weighted record that points to aliases for each ALB. Implement an Amazon CloudFront distribution that uses the weighted record as an origin.",
          "explanation": "It does not support UDP connections, which are required for the real-time gaming service. Additionally, it does not provide automated failover between Regions, which is a requirement for this scenario."
        }
      ],
      "references": [
        "https://aws.amazon.com/elasticloadbalancing/network-load-balancer",
        "https://aws.amazon.com/blogs/networking-and-content-delivery/improving-real-time-communication-rtc-client-experience-with-aws-global-accelerator"
      ]
    },
    {
      "id": 8,
      "question": "A company is migrating its on-premises applications to AWS and wants to set up a single sign-on (SSO) solution for all its applications across multiple AWS accounts. The company's applications are currently deployed on self-managed Microsoft Active Directory (AD) and the company wants to continue managing users and groups in its existing AD.\n\nWhich solution will best fulfill these requirements?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Enable AWS Single Sign-On (AWS SSO) in the master account. Use AWS Directory Service for Microsoft Active Directory to set up a two-way forest trust between the on-premises AD and the AWS SSO directory.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Set up AWS Directory Service AD Connector to create a trust relationship with the company's self-managed Microsoft Active Directory.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Enable AWS Single Sign-On (AWS SSO) in the master account. Use AWS Directory Service for Microsoft Active Directory to set up a one-way forest trust between the on-premises AD and the AWS SSO directory.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Deploy a third-party identity provider (IdP) on-premises and configure AWS SSO with the IdP in the master account.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nEnable AWS Single Sign-On (AWS SSO) in the master account. Use AWS Directory Service for Microsoft Active Directory to set up a two-way forest trust between the on-premises AD and the AWS SSO directory.\n\nA two-way trust is required for AWS Enterprise Apps such as Amazon Chime, Amazon Connect, Amazon QuickSight, AWS IAM Identity Center (successor to AWS Single Sign-On), Amazon WorkDocs, Amazon WorkMail, Amazon WorkSpaces, and the AWS Management Console. AWS Managed Microsoft AD must be able to query the users and groups in your self-managed AD.\n\nAmazon EC2, Amazon RDS, and Amazon FSx will work with either a one-way or two-way trust.\n\nThis solution allows the company to continue managing users and groups in its existing self-managed Microsoft Active Directory while enabling a single sign-on solution for all its applications across multiple AWS accounts. AWS SSO can be set up in the master account, and AWS Directory Service for Microsoft Active Directory can be used to set up a two-way forest trust between the on-premises AD and the AWS SSO directory. This trust enables AWS SSO to query the users and groups in the self-managed AD, providing seamless access to AWS applications and resources.\n\n\n\n\n\n\n\nIncorrect Options:\n\nEnable AWS Single Sign-On (AWS SSO) in the master account. Use AWS Directory Service for Microsoft Active Directory to set up a one-way forest trust between the on-premises AD and the AWS SSO directory.\n\nThis option is not suitable as a one-way forest trust only allows AWS SSO to read the user and group information from the on-premises AD, but does not allow changes to be made to the on-premises AD. This can limit the capabilities of AWS SSO and make user management more complex.\n\n\n\n\nSet up AWS Directory Service AD Connector to create a trust relationship with the company's self-managed Microsoft Active Directory.\n\nThis solution can work for some use cases, but it does not provide a single sign-on solution for all the company's applications across multiple AWS accounts. Additionally, it requires additional configuration and maintenance efforts for AD Connector.\n\n\n\n\nDeploy a third-party identity provider (IdP) on-premises and configure AWS SSO with the IdP in the master account.\n\nThis option can be more complex and may require additional licensing costs for the third-party IdP. Additionally, it may not align with the company's preference for managing users and groups in its existing AD.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_setup_trust.html",
      "correctAnswerExplanations": [
        {
          "answer": "Enable AWS Single Sign-On (AWS SSO) in the master account. Use AWS Directory Service for Microsoft Active Directory to set up a two-way forest trust between the on-premises AD and the AWS SSO directory.",
          "explanation": "A two-way trust is required for AWS Enterprise Apps such as Amazon Chime, Amazon Connect, Amazon QuickSight, AWS IAM Identity Center (successor to AWS Single Sign-On), Amazon WorkDocs, Amazon WorkMail, Amazon WorkSpaces, and the AWS Management Console. AWS Managed Microsoft AD must be able to query the users and groups in your self-managed AD."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Enable AWS Single Sign-On (AWS SSO) in the master account. Use AWS Directory Service for Microsoft Active Directory to set up a one-way forest trust between the on-premises AD and the AWS SSO directory.",
          "explanation": "This option is not suitable as a one-way forest trust only allows AWS SSO to read the user and group information from the on-premises AD, but does not allow changes to be made to the on-premises AD. This can limit the capabilities of AWS SSO and make user management more complex."
        },
        {
          "answer": "Set up AWS Directory Service AD Connector to create a trust relationship with the company's self-managed Microsoft Active Directory.",
          "explanation": "This solution can work for some use cases, but it does not provide a single sign-on solution for all the company's applications across multiple AWS accounts. Additionally, it requires additional configuration and maintenance efforts for AD Connector."
        },
        {
          "answer": "Deploy a third-party identity provider (IdP) on-premises and configure AWS SSO with the IdP in the master account.",
          "explanation": "This option can be more complex and may require additional licensing costs for the third-party IdP. Additionally, it may not align with the company's preference for managing users and groups in its existing AD."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_setup_trust.html"
      ]
    },
    {
      "id": 9,
      "question": "A company hosts a web application on AWS using two Amazon EC2 instances in separate Availability Zones, each with its own Amazon EBS volume. Both instances are placed behind an Application Load Balancer. Users report that when they refresh the website, they can only see a subset of their documents at a time, not all of them.\n\nWhat should a solutions architect propose to ensure users see all of their documents at once?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Configure Amazon RDS to store the documents and modify the application to use RDS.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Synchronize the data between both EBS volumes using Amazon S3.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Store the documents on Amazon EFS and modify the application to save new documents to Amazon EFS.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Configure Amazon EC2 Auto Scaling with lifecycle hooks to copy the documents between instances.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nStore the documents on Amazon EFS and modify the application to save new documents to Amazon EFS.\n\nAmazon EFS provides a scalable, elastic, and shared file storage solution that can be accessed concurrently by multiple EC2 instances. By storing the documents on Amazon EFS and modifying the application to save new documents there, all instances can access the same set of documents, allowing users to see all their documents at once.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSynchronize the data between both EBS volumes using Amazon S3.\n\nWhile Amazon S3 can be used for synchronization, it requires additional complexity and does not provide a real-time shared file storage solution that can be accessed concurrently by multiple EC2 instances.\n\n\n\n\nConfigure Amazon RDS to store the documents and modify the application to use RDS.\n\nAmazon RDS is a relational database service, not a file storage solution. It is not suitable for storing and serving user-uploaded documents directly.\n\n\n\n\nConfigure Amazon EC2 Auto Scaling with lifecycle hooks to copy the documents between instances.\n\nThis option adds unnecessary complexity and still does not provide a real-time shared file storage solution that can be accessed concurrently by multiple EC2 instances.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/efs/when-to-choose-efs",
      "correctAnswerExplanations": [
        {
          "answer": "Store the documents on Amazon EFS and modify the application to save new documents to Amazon EFS.",
          "explanation": "Amazon EFS provides a scalable, elastic, and shared file storage solution that can be accessed concurrently by multiple EC2 instances. By storing the documents on Amazon EFS and modifying the application to save new documents there, all instances can access the same set of documents, allowing users to see all their documents at once."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Synchronize the data between both EBS volumes using Amazon S3.",
          "explanation": "While Amazon S3 can be used for synchronization, it requires additional complexity and does not provide a real-time shared file storage solution that can be accessed concurrently by multiple EC2 instances."
        },
        {
          "answer": "Configure Amazon RDS to store the documents and modify the application to use RDS.",
          "explanation": "Amazon RDS is a relational database service, not a file storage solution. It is not suitable for storing and serving user-uploaded documents directly."
        },
        {
          "answer": "Configure Amazon EC2 Auto Scaling with lifecycle hooks to copy the documents between instances.",
          "explanation": "This option adds unnecessary complexity and still does not provide a real-time shared file storage solution that can be accessed concurrently by multiple EC2 instances."
        }
      ],
      "references": [
        "https://aws.amazon.com/efs/when-to-choose-efs"
      ]
    },
    {
      "id": 10,
      "question": "An organization is launching a public-facing web application on AWS. The web application will use an Application Load Balancer (ALB) to distribute traffic. The application requires encryption with an SSL/TLS certificate provided by an external certificate authority (CA). The certificate needs to be rotated annually before expiration.\n\nWhat approach should a solutions architect take to fulfill these requirements?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Import the SSL/TLS certificate into AWS Certificate Manager (ACM). Attach the certificate to the ALB. Configure an Amazon EventBridge (Amazon CloudWatch Events) rule to send a notification when the certificate is nearing expiration. Manually rotate the certificate.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Request an SSL/TLS certificate from AWS Certificate Manager (ACM). Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate from the root CA. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Request an SSL/TLS certificate from AWS Certificate Manager (ACM). Import the key material from the certificate. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nImport the SSL/TLS certificate into AWS Certificate Manager (ACM). Attach the certificate to the ALB. Configure an Amazon EventBridge (Amazon CloudWatch Events) rule to send a notification when the certificate is nearing expiration. Manually rotate the certificate.\n\nAWS Certificate Manager (ACM) can be used to import the SSL/TLS certificate issued by an external CA. The imported certificate can then be attached to the ALB. To ensure timely rotation of the certificate, an Amazon EventBridge (Amazon CloudWatch Events) rule can be set up to send a notification when the certificate is close to expiring. The certificate can then be manually rotated to maintain security.\n\n\n\n\n\n\n\nIncorrect Options:\n\nRequest an SSL/TLS certificate from AWS Certificate Manager (ACM). Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.\n\nThe requirement is to use an SSL/TLS certificate issued by an external CA, not one issued by ACM.\n\n\n\n\nUse AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate from the root CA. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.\n\nThe requirement is to use an SSL/TLS certificate provided by an external CA, not one issued by ACM Private Certificate Authority.\n\n\n\n\nRequest an SSL/TLS certificate from AWS Certificate Manager (ACM). Import the key material from the certificate. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.\n\nIt combines elements of requesting a certificate from ACM and importing a certificate from an external CA. The correct approach is to import the externally issued certificate into ACM, as specified in option A.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Rule.html",
      "correctAnswerExplanations": [
        {
          "answer": "Import the SSL/TLS certificate into AWS Certificate Manager (ACM). Attach the certificate to the ALB. Configure an Amazon EventBridge (Amazon CloudWatch Events) rule to send a notification when the certificate is nearing expiration. Manually rotate the certificate.",
          "explanation": "AWS Certificate Manager (ACM) can be used to import the SSL/TLS certificate issued by an external CA. The imported certificate can then be attached to the ALB. To ensure timely rotation of the certificate, an Amazon EventBridge (Amazon CloudWatch Events) rule can be set up to send a notification when the certificate is close to expiring. The certificate can then be manually rotated to maintain security."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Request an SSL/TLS certificate from AWS Certificate Manager (ACM). Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.",
          "explanation": "The requirement is to use an SSL/TLS certificate issued by an external CA, not one issued by ACM."
        },
        {
          "answer": "Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate from the root CA. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.",
          "explanation": "The requirement is to use an SSL/TLS certificate provided by an external CA, not one issued by ACM Private Certificate Authority."
        },
        {
          "answer": "Request an SSL/TLS certificate from AWS Certificate Manager (ACM). Import the key material from the certificate. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.",
          "explanation": "It combines elements of requesting a certificate from ACM and importing a certificate from an external CA. The correct approach is to import the externally issued certificate into ACM, as specified in option A."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html",
        "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Rule.html"
      ]
    },
    {
      "id": 11,
      "question": "A company operates an SMB file server in its data center, storing large files that are frequently accessed during the first few days after creation. After 7 days, these files are rarely accessed. As the total data size approaches the company's storage capacity, a solutions architect must increase storage space while maintaining low-latency access to recently accessed files and implementing file lifecycle management to prevent future storage issues.\n\nWhich solution meets these requirements?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Deploy an Amazon S3 File Gateway to extend the company's storage space and create an S3 Lifecycle policy to transition data to S3 Glacier Deep Archive after 7 days.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Deploy an Amazon FSx for Windows File Server file system to extend the company's storage space.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Install a utility on each user's computer to access Amazon S3 and create an S3 Lifecycle policy to transition data to S3 Glacier Flexible Retrieval after 7 days.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use AWS DataSync to copy data older than 7 days from the SMB file server to AWS.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nDeploy an Amazon S3 File Gateway to extend the company's storage space and create an S3 Lifecycle policy to transition data to S3 Glacier Deep Archive after 7 days.\n\nBy using Amazon S3 File Gateway, the company can extend its storage space while maintaining low-latency access to recently accessed files. The S3 Lifecycle policy will automatically transition data to S3 Glacier Deep Archive after 7 days, providing file lifecycle management and reducing storage costs.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS DataSync to copy data older than 7 days from the SMB file server to AWS.\n\nThis solution does not provide low-latency access to the most recently accessed files or file lifecycle management.\n\n\n\n\nDeploy an Amazon FSx for Windows File Server file system to extend the company's storage space.\n\nWhile Amazon FSx for Windows File Server provides additional storage, it does not provide file lifecycle management or automatically transition data to lower-cost storage tiers.\n\n\n\n\nInstall a utility on each user's computer to access Amazon S3 and create an S3 Lifecycle policy to transition data to S3 Glacier Flexible Retrieval after 7 days.\n\nThis solution does not provide low-latency access to the most recently accessed files and may not be efficient for managing large files across multiple users.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/filegateway/latest/files3/CreatingAnSMBFileShare.html",
      "correctAnswerExplanations": [
        {
          "answer": "Deploy an Amazon S3 File Gateway to extend the company's storage space and create an S3 Lifecycle policy to transition data to S3 Glacier Deep Archive after 7 days.",
          "explanation": "By using Amazon S3 File Gateway, the company can extend its storage space while maintaining low-latency access to recently accessed files. The S3 Lifecycle policy will automatically transition data to S3 Glacier Deep Archive after 7 days, providing file lifecycle management and reducing storage costs."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use AWS DataSync to copy data older than 7 days from the SMB file server to AWS.",
          "explanation": "This solution does not provide low-latency access to the most recently accessed files or file lifecycle management."
        },
        {
          "answer": "Deploy an Amazon FSx for Windows File Server file system to extend the company's storage space.",
          "explanation": "While Amazon FSx for Windows File Server provides additional storage, it does not provide file lifecycle management or automatically transition data to lower-cost storage tiers."
        },
        {
          "answer": "Install a utility on each user's computer to access Amazon S3 and create an S3 Lifecycle policy to transition data to S3 Glacier Flexible Retrieval after 7 days.",
          "explanation": "This solution does not provide low-latency access to the most recently accessed files and may not be efficient for managing large files across multiple users."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/filegateway/latest/files3/CreatingAnSMBFileShare.html"
      ]
    },
    {
      "id": 12,
      "question": "A company runs a mission-critical application on 500 Amazon EC2 Windows instances. The application relies on third-party software, and the company needs to update the third-party software on all EC2 instances urgently to address a high-priority security vulnerability.\n\nWhat should a solutions architect do to meet these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use AWS Systems Manager Run Command to execute a custom command that applies the patch to all EC2 instances.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use AWS Systems Manager Automation to create a document that applies the patch to all EC2 instances.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nUse AWS Systems Manager Run Command to execute a custom command that applies the patch to all EC2 instances.\n\nThis solution is the best choice for quickly applying a patch to all EC2 instances. AWS Systems Manager Run Command allows the execution of custom commands on multiple instances simultaneously, which helps to ensure rapid patch deployment across all instances. By using Run Command, the company can maintain control over the patch process and monitor the progress in real-time.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS Systems Manager Automation to create a document that applies the patch to all EC2 instances.\n\nAutomation is suitable for simplifying common maintenance and deployment tasks, but it is not the best choice for urgent patching needs. Run Command is more suitable for quickly applying a patch to all instances simultaneously.\n\n\n\n\nConfigure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.\n\nPatch Manager is designed for managing patches for operating systems and applications, but it might not be the best choice for an urgent patch deployment for third-party software. Run Command provides faster and more direct control over the patching process.\n\n\n\n\nSchedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.\n\nMaintenance windows are designed for performing routine maintenance tasks on a predefined schedule. In this scenario, the company needs to update the third-party software urgently, making Run Command a better option for quickly applying the patch.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use AWS Systems Manager Run Command to execute a custom command that applies the patch to all EC2 instances.",
          "explanation": "This solution is the best choice for quickly applying a patch to all EC2 instances. AWS Systems Manager Run Command allows the execution of custom commands on multiple instances simultaneously, which helps to ensure rapid patch deployment across all instances. By using Run Command, the company can maintain control over the patch process and monitor the progress in real-time."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use AWS Systems Manager Automation to create a document that applies the patch to all EC2 instances.",
          "explanation": "Automation is suitable for simplifying common maintenance and deployment tasks, but it is not the best choice for urgent patching needs. Run Command is more suitable for quickly applying a patch to all instances simultaneously."
        },
        {
          "answer": "Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.",
          "explanation": "Patch Manager is designed for managing patches for operating systems and applications, but it might not be the best choice for an urgent patch deployment for third-party software. Run Command provides faster and more direct control over the patching process."
        },
        {
          "answer": "Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.",
          "explanation": "Maintenance windows are designed for performing routine maintenance tasks on a predefined schedule. In this scenario, the company needs to update the third-party software urgently, making Run Command a better option for quickly applying the patch."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html"
      ]
    },
    {
      "id": 13,
      "question": "A multinational corporation has a web application deployed on Amazon EC2 instances, which are situated behind an Application Load Balancer (ALB). The web application contains both static and dynamic data, with the static data stored in an Amazon S3 bucket. The corporation aims to enhance performance and decrease latency for both types of data while using its domain name registered with Amazon Route 53.\n\nWhat should a solutions architect do to achieve these goals?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint. Configure Route 53 to route traffic to the CloudFront distribution.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint. Create two domain names. Point one domain name to the CloudFront DNS name for dynamic content. Point the other domain name to the accelerator DNS name for static content. Use the domain names as endpoints for the web application.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Create an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS Global Accelerator standard accelerator that has the ALB and the CloudFront distribution as endpoints. Create a custom domain name that points to the accelerator DNS name. Use the custom domain name as an endpoint for the web application.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Set up an Amazon CloudFront distribution with both the S3 bucket and the ALB as origins. Configure Route 53 to direct traffic to the CloudFront distribution.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nSet up an Amazon CloudFront distribution with both the S3 bucket and the ALB as origins. Configure Route 53 to direct traffic to the CloudFront distribution.\n\nBy creating an Amazon CloudFront distribution with both the S3 bucket (for static data) and the ALB (for dynamic data) as origins, performance will be enhanced by caching content at edge locations. Configuring Route 53 to route traffic to the CloudFront distribution ensures that both static and dynamic data will be served with reduced latency.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint. Configure Route 53 to route traffic to the CloudFront distribution.\n\nIt doesn't involve using the S3 bucket as an origin in the CloudFront distribution, which is necessary for improving the performance of static content delivery.\n\n\n\n\nCreate an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS Global Accelerator standard accelerator that has the ALB and the CloudFront distribution as endpoints. Create a custom domain name that points to the accelerator DNS name. Use the custom domain name as an endpoint for the web application.\n\nIt doesn't utilize the CloudFront distribution for both static and dynamic content. Additionally, the AWS Global Accelerator would add unnecessary complexity in this scenario.\n\n\n\n\nCreate an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint. Create two domain names. Point one domain name to the CloudFront DNS name for dynamic content. Point the other domain name to the accelerator DNS name for static content. Use the domain names as endpoints for the web application.\n\nIt doesn't use the S3 bucket as an origin in the CloudFront distribution, which is necessary for improving the performance of static content delivery. Also, the AWS Global Accelerator would add complexity without providing significant benefits.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
      "correctAnswerExplanations": [
        {
          "answer": "Set up an Amazon CloudFront distribution with both the S3 bucket and the ALB as origins. Configure Route 53 to direct traffic to the CloudFront distribution.",
          "explanation": "By creating an Amazon CloudFront distribution with both the S3 bucket (for static data) and the ALB (for dynamic data) as origins, performance will be enhanced by caching content at edge locations. Configuring Route 53 to route traffic to the CloudFront distribution ensures that both static and dynamic data will be served with reduced latency."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint. Configure Route 53 to route traffic to the CloudFront distribution.",
          "explanation": "It doesn't involve using the S3 bucket as an origin in the CloudFront distribution, which is necessary for improving the performance of static content delivery."
        },
        {
          "answer": "Create an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS Global Accelerator standard accelerator that has the ALB and the CloudFront distribution as endpoints. Create a custom domain name that points to the accelerator DNS name. Use the custom domain name as an endpoint for the web application.",
          "explanation": "It doesn't utilize the CloudFront distribution for both static and dynamic content. Additionally, the AWS Global Accelerator would add unnecessary complexity in this scenario."
        },
        {
          "answer": "Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint. Create two domain names. Point one domain name to the CloudFront DNS name for dynamic content. Point the other domain name to the accelerator DNS name for static content. Use the domain names as endpoints for the web application.",
          "explanation": "It doesn't use the S3 bucket as an origin in the CloudFront distribution, which is necessary for improving the performance of static content delivery. Also, the AWS Global Accelerator would add complexity without providing significant benefits."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html"
      ]
    },
    {
      "id": 14,
      "question": "A company wants to analyze its web server logs, which are stored in CSV format in an Amazon S3 bucket. The analysis will consist of simple, on-demand queries. A solutions architect needs to facilitate the analysis while making minimal modifications to the existing architecture.\n\nWhich solution should the solutions architect recommend to meet these requirements with the LEAST amount of operational overhead?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Load the log data into an Amazon Redshift cluster and run the SQL queries as needed.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use AWS Glue to catalog the logs and use a transient Apache Spark cluster on Amazon EMR to execute the SQL queries as needed.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Configure Amazon CloudWatch Logs to store the logs and execute SQL queries as needed from the Amazon CloudWatch console.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use Amazon Athena directly with Amazon S3 to execute the queries as needed.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nUse Amazon Athena directly with Amazon S3 to execute the queries as needed.\n\nAmazon Athena is a serverless, interactive query service that allows users to analyze data in Amazon S3 using standard SQL queries. Athena is an ideal choice for this scenario because it does not require any infrastructure setup or management, reducing operational overhead. The company can run simple, on-demand queries directly on the logs stored in Amazon S3 without the need for ETL processes or data transfers. This approach minimizes changes to the existing architecture while providing the desired analysis capabilities.\n\n\n\n\n\n\n\nIncorrect Options:\n\nLoad the log data into an Amazon Redshift cluster and run the SQL queries as needed.\n\nIt involves additional operational overhead, such as managing and maintaining a Redshift cluster.\n\n\n\n\nConfigure Amazon CloudWatch Logs to store the logs and execute SQL queries as needed from the Amazon CloudWatch console.\n\nIt requires migrating the logs from S3 to CloudWatch Logs, which adds unnecessary complexity and operational overhead.\n\n\n\n\nUse AWS Glue to catalog the logs and use a transient Apache Spark cluster on Amazon EMR to execute the SQL queries as needed.\n\nIt introduces additional operational overhead by requiring the management of an EMR cluster and the use of AWS Glue for cataloging.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/athena/latest/ug/what-is.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use Amazon Athena directly with Amazon S3 to execute the queries as needed.",
          "explanation": "Amazon Athena is a serverless, interactive query service that allows users to analyze data in Amazon S3 using standard SQL queries. Athena is an ideal choice for this scenario because it does not require any infrastructure setup or management, reducing operational overhead. The company can run simple, on-demand queries directly on the logs stored in Amazon S3 without the need for ETL processes or data transfers. This approach minimizes changes to the existing architecture while providing the desired analysis capabilities."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Load the log data into an Amazon Redshift cluster and run the SQL queries as needed.",
          "explanation": "It involves additional operational overhead, such as managing and maintaining a Redshift cluster."
        },
        {
          "answer": "Configure Amazon CloudWatch Logs to store the logs and execute SQL queries as needed from the Amazon CloudWatch console.",
          "explanation": "It requires migrating the logs from S3 to CloudWatch Logs, which adds unnecessary complexity and operational overhead."
        },
        {
          "answer": "Use AWS Glue to catalog the logs and use a transient Apache Spark cluster on Amazon EMR to execute the SQL queries as needed.",
          "explanation": "It introduces additional operational overhead by requiring the management of an EMR cluster and the use of AWS Glue for cataloging."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/athena/latest/ug/what-is.html"
      ]
    },
    {
      "id": 15,
      "question": "A company needs to store its legal documents in Amazon S3. The documents must be readily available for 3 years and then must be archived for an additional 7 years. During the entire 10-year period, no one, including administrative users and root users, should be able to delete the documents. The documents must be stored with the highest durability possible.\n\nWhich solution will meet these requirements?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use an S3 Lifecycle policy to transition the documents from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 3 years. Use S3 Object Lock in governance mode for a period of 10 years.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Store the documents in S3 Intelligent-Tiering for the entire 10-year period. Use an access control policy to deny deletion of the documents for a period of 10 years.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Store the documents by using S3 One Zone-Infrequent Access. Use an IAM policy to deny deletion of the documents. After 10 years, change the IAM policy to allow deletion.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use an S3 Lifecycle policy to transition the documents from S3 Standard to S3 Glacier Deep Archive after 3 years. Use S3 Object Lock in compliance mode for a period of 10 years.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nUse an S3 Lifecycle policy to transition the documents from S3 Standard to S3 Glacier Deep Archive after 3 years. Use S3 Object Lock in compliance mode for a period of 10 years.\n\nThis ensures that the documents are readily available for 3 years, and then archives them for the remaining 7 years. By using S3 Object Lock in compliance mode, the documents cannot be deleted, even by administrative or root users, for the entire 10-year period.\n\nThe S3 Lifecycle policy ensures that the objects are automatically transitioned to a lower-cost storage class after three years of use, which provides cost optimization to the company. S3 Glacier Deep Archive provides the highest durability of any Amazon S3 storage class. Once the documents are transitioned to Glacier Deep Archive, they can be stored for long-term archive retention at a lower cost. S3 Object Lock in compliance mode ensures that the documents cannot be deleted or overwritten by any user, including administrative and root users, for the retention period. This is because the compliance mode provides a WORM (Write Once Read Many) storage feature that prohibits any modification or deletion of the objects stored.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStore the documents in S3 Intelligent-Tiering for the entire 10-year period. Use an access control policy to deny deletion of the documents for a period of 10 years.\n\nWhile this option prevents deletion for the specified period, it does not move the documents to a more cost-effective storage class for long-term archiving.\n\n\n\n\nStore the documents by using S3 One Zone-Infrequent Access. Use an IAM policy to deny deletion of the documents. After 10 years, change the IAM policy to allow deletion.\n\nThis doesn't offer the highest durability possible since S3 One Zone-IA stores data in a single availability zone, which increases the risk of data loss.\n\n\n\n\nUse an S3 Lifecycle policy to transition the documents from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 3 years. Use S3 Object Lock in governance mode for a period of 10 years.\n\nWhile this option utilizes S3 Object Lock, it moves the documents to S3 One Zone-IA, which doesn't offer the highest durability possible due to its single availability zone storage.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use an S3 Lifecycle policy to transition the documents from S3 Standard to S3 Glacier Deep Archive after 3 years. Use S3 Object Lock in compliance mode for a period of 10 years.",
          "explanation": "This ensures that the documents are readily available for 3 years, and then archives them for the remaining 7 years. By using S3 Object Lock in compliance mode, the documents cannot be deleted, even by administrative or root users, for the entire 10-year period.<br><br>The S3 Lifecycle policy ensures that the objects are automatically transitioned to a lower-cost storage class after three years of use, which provides cost optimization to the company. S3 Glacier Deep Archive provides the highest durability of any Amazon S3 storage class. Once the documents are transitioned to Glacier Deep Archive, they can be stored for long-term archive retention at a lower cost. S3 Object Lock in compliance mode ensures that the documents cannot be deleted or overwritten by any user, including administrative and root users, for the retention period. This is because the compliance mode provides a WORM (Write Once Read Many) storage feature that prohibits any modification or deletion of the objects stored."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Store the documents in S3 Intelligent-Tiering for the entire 10-year period. Use an access control policy to deny deletion of the documents for a period of 10 years.",
          "explanation": "While this option prevents deletion for the specified period, it does not move the documents to a more cost-effective storage class for long-term archiving."
        },
        {
          "answer": "Store the documents by using S3 One Zone-Infrequent Access. Use an IAM policy to deny deletion of the documents. After 10 years, change the IAM policy to allow deletion.",
          "explanation": "This doesn't offer the highest durability possible since S3 One Zone-IA stores data in a single availability zone, which increases the risk of data loss."
        },
        {
          "answer": "Use an S3 Lifecycle policy to transition the documents from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 3 years. Use S3 Object Lock in governance mode for a period of 10 years.",
          "explanation": "While this option utilizes S3 Object Lock, it moves the documents to S3 One Zone-IA, which doesn't offer the highest durability possible due to its single availability zone storage."
        }
      ],
      "references": [
        "https://aws.amazon.com/s3/storage-classes/",
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html"
      ]
    },
    {
      "id": 16,
      "question": "A company is developing an application that provides sales performance metrics for retrieval via a REST API. The company wants to extract the sales performance metrics, present the data in a visually appealing PDF format, and distribute the report to multiple email recipients every week.\n\nWhich combination of steps should a solutions architect take to meet these requirements? (Select TWO.)",
      "corrects": [
        1,
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Configure the application to stream the data to Amazon Kinesis Data Firehose.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that triggers an AWS Lambda function to query the application's API for the data.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email.",
          "correct": false
        },
        {
          "id": 5,
          "answer": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.",
          "correct": false
        }
      ],
      "multiple": true,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nCreate an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that triggers an AWS Lambda function to query the application's API for the data.\n\nUse Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.\n\nThe company should use Amazon EventBridge to schedule a Lambda function that queries the REST API for the sales performance metrics. The Lambda function can then transform the data into a visually appealing PDF format. Finally, Amazon SES can be used to send the formatted report to multiple recipients via email.\n\nAmazon SES is an email sending service that allows you to send emails from your applications. It is used to format the data and send the report to multiple email recipients. Amazon SES provides an API to send emails and can also be used to receive emails.\n\nAmazon EventBridge (Amazon CloudWatch Events) is a serverless event bus service that allows you to route events between different AWS services. It can be used to trigger an AWS Lambda function to query the application's REST API for the sales performance data.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure the application to stream the data to Amazon Kinesis Data Firehose.\n\nKinesis Data Firehose is designed for real-time streaming data analysis and is not the best choice for this use case, which involves querying data via a REST API and sending weekly reports.\n\n\n\n\nCreate an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.\n\nAWS Glue is a serverless data integration service designed for ETL jobs, not for querying REST APIs and generating reports. Lambda is a better fit for this requirement.\n\n\n\n\nStore the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email.\n\nAmazon SNS is used for messaging and notifications, not for querying REST APIs, formatting data, or generating reports. This option does not address the company's requirements.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html\n\nhttps://docs.aws.amazon.com/ses/latest/dg/send-email-formatted.html",
      "correctAnswerExplanations": [
        {
          "answer": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that triggers an AWS Lambda function to query the application's API for the data.",
          "explanation": "<strong>Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.</strong>"
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Configure the application to stream the data to Amazon Kinesis Data Firehose.",
          "explanation": "Kinesis Data Firehose is designed for real-time streaming data analysis and is not the best choice for this use case, which involves querying data via a REST API and sending weekly reports."
        },
        {
          "answer": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.",
          "explanation": "AWS Glue is a serverless data integration service designed for ETL jobs, not for querying REST APIs and generating reports. Lambda is a better fit for this requirement."
        },
        {
          "answer": "Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email.",
          "explanation": "Amazon SNS is used for messaging and notifications, not for querying REST APIs, formatting data, or generating reports. This option does not address the company's requirements."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html",
        "https://docs.aws.amazon.com/ses/latest/dg/send-email-formatted.html"
      ]
    },
    {
      "id": 17,
      "question": "A startup is planning to build a web application that relies on microservices. The company wants to deploy these microservices in containers for better scalability and availability, while avoiding the responsibility of managing the underlying infrastructure. They prefer to focus on developing and maintaining the application itself.\n\nWhat should a solutions architect recommend to meet these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Deploy Amazon EC2 instances and manually install Docker to manage the containers.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use Amazon Elastic Kubernetes Service (EKS) with EC2 worker nodes.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use Amazon Elastic Container Service (ECS) with AWS Fargate.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Use AWS Lambda with container image support.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nUse Amazon Elastic Container Service (ECS) with AWS Fargate.\n\nAmazon ECS is a fully managed container orchestration service that enables the startup to run and scale containerized applications quickly and easily. AWS Fargate is a serverless compute engine for containers that manages the infrastructure underlying the containers, such as EC2 instances and the operating system, so the startup can focus solely on building and deploying their application. By using ECS with Fargate, the startup does not need to manage the underlying infrastructure, such as provisioning or patching servers, which reduces the operational overhead and enables them to focus more on the application. ECS with Fargate also provides automatic scaling and load balancing capabilities, so the application can handle variable traffic loads without manual intervention.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS Lambda with container image support.\n\nAWS Lambda is a serverless computing service, not a container orchestration platform. While Lambda now supports container images, it is not the best fit for running containerized microservices that require fine-grained control and scaling.\n\n\n\n\nUse Amazon Elastic Kubernetes Service (EKS) with EC2 worker nodes.\n\nIt involves managing the underlying EC2 instances that act as worker nodes. The company prefers not to be responsible for provisioning and managing the underlying infrastructure.\n\n\n\n\nDeploy Amazon EC2 instances and manually install Docker to manage the containers.\n\nIt requires the company to manage the underlying EC2 instances and manually install Docker, which does not align with the company's preference to avoid infrastructure management responsibilities.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/ecs/features/\n\nhttps://aws.amazon.com/fargate/",
      "correctAnswerExplanations": [
        {
          "answer": "Use Amazon Elastic Container Service (ECS) with AWS Fargate.",
          "explanation": "Amazon ECS is a fully managed container orchestration service that enables the startup to run and scale containerized applications quickly and easily. AWS Fargate is a serverless compute engine for containers that manages the infrastructure underlying the containers, such as EC2 instances and the operating system, so the startup can focus solely on building and deploying their application. By using ECS with Fargate, the startup does not need to manage the underlying infrastructure, such as provisioning or patching servers, which reduces the operational overhead and enables them to focus more on the application. ECS with Fargate also provides automatic scaling and load balancing capabilities, so the application can handle variable traffic loads without manual intervention."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use AWS Lambda with container image support.",
          "explanation": "AWS Lambda is a serverless computing service, not a container orchestration platform. While Lambda now supports container images, it is not the best fit for running containerized microservices that require fine-grained control and scaling."
        },
        {
          "answer": "Use Amazon Elastic Kubernetes Service (EKS) with EC2 worker nodes.",
          "explanation": "It involves managing the underlying EC2 instances that act as worker nodes. The company prefers not to be responsible for provisioning and managing the underlying infrastructure."
        },
        {
          "answer": "Deploy Amazon EC2 instances and manually install Docker to manage the containers.",
          "explanation": "It requires the company to manage the underlying EC2 instances and manually install Docker, which does not align with the company's preference to avoid infrastructure management responsibilities."
        }
      ],
      "references": [
        "https://aws.amazon.com/ecs/features/",
        "https://aws.amazon.com/fargate/"
      ]
    },
    {
      "id": 18,
      "question": "A media company has a web application that runs on Amazon EC2 instances and uses Amazon RDS for PostgreSQL as its database. The EC2 instances access the database using credentials stored in a local configuration file. The company wants to reduce the maintenance burden of managing these credentials.\n\nWhat should a solutions architect recommend to achieve this objective?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use AWS Systems Manager Parameter Store and enable automatic rotation.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Store the credentials in an Amazon S3 bucket with AWS KMS encryption and update the application to access the S3 bucket.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use AWS Secrets Manager and enable automatic rotation.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Create encrypted Amazon EBS volumes for each EC2 instance, move the credentials to these volumes, and update the application accordingly.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nUse AWS Secrets Manager and enable automatic rotation.\n\nAWS Secrets Manager is a service designed for managing secrets, such as database credentials. By using Secrets Manager, the company can store, retrieve, and rotate the credentials securely. Automatic rotation of credentials reduces operational overhead and improves security by regularly updating the database passwords. This is the most appropriate solution for managing and securing credentials in this scenario.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS Systems Manager Parameter Store and enable automatic rotation.\n\nParameter Store is useful for storing configuration data and plain-text secrets, but it does not provide automatic rotation of secrets. This makes it less suitable for this scenario where minimizing operational overhead is the goal.\n\n\n\n\nStore the credentials in an Amazon S3 bucket with AWS KMS encryption and update the application to access the S3 bucket.\n\nWhile this option allows storing and retrieving credentials securely, it does not address the operational overhead associated with rotating and managing credentials. Additionally, it introduces the complexity of managing access to the S3 bucket.\n\n\n\n\nCreate encrypted Amazon EBS volumes for each EC2 instance, move the credentials to these volumes, and update the application accordingly.\n\nAlthough this option encrypts the credentials at rest, it does not help with managing and rotating credentials, which is the main objective. It also adds the complexity of managing EBS volumes for each EC2 instance.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/secrets-manager",
      "correctAnswerExplanations": [
        {
          "answer": "Use AWS Secrets Manager and enable automatic rotation.",
          "explanation": "AWS Secrets Manager is a service designed for managing secrets, such as database credentials. By using Secrets Manager, the company can store, retrieve, and rotate the credentials securely. Automatic rotation of credentials reduces operational overhead and improves security by regularly updating the database passwords. This is the most appropriate solution for managing and securing credentials in this scenario."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use AWS Systems Manager Parameter Store and enable automatic rotation.",
          "explanation": "Parameter Store is useful for storing configuration data and plain-text secrets, but it does not provide automatic rotation of secrets. This makes it less suitable for this scenario where minimizing operational overhead is the goal."
        },
        {
          "answer": "Store the credentials in an Amazon S3 bucket with AWS KMS encryption and update the application to access the S3 bucket.",
          "explanation": "While this option allows storing and retrieving credentials securely, it does not address the operational overhead associated with rotating and managing credentials. Additionally, it introduces the complexity of managing access to the S3 bucket."
        },
        {
          "answer": "Create encrypted Amazon EBS volumes for each EC2 instance, move the credentials to these volumes, and update the application accordingly.",
          "explanation": "Although this option encrypts the credentials at rest, it does not help with managing and rotating credentials, which is the main objective. It also adds the complexity of managing EBS volumes for each EC2 instance."
        }
      ],
      "references": [
        "https://aws.amazon.com/secrets-manager"
      ]
    },
    {
      "id": 19,
      "question": "A company wants to enhance its ability to copy large volumes of production data to a development environment within the same AWS Region. The data is stored on Amazon Elastic Block Store (Amazon EBS) volumes attached to Amazon EC2 instances. Changes to the copied data must not impact the production environment. The applications that access this data require consistently high I/O performance.\n\nA solutions architect needs to minimize the time required to copy the production data to the development environment.\n\nWhich solution will meet these requirements?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the development environment before restoring the volumes from the production EBS snapshots.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the development environment.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use the EBS Multi-Attach feature for the production EBS volumes. Create EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the development environment.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Create Amazon EBS snapshots of the production EBS volumes. Enable EBS fast snapshot restore for the snapshots. Restore the snapshots to new EBS volumes. Attach the new EBS volumes to EC2 instances in the development environment.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nCreate Amazon EBS snapshots of the production EBS volumes. Enable EBS fast snapshot restore for the snapshots. Restore the snapshots to new EBS volumes. Attach the new EBS volumes to EC2 instances in the development environment.\n\nThis solution leverages EBS fast snapshot restore, which helps reduce the time needed to copy production data to the development environment. Fast snapshot restore enables immediate and consistent high I/O performance for EBS volumes created from EBS snapshots. This meets the requirement of the applications that access the data.\n\n\n\n\n\n\n\nIncorrect Options:\n\nTake EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the development environment.\n\nEC2 instance store volumes do not support EBS snapshots, making this option invalid.\n\n\n\n\nUse the EBS Multi-Attach feature for the production EBS volumes. Create EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the development environment.\n\nThe EBS Multi-Attach feature allows multiple EC2 instances to access the same EBS volume concurrently, which can lead to data corruption if not managed properly. Additionally, modifications to the data in the development environment would affect the production environment.\n\n\n\n\nTake EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the development environment before restoring the volumes from the production EBS snapshots.\n\nThis option is not optimal because it does not utilize EBS fast snapshot restore, resulting in increased time to copy the data and potentially inconsistent I/O performance.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html",
      "correctAnswerExplanations": [
        {
          "answer": "Create Amazon EBS snapshots of the production EBS volumes. Enable EBS fast snapshot restore for the snapshots. Restore the snapshots to new EBS volumes. Attach the new EBS volumes to EC2 instances in the development environment.",
          "explanation": "This solution leverages EBS fast snapshot restore, which helps reduce the time needed to copy production data to the development environment. Fast snapshot restore enables immediate and consistent high I/O performance for EBS volumes created from EBS snapshots. This meets the requirement of the applications that access the data."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the development environment.",
          "explanation": "EC2 instance store volumes do not support EBS snapshots, making this option invalid."
        },
        {
          "answer": "Use the EBS Multi-Attach feature for the production EBS volumes. Create EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the development environment.",
          "explanation": "The EBS Multi-Attach feature allows multiple EC2 instances to access the same EBS volume concurrently, which can lead to data corruption if not managed properly. Additionally, modifications to the data in the development environment would affect the production environment."
        },
        {
          "answer": "Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the development environment before restoring the volumes from the production EBS snapshots.",
          "explanation": "This option is not optimal because it does not utilize EBS fast snapshot restore, resulting in increased time to copy the data and potentially inconsistent I/O performance."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html"
      ]
    },
    {
      "id": 20,
      "question": "A company has a data processing workflow. Amazon Simple Notification Service (Amazon SNS) is used to notify about new data arrivals, and an AWS Lambda function is used to analyze the data and store the results.\n\nThe company experiences occasional failures in the processing workflow due to network connectivity issues. When a failure occurs, the Lambda function does not process the associated data unless the company manually restarts the job.\n\nWhat combination of steps should a solutions architect take to ensure that the Lambda function processes all data in the future? (Select TWO.)",
      "corrects": [
        1,
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Configure the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Increase the memory and CPU allocated to the Lambda function.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Implement an Amazon Simple Queue Service (Amazon SQS) queue and subscribe it to the SNS topic.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Increase the provisioned concurrency for the Lambda function.",
          "correct": false
        },
        {
          "id": 5,
          "answer": "Deploy the Lambda function across multiple Availability Zones.",
          "correct": false
        }
      ],
      "multiple": true,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nImplement an Amazon Simple Queue Service (Amazon SQS) queue and subscribe it to the SNS topic.\n\nBy creating an Amazon SQS queue and subscribing it to the SNS topic, messages will be stored in the queue when there are network connectivity issues. This ensures that the messages are not lost and can be processed by the Lambda function when the connectivity issue is resolved.\n\n\n\n\nConfigure the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.\n\nModifying the Lambda function to read from the Amazon SQS queue allows the function to process messages in the queue. This ensures that all data is processed, even in cases of network connectivity issues, as the messages are stored in the queue and can be retried.\n\n\n\n\n\n\n\nIncorrect Options:\n\nIncrease the memory and CPU allocated to the Lambda function.\n\nWhile increasing memory and CPU can improve the performance of the Lambda function, it does not specifically address the issue of network connectivity failures.\n\n\n\n\nIncrease the provisioned concurrency for the Lambda function.\n\nIncreasing provisioned concurrency can help reduce the cold start time for the Lambda function but does not directly address the network connectivity issue that causes processing failures.\n\n\n\n\nDeploy the Lambda function across multiple Availability Zones.\n\nDeploying the Lambda function across multiple Availability Zones can improve its availability but does not specifically address the issue of network connectivity failures.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
      "correctAnswerExplanations": [
        {
          "answer": "Implement an Amazon Simple Queue Service (Amazon SQS) queue and subscribe it to the SNS topic.",
          "explanation": "By creating an Amazon SQS queue and subscribing it to the SNS topic, messages will be stored in the queue when there are network connectivity issues. This ensures that the messages are not lost and can be processed by the Lambda function when the connectivity issue is resolved."
        },
        {
          "answer": "Configure the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.",
          "explanation": "Modifying the Lambda function to read from the Amazon SQS queue allows the function to process messages in the queue. This ensures that all data is processed, even in cases of network connectivity issues, as the messages are stored in the queue and can be retried."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Increase the memory and CPU allocated to the Lambda function.",
          "explanation": "While increasing memory and CPU can improve the performance of the Lambda function, it does not specifically address the issue of network connectivity failures."
        },
        {
          "answer": "Increase the provisioned concurrency for the Lambda function.",
          "explanation": "Increasing provisioned concurrency can help reduce the cold start time for the Lambda function but does not directly address the network connectivity issue that causes processing failures."
        },
        {
          "answer": "Deploy the Lambda function across multiple Availability Zones.",
          "explanation": "Deploying the Lambda function across multiple Availability Zones can improve its availability but does not specifically address the issue of network connectivity failures."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html",
        "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"
      ]
    },
    {
      "id": 21,
      "question": "A healthcare organization has a data lake on AWS, consisting of data stored in Amazon S3 and Amazon Aurora MySQL. The organization requires a data visualization solution that incorporates all data sources within the data lake. The management team should have full access to all visualizations, while the rest of the organization should have limited access.\n\nWhich solution will satisfy these requirements?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Create an analysis in Amazon QuickSight, connecting all data sources and creating new datasets. Publish dashboards for data visualization, and share them with appropriate users and groups.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Create an analysis in Amazon QuickSight, connecting all data sources and creating new datasets. Publish dashboards for data visualization, and share them with relevant IAM roles.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Create an AWS Glue table and crawler for the data in Amazon S3. Utilize Amazon Athena Federated Query to access data within Amazon Aurora MySQL. Generate reports using Amazon Athena, publish them to Amazon S3, and apply S3 bucket policies to limit access to the reports.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Create an AWS Glue table and crawler for the data in Amazon S3. Design an AWS Glue ETL job to generate reports, and publish them to Amazon S3. Use S3 bucket policies to restrict access to the reports.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nCreate an analysis in Amazon QuickSight, connecting all data sources and creating new datasets. Publish dashboards for data visualization, and share them with appropriate users and groups.\n\nAmazon QuickSight is a fully managed business intelligence service that provides quick and easy-to-use data visualization capabilities. By connecting all data sources from the data lake, you can create new datasets and develop insightful analyses. After publishing the dashboards, you can share them with the management team and other users in the organization based on their user or group permissions. This approach ensures that the management team has full access to all visualizations, while the rest of the organization has limited access, meeting the healthcare organization's requirements.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an analysis in Amazon QuickSight, connecting all data sources and creating new datasets. Publish dashboards for data visualization, and share them with relevant IAM roles.\n\nAmazon QuickSight uses its native user and group-based permission system, not IAM roles, to share dashboards and control access. Therefore, this option does not meet the organization's requirements.\n\n\n\n\nCreate an AWS Glue table and crawler for the data in Amazon S3. Design an AWS Glue ETL job to generate reports, and publish them to Amazon S3. Use S3 bucket policies to restrict access to the reports.\n\nWhile this option provides a way to create reports and control access using S3 bucket policies, it does not offer a data visualization solution as required by the organization.\n\n\n\n\nCreate an AWS Glue table and crawler for the data in Amazon S3. Utilize Amazon Athena Federated Query to access data within Amazon Aurora MySQL. Generate reports using Amazon Athena, publish them to Amazon S3, and apply S3 bucket policies to limit access to the reports.\n\nWhile this option provides a way to create reports and control access using S3 bucket policies. However, it does not provide a data visualization solution as required by the organization.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/share-a-dashboard-grant-access-users.html",
      "correctAnswerExplanations": [
        {
          "answer": "Create an analysis in Amazon QuickSight, connecting all data sources and creating new datasets. Publish dashboards for data visualization, and share them with appropriate users and groups.",
          "explanation": "Amazon QuickSight is a fully managed business intelligence service that provides quick and easy-to-use data visualization capabilities. By connecting all data sources from the data lake, you can create new datasets and develop insightful analyses. After publishing the dashboards, you can share them with the management team and other users in the organization based on their user or group permissions. This approach ensures that the management team has full access to all visualizations, while the rest of the organization has limited access, meeting the healthcare organization's requirements."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Create an analysis in Amazon QuickSight, connecting all data sources and creating new datasets. Publish dashboards for data visualization, and share them with relevant IAM roles.",
          "explanation": "Amazon QuickSight uses its native user and group-based permission system, not IAM roles, to share dashboards and control access. Therefore, this option does not meet the organization's requirements."
        },
        {
          "answer": "Create an AWS Glue table and crawler for the data in Amazon S3. Design an AWS Glue ETL job to generate reports, and publish them to Amazon S3. Use S3 bucket policies to restrict access to the reports.",
          "explanation": "While this option provides a way to create reports and control access using S3 bucket policies, it does not offer a data visualization solution as required by the organization."
        },
        {
          "answer": "Create an AWS Glue table and crawler for the data in Amazon S3. Utilize Amazon Athena Federated Query to access data within Amazon Aurora MySQL. Generate reports using Amazon Athena, publish them to Amazon S3, and apply S3 bucket policies to limit access to the reports.",
          "explanation": "While this option provides a way to create reports and control access using S3 bucket policies. However, it does not provide a data visualization solution as required by the organization."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/quicksight/latest/user/share-a-dashboard-grant-access-users.html"
      ]
    },
    {
      "id": 22,
      "question": "A company wants to continuously monitor its AWS infrastructure to detect and react to unauthorized changes in Amazon EC2 security groups.\n\nWhat should a solutions architect implement to achieve this objective?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Enable AWS Config with the appropriate managed rules.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Enable Amazon CloudTrail logs and configure Amazon EventBridge (Amazon CloudWatch Events) for monitoring.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Enable Amazon GuardDuty with the suitable threat detection settings.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Enable AWS Trusted Advisor with the relevant security checks.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nEnable AWS Config with the appropriate managed rules.\n\nAWS Config is a fully managed service that provides a detailed inventory of AWS resources and their current configurations. By enabling AWS Config and configuring the appropriate managed rules, they can continuously monitor the infrastructure and detect unauthorized changes to Amazon EC2 security groups. AWS Config can send notifications via Amazon SNS or AWS Lambda, allowing the company to quickly react and remediate any detected issues.\n\nAWS Config also provides a history of resource changes, which can help the company identify the root cause of any security issues and prevent similar issues in the future.\n\n\n\n\n\n\n\nIncorrect Options:\n\nEnable AWS Trusted Advisor with the relevant security checks.\n\nTrusted Advisor provides best practice recommendations but does not offer continuous monitoring or detection of unauthorized changes in security groups.\n\n\n\n\nEnable Amazon GuardDuty with the suitable threat detection settings.\n\nAmazon GuardDuty is a threat detection service that monitors for malicious activity, not for unauthorized configuration changes in security groups.\n\n\n\n\nEnable Amazon CloudTrail logs and configure Amazon EventBridge (Amazon CloudWatch Events) for monitoring.\n\nCloudTrail logs API calls, while EventBridge monitors events in real-time. However, this combination does not provide the continuous monitoring and detection of unauthorized security group changes like AWS Config does.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html",
      "correctAnswerExplanations": [
        {
          "answer": "Enable AWS Config with the appropriate managed rules.",
          "explanation": "AWS Config is a fully managed service that provides a detailed inventory of AWS resources and their current configurations. By enabling AWS Config and configuring the appropriate managed rules, they can continuously monitor the infrastructure and detect unauthorized changes to Amazon EC2 security groups. AWS Config can send notifications via Amazon SNS or AWS Lambda, allowing the company to quickly react and remediate any detected issues."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Enable AWS Trusted Advisor with the relevant security checks.",
          "explanation": "Trusted Advisor provides best practice recommendations but does not offer continuous monitoring or detection of unauthorized changes in security groups."
        },
        {
          "answer": "Enable Amazon GuardDuty with the suitable threat detection settings.",
          "explanation": "Amazon GuardDuty is a threat detection service that monitors for malicious activity, not for unauthorized configuration changes in security groups."
        },
        {
          "answer": "Enable Amazon CloudTrail logs and configure Amazon EventBridge (Amazon CloudWatch Events) for monitoring.",
          "explanation": "CloudTrail logs API calls, while EventBridge monitors events in real-time. However, this combination does not provide the continuous monitoring and detection of unauthorized security group changes like AWS Config does."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html"
      ]
    },
    {
      "id": 23,
      "question": "A data analysis team performs resource-intensive computations on their Amazon RDS for PostgreSQL DB instance with Performance Insights enabled. The computations take 72 hours and occur quarterly. No other processes utilize the database during this time. The team aims to reduce the cost of running these computations without diminishing the compute and memory attributes of the DB instance.\n\nWhich solution MOST cost-effectively meets these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Implement an Auto Scaling policy with the DB instance to automatically scale upon completion of computations.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Stop the DB instance upon completing computations. Restart the DB instance when needed.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Create a snapshot upon completing computations. Terminate the DB instance and restore the snapshot when needed.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Modify the DB instance to a lower-capacity instance upon completing computations. Modify the DB instance again when needed.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nCreate a snapshot upon completing computations. Terminate the DB instance and restore the snapshot when needed.\n\nThis solution reduces the cost of running computations without diminishing the compute and memory attributes of the DB instance. By creating a snapshot upon completing computations and terminating the DB instance, the team can avoid paying for the resources that are not in use during the remaining period. This allows the team to significantly reduce the cost of running the computations.\n\nWhen the computations are needed again, the team can simply restore the snapshot and resume using the DB instance as usual. This approach ensures that the compute and memory attributes of the DB instance are not compromised in any way, providing the team with the necessary resources to perform the resource-intensive computations.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStop the DB instance upon completing computations. Restart the DB instance when needed.\n\nIn our case the test will perform monthly basis, and you can stop a DB instance for up to seven days. If you don't manually start your DB instance after seven days, your DB instance is automatically started. So, if it run a test once a month, creating a snapshot is more appropriate and cost-effective way.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html\n\n\n\n\nImplement an Auto Scaling policy with the DB instance to automatically scale upon completion of computations.\n\nAuto Scaling is not available for Amazon RDS instances, making this option invalid.\n\n\n\n\nModify the DB instance to a lower-capacity instance upon completing computations. Modify the DB instance again when needed.\n\nModifying the DB instance to a lower-capacity version contradicts the requirement to maintain the compute and memory attributes, making this option unsuitable.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html",
      "correctAnswerExplanations": [
        {
          "answer": "Create a snapshot upon completing computations. Terminate the DB instance and restore the snapshot when needed.",
          "explanation": "This solution reduces the cost of running computations without diminishing the compute and memory attributes of the DB instance. By creating a snapshot upon completing computations and terminating the DB instance, the team can avoid paying for the resources that are not in use during the remaining period. This allows the team to significantly reduce the cost of running the computations."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Stop the DB instance upon completing computations. Restart the DB instance when needed.",
          "explanation": "In our case the test will perform monthly basis, and you can stop a DB instance for up to seven days. If you don't manually start your DB instance after seven days, your DB instance is automatically started. So, if it run a test once a month, creating a snapshot is more appropriate and cost-effective way."
        },
        {
          "answer": "Implement an Auto Scaling policy with the DB instance to automatically scale upon completion of computations.",
          "explanation": "Auto Scaling is not available for Amazon RDS instances, making this option invalid."
        },
        {
          "answer": "Modify the DB instance to a lower-capacity instance upon completing computations. Modify the DB instance again when needed.",
          "explanation": "Modifying the DB instance to a lower-capacity version contradicts the requirement to maintain the compute and memory attributes, making this option unsuitable."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html",
        "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html"
      ]
    },
    {
      "id": 24,
      "question": "A startup company is running several applications on Amazon EC2 instances in its AWS account. The company needs a secure and scalable solution to manage and maintain these instances remotely. The solution should be easy to implement, align with the AWS Well-Architected Framework, and minimize operational overhead.\n\nWhich solution will meet these requirements with the LEAST operational effort?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Set up an AWS Site-to-Site VPN connection and instruct administrators to use their local on-premises machines to connect directly to the instances using SSH keys across the VPN tunnel.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Access each instance's terminal interface directly through the EC2 serial console for administration purposes.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Attach the required IAM roles to existing and new instances. Use AWS Systems Manager Session Manager to establish a remote SSH session.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Configure a bastion host in a public subnet, create an administrative SSH key pair, and load the public key into each EC2 instance for remote management.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nAttach the required IAM roles to existing and new instances. Use AWS Systems Manager Session Manager to establish a remote SSH session.\n\nThis method provides a secure and scalable solution with minimal operational overhead. By using AWS Systems Manager, administrators can establish a remote SSH session directly to the instance without the need to open any additional inbound ports. Additionally, this solution does not require any bastion hosts or VPN connections, reducing operational complexity.\n\nAWS Systems Manager Session Manager also provides detailed logging and auditing, which allows for monitoring of who is accessing which instances and when. It also supports role-based access control (RBAC), which enables administrators to restrict access to specific instances based on user roles and permissions.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure a bastion host in a public subnet, create an administrative SSH key pair, and load the public key into each EC2 instance for remote management.\n\nAlthough this solution allows remote management, it adds operational overhead and complexity. AWS Systems Manager Session Manager is a more efficient and easier-to-implement option.\n\n\n\n\nSet up an AWS Site-to-Site VPN connection and instruct administrators to use their local on-premises machines to connect directly to the instances using SSH keys across the VPN tunnel.\n\nThis option increases operational complexity and may introduce latency issues. AWS Systems Manager Session Manager is a more efficient and easier-to-implement solution.\n\n\n\n\nAccess each instance's terminal interface directly through the EC2 serial console for administration purposes.\n\nWhile the EC2 serial console allows direct access to the terminal interface, it does not provide a scalable solution for managing multiple instances. AWS Systems Manager Session Manager is better suited for this requirement.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html",
      "correctAnswerExplanations": [
        {
          "answer": "Attach the required IAM roles to existing and new instances. Use AWS Systems Manager Session Manager to establish a remote SSH session.",
          "explanation": "This method provides a secure and scalable solution with minimal operational overhead. By using AWS Systems Manager, administrators can establish a remote SSH session directly to the instance without the need to open any additional inbound ports. Additionally, this solution does not require any bastion hosts or VPN connections, reducing operational complexity."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Configure a bastion host in a public subnet, create an administrative SSH key pair, and load the public key into each EC2 instance for remote management.",
          "explanation": "Although this solution allows remote management, it adds operational overhead and complexity. AWS Systems Manager Session Manager is a more efficient and easier-to-implement option."
        },
        {
          "answer": "Set up an AWS Site-to-Site VPN connection and instruct administrators to use their local on-premises machines to connect directly to the instances using SSH keys across the VPN tunnel.",
          "explanation": "This option increases operational complexity and may introduce latency issues. AWS Systems Manager Session Manager is a more efficient and easier-to-implement solution."
        },
        {
          "answer": "Access each instance's terminal interface directly through the EC2 serial console for administration purposes.",
          "explanation": "While the EC2 serial console allows direct access to the terminal interface, it does not provide a scalable solution for managing multiple instances. AWS Systems Manager Session Manager is better suited for this requirement."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html"
      ]
    },
    {
      "id": 25,
      "question": "A company operates a network of IoT devices that generate 1 TB of telemetry data per day, with each data point being approximately 2 KB. A solutions architect must design a solution to collect and store this data for future analysis. The company requires a highly available solution with minimal costs and infrastructure management. The company also wants to retain 14 days of data for immediate analysis while archiving older data.\n\nWhat is the MOST operationally efficient solution that fulfills these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Deploy Amazon EC2 instances in two Availability Zones behind an Elastic Load Balancer to collect the telemetry data. Use a script on the EC2 instances to store the data in an Amazon S3 bucket. Implement an S3 Lifecycle policy to transition data to Amazon S3 Glacier after 14 days.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Set up an Amazon Kinesis Data Firehose delivery stream to collect the telemetry data and store it in an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Configure the cluster to take manual snapshots daily and delete data older than 14 days.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Configure an Amazon Kinesis Data Firehose delivery stream to ingest the telemetry data and store it in an Amazon S3 bucket. Set up an S3 Lifecycle policy to transition data to Amazon S3 Glacier after 14 days.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to collect the telemetry data with a 14-day message retention period. Configure consumers to poll the SQS queue, evaluate the message age, and analyze the data as needed. If a message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete it from the SQS queue.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nConfigure an Amazon Kinesis Data Firehose delivery stream to ingest the telemetry data and store it in an Amazon S3 bucket. Set up an S3 Lifecycle policy to transition data to Amazon S3 Glacier after 14 days.\n\nAmazon Kinesis Data Firehose can ingest and transform streaming data, and store it in S3 for cost-effective and durable storage. The S3 Lifecycle policy can be configured to transition data to Amazon S3 Glacier after 14 days, providing long-term storage for data archiving at a lower cost. This solution is highly available, as Kinesis Data Firehose can be deployed across multiple Availability Zones to ensure continuous data delivery.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDeploy Amazon EC2 instances in two Availability Zones behind an Elastic Load Balancer to collect the telemetry data.\n\nThis requires manual infrastructure management, which increases operational overhead and does not fulfill the requirement for minimal infrastructure management.\n\n\n\n\nSet up an Amazon Kinesis Data Firehose delivery stream to collect the telemetry data and store it in an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster.\n\nAmazon OpenSearch Service (Amazon Elasticsearch Service) is designed for search and analytics use cases, and it is not the most cost-effective storage solution for the given requirements.\n\n\n\n\nCreate an Amazon Simple Queue Service (Amazon SQS) standard queue to collect the telemetry data.\n\nUsing Amazon SQS in this context adds unnecessary complexity in managing message processing and handling, increasing operational overhead.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html",
      "correctAnswerExplanations": [
        {
          "answer": "Configure an Amazon Kinesis Data Firehose delivery stream to ingest the telemetry data and store it in an Amazon S3 bucket. Set up an S3 Lifecycle policy to transition data to Amazon S3 Glacier after 14 days.",
          "explanation": "Amazon Kinesis Data Firehose can ingest and transform streaming data, and store it in S3 for cost-effective and durable storage. The S3 Lifecycle policy can be configured to transition data to Amazon S3 Glacier after 14 days, providing long-term storage for data archiving at a lower cost. This solution is highly available, as Kinesis Data Firehose can be deployed across multiple Availability Zones to ensure continuous data delivery."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Deploy Amazon EC2 instances in two Availability Zones behind an Elastic Load Balancer to collect the telemetry data.",
          "explanation": "This requires manual infrastructure management, which increases operational overhead and does not fulfill the requirement for minimal infrastructure management."
        },
        {
          "answer": "Set up an Amazon Kinesis Data Firehose delivery stream to collect the telemetry data and store it in an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster.",
          "explanation": "Amazon OpenSearch Service (Amazon Elasticsearch Service) is designed for search and analytics use cases, and it is not the most cost-effective storage solution for the given requirements."
        },
        {
          "answer": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to collect the telemetry data.",
          "explanation": "Using Amazon SQS in this context adds unnecessary complexity in managing message processing and handling, increasing operational overhead."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"
      ]
    },
    {
      "id": 26,
      "question": "An organization wants to analyze data stored in an Amazon S3 bucket using an application running on an Amazon EC2 instance within a VPC. The organization needs to ensure that the EC2 instance accesses the S3 bucket privately without using the public internet.\n\nWhich solution will provide private network connectivity to Amazon S3?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Create an Amazon API Gateway API with a private link to the S3 bucket.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Configure a VPC endpoint for the S3 bucket.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Attach an IAM role to the EC2 instance for S3 access.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use Amazon S3 Transfer Acceleration to access the S3 bucket.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nConfigure a VPC endpoint for the S3 bucket.\n\nCreating a gateway VPC endpoint for the S3 bucket allows the Amazon EC2 instance to access the S3 bucket privately within the Amazon network. It routes traffic between the VPC and S3 without traversing the internet, ensuring private network connectivity.\n\n\n\n\n\n\n\nIncorrect Options:\n\nAttach an IAM role to the EC2 instance for S3 access.\n\nAttaching an IAM role to the EC2 instance provides the necessary permissions to access the S3 bucket. However, it does not ensure private network connectivity between the EC2 instance and the S3 bucket.\n\n\n\n\nUse Amazon S3 Transfer Acceleration to access the S3 bucket.\n\nAmazon S3 Transfer Acceleration is designed to improve data transfer speeds over the public internet. It does not provide private network connectivity between the EC2 instance and the S3 bucket.\n\n\n\n\nCreate an Amazon API Gateway API with a private link to the S3 bucket.\n\nCreating an Amazon API Gateway API with a private link allows access to the S3 bucket through a VPC endpoint, but it adds unnecessary complexity to the solution. Configuring a VPC endpoint for the S3 bucket is a more direct and efficient approach.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html",
      "correctAnswerExplanations": [
        {
          "answer": "Configure a VPC endpoint for the S3 bucket.",
          "explanation": "Creating a gateway VPC endpoint for the S3 bucket allows the Amazon EC2 instance to access the S3 bucket privately within the Amazon network. It routes traffic between the VPC and S3 without traversing the internet, ensuring private network connectivity."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Attach an IAM role to the EC2 instance for S3 access.",
          "explanation": "Attaching an IAM role to the EC2 instance provides the necessary permissions to access the S3 bucket. However, it does not ensure private network connectivity between the EC2 instance and the S3 bucket."
        },
        {
          "answer": "Use Amazon S3 Transfer Acceleration to access the S3 bucket.",
          "explanation": "Amazon S3 Transfer Acceleration is designed to improve data transfer speeds over the public internet. It does not provide private network connectivity between the EC2 instance and the S3 bucket."
        },
        {
          "answer": "Create an Amazon API Gateway API with a private link to the S3 bucket.",
          "explanation": "Creating an Amazon API Gateway API with a private link allows access to the S3 bucket through a VPC endpoint, but it adds unnecessary complexity to the solution. Configuring a VPC endpoint for the S3 bucket is a more direct and efficient approach."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html"
      ]
    },
    {
      "id": 27,
      "question": "A startup is developing a web application that runs on two Amazon EC2 instances and utilizes an Amazon S3 bucket for storing images. A solutions architect needs to ensure that the EC2 instances have the necessary permissions to access the S3 bucket.\n\nWhat should the solutions architect do to fulfill this requirement?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Create an IAM policy with permissions to access the S3 bucket and attach the policy to the EC2 instances.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an IAM group with permissions to access the S3 bucket and add the EC2 instances to the group.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Create an IAM role with permissions to access the S3 bucket and associate the role with the EC2 instances.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Create an IAM user with permissions to access the S3 bucket and link the user account to the EC2 instances.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nCreate an IAM role with permissions to access the S3 bucket and associate the role with the EC2 instances.\n\nTo grant the EC2 instances access to the S3 bucket, the solutions architect should create an IAM role with the necessary permissions and then associate the role with the EC2 instances. IAM roles provide a secure and scalable way to grant permissions to AWS services, allowing them to access other AWS resources without having to manage and store AWS access keys.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an IAM policy with permissions to access the S3 bucket and attach the policy to the EC2 instances.\n\nIAM policies can be attached to IAM users, groups, or roles but cannot be directly attached to EC2 instances.\n\n\n\n\nCreate an IAM group with permissions to access the S3 bucket and add the EC2 instances to the group.\n\nIAM groups are used to manage permissions for IAM users, not for AWS resources like EC2 instances.\n\n\n\n\nCreate an IAM user with permissions to access the S3 bucket and link the user account to the EC2 instances.\n\nCreating an IAM user and linking it to the EC2 instances would involve managing and storing AWS access keys, which is not a secure or scalable way to grant permissions to AWS services.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\n\nhttps://repost.aws/knowledge-center/ec2-instance-access-s3-bucket",
      "correctAnswerExplanations": [
        {
          "answer": "Create an IAM role with permissions to access the S3 bucket and associate the role with the EC2 instances.",
          "explanation": "To grant the EC2 instances access to the S3 bucket, the solutions architect should create an IAM role with the necessary permissions and then associate the role with the EC2 instances. IAM roles provide a secure and scalable way to grant permissions to AWS services, allowing them to access other AWS resources without having to manage and store AWS access keys."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Create an IAM policy with permissions to access the S3 bucket and attach the policy to the EC2 instances.",
          "explanation": "IAM policies can be attached to IAM users, groups, or roles but cannot be directly attached to EC2 instances."
        },
        {
          "answer": "Create an IAM group with permissions to access the S3 bucket and add the EC2 instances to the group.",
          "explanation": "IAM groups are used to manage permissions for IAM users, not for AWS resources like EC2 instances."
        },
        {
          "answer": "Create an IAM user with permissions to access the S3 bucket and link the user account to the EC2 instances.",
          "explanation": "Creating an IAM user and linking it to the EC2 instances would involve managing and storing AWS access keys, which is not a secure or scalable way to grant permissions to AWS services."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html",
        "https://repost.aws/knowledge-center/ec2-instance-access-s3-bucket"
      ]
    },
    {
      "id": 28,
      "question": "A company is planning to migrate its on-premises data processing application to AWS. The application generates output files ranging from a few gigabytes to multiple terabytes in size. The application data needs to be stored in a standard file system structure, and the company requires a solution that is highly available, scales automatically, and has minimal operational overhead.\n\nWhich solution will meet these requirements?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon Elastic Block Store (Amazon EBS) for storage.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Migrate the application to run on AWS Lambda. Use Amazon S3 for storage.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Migrate the application to run on Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic Block Store (Amazon EBS) for storage.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Migrate the application to run on Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nMigrate the application to run on Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.\n\nThe company should migrate the application to run on Amazon EC2 instances in a Multi-AZ Auto Scaling group and use Amazon Elastic File System (Amazon EFS) for storage. Amazon EFS is a scalable, highly available, and fully managed file system that provides file storage for use with Amazon EC2 instances. Amazon EFS is a simple and easy-to-use storage solution that eliminates the need for managing file servers or storage clusters. It is designed to scale on demand, providing the company with unlimited storage capacity and performance.\n\n\n\n\n\n\n\nIncorrect Options:\n\nMigrate the application to run on AWS Lambda. Use Amazon S3 for storage.\n\nAWS Lambda is a serverless compute service that is best suited for event-driven and stateless workloads. Given the size of the output files, Lambda may not be the most suitable choice for this use case.\n\n\n\n\nMigrate the application to run on Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic Block Store (Amazon EBS) for storage.\n\nAlthough this option provides high availability and auto-scaling, Amazon EBS volumes are limited to a single EC2 instance and do not provide a shared file system structure required by the company.\n\n\n\n\nMigrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon Elastic Block Store (Amazon EBS) for storage.\n\nWhile Amazon EKS offers container orchestration, using Amazon EBS for storage does not provide a shared file system structure, which is a key requirement for the company's application.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/efs/\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html",
      "correctAnswerExplanations": [
        {
          "answer": "Migrate the application to run on Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.",
          "explanation": "The company should migrate the application to run on Amazon EC2 instances in a Multi-AZ Auto Scaling group and use Amazon Elastic File System (Amazon EFS) for storage. Amazon EFS is a scalable, highly available, and fully managed file system that provides file storage for use with Amazon EC2 instances. Amazon EFS is a simple and easy-to-use storage solution that eliminates the need for managing file servers or storage clusters. It is designed to scale on demand, providing the company with unlimited storage capacity and performance."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Migrate the application to run on AWS Lambda. Use Amazon S3 for storage.",
          "explanation": "AWS Lambda is a serverless compute service that is best suited for event-driven and stateless workloads. Given the size of the output files, Lambda may not be the most suitable choice for this use case."
        },
        {
          "answer": "Migrate the application to run on Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic Block Store (Amazon EBS) for storage.",
          "explanation": "Although this option provides high availability and auto-scaling, Amazon EBS volumes are limited to a single EC2 instance and do not provide a shared file system structure required by the company."
        },
        {
          "answer": "Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon Elastic Block Store (Amazon EBS) for storage.",
          "explanation": "While Amazon EKS offers container orchestration, using Amazon EBS for storage does not provide a shared file system structure, which is a key requirement for the company's application."
        }
      ],
      "references": [
        "https://aws.amazon.com/efs/",
        "https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html"
      ]
    },
    {
      "id": 29,
      "question": "A company plans to migrate a distributed application with variable workloads to AWS. The legacy platform consists of a primary server coordinating jobs across multiple compute nodes. The company wants to modernize the application to maximize resiliency and scalability.\n\nWhat should a solutions architect design for the architecture to meet these requirements?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Set up an Amazon Simple Queue Service (Amazon SQS) queue for job distribution, deploy compute nodes as Amazon EC2 instances managed in an Auto Scaling group, and use scheduled scaling for EC2 Auto Scaling.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Deploy the primary server and compute nodes as Amazon EC2 instances managed in an Auto Scaling group, use AWS CloudTrail for job distribution, and configure EC2 Auto Scaling based on the load on the primary server.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Deploy the primary server and compute nodes as Amazon EC2 instances managed in an Auto Scaling group, use Amazon EventBridge (Amazon CloudWatch Events) for job distribution, and configure EC2 Auto Scaling based on the load on the compute nodes.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Set up an Amazon Simple Queue Service (Amazon SQS) queue for job distribution, deploy compute nodes as Amazon EC2 instances managed in an Auto Scaling group, and configure EC2 Auto Scaling based on the size of the queue.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nSet up an Amazon Simple Queue Service (Amazon SQS) queue for job distribution, deploy compute nodes as Amazon EC2 instances managed in an Auto Scaling group, and configure EC2 Auto Scaling based on the size of the queue.\n\nBy using Amazon SQS for job distribution and Amazon EC2 instances with Auto Scaling, the solution can handle variable workloads while maximizing resiliency and scalability. Configuring EC2 Auto Scaling based on the size of the SQS queue ensures that the compute capacity adjusts according to the workload.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up an Amazon Simple Queue Service (Amazon SQS) queue for job distribution, deploy compute nodes as Amazon EC2 instances managed in an Auto Scaling group, and use scheduled scaling for EC2 Auto Scaling.\n\nScheduled scaling isn't ideal for variable workloads, as it relies on predefined schedules rather than adjusting dynamically based on workload changes.\n\n\n\n\nDeploy the primary server and compute nodes as Amazon EC2 instances managed in an Auto Scaling group, use AWS CloudTrail for job distribution, and configure EC2 Auto Scaling based on the load on the primary server.\n\nAWS CloudTrail is designed for governance, compliance, risk auditing, and operational auditing. It is not suitable for job distribution in a distributed application.\n\n\n\n\nDeploy the primary server and compute nodes as Amazon EC2 instances managed in an Auto Scaling group, use Amazon EventBridge (Amazon CloudWatch Events) for job distribution, and configure EC2 Auto Scaling based on the load on the compute nodes.\n\nAmazon EventBridge is used for event-driven architectures and not for distributing jobs in a distributed application. Amazon SQS is a better choice for job distribution in this scenario.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html",
      "correctAnswerExplanations": [
        {
          "answer": "Set up an Amazon Simple Queue Service (Amazon SQS) queue for job distribution, deploy compute nodes as Amazon EC2 instances managed in an Auto Scaling group, and configure EC2 Auto Scaling based on the size of the queue.",
          "explanation": "By using Amazon SQS for job distribution and Amazon EC2 instances with Auto Scaling, the solution can handle variable workloads while maximizing resiliency and scalability. Configuring EC2 Auto Scaling based on the size of the SQS queue ensures that the compute capacity adjusts according to the workload."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Set up an Amazon Simple Queue Service (Amazon SQS) queue for job distribution, deploy compute nodes as Amazon EC2 instances managed in an Auto Scaling group, and use scheduled scaling for EC2 Auto Scaling.",
          "explanation": "Scheduled scaling isn't ideal for variable workloads, as it relies on predefined schedules rather than adjusting dynamically based on workload changes."
        },
        {
          "answer": "Deploy the primary server and compute nodes as Amazon EC2 instances managed in an Auto Scaling group, use AWS CloudTrail for job distribution, and configure EC2 Auto Scaling based on the load on the primary server.",
          "explanation": "AWS CloudTrail is designed for governance, compliance, risk auditing, and operational auditing. It is not suitable for job distribution in a distributed application."
        },
        {
          "answer": "Deploy the primary server and compute nodes as Amazon EC2 instances managed in an Auto Scaling group, use Amazon EventBridge (Amazon CloudWatch Events) for job distribution, and configure EC2 Auto Scaling based on the load on the compute nodes.",
          "explanation": "Amazon EventBridge is used for event-driven architectures and not for distributing jobs in a distributed application. Amazon SQS is a better choice for job distribution in this scenario."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html"
      ]
    },
    {
      "id": 30,
      "question": "A company's web application integrates with various third-party SaaS services to aggregate data for analysis. The application utilizes Amazon EC2 instances to process and transfer data to an Amazon S3 bucket. Once the data transfer is complete, the EC2 instance sends a notification to users. The company is experiencing slow application performance and seeks a solution with minimal operational overhead to improve it.\n\nWhich option best addresses these requirements?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Create an Auto Scaling group to enable the scaling of EC2 instances. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic upon completing the upload to the S3 bucket.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Replace the EC2 instances with Docker containers hosted on Amazon Elastic Container Service (Amazon ECS). Configure Amazon CloudWatch Container Insights to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Set up an Amazon EventBridge (Amazon CloudWatch Events) rule for each third-party SaaS service to forward output data. Configure the S3 bucket as the rule's target. Create a second EventBridge (CloudWatch Events) rule to send events upon completing the upload to the S3 bucket, with an Amazon Simple Notification Service (Amazon SNS) topic as the second rule's target.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Configure an Amazon AppFlow flow to transfer data between third-party SaaS services and the S3 bucket. Set up an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nConfigure an Amazon AppFlow flow to transfer data between third-party SaaS services and the S3 bucket. Set up an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.\n\nAmazon AppFlow is a fully managed integration service that allows developers to easily transfer data between Software as a Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift. S3 event notifications enable automated workflows and integration with third-party services. Using Amazon SNS, the notification service can deliver messages to a variety of endpoints, including email, HTTP/S, SMS, mobile push notifications, and Amazon Simple Queue Service (SQS).\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an Auto Scaling group to enable the scaling of EC2 instances. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic upon completing the upload to the S3 bucket.\n\nWhile this option may improve performance, it increases operational overhead and complexity, as the company still needs to manage the EC2 instances.\n\n\n\n\nSet up an Amazon EventBridge (Amazon CloudWatch Events) rule for each third-party SaaS service to forward output data. Configure the S3 bucket as the rule's target. Create a second EventBridge (CloudWatch Events) rule to send events upon completing the upload to the S3 bucket, with an Amazon Simple Notification Service (Amazon SNS) topic as the second rule's target.\n\nThis option adds unnecessary complexity by requiring separate rules for each third-party service and still relies on EC2 instances for data processing.\n\n\n\n\nReplace the EC2 instances with Docker containers hosted on Amazon Elastic Container Service (Amazon ECS). Configure Amazon CloudWatch Container Insights to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.\n\nAlthough this option could improve performance, it introduces the operational overhead of managing Docker containers and Amazon ECS.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/appflow/latest/userguide/what-is-appflow.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html",
      "correctAnswerExplanations": [
        {
          "answer": "Configure an Amazon AppFlow flow to transfer data between third-party SaaS services and the S3 bucket. Set up an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.",
          "explanation": "Amazon AppFlow is a fully managed integration service that allows developers to easily transfer data between Software as a Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift. S3 event notifications enable automated workflows and integration with third-party services. Using Amazon SNS, the notification service can deliver messages to a variety of endpoints, including email, HTTP/S, SMS, mobile push notifications, and Amazon Simple Queue Service (SQS)."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Create an Auto Scaling group to enable the scaling of EC2 instances. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic upon completing the upload to the S3 bucket.",
          "explanation": "While this option may improve performance, it increases operational overhead and complexity, as the company still needs to manage the EC2 instances."
        },
        {
          "answer": "Set up an Amazon EventBridge (Amazon CloudWatch Events) rule for each third-party SaaS service to forward output data. Configure the S3 bucket as the rule's target. Create a second EventBridge (CloudWatch Events) rule to send events upon completing the upload to the S3 bucket, with an Amazon Simple Notification Service (Amazon SNS) topic as the second rule's target.",
          "explanation": "This option adds unnecessary complexity by requiring separate rules for each third-party service and still relies on EC2 instances for data processing."
        },
        {
          "answer": "Replace the EC2 instances with Docker containers hosted on Amazon Elastic Container Service (Amazon ECS). Configure Amazon CloudWatch Container Insights to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.",
          "explanation": "Although this option could improve performance, it introduces the operational overhead of managing Docker containers and Amazon ECS."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/appflow/latest/userguide/what-is-appflow.html",
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html"
      ]
    },
    {
      "id": 31,
      "question": "A solutions architect is designing the storage architecture for a data analytics application using Amazon S3. The application processes a mix of frequently accessed and infrequently accessed data files. The data files must be resilient to the loss of an Availability Zone. The solutions architect needs to optimize the costs of storing and retrieving these data files.\n\nWhich storage option meets these requirements?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "S3 Intelligent-Tiering",
          "correct": true
        },
        {
          "id": 2,
          "answer": "S3 One Zone-Infrequent Access (S3 One Zone-IA)",
          "correct": false
        },
        {
          "id": 3,
          "answer": "S3 Standard",
          "correct": false
        },
        {
          "id": 4,
          "answer": "S3 Standard-Infrequent Access (S3 Standard-IA)",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nS3 Intelligent-Tiering\n\nS3 Intelligent-Tiering is designed to optimize storage costs for datasets with unpredictable access patterns. It automatically moves objects between two access tiers (frequent and infrequent access) based on changing access patterns, ensuring cost-effective storage for both frequently accessed and infrequently accessed data files. It also provides resilience to the loss of an Availability Zone.\n\n\n\n\n\n\n\nIncorrect Options:\n\nS3 One Zone-Infrequent Access (S3 One Zone-IA)\n\nS3 One Zone-IA stores data in a single Availability Zone, which does not meet the requirement for resilience to the loss of an Availability Zone.\n\n\n\n\nS3 Standard-Infrequent Access (S3 Standard-IA)\n\nS3 Standard-IA is designed for infrequently accessed data with a lower cost compared to S3 Standard. However, it does not automatically optimize storage costs for datasets with unpredictable access patterns like S3 Intelligent-Tiering.\n\n\n\n\nS3 Standard\n\nS3 Standard is designed for frequently accessed data, but it does not provide cost optimization for infrequently accessed data in the same way S3 Intelligent-Tiering does.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes",
      "correctAnswerExplanations": [
        {
          "answer": "S3 Intelligent-Tiering",
          "explanation": "S3 Intelligent-Tiering is designed to optimize storage costs for datasets with unpredictable access patterns. It automatically moves objects between two access tiers (frequent and infrequent access) based on changing access patterns, ensuring cost-effective storage for both frequently accessed and infrequently accessed data files. It also provides resilience to the loss of an Availability Zone."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "S3 One Zone-Infrequent Access (S3 One Zone-IA)",
          "explanation": "S3 One Zone-IA stores data in a single Availability Zone, which does not meet the requirement for resilience to the loss of an Availability Zone."
        },
        {
          "answer": "S3 Standard-Infrequent Access (S3 Standard-IA)",
          "explanation": "S3 Standard-IA is designed for infrequently accessed data with a lower cost compared to S3 Standard. However, it does not automatically optimize storage costs for datasets with unpredictable access patterns like S3 Intelligent-Tiering."
        },
        {
          "answer": "S3 Standard",
          "explanation": "S3 Standard is designed for frequently accessed data, but it does not provide cost optimization for infrequently accessed data in the same way S3 Intelligent-Tiering does."
        }
      ],
      "references": [
        "https://aws.amazon.com/s3/storage-classes"
      ]
    },
    {
      "id": 32,
      "question": "A financial organization recently transitioned its infrastructure to AWS and seeks a solution to secure the network traffic within its VPC. In the company's on-premises data center, a network security device provided features such as deep packet inspection and traffic filtering. The organization wants to implement similar functionality in the AWS Cloud.\n\nWhich solution will best address these requirements?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon Inspector for traffic inspection and traffic filtering in the VPC.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use AWS Firewall Manager to establish the required rules for traffic inspection and traffic filtering in the VPC.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use VPC Traffic Mirroring to copy traffic from the VPC for traffic inspection and filtering.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use AWS Network Firewall to define the necessary rules for traffic inspection and traffic filtering in the VPC.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nUse AWS Network Firewall to define the necessary rules for traffic inspection and traffic filtering in the VPC.\n\nAWS Network Firewall is a managed service that provides network protection and visibility for VPCs. It enables the creation of stateful inspection rules, allowing organizations to enforce fine-grained security policies for inbound and outbound traffic. AWS Network Firewall supports deep packet inspection and traffic filtering, providing similar functionality to the network security device used in the on-premises data center. By implementing AWS Network Firewall, the financial organization can achieve the desired traffic inspection and filtering capabilities in the AWS Cloud, ensuring a secure network environment for its VPC.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon Inspector for traffic inspection and traffic filtering in the VPC.\n\nAmazon Inspector is a security assessment service that helps improve the security and compliance of applications deployed on AWS. However, it does not provide traffic inspection and filtering capabilities at the network level.\n\n\n\n\nUse VPC Traffic Mirroring to copy traffic from the VPC for traffic inspection and filtering.\n\nVPC Traffic Mirroring allows you to capture network traffic from an Elastic Network Interface (ENI) and send it to a security appliance for analysis. While Traffic Mirroring can help monitor traffic, it does not provide traffic inspection and filtering functionalities directly.\n\n\n\n\nUse AWS Firewall Manager to establish the required rules for traffic inspection and traffic filtering in the VPC.\n\nAWS Firewall Manager is a security management service that helps centrally configure and manage firewall rules across accounts and applications. It simplifies the management of AWS WAF, AWS Shield Advanced, and VPC security groups but does not provide deep packet inspection and traffic filtering capabilities itself.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use AWS Network Firewall to define the necessary rules for traffic inspection and traffic filtering in the VPC.",
          "explanation": "AWS Network Firewall is a managed service that provides network protection and visibility for VPCs. It enables the creation of stateful inspection rules, allowing organizations to enforce fine-grained security policies for inbound and outbound traffic. AWS Network Firewall supports deep packet inspection and traffic filtering, providing similar functionality to the network security device used in the on-premises data center. By implementing AWS Network Firewall, the financial organization can achieve the desired traffic inspection and filtering capabilities in the AWS Cloud, ensuring a secure network environment for its VPC."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use Amazon Inspector for traffic inspection and traffic filtering in the VPC.",
          "explanation": "Amazon Inspector is a security assessment service that helps improve the security and compliance of applications deployed on AWS. However, it does not provide traffic inspection and filtering capabilities at the network level."
        },
        {
          "answer": "Use VPC Traffic Mirroring to copy traffic from the VPC for traffic inspection and filtering.",
          "explanation": "VPC Traffic Mirroring allows you to capture network traffic from an Elastic Network Interface (ENI) and send it to a security appliance for analysis. While Traffic Mirroring can help monitor traffic, it does not provide traffic inspection and filtering functionalities directly."
        },
        {
          "answer": "Use AWS Firewall Manager to establish the required rules for traffic inspection and traffic filtering in the VPC.",
          "explanation": "AWS Firewall Manager is a security management service that helps centrally configure and manage firewall rules across accounts and applications. It simplifies the management of AWS WAF, AWS Shield Advanced, and VPC security groups but does not provide deep packet inspection and traffic filtering capabilities itself."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html"
      ]
    },
    {
      "id": 33,
      "question": "A media company runs multiple Windows workloads on AWS. The company's employees use Windows file shares that are hosted on four Amazon EC2 instances. The file shares synchronize data between themselves and maintain duplicate copies. The company wants a highly available, durable, and cost-effective storage solution that maintains the current user access methods.\n\nWhat should a solutions architect do to meet these requirements?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Migrate the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Transfer all the data to FSx for Windows File Server.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Set up an AWS Storage Gateway File Gateway. Mount the File Gateway on the existing EC2 instances.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Migrate all the data to Amazon S3. Set up IAM authentication for users to access files.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Migrate the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Transfer all the data to Amazon EFS.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nMigrate the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Transfer all the data to FSx for Windows File Server.\n\nAmazon FSx for Windows File Server is a fully managed native Windows file system that is accessible over the industry-standard SMB (Server Message Block) protocol. By migrating the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration, the media company can maintain its current user access methods while benefiting from automatic backups, data replication, and fault tolerance. The Multi-AZ configuration ensures that the data remains available even if one of the AZs fails.\n\n\n\n\n\n\n\nIncorrect Options:\n\nMigrate all the data to Amazon S3. Set up IAM authentication for users to access files.\n\nThis option doesn't maintain the current user access methods because it requires users to access files through S3, rather than the familiar SMB protocol.\n\n\n\n\nSet up an AWS Storage Gateway File Gateway. Mount the File Gateway on the existing EC2 instances.\n\nAlthough this option utilizes the SMB protocol, it is not as cost-effective as FSx for Windows File Server, as it requires additional resources to manage and maintain the existing EC2 instances and the File Gateway.\n\n\n\n\nMigrate the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Transfer all the data to Amazon EFS.\n\nWhile this option provides a highly available and durable storage solution, it is not suitable for Windows workloads because EFS uses the Network File System (NFS) protocol, rather than the SMB protocol required for Windows file shares.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html\n\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multiAZ.html",
      "correctAnswerExplanations": [
        {
          "answer": "Migrate the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Transfer all the data to FSx for Windows File Server.",
          "explanation": "Amazon FSx for Windows File Server is a fully managed native Windows file system that is accessible over the industry-standard SMB (Server Message Block) protocol. By migrating the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration, the media company can maintain its current user access methods while benefiting from automatic backups, data replication, and fault tolerance. The Multi-AZ configuration ensures that the data remains available even if one of the AZs fails."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Migrate all the data to Amazon S3. Set up IAM authentication for users to access files.",
          "explanation": "This option doesn't maintain the current user access methods because it requires users to access files through S3, rather than the familiar SMB protocol."
        },
        {
          "answer": "Set up an AWS Storage Gateway File Gateway. Mount the File Gateway on the existing EC2 instances.",
          "explanation": "Although this option utilizes the SMB protocol, it is not as cost-effective as FSx for Windows File Server, as it requires additional resources to manage and maintain the existing EC2 instances and the File Gateway."
        },
        {
          "answer": "Migrate the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Transfer all the data to Amazon EFS.",
          "explanation": "While this option provides a highly available and durable storage solution, it is not suitable for Windows workloads because EFS uses the Network File System (NFS) protocol, rather than the SMB protocol required for Windows file shares."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html",
        "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/high-availability-multiAZ.html"
      ]
    },
    {
      "id": 34,
      "question": "A company stores important files in an Amazon S3 bucket and wants to ensure that these files are protected from accidental deletion.\n\nWhat combination of steps should a solutions architect take to meet these requirements? (Select TWO.)",
      "corrects": [
        1,
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Enable object versioning on the S3 bucket.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Enable MFA Delete on the S3 bucket.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Set up a bucket policy to restrict delete operations.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Configure a lifecycle policy for the objects in the S3 bucket.",
          "correct": false
        },
        {
          "id": 5,
          "answer": "Activate default encryption for the S3 bucket.",
          "correct": false
        }
      ],
      "multiple": true,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nEnable object versioning on the S3 bucket.\n\nBy enabling object versioning, the S3 bucket can preserve multiple versions of an object, including all writes and deletes. This feature helps protect data from accidental deletion or overwrites, as previous versions of the object can be retrieved even if the current version is deleted.\n\n\n\n\nEnable MFA Delete on the S3 bucket.\n\nMFA Delete adds an additional layer of security by requiring multi-factor authentication (MFA) to permanently delete an object version or change the versioning state of a bucket. This ensures that accidental or unauthorized deletions are minimized, providing better protection for critical data.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up a bucket policy to restrict delete operations.\n\nAlthough a bucket policy can be used to restrict delete operations, it does not provide protection against accidental deletion or modification. Enabling object versioning and MFA Delete offers more comprehensive protection for critical data.\n\n\n\n\nActivate default encryption for the S3 bucket.\n\nDefault encryption protects the data at rest by encrypting the objects stored in the bucket. While this feature is important for securing data, it does not specifically protect against accidental deletion or modification.\n\n\n\n\nConfigure a lifecycle policy for the objects in the S3 bucket.\n\nLifecycle policies help manage object versions and transition them to different storage classes or expiration based on defined rules. However, they do not provide direct protection against accidental deletion or modification of data.\n\n\n\n\n\n\n\nReferences:\n\nhttps://repost.aws/it/knowledge-center/s3-audit-deleted-missing-objects",
      "correctAnswerExplanations": [
        {
          "answer": "Enable object versioning on the S3 bucket.",
          "explanation": "By enabling object versioning, the S3 bucket can preserve multiple versions of an object, including all writes and deletes. This feature helps protect data from accidental deletion or overwrites, as previous versions of the object can be retrieved even if the current version is deleted."
        },
        {
          "answer": "Enable MFA Delete on the S3 bucket.",
          "explanation": "MFA Delete adds an additional layer of security by requiring multi-factor authentication (MFA) to permanently delete an object version or change the versioning state of a bucket. This ensures that accidental or unauthorized deletions are minimized, providing better protection for critical data."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Set up a bucket policy to restrict delete operations.",
          "explanation": "Although a bucket policy can be used to restrict delete operations, it does not provide protection against accidental deletion or modification. Enabling object versioning and MFA Delete offers more comprehensive protection for critical data."
        },
        {
          "answer": "Activate default encryption for the S3 bucket.",
          "explanation": "Default encryption protects the data at rest by encrypting the objects stored in the bucket. While this feature is important for securing data, it does not specifically protect against accidental deletion or modification."
        },
        {
          "answer": "Configure a lifecycle policy for the objects in the S3 bucket.",
          "explanation": "Lifecycle policies help manage object versions and transition them to different storage classes or expiration based on defined rules. However, they do not provide direct protection against accidental deletion or modification of data."
        }
      ],
      "references": [
        "https://repost.aws/it/knowledge-center/s3-audit-deleted-missing-objects"
      ]
    },
    {
      "id": 35,
      "question": "A Medical analytical company has implemented a RESTful API using Amazon API Gateway and AWS Lambda to process and store patient records in various formats, including PDF and image files. They need to update the Lambda function to detect sensitive medical data in the records.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon Rekognition to extract text from the patient records and apply Amazon Comprehend Medical to identify sensitive medical data within the extracted text.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use available Python libraries to extract text from the patient records and identify sensitive medical data within the extracted text.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use Amazon Textract to extract text from the patient records and apply Amazon Comprehend Medical to identify sensitive medical data within the extracted text.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Use Amazon Textract to extract text from the patient records and use Amazon SageMaker to identify sensitive medical data within the extracted text.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nUse Amazon Textract to extract text from the patient records and apply Amazon Comprehend Medical to identify sensitive medical data within the extracted text.\n\nTextract is a machine learning service that can extract text and data from a wide range of document types, including PDF and image files, which is suitable for this use case. Comprehend Medical, on the other hand, is a natural language processing service that can accurately identify medical information such as medical conditions, medication, and dosage from unstructured text.\n\nBy using Textract to extract the text from the patient records and applying Comprehend Medical to identify sensitive medical data within the extracted text, the Lambda function can efficiently detect and redact any sensitive information. This solution is cost-effective and scalable, as both services are fully managed and can handle large volumes of data without the need for additional infrastructure or maintenance.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse available Python libraries to extract text from the patient records and identify sensitive medical data within the extracted text.\n\nThis option requires manual maintenance and updates to the Python libraries, which increases operational overhead compared to using managed AWS services.\n\n\n\n\nUse Amazon Textract to extract text from the patient records and use Amazon SageMaker to identify sensitive medical data within the extracted text.\n\nThis option involves more operational overhead due to the need to create, train, and maintain custom machine learning models with Amazon SageMaker.\n\n\n\n\nUse Amazon Rekognition to extract text from the patient records and apply Amazon Comprehend Medical to identify sensitive medical data within the extracted text.\n\nAmazon Rekognition is primarily designed for image and video analysis, not for extracting text from documents. Therefore, this option is less effective for the given requirements.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/textract/\n\nhttps://aws.amazon.com/comprehend/medical/",
      "correctAnswerExplanations": [
        {
          "answer": "Use Amazon Textract to extract text from the patient records and apply Amazon Comprehend Medical to identify sensitive medical data within the extracted text.",
          "explanation": "Textract is a machine learning service that can extract text and data from a wide range of document types, including PDF and image files, which is suitable for this use case. Comprehend Medical, on the other hand, is a natural language processing service that can accurately identify medical information such as medical conditions, medication, and dosage from unstructured text."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use available Python libraries to extract text from the patient records and identify sensitive medical data within the extracted text.",
          "explanation": "This option requires manual maintenance and updates to the Python libraries, which increases operational overhead compared to using managed AWS services."
        },
        {
          "answer": "Use Amazon Textract to extract text from the patient records and use Amazon SageMaker to identify sensitive medical data within the extracted text.",
          "explanation": "This option involves more operational overhead due to the need to create, train, and maintain custom machine learning models with Amazon SageMaker."
        },
        {
          "answer": "Use Amazon Rekognition to extract text from the patient records and apply Amazon Comprehend Medical to identify sensitive medical data within the extracted text.",
          "explanation": "Amazon Rekognition is primarily designed for image and video analysis, not for extracting text from documents. Therefore, this option is less effective for the given requirements."
        }
      ],
      "references": [
        "https://aws.amazon.com/textract/",
        "https://aws.amazon.com/comprehend/medical/"
      ]
    },
    {
      "id": 36,
      "question": "A company hosts a media-rich website on Amazon S3, using Amazon Route 53 for DNS management. The website serves a global audience and experiences high traffic. The company needs to minimize latency for users accessing the website in a cost-effective manner.\n\nWhich solution meets these requirements MOST cost-effectively?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Configure accelerators in AWS Global Accelerator, associate the provided IP addresses with the S3 bucket, and update the Route 53 entries to point to the accelerators' IP addresses.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Enable S3 Transfer Acceleration on the bucket and update the Route 53 entries to point to the new endpoint.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Replicate the S3 bucket to all AWS Regions and configure Route 53 geolocation routing entries.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Set up an Amazon CloudFront distribution in front of the S3 bucket and update the Route 53 entries to point to the CloudFront distribution.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nSet up an Amazon CloudFront distribution in front of the S3 bucket and update the Route 53 entries to point to the CloudFront distribution.\n\nAmazon CloudFront is a content delivery network that caches content at edge locations around the world, reducing the latency for users by serving content from the edge location closest to them. This will enable users to access content more quickly and help provide a better user experience. Additionally, updating the Route 53 entries to point to the CloudFront distribution will help ensure that traffic is routed to the closest edge location, further reducing latency.\n\n\n\n\n\n\n\nIncorrect Options:\n\nEnable S3 Transfer Acceleration on the bucket and update the Route 53 entries to point to the new endpoint.\n\nS3 Transfer Acceleration is designed for faster transfer of large files over the public internet but is not a suitable solution for minimizing latency for website users.\n\n\n\n\nReplicate the S3 bucket to all AWS Regions and configure Route 53 geolocation routing entries.\n\nWhile this solution may decrease latency, it adds complexity and increases costs due to data replication and storage in multiple regions.\n\n\n\n\nConfigure accelerators in AWS Global Accelerator, associate the provided IP addresses with the S3 bucket, and update the Route 53 entries to point to the accelerators' IP addresses.\n\nAWS Global Accelerator is not directly compatible with Amazon S3. Moreover, using CloudFront is a more cost-effective and straightforward solution to minimize latency for website users.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/cloudfront",
      "correctAnswerExplanations": [
        {
          "answer": "Set up an Amazon CloudFront distribution in front of the S3 bucket and update the Route 53 entries to point to the CloudFront distribution.",
          "explanation": "Amazon CloudFront is a content delivery network that caches content at edge locations around the world, reducing the latency for users by serving content from the edge location closest to them. This will enable users to access content more quickly and help provide a better user experience. Additionally, updating the Route 53 entries to point to the CloudFront distribution will help ensure that traffic is routed to the closest edge location, further reducing latency."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Enable S3 Transfer Acceleration on the bucket and update the Route 53 entries to point to the new endpoint.",
          "explanation": "S3 Transfer Acceleration is designed for faster transfer of large files over the public internet but is not a suitable solution for minimizing latency for website users."
        },
        {
          "answer": "Replicate the S3 bucket to all AWS Regions and configure Route 53 geolocation routing entries.",
          "explanation": "While this solution may decrease latency, it adds complexity and increases costs due to data replication and storage in multiple regions."
        },
        {
          "answer": "Configure accelerators in AWS Global Accelerator, associate the provided IP addresses with the S3 bucket, and update the Route 53 entries to point to the accelerators' IP addresses.",
          "explanation": "AWS Global Accelerator is not directly compatible with Amazon S3. Moreover, using CloudFront is a more cost-effective and straightforward solution to minimize latency for website users."
        }
      ],
      "references": [
        "https://aws.amazon.com/cloudfront"
      ]
    },
    {
      "id": 37,
      "question": "A media company stores podcast episodes and their metadata on a monthly basis. Users frequently access episodes within 1 year of the release date, but access drops significantly after 1 year. The company aims to optimize its storage solution by ensuring quick access to episodes less than 1-year-old, while tolerating delays in retrieving older episodes.\n\nWhich option is the MOST cost-effective solution to meet these requirements?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Store individual episodes with tags in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the episodes to S3 Glacier Deep Archive after 1 year. Query and retrieve the episodes by searching for metadata in Amazon S3.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Store individual episodes in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the episodes to S3 Glacier Flexible Retrieval after 1 year. Store metadata in Amazon S3 Standard storage. Query and retrieve the episodes using Amazon Athena and S3 Glacier Select.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Store individual episodes in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the episodes to S3 Glacier Flexible Retrieval after 1 year. Store metadata in Amazon RDS. Query the metadata in Amazon RDS and retrieve the episodes from S3 Glacier Flexible Retrieval.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Store individual episodes in Amazon S3 One Zone-Infrequent Access. Use S3 Lifecycle policies to move the episodes to S3 Glacier Deep Archive after 1 year. Store metadata in Amazon RDS. Query the metadata in Amazon RDS and retrieve the episodes from S3 Glacier Deep Archive.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nStore individual episodes with tags in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the episodes to S3 Glacier Deep Archive after 1 year. Query and retrieve the episodes by searching for metadata in Amazon S3.\n\nThis solution is cost-effective and meets the requirements. Amazon S3 Intelligent-Tiering automatically optimizes storage costs by moving objects between two access tiers based on changing access patterns. By using S3 Lifecycle policies, the company can automatically move episodes to S3 Glacier Deep Archive after 1 year, reducing storage costs for infrequently accessed data. Storing metadata as tags allows users to quickly search and retrieve episodes using Amazon S3.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStore individual episodes in Amazon S3 One Zone-Infrequent Access. Use S3 Lifecycle policies to move the episodes to S3 Glacier Deep Archive after 1 year. Store metadata in Amazon RDS. Query the metadata in Amazon RDS and retrieve the episodes from S3 Glacier Deep Archive.\n\nThis option is not as cost-effective as storing the episodes in Amazon S3 Intelligent-Tiering, which automatically optimizes storage costs based on access patterns.\n\n\n\n\nStore individual episodes in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the episodes to S3 Glacier Flexible Retrieval after 1 year. Store metadata in Amazon S3 Standard storage. Query and retrieve the episodes using Amazon Athena and S3 Glacier Select.\n\nUsing Amazon S3 Standard storage for episodes that are accessed infrequently after 1 year is not as cost-effective as using Amazon S3 Intelligent-Tiering.\n\n\n\n\nStore individual episodes in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the episodes to S3 Glacier Flexible Retrieval after 1 year. Store metadata in Amazon RDS. Query the metadata in Amazon RDS and retrieve the episodes from S3 Glacier Flexible Retrieval.\n\nUsing Amazon RDS for metadata storage is not as cost-effective as storing metadata as tags in Amazon S3.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html",
      "correctAnswerExplanations": [
        {
          "answer": "Store individual episodes with tags in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the episodes to S3 Glacier Deep Archive after 1 year. Query and retrieve the episodes by searching for metadata in Amazon S3.",
          "explanation": "This solution is cost-effective and meets the requirements. Amazon S3 Intelligent-Tiering automatically optimizes storage costs by moving objects between two access tiers based on changing access patterns. By using S3 Lifecycle policies, the company can automatically move episodes to S3 Glacier Deep Archive after 1 year, reducing storage costs for infrequently accessed data. Storing metadata as tags allows users to quickly search and retrieve episodes using Amazon S3."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Store individual episodes in Amazon S3 One Zone-Infrequent Access. Use S3 Lifecycle policies to move the episodes to S3 Glacier Deep Archive after 1 year. Store metadata in Amazon RDS. Query the metadata in Amazon RDS and retrieve the episodes from S3 Glacier Deep Archive.",
          "explanation": "This option is not as cost-effective as storing the episodes in Amazon S3 Intelligent-Tiering, which automatically optimizes storage costs based on access patterns."
        },
        {
          "answer": "Store individual episodes in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the episodes to S3 Glacier Flexible Retrieval after 1 year. Store metadata in Amazon S3 Standard storage. Query and retrieve the episodes using Amazon Athena and S3 Glacier Select.",
          "explanation": "Using Amazon S3 Standard storage for episodes that are accessed infrequently after 1 year is not as cost-effective as using Amazon S3 Intelligent-Tiering."
        },
        {
          "answer": "Store individual episodes in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the episodes to S3 Glacier Flexible Retrieval after 1 year. Store metadata in Amazon RDS. Query the metadata in Amazon RDS and retrieve the episodes from S3 Glacier Flexible Retrieval.",
          "explanation": "Using Amazon RDS for metadata storage is not as cost-effective as storing metadata as tags in Amazon S3."
        }
      ],
      "references": [
        "https://aws.amazon.com/s3/storage-classes/",
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html"
      ]
    },
    {
      "id": 38,
      "question": "A software company is building a multi-tier web application on AWS. The application is hosted on Amazon EC2 instances and interacts with a backend Amazon RDS database. To enhance security, the company wants to avoid hardcoding database credentials in the application and implement a method to automatically rotate the credentials regularly.\n\nWhich approach minimizes operational overhead while meeting these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Save the database credentials as encrypted parameters in AWS Systems Manager Parameter Store, enable automatic rotation, and grant the EC2 instance role access to the encrypted parameters.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Store the database credentials in instance metadata, use Amazon EventBridge (Amazon CloudWatch Events) rules to trigger an AWS Lambda function to update the RDS credentials and instance metadata concurrently.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use AWS Secrets Manager to store the database credentials as a secret, enable automatic rotation, and grant the EC2 instance role access to the secret.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Store the database credentials in an encrypted Amazon S3 bucket, use AWS Lambda to rotate the credentials periodically, and grant the EC2 instance role access to the bucket.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nUse AWS Secrets Manager to store the database credentials as a secret, enable automatic rotation, and grant the EC2 instance role access to the secret.\n\nAWS Secrets Manager provides a secure and scalable solution to store, retrieve, and manage secrets, including database credentials. By enabling automatic rotation, the credentials are automatically rotated without manual intervention, reducing operational overhead. Granting the EC2 instance role access to the secret ensures that the application can access the credentials without hardcoding them.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStore the database credentials in an encrypted Amazon S3 bucket, use AWS Lambda to rotate the credentials periodically, and grant the EC2 instance role access to the bucket.\n\nThis option introduces unnecessary complexity and operational overhead, as it requires setting up Lambda functions for credential rotation and managing access to the S3 bucket.\n\n\n\n\nSave the database credentials as encrypted parameters in AWS Systems Manager Parameter Store, enable automatic rotation, and grant the EC2 instance role access to the encrypted parameters.\n\nAWS Systems Manager Parameter Store does not support automatic rotation of secrets. Secrets Manager is a more suitable choice for this requirement.\n\n\n\n\nStore the database credentials in instance metadata, use Amazon EventBridge (Amazon CloudWatch Events) rules to trigger an AWS Lambda function to update the RDS credentials and instance metadata concurrently.\n\nInstance metadata is not intended for storing sensitive information such as database credentials. Secrets Manager offers a more secure and scalable solution for this purpose.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/secrets-manager/\n\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/create_database_secret.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use AWS Secrets Manager to store the database credentials as a secret, enable automatic rotation, and grant the EC2 instance role access to the secret.",
          "explanation": "AWS Secrets Manager provides a secure and scalable solution to store, retrieve, and manage secrets, including database credentials. By enabling automatic rotation, the credentials are automatically rotated without manual intervention, reducing operational overhead. Granting the EC2 instance role access to the secret ensures that the application can access the credentials without hardcoding them."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Store the database credentials in an encrypted Amazon S3 bucket, use AWS Lambda to rotate the credentials periodically, and grant the EC2 instance role access to the bucket.",
          "explanation": "This option introduces unnecessary complexity and operational overhead, as it requires setting up Lambda functions for credential rotation and managing access to the S3 bucket."
        },
        {
          "answer": "Save the database credentials as encrypted parameters in AWS Systems Manager Parameter Store, enable automatic rotation, and grant the EC2 instance role access to the encrypted parameters.",
          "explanation": "AWS Systems Manager Parameter Store does not support automatic rotation of secrets. Secrets Manager is a more suitable choice for this requirement."
        },
        {
          "answer": "Store the database credentials in instance metadata, use Amazon EventBridge (Amazon CloudWatch Events) rules to trigger an AWS Lambda function to update the RDS credentials and instance metadata concurrently.",
          "explanation": "Instance metadata is not intended for storing sensitive information such as database credentials. Secrets Manager offers a more secure and scalable solution for this purpose."
        }
      ],
      "references": [
        "https://aws.amazon.com/secrets-manager/",
        "https://docs.aws.amazon.com/secretsmanager/latest/userguide/create_database_secret.html"
      ]
    },
    {
      "id": 39,
      "question": "A company operates an e-commerce platform that maintains a product catalog with millions of items. The product data is stored in an Amazon RDS for MySQL database table with over 10 million rows, utilizing 2 TB of General Purpose SSD storage. The platform experiences millions of updates daily through user interactions.\n\nThe company has observed that some insert operations take more than 10 seconds to complete and identified the database storage performance as the bottleneck.\n\nWhich solution resolves this performance issue?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Change the storage type to Provisioned IOPS SSD.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Alter the DB instance to utilize a memory-optimized instance class.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Modify the DB instance to use a burstable performance instance class.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Configure Multi-AZ RDS read replicas using MySQL native asynchronous replication.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nChange the storage type to Provisioned IOPS SSD.\n\nProvisioned IOPS SSD (io1 and io2) storage is designed to provide fast, predictable, and consistent I/O performance. By switching to Provisioned IOPS SSD, the company can provision the IOPS tailored to their workload requirements, ensuring faster and more consistent performance for write-intensive operations like insertions, addressing the performance bottleneck.\n\n\n\n\n\n\n\nIncorrect Options:\n\nModify the DB instance to use a burstable performance instance class.\n\nBurstable performance instances are designed to provide a baseline level of CPU performance with the ability to burst to a higher level when required. They are not the best choice for resolving storage performance issues.\n\n\n\n\nAlter the DB instance to utilize a memory-optimized instance class.\n\nMemory-optimized instances are designed for workloads that require high memory access speed but do not directly address storage performance issues.\n\n\n\n\nConfigure Multi-AZ RDS read replicas using MySQL native asynchronous replication.\n\nWhile read replicas can help distribute read traffic and improve read performance, they do not directly address storage performance issues related to write operations like insertions.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html",
      "correctAnswerExplanations": [
        {
          "answer": "Change the storage type to Provisioned IOPS SSD.",
          "explanation": "Provisioned IOPS SSD (io1 and io2) storage is designed to provide fast, predictable, and consistent I/O performance. By switching to Provisioned IOPS SSD, the company can provision the IOPS tailored to their workload requirements, ensuring faster and more consistent performance for write-intensive operations like insertions, addressing the performance bottleneck."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Modify the DB instance to use a burstable performance instance class.",
          "explanation": "Burstable performance instances are designed to provide a baseline level of CPU performance with the ability to burst to a higher level when required. They are not the best choice for resolving storage performance issues."
        },
        {
          "answer": "Alter the DB instance to utilize a memory-optimized instance class.",
          "explanation": "Memory-optimized instances are designed for workloads that require high memory access speed but do not directly address storage performance issues."
        },
        {
          "answer": "Configure Multi-AZ RDS read replicas using MySQL native asynchronous replication.",
          "explanation": "While read replicas can help distribute read traffic and improve read performance, they do not directly address storage performance issues related to write operations like insertions."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html"
      ]
    },
    {
      "id": 40,
      "question": "A solutions architect is designing a hybrid cloud solution to extend a company's on-premises infrastructure to AWS. The company needs a reliable connection with consistently low latency to an AWS Region. The company is willing to minimize costs and accept slower traffic if the primary connection fails.\n\nWhich solution should the solutions architect recommend to meet these requirements?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Set up an AWS Direct Connect connection to a Region, and use a VPN connection as a backup if the primary Direct Connect connection fails.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Create an AWS Direct Connect connection to a Region, and utilize the Direct Connect failover attribute from the AWS CLI to automatically generate a backup connection if the primary Direct Connect connection fails.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Configure a VPN connection to a Region for private connectivity, and establish a second VPN connection for private connectivity and as a backup if the primary VPN connection fails.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Establish an AWS Direct Connect connection to a Region, and create a second Direct Connect connection to the same Region as a backup if the primary Direct Connect connection fails.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nSet up an AWS Direct Connect connection to a Region, and use a VPN connection as a backup if the primary Direct Connect connection fails.\n\nAWS Direct Connect provides a dedicated, private connection between the company's on-premises infrastructure and an AWS Region. It offers a reliable, high-bandwidth connection with low latency, making it ideal for the company's needs. In the event of a primary connection failure, a VPN connection can serve as a backup, providing slower traffic but maintaining connectivity until the primary connection is restored.\n\nThe VPN connection provides a cost-effective backup option and enables the organization to establish a secure connection between their on-premises infrastructure and AWS over the internet.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure a VPN connection to a Region for private connectivity, and establish a second VPN connection for private connectivity and as a backup if the primary VPN connection fails.\n\nThis does not provide the low-latency connection the company requires, as both primary and backup connections rely on VPNs, which typically have higher latency compared to Direct Connect.\n\n\n\n\nEstablish an AWS Direct Connect connection to a Region, and create a second Direct Connect connection to the same Region as a backup if the primary Direct Connect connection fails.\n\nThis offers high reliability and low latency but does not minimize costs, as it requires two Direct Connect connections, which are more expensive than using a VPN connection as a backup.\n\n\n\n\nCreate an AWS Direct Connect connection to a Region, and utilize the Direct Connect failover attribute from the AWS CLI to automatically generate a backup connection if the primary Direct Connect connection fails.\n\nThere is no such attribute as Direct Connect failover in the AWS CLI. It is a misleading option.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/directconnect/\n\nhttps://aws.amazon.com/vpn/",
      "correctAnswerExplanations": [
        {
          "answer": "Set up an AWS Direct Connect connection to a Region, and use a VPN connection as a backup if the primary Direct Connect connection fails.",
          "explanation": "AWS Direct Connect provides a dedicated, private connection between the company's on-premises infrastructure and an AWS Region. It offers a reliable, high-bandwidth connection with low latency, making it ideal for the company's needs. In the event of a primary connection failure, a VPN connection can serve as a backup, providing slower traffic but maintaining connectivity until the primary connection is restored."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Configure a VPN connection to a Region for private connectivity, and establish a second VPN connection for private connectivity and as a backup if the primary VPN connection fails.",
          "explanation": "This does not provide the low-latency connection the company requires, as both primary and backup connections rely on VPNs, which typically have higher latency compared to Direct Connect."
        },
        {
          "answer": "Establish an AWS Direct Connect connection to a Region, and create a second Direct Connect connection to the same Region as a backup if the primary Direct Connect connection fails.",
          "explanation": "This offers high reliability and low latency but does not minimize costs, as it requires two Direct Connect connections, which are more expensive than using a VPN connection as a backup."
        },
        {
          "answer": "Create an AWS Direct Connect connection to a Region, and utilize the Direct Connect failover attribute from the AWS CLI to automatically generate a backup connection if the primary Direct Connect connection fails.",
          "explanation": "There is no such attribute as Direct Connect failover in the AWS CLI. It is a misleading option."
        }
      ],
      "references": [
        "https://aws.amazon.com/directconnect/",
        "https://aws.amazon.com/vpn/"
      ]
    },
    {
      "id": 41,
      "question": "A company is developing a web application that processes user-generated content. The application uses an AWS Lambda function to receive data via Amazon API Gateway and stores the data in an Amazon RDS MySQL database. During peak hours, the Lambda function must handle high volumes of incoming data, causing the company to significantly increase Lambda quotas.\n\nA solutions architect needs to recommend a new design to enhance scalability and minimize configuration efforts. Which solution meets these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Migrate the database from RDS MySQL to Amazon DynamoDB. Utilize DynamoDB Streams to trigger a Lambda function for processing the incoming data.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Set up two Lambda functions. Configure one function to receive the data. Configure the other function to load the data into the database. Integrate the Lambda functions using Amazon Simple Queue Service (Amazon SQS) queue.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Create two Lambda functions. One function receives the data, and the other loads the data into the database. Use Amazon EventBridge to trigger the second Lambda function based on events from the first Lambda function.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Refactor the Lambda function to a Node.js application running on Amazon EC2 instances. Connect to the database using native Node.js MySQL drivers.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nSet up two Lambda functions. Configure one function to receive the data. Configure the other function to load the data into the database. Integrate the Lambda functions using Amazon Simple Queue Service (Amazon SQS) queue.\n\nThis solution enhances scalability by splitting the functionality of the Lambda function into two separate functions. One function is responsible for receiving the data, while the other function is responsible for loading the data into the database. This design allows for horizontal scaling of the load and efficient use of resources. Additionally, the use of an Amazon SQS queue allows for asynchronous processing of incoming data, further enhancing scalability.\n\nThis solution also minimizes configuration efforts, as the company can configure the Lambda functions and the SQS queue using AWS Management Console or AWS CLI. There is no need to manage infrastructure or handle scaling manually.\n\n\n\n\n\n\n\nIncorrect Options:\n\nRefactor the Lambda function to a Node.js application running on Amazon EC2 instances.\n\nAlthough this solution might improve scalability, it increases the configuration efforts and moves away from the serverless architecture, which may not be the most cost-effective approach.\n\n\n\n\nMigrate the database from RDS MySQL to Amazon DynamoDB.\n\nThis solution requires a significant architectural change and may not be the most cost-effective choice, as it introduces the additional complexity of using DynamoDB Streams.\n\n\n\n\nCreate two Lambda functions and use Amazon EventBridge to trigger the second Lambda function based on events from the first Lambda function.\n\nWhile using EventBridge can provide some scalability improvements, it doesn't offer the same level of scalability and reliability as an Amazon SQS queue.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/aws-lambda-adds-amazon-simple-queue-service-to-supported-event-sources",
      "correctAnswerExplanations": [
        {
          "answer": "Set up two Lambda functions. Configure one function to receive the data. Configure the other function to load the data into the database. Integrate the Lambda functions using Amazon Simple Queue Service (Amazon SQS) queue.",
          "explanation": "This solution enhances scalability by splitting the functionality of the Lambda function into two separate functions. One function is responsible for receiving the data, while the other function is responsible for loading the data into the database. This design allows for horizontal scaling of the load and efficient use of resources. Additionally, the use of an Amazon SQS queue allows for asynchronous processing of incoming data, further enhancing scalability."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Refactor the Lambda function to a Node.js application running on Amazon EC2 instances.",
          "explanation": "Although this solution might improve scalability, it increases the configuration efforts and moves away from the serverless architecture, which may not be the most cost-effective approach."
        },
        {
          "answer": "Migrate the database from RDS MySQL to Amazon DynamoDB.",
          "explanation": "This solution requires a significant architectural change and may not be the most cost-effective choice, as it introduces the additional complexity of using DynamoDB Streams."
        },
        {
          "answer": "Create two Lambda functions and use Amazon EventBridge to trigger the second Lambda function based on events from the first Lambda function.",
          "explanation": "While using EventBridge can provide some scalability improvements, it doesn't offer the same level of scalability and reliability as an Amazon SQS queue."
        }
      ],
      "references": [
        "https://aws.amazon.com/blogs/aws/aws-lambda-adds-amazon-simple-queue-service-to-supported-event-sources"
      ]
    },
    {
      "id": 42,
      "question": "A company has an e-commerce platform that stores its product inventory data on an Amazon EC2 instance store. The company wants to enhance the availability and durability of its inventory data.\n\nWhat should a solutions architect do to achieve these objectives with the LEAST operational overhead?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Scale up the EC2 instance with a larger instance store.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Move the inventory data to an Amazon RDS database.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Migrate the inventory data to Amazon Elastic Block (Amazon EBS).",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Transfer the inventory data to Amazon S3 Glacier Deep Archive.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nMigrate the inventory data to Amazon Elastic Block (Amazon EBS).\n\nBy migrating the inventory data from the EC2 instance store to an Amazon EBS volume, the company can achieve higher durability and availability for its data. Amazon EBS volumes offer replication within an Availability Zone, which protects the data from component failure. Moreover, EBS volumes can be easily backed up as snapshots and can be attached to other EC2 instances, providing additional flexibility for failover and scaling operations.\n\n\n\n\n\n\n\nIncorrect Options:\n\nScale up the EC2 instance with a larger instance store.\n\nScaling up the EC2 instance with a larger instance store does not improve the durability or availability of the inventory data. Instance stores are ephemeral and do not persist data upon instance stop or termination, which could lead to data loss.\n\n\n\n\nTransfer the inventory data to Amazon S3 Glacier Deep Archive.\n\nAmazon S3 Glacier Deep Archive is designed for long-term, low-cost storage of infrequently accessed data. It is not suitable for the inventory data of an e-commerce platform, which requires frequent and low-latency access for optimal performance.\n\n\n\n\nMove the inventory data to an Amazon RDS database.\n\nAlthough an Amazon RDS database could improve data availability and durability, it would require significant changes to the application and may not be the most effective solution for our case. Amazon EBS provides a simpler and more efficient option for enhancing the data's durability and availability.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/ebs/\n\nhttps://aws.amazon.com/ec2/instance-store/",
      "correctAnswerExplanations": [
        {
          "answer": "Migrate the inventory data to Amazon Elastic Block (Amazon EBS).",
          "explanation": "By migrating the inventory data from the EC2 instance store to an Amazon EBS volume, the company can achieve higher durability and availability for its data. Amazon EBS volumes offer replication within an Availability Zone, which protects the data from component failure. Moreover, EBS volumes can be easily backed up as snapshots and can be attached to other EC2 instances, providing additional flexibility for failover and scaling operations."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Scale up the EC2 instance with a larger instance store.",
          "explanation": "Scaling up the EC2 instance with a larger instance store does not improve the durability or availability of the inventory data. Instance stores are ephemeral and do not persist data upon instance stop or termination, which could lead to data loss."
        },
        {
          "answer": "Transfer the inventory data to Amazon S3 Glacier Deep Archive.",
          "explanation": "Amazon S3 Glacier Deep Archive is designed for long-term, low-cost storage of infrequently accessed data. It is not suitable for the inventory data of an e-commerce platform, which requires frequent and low-latency access for optimal performance."
        },
        {
          "answer": "Move the inventory data to an Amazon RDS database.",
          "explanation": "Although an Amazon RDS database could improve data availability and durability, it would require significant changes to the application and may not be the most effective solution for our case. Amazon EBS provides a simpler and more efficient option for enhancing the data's durability and availability."
        }
      ],
      "references": [
        "https://aws.amazon.com/ebs/",
        "https://aws.amazon.com/ec2/instance-store/"
      ]
    },
    {
      "id": 43,
      "question": "A company is using Amazon CloudWatch dashboard to visualize application metrics. The company's project manager needs to access these visualizations periodically, but does not have an AWS account. A solutions architect must provide access to the project manager while adhering to the principle of least privilege.\n\nWhich solution will meet these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Deploy a bastion server in a public subnet. When the project manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an IAM user for the company's employees. Attach the ViewOnlyAccess AWS managed policy to the IAM user. Share the new login credentials with the project manager. Ask the project manager to navigate to the QuickSight console and locate the dashboard by name in the Dashboards section.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Share the CloudWatch dashboard from the console. Enter the project manager's email address, and complete the sharing process. Provide a shareable link for the dashboard to the project manager.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Create an IAM user specifically for the project manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentials with the project manager. Share the browser URL of the correct dashboard with the project manager.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nShare the CloudWatch dashboard from the console. Enter the project manager's email address, and complete the sharing process. Provide a shareable link for the dashboard to the project manager.\n\nThis solution adheres to the principle of least privilege by not creating unnecessary IAM users or giving the project manager access to an AWS account. Sharing the dashboard from the console allows the solutions architect to specify the exact dashboard the project manager can access, reducing the risk of unauthorized access to other AWS resources.\n\nThe shareable link for the dashboard can be accessed without logging in to an AWS account, making it easy for the project manager to view the dashboard periodically. The link can also be set to expire after a specific time period to ensure that access is limited to the project manager.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an IAM user specifically for the project manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentials with the project manager. Share the browser URL of the correct dashboard with the project manager.\n\nCreating a new IAM user for the project manager can be unnecessary, as it may not align with the principle of least privilege. Additionally, sharing login credentials for an IAM user can pose a security risk if the credentials fall into the wrong hands.\n\n\n\n\nCreate an IAM user for the company's employees. Attach the ViewOnlyAccess AWS managed policy to the IAM user. Share the new login credentials with the project manager. Ask the project manager to navigate to the QuickSight console and locate the dashboard by name in the Dashboards section.\n\nThis is not suitable as it provides more access than necessary to the project manager. It also requires the project manager to navigate to the QuickSight console, which can be confusing and time-consuming.\n\n\n\n\nDeploy a bastion server in a public subnet. When the project manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard.\n\nThis is overly complex and can be expensive to maintain. Deploying a bastion server can also pose a security risk if not configured properly. Sharing RDP credentials can also pose a security risk if the credentials fall into the wrong hands.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html",
      "correctAnswerExplanations": [
        {
          "answer": "Share the CloudWatch dashboard from the console. Enter the project manager's email address, and complete the sharing process. Provide a shareable link for the dashboard to the project manager.",
          "explanation": "This solution adheres to the principle of least privilege by not creating unnecessary IAM users or giving the project manager access to an AWS account. Sharing the dashboard from the console allows the solutions architect to specify the exact dashboard the project manager can access, reducing the risk of unauthorized access to other AWS resources."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Create an IAM user specifically for the project manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentials with the project manager. Share the browser URL of the correct dashboard with the project manager.",
          "explanation": "Creating a new IAM user for the project manager can be unnecessary, as it may not align with the principle of least privilege. Additionally, sharing login credentials for an IAM user can pose a security risk if the credentials fall into the wrong hands."
        },
        {
          "answer": "Create an IAM user for the company's employees. Attach the ViewOnlyAccess AWS managed policy to the IAM user. Share the new login credentials with the project manager. Ask the project manager to navigate to the QuickSight console and locate the dashboard by name in the Dashboards section.",
          "explanation": "This is not suitable as it provides more access than necessary to the project manager. It also requires the project manager to navigate to the QuickSight console, which can be confusing and time-consuming."
        },
        {
          "answer": "Deploy a bastion server in a public subnet. When the project manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard.",
          "explanation": "This is overly complex and can be expensive to maintain. Deploying a bastion server can also pose a security risk if not configured properly. Sharing RDP credentials can also pose a security risk if the credentials fall into the wrong hands."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html"
      ]
    },
    {
      "id": 44,
      "question": "A weather analytics company collects over 50 TB of data worldwide daily. The company needs a scalable and cost-effective solution to process and analyze this data in real-time to generate insights and provide forecasts.\n\nWhat should a solutions architect recommend for processing and analyzing the weather data?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon Kinesis Data Streams for real-time data collection, then use Amazon Kinesis Data Analytics for processing, and store the processed data in Amazon S3 for further analysis using Amazon Athena.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Configure an Auto Scaling group of Amazon EC2 instances to process the data, store the processed data in Amazon S3, and load the data into Amazon Redshift for analysis.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use Amazon CloudFront for caching, store the data in Amazon S3, and trigger an AWS Lambda function upon object creation in the S3 bucket for processing and analysis.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Create an Amazon S3 bucket to store the data, then set up an Amazon EMR cluster for processing, and finally, use Amazon QuickSight for analysis and visualization.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nUse Amazon Kinesis Data Streams for real-time data collection, then use Amazon Kinesis Data Analytics for processing, and store the processed data in Amazon S3 for further analysis using Amazon Athena.\n\nAmazon Kinesis Data Streams is an ideal solution for real-time data collection from a variety of sources worldwide. The solution can handle large volumes of data with low latency and provide real-time analytics capabilities. The data can be processed using Amazon Kinesis Data Analytics, which can easily scale to accommodate the high volume of data. The processed data can then be stored in Amazon S3, which provides a scalable, durable, and cost-effective storage solution. Amazon Athena can then be used to query and analyze the data stored in S3. This solution offers real-time processing and analysis of large volumes of data at a low cost with minimal operational overhead.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an Amazon S3 bucket to store the data, then set up an Amazon EMR cluster for processing, and finally, use Amazon QuickSight for analysis and visualization.\n\nIt does not address the real-time data processing requirement. Amazon EMR is better suited for batch processing, rather than real-time processing.\n\n\n\n\nUse Amazon CloudFront for caching, store the data in Amazon S3, and trigger an AWS Lambda function upon object creation in the S3 bucket for processing and analysis.\n\nAmazon CloudFront is a content delivery network (CDN) service, not a real-time data processing solution. Additionally, using AWS Lambda for such a large volume of data would not be cost-effective or scalable.\n\n\n\n\nConfigure an Auto Scaling group of Amazon EC2 instances to process the data, store the processed data in Amazon S3, and load the data into Amazon Redshift for analysis.\n\nIt requires manual management of EC2 instances and does not provide a real-time data processing solution. Furthermore, it does not address the cost-effectiveness requirement.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/kinesis/data-streams/\n\nhttps://aws.amazon.com/kinesis/data-analytics/",
      "correctAnswerExplanations": [
        {
          "answer": "Use Amazon Kinesis Data Streams for real-time data collection, then use Amazon Kinesis Data Analytics for processing, and store the processed data in Amazon S3 for further analysis using Amazon Athena.",
          "explanation": "Amazon Kinesis Data Streams is an ideal solution for real-time data collection from a variety of sources worldwide. The solution can handle large volumes of data with low latency and provide real-time analytics capabilities. The data can be processed using Amazon Kinesis Data Analytics, which can easily scale to accommodate the high volume of data. The processed data can then be stored in Amazon S3, which provides a scalable, durable, and cost-effective storage solution. Amazon Athena can then be used to query and analyze the data stored in S3. This solution offers real-time processing and analysis of large volumes of data at a low cost with minimal operational overhead."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Create an Amazon S3 bucket to store the data, then set up an Amazon EMR cluster for processing, and finally, use Amazon QuickSight for analysis and visualization.",
          "explanation": "It does not address the real-time data processing requirement. Amazon EMR is better suited for batch processing, rather than real-time processing."
        },
        {
          "answer": "Use Amazon CloudFront for caching, store the data in Amazon S3, and trigger an AWS Lambda function upon object creation in the S3 bucket for processing and analysis.",
          "explanation": "Amazon CloudFront is a content delivery network (CDN) service, not a real-time data processing solution. Additionally, using AWS Lambda for such a large volume of data would not be cost-effective or scalable."
        },
        {
          "answer": "Configure an Auto Scaling group of Amazon EC2 instances to process the data, store the processed data in Amazon S3, and load the data into Amazon Redshift for analysis.",
          "explanation": "It requires manual management of EC2 instances and does not provide a real-time data processing solution. Furthermore, it does not address the cost-effectiveness requirement."
        }
      ],
      "references": [
        "https://aws.amazon.com/kinesis/data-streams/",
        "https://aws.amazon.com/kinesis/data-analytics/"
      ]
    },
    {
      "id": 45,
      "question": "A solutions architect is designing a VPC infrastructure that will host applications using Amazon EC2 instances and Amazon RDS DB instances. The infrastructure will consist of six subnets in two Availability Zones. Each Availability Zone will have a public subnet, a private subnet, and a dedicated subnet for databases. The requirement is that only EC2 instances in the private subnets can access the RDS databases.\n\nWhich solution will fulfill these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Develop a security group that denies inbound traffic from the security group assigned to instances in the public subnets. Attach the security group to the DB instances.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Set up a new peering connection between the public and private subnets. Create another peering connection between the private subnets and the database subnets.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Construct a security group that permits inbound traffic from the security group assigned to instances in the private subnets. Attach the security group to the DB instances.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Establish a new route table that omits the route to the public subnets' CIDR blocks. Associate the route table with the database subnets.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nConstruct a security group that permits inbound traffic from the security group assigned to instances in the private subnets. Attach the security group to the DB instances.\n\nTo allow only EC2 instances in private subnets to access RDS DB instances, we can use security groups. Security groups act as virtual firewalls that control inbound and outbound traffic to instances.\n\nWe can create a security group for the RDS instances that only permits inbound traffic from the security group assigned to instances in the private subnets. By doing so, it ensure that only the instances in private subnets can access the RDS databases, and instances in public subnets are denied access. This provides a secure and scalable solution that allows the private subnets to access the RDS databases without exposing them to the public internet.\n\n\n\n\n\n\n\nIncorrect Options:\n\nEstablish a new route table that omits the route to the public subnets' CIDR blocks. Associate the route table with the database subnets.\n\nThis does not restrict access to the RDS databases, as it only alters the route table for the database subnets and does not control inbound traffic.\n\n\n\n\nDevelop a security group that denies inbound traffic from the security group assigned to instances in the public subnets. Attach the security group to the DB instances.\n\nIt only denies traffic from the public subnets and does not explicitly allow traffic from the private subnets, which is required to meet the requirement.\n\n\n\n\nSet up a new peering connection between the public and private subnets. Create another peering connection between the private subnets and the database subnets.\n\nPeering connections are used to establish communication between VPCs, not between subnets within the same VPC.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html",
      "correctAnswerExplanations": [
        {
          "answer": "Construct a security group that permits inbound traffic from the security group assigned to instances in the private subnets. Attach the security group to the DB instances.",
          "explanation": "To allow only EC2 instances in private subnets to access RDS DB instances, we can use security groups. Security groups act as virtual firewalls that control inbound and outbound traffic to instances."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Establish a new route table that omits the route to the public subnets' CIDR blocks. Associate the route table with the database subnets.",
          "explanation": "This does not restrict access to the RDS databases, as it only alters the route table for the database subnets and does not control inbound traffic."
        },
        {
          "answer": "Develop a security group that denies inbound traffic from the security group assigned to instances in the public subnets. Attach the security group to the DB instances.",
          "explanation": "It only denies traffic from the public subnets and does not explicitly allow traffic from the private subnets, which is required to meet the requirement."
        },
        {
          "answer": "Set up a new peering connection between the public and private subnets. Create another peering connection between the private subnets and the database subnets.",
          "explanation": "Peering connections are used to establish communication between VPCs, not between subnets within the same VPC."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html"
      ]
    },
    {
      "id": 46,
      "question": "A company has an application that processes incoming messages from various sources. These messages are then consumed by multiple applications and microservices. The volume of messages can vary greatly and may suddenly spike to 100,000 per second. The company wants to decouple the architecture and improve scalability.\n\nWhich solution meets these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Persist the messages to Amazon Kinesis Data Analytics, then configure the consumer applications to read and process the messages.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Publish the messages to an Amazon Simple Notification Service (SNS) topic with multiple Amazon Simple Queue Service (SQS) subscriptions. Configure the consumer applications to process the messages from the queues.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group, scaling the number of instances based on CPU metrics.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Store the messages in Amazon Kinesis Data Streams, then use an AWS Lambda function to preprocess them and store them in Amazon DynamoDB. Configure the consumer applications to read and process the messages from DynamoDB.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nPublish the messages to an Amazon Simple Notification Service (SNS) topic with multiple Amazon Simple Queue Service (SQS) subscriptions. Configure the consumer applications to process the messages from the queues.\n\nBy using Amazon SNS to publish messages to a topic and Amazon SQS for multiple subscriptions, the company can effectively decouple the ingestion application from the consumer applications. This solution provides increased scalability, as it allows the consumer applications to process the messages from the queues independently and at their own pace.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStore the messages in Amazon Kinesis Data Streams, then use an AWS Lambda function to preprocess them and store them in Amazon DynamoDB. Configure the consumer applications to read and process the messages from DynamoDB.\n\nWhile this solution decouples the architecture, it does not provide optimal scalability for a sudden spike of messages. A single shard in Kinesis Data Streams may not be sufficient to handle the volume of incoming messages.\n\n\n\n\nDeploy the ingestion application on Amazon EC2 instances in an Auto Scaling group, scaling the number of instances based on CPU metrics.\n\nThis does not decouple the solution, and the ingestion application still remains a potential bottleneck for processing messages.\n\n\n\n\nPersist the messages to Amazon Kinesis Data Analytics, then configure the consumer applications to read and process the messages.\n\nAmazon Kinesis Data Analytics is primarily used for processing real-time streaming data for analytics, not for handling and scaling message ingestion for multiple consumer applications.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-common-scenarios.html",
      "correctAnswerExplanations": [
        {
          "answer": "Publish the messages to an Amazon Simple Notification Service (SNS) topic with multiple Amazon Simple Queue Service (SQS) subscriptions. Configure the consumer applications to process the messages from the queues.",
          "explanation": "By using Amazon SNS to publish messages to a topic and Amazon SQS for multiple subscriptions, the company can effectively decouple the ingestion application from the consumer applications. This solution provides increased scalability, as it allows the consumer applications to process the messages from the queues independently and at their own pace."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Store the messages in Amazon Kinesis Data Streams, then use an AWS Lambda function to preprocess them and store them in Amazon DynamoDB. Configure the consumer applications to read and process the messages from DynamoDB.",
          "explanation": "While this solution decouples the architecture, it does not provide optimal scalability for a sudden spike of messages. A single shard in Kinesis Data Streams may not be sufficient to handle the volume of incoming messages."
        },
        {
          "answer": "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group, scaling the number of instances based on CPU metrics.",
          "explanation": "This does not decouple the solution, and the ingestion application still remains a potential bottleneck for processing messages."
        },
        {
          "answer": "Persist the messages to Amazon Kinesis Data Analytics, then configure the consumer applications to read and process the messages.",
          "explanation": "Amazon Kinesis Data Analytics is primarily used for processing real-time streaming data for analytics, not for handling and scaling message ingestion for multiple consumer applications."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/sns/latest/dg/sns-common-scenarios.html"
      ]
    },
    {
      "id": 47,
      "question": "A company has a platform for businesses to upload their customer data for analysis to provide personalized marketing solutions. The businesses upload their data through SFTP, and the data is processed to generate marketing insights. Some files can be larger than 200 GB. The company discovered that some businesses have uploaded files containing personally identifiable information (PII), which should not be included. The company wants to be alerted if PII is uploaded again and automate the remediation process.\n\nWhat should a solutions architect do to meet these requirements with the LEAST development effort?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use an Amazon S3 bucket for secure data transfer. Use Amazon GuardDuty to scan the objects in the bucket. If objects contain PII, configure Amazon Simple Notification Service (Amazon SNS) to alert the administrators and trigger an S3 Lifecycle policy to remove the objects containing PII.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use an Amazon S3 bucket for secure data transfer. Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to alert the administrators and automate the removal of objects containing PII.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Use an Amazon S3 bucket for secure data transfer. Use Amazon Macie to scan the objects in the bucket. If objects contain PII, configure Amazon Simple Notification Service (Amazon SNS) to alert the administrators and automate the removal of objects containing PII.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Use an Amazon S3 bucket for secure data transfer. Use Amazon Inspector to scan the objects in the bucket. If objects contain PII, configure Amazon Simple Email Service (Amazon SES) to alert the administrators and trigger an S3 Lifecycle policy to remove the objects containing PII.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nUse an Amazon S3 bucket for secure data transfer. Use Amazon Macie to scan the objects in the bucket. If objects contain PII, configure Amazon Simple Notification Service (Amazon SNS) to alert the administrators and automate the removal of objects containing PII.\n\nAmazon Macie is a fully managed data security and data privacy service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. It is the appropriate service for scanning and identifying PII in the objects stored in an S3 bucket. By configuring Amazon SNS to alert administrators when PII is detected, the company can stay informed and automate the removal of objects containing PII.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse an Amazon S3 bucket for secure data transfer. Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to alert the administrators and automate the removal of objects containing PII.\n\nDeveloping custom scanning algorithms requires more development effort than using Amazon Macie, which is designed to discover and protect sensitive data.\n\n\n\n\nUse an Amazon S3 bucket for secure data transfer. Use Amazon GuardDuty to scan the objects in the bucket. If objects contain PII, configure Amazon Simple Notification Service (Amazon SNS) to alert the administrators and trigger an S3 Lifecycle policy to remove the objects containing PII.\n\nAmazon GuardDuty is a threat detection service, not specifically designed to detect PII in objects stored in an S3 bucket. Amazon Macie is the appropriate service for this use case.\n\n\n\n\nUse an Amazon S3 bucket for secure data transfer. Use Amazon Inspector to scan the objects in the bucket. If objects contain PII, configure Amazon Simple Email Service (Amazon SES) to alert the administrators and trigger an S3 Lifecycle policy to remove the objects containing PII.\n\nAmazon Inspector is a security assessment service for applications running on Amazon EC2, not for detecting PII in objects stored in an S3 bucket\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/macie/latest/user/what-is-macie.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use an Amazon S3 bucket for secure data transfer. Use Amazon Macie to scan the objects in the bucket. If objects contain PII, configure Amazon Simple Notification Service (Amazon SNS) to alert the administrators and automate the removal of objects containing PII.",
          "explanation": "Amazon Macie is a fully managed data security and data privacy service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. It is the appropriate service for scanning and identifying PII in the objects stored in an S3 bucket. By configuring Amazon SNS to alert administrators when PII is detected, the company can stay informed and automate the removal of objects containing PII."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use an Amazon S3 bucket for secure data transfer. Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to alert the administrators and automate the removal of objects containing PII.",
          "explanation": "Developing custom scanning algorithms requires more development effort than using Amazon Macie, which is designed to discover and protect sensitive data."
        },
        {
          "answer": "Use an Amazon S3 bucket for secure data transfer. Use Amazon GuardDuty to scan the objects in the bucket. If objects contain PII, configure Amazon Simple Notification Service (Amazon SNS) to alert the administrators and trigger an S3 Lifecycle policy to remove the objects containing PII.",
          "explanation": "Amazon GuardDuty is a threat detection service, not specifically designed to detect PII in objects stored in an S3 bucket. Amazon Macie is the appropriate service for this use case."
        },
        {
          "answer": "Use an Amazon S3 bucket for secure data transfer. Use Amazon Inspector to scan the objects in the bucket. If objects contain PII, configure Amazon Simple Email Service (Amazon SES) to alert the administrators and trigger an S3 Lifecycle policy to remove the objects containing PII.",
          "explanation": "Amazon Inspector is a security assessment service for applications running on Amazon EC2, not for detecting PII in objects stored in an S3 bucket"
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/macie/latest/user/what-is-macie.html"
      ]
    },
    {
      "id": 48,
      "question": "A company is developing a video-sharing platform where users can upload and share their videos. To ensure the platform remains family-friendly, the company wants to automatically detect and flag any inappropriate content in the uploaded videos while keeping development effort to a minimum.\n\nWhat should a solutions architect recommend to meet these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon Transcribe to analyze video content for inappropriate language. Use human review for transcripts with uncertain predictions.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use Amazon Rekognition Video to analyze and identify inappropriate content in videos. Use human review for videos with low-confidence results.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Use Amazon SageMaker to build a custom machine learning model for detecting inappropriate content. Use ground truth labeling for videos with low-confidence predictions.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Deploy a custom machine learning model using AWS Fargate to analyze video content. Use ground truth labeling for videos with low-confidence predictions.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nUse Amazon Rekognition Video to analyze and identify inappropriate content in videos. Use human review for videos with low-confidence results.\n\nAmazon Rekognition Video is a computer vision service that can analyze video content to detect objects, scenes, and activities. It also supports text detection and recognition, and face analysis. To detect inappropriate content in videos, Amazon Rekognition Video can be trained to identify specific content categories such as nudity, violence, and explicit language. This process is known as content moderation. If the service detects content that matches the specified categories, it can flag the video for human review. Human review can then be employed to handle videos with low-confidence results, ensuring accurate content moderation.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon Transcribe to analyze video content for inappropriate language. Use human review for transcripts with uncertain predictions.\n\nAmazon Transcribe focuses on converting speech to text and does not analyze visual content in videos.\n\n\n\n\nUse Amazon SageMaker to build a custom machine learning model for detecting inappropriate content. Use ground truth labeling for videos with low-confidence predictions.\n\nIt requires more development effort compared to using the pre-trained Amazon Rekognition Video service.\n\n\n\n\nDeploy a custom machine learning model using AWS Fargate to analyze video content. Utilize ground truth labeling for videos with low-confidence predictions.\n\nIt involves developing and deploying a custom machine learning model, which is more time-consuming and requires more effort compared to using Amazon Rekognition Video.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rekognition/video-features/",
      "correctAnswerExplanations": [
        {
          "answer": "Use Amazon Rekognition Video to analyze and identify inappropriate content in videos. Use human review for videos with low-confidence results.",
          "explanation": "Amazon Rekognition Video is a computer vision service that can analyze video content to detect objects, scenes, and activities. It also supports text detection and recognition, and face analysis. To detect inappropriate content in videos, Amazon Rekognition Video can be trained to identify specific content categories such as nudity, violence, and explicit language. This process is known as content moderation. If the service detects content that matches the specified categories, it can flag the video for human review. Human review can then be employed to handle videos with low-confidence results, ensuring accurate content moderation."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use Amazon Transcribe to analyze video content for inappropriate language. Use human review for transcripts with uncertain predictions.",
          "explanation": "Amazon Transcribe focuses on converting speech to text and does not analyze visual content in videos."
        },
        {
          "answer": "Use Amazon SageMaker to build a custom machine learning model for detecting inappropriate content. Use ground truth labeling for videos with low-confidence predictions.",
          "explanation": "It requires more development effort compared to using the pre-trained Amazon Rekognition Video service."
        },
        {
          "answer": "Deploy a custom machine learning model using AWS Fargate to analyze video content. Utilize ground truth labeling for videos with low-confidence predictions.",
          "explanation": "It involves developing and deploying a custom machine learning model, which is more time-consuming and requires more effort compared to using Amazon Rekognition Video."
        }
      ],
      "references": [
        "https://aws.amazon.com/rekognition/video-features/"
      ]
    },
    {
      "id": 49,
      "question": "A media company wants to launch a streaming platform on AWS that promotes a movie every day for a period of 24 hours. The company needs to handle millions of requests each hour with millisecond latency during peak hours.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Deploy the streaming platform on Amazon EC2 instances in Auto Scaling groups across multiple Availability Zones. Use an Application Load Balancer (ALB) for distributing traffic. Add another ALB for backend APIs. Store the data in Amazon RDS for MySQL.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Migrate the entire streaming platform to containers. Host the containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use the Kubernetes Cluster Autoscaler to adjust the number of pods based on traffic. Store the data in Amazon RDS for MySQL.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Host the static contents in an Amazon S3 bucket. Use Amazon CloudFront to distribute the content. Set the S3 bucket as the origin. Implement Amazon API Gateway and AWS Lambda functions for backend APIs. Store user and movie data in Amazon DynamoDB.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Host the full streaming platform in different Amazon S3 buckets. Add Amazon CloudFront distributions. Set the S3 buckets as origins for the distributions. Store user and movie data in Amazon S3.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nHost the static contents in an Amazon S3 bucket. Use Amazon CloudFront to distribute the content. Set the S3 bucket as the origin. Implement Amazon API Gateway and AWS Lambda functions for backend APIs. Store user and movie data in Amazon DynamoDB.\n\nthe static contents are hosted on Amazon S3, which is a cost-effective and scalable storage solution. Amazon CloudFront is used to distribute content, which ensures low latency and high transfer speeds. Amazon API Gateway and AWS Lambda functions are used for backend APIs, which allows for automatic scaling and minimal operational overhead. User and movie data is stored in Amazon DynamoDB, which is a managed NoSQL database that can handle high read and write throughput with low latency.\n\nThis solution meets the requirements of handling millions of requests each hour with millisecond latency during peak hours, and it also minimizes operational overhead. Amazon S3 and DynamoDB are fully managed services, so the media company doesn't have to worry about managing infrastructure or scaling resources.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDeploy the streaming platform on Amazon EC2 instances in Auto Scaling groups across multiple Availability Zones. Use an Application Load Balancer (ALB) for distributing traffic. Add another ALB for backend APIs. Store the data in Amazon RDS for MySQL.\n\nThis option requires more operational overhead as it involves managing EC2 instances, Auto Scaling groups, and ALBs.\n\n\n\n\nMigrate the entire streaming platform to containers. Host the containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use the Kubernetes Cluster Autoscaler to adjust the number of pods based on traffic. Store the data in Amazon RDS for MySQL.\n\nThis option involves managing Kubernetes and EKS, which increases operational overhead compared to the serverless solutions.\n\n\n\n\nHost the full streaming platform in different Amazon S3 buckets. Add Amazon CloudFront distributions. Set the S3 buckets as origins for the distributions. Store user and movie data in Amazon S3.\n\nAmazon S3 is not suitable for storing and querying user and movie data as it is an object storage service and does not provide the same performance and query capabilities as DynamoDB.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html",
      "correctAnswerExplanations": [
        {
          "answer": "Host the static contents in an Amazon S3 bucket. Use Amazon CloudFront to distribute the content. Set the S3 bucket as the origin. Implement Amazon API Gateway and AWS Lambda functions for backend APIs. Store user and movie data in Amazon DynamoDB.",
          "explanation": "the static contents are hosted on Amazon S3, which is a cost-effective and scalable storage solution. Amazon CloudFront is used to distribute content, which ensures low latency and high transfer speeds. Amazon API Gateway and AWS Lambda functions are used for backend APIs, which allows for automatic scaling and minimal operational overhead. User and movie data is stored in Amazon DynamoDB, which is a managed NoSQL database that can handle high read and write throughput with low latency."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Deploy the streaming platform on Amazon EC2 instances in Auto Scaling groups across multiple Availability Zones. Use an Application Load Balancer (ALB) for distributing traffic. Add another ALB for backend APIs. Store the data in Amazon RDS for MySQL.",
          "explanation": "This option requires more operational overhead as it involves managing EC2 instances, Auto Scaling groups, and ALBs."
        },
        {
          "answer": "Migrate the entire streaming platform to containers. Host the containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use the Kubernetes Cluster Autoscaler to adjust the number of pods based on traffic. Store the data in Amazon RDS for MySQL.",
          "explanation": "This option involves managing Kubernetes and EKS, which increases operational overhead compared to the serverless solutions."
        },
        {
          "answer": "Host the full streaming platform in different Amazon S3 buckets. Add Amazon CloudFront distributions. Set the S3 buckets as origins for the distributions. Store user and movie data in Amazon S3.",
          "explanation": "Amazon S3 is not suitable for storing and querying user and movie data as it is an object storage service and does not provide the same performance and query capabilities as DynamoDB."
        }
      ],
      "references": [
        "https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud",
        "https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html"
      ]
    },
    {
      "id": 50,
      "question": "A media company is planning to host a live streaming event on AWS. The architecture consists of Amazon EC2 instances within a VPC, placed behind an Application Load Balancer (ALB). A custom DNS service is used for domain resolution. The company's solutions architect must recommend a solution to detect and mitigate potential DDoS attacks during the event.\n\nWhich solution meets these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Implement Amazon GuardDuty for threat detection.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Enable AWS Shield Advanced and associate it with the ALB.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Enable AWS Shield and associate it with Amazon Route 53.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Implement Amazon Inspector for security assessments.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nEnable AWS Shield Advanced and associate it with the ALB.\n\nAWS Shield Advanced provides comprehensive protection against DDoS attacks for various AWS resources, including ALBs. By associating the ALB with AWS Shield Advanced, the company can effectively detect and mitigate large-scale DDoS attacks during the live streaming event.\n\n\n\n\n\n\n\nIncorrect Options:\n\nImplement Amazon GuardDuty for threat detection.\n\nAmazon GuardDuty is a threat detection service that focuses on detecting unauthorized or malicious activity but does not provide DDoS protection. AWS Shield Advanced is better suited for DDoS mitigation.\n\n\n\n\nImplement Amazon Inspector for security assessments.\n\nAmazon Inspector is a security assessment service that helps identify vulnerabilities in applications but does not provide DDoS protection. AWS Shield Advanced is the appropriate choice for DDoS mitigation.\n\n\n\n\nEnable AWS Shield and associate it with Amazon Route 53.\n\nAWS Shield provides basic DDoS protection for all AWS customers. However, the company requires advanced DDoS protection for the ALB, which is available with AWS Shield Advanced.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/shield/advanced",
      "correctAnswerExplanations": [
        {
          "answer": "Enable AWS Shield Advanced and associate it with the ALB.",
          "explanation": "AWS Shield Advanced provides comprehensive protection against DDoS attacks for various AWS resources, including ALBs. By associating the ALB with AWS Shield Advanced, the company can effectively detect and mitigate large-scale DDoS attacks during the live streaming event."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Implement Amazon GuardDuty for threat detection.",
          "explanation": "Amazon GuardDuty is a threat detection service that focuses on detecting unauthorized or malicious activity but does not provide DDoS protection. AWS Shield Advanced is better suited for DDoS mitigation."
        },
        {
          "answer": "Implement Amazon Inspector for security assessments.",
          "explanation": "Amazon Inspector is a security assessment service that helps identify vulnerabilities in applications but does not provide DDoS protection. AWS Shield Advanced is the appropriate choice for DDoS mitigation."
        },
        {
          "answer": "Enable AWS Shield and associate it with Amazon Route 53.",
          "explanation": "AWS Shield provides basic DDoS protection for all AWS customers. However, the company requires advanced DDoS protection for the ALB, which is available with AWS Shield Advanced."
        }
      ],
      "references": [
        "https://aws.amazon.com/shield/advanced"
      ]
    },
    {
      "id": 51,
      "question": "A media company stores large video files using Amazon S3 Standard storage. The files are frequently accessed for the first 30 days. After that period, the files are rarely accessed, but the company needs to retain them for the long term.\n\nWhich storage solution is the MOST cost-effective for these requirements?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use S3 Intelligent-Tiering to automatically move objects between tiers.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier after 30 days.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nCreate an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.\n\nS3 Standard-IA is designed for data that is infrequently accessed but requires low latency when it is accessed. Transitioning objects from S3 Standard to S3 Standard-IA after 30 days is a cost-effective solution, considering the files' access patterns and long-term retention requirements.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse S3 Intelligent-Tiering to automatically move objects between tiers.\n\nWhile S3 Intelligent-Tiering can optimize storage costs for unpredictable access patterns, it may not be the most cost-effective solution for the given access pattern in this scenario.\n\n\n\n\nCreate an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier after 30 days.\n\nS3 Glacier is designed for long-term storage of infrequently accessed data. However, it does not offer low latency access when needed, making it unsuitable for this scenario.\n\n\n\n\nCreate an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.\n\nS3 One Zone-IA stores data in a single Availability Zone, which may not provide the same level of durability as S3 Standard-IA.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes",
      "correctAnswerExplanations": [
        {
          "answer": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
          "explanation": "S3 Standard-IA is designed for data that is infrequently accessed but requires low latency when it is accessed. Transitioning objects from S3 Standard to S3 Standard-IA after 30 days is a cost-effective solution, considering the files' access patterns and long-term retention requirements."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use S3 Intelligent-Tiering to automatically move objects between tiers.",
          "explanation": "While S3 Intelligent-Tiering can optimize storage costs for unpredictable access patterns, it may not be the most cost-effective solution for the given access pattern in this scenario."
        },
        {
          "answer": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier after 30 days.",
          "explanation": "S3 Glacier is designed for long-term storage of infrequently accessed data. However, it does not offer low latency access when needed, making it unsuitable for this scenario."
        },
        {
          "answer": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.",
          "explanation": "S3 One Zone-IA stores data in a single Availability Zone, which may not provide the same level of durability as S3 Standard-IA."
        }
      ],
      "references": [
        "https://aws.amazon.com/s3/storage-classes"
      ]
    },
    {
      "id": 52,
      "question": "A financial institution operates its infrastructure on AWS and needs to adhere to regulatory requirements. The institution must monitor AWS resource configurations for any changes and maintain a record of API calls made to these resources for auditing purposes.\n\nWhat should a solutions architect recommend to fulfill these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon CloudWatch to monitor configuration changes and AWS CloudTrail to log API calls.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use AWS Config to monitor configuration changes and AWS CloudTrail to log API calls.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Use AWS CloudTrail to monitor configuration changes and Amazon CloudWatch to log API calls.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use AWS Config to monitor configuration changes and Amazon CloudWatch to log API calls.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nUse AWS Config to monitor configuration changes and AWS CloudTrail to log API calls.\n\nAWS Config is a service that provides resource inventory, configuration history, and configuration change notifications for AWS resources. AWS CloudTrail is a service that records API calls made in AWS services and delivers the resulting log files to Amazon S3 or CloudWatch Logs. By combining these services, the institution can monitor configuration changes and maintain a record of API calls made to resources, which is necessary for auditing purposes. Thus, the recommended solution is to use AWS Config to monitor configuration changes and AWS CloudTrail to log API calls.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon CloudWatch to monitor configuration changes and AWS CloudTrail to log API calls.\n\nAmazon CloudWatch primarily focuses on monitoring performance metrics and logging, not tracking configuration changes. AWS Config is better suited for this purpose.\n\n\n\n\nUse AWS CloudTrail to monitor configuration changes and Amazon CloudWatch to log API calls.\n\nAWS CloudTrail is designed for logging API calls, not tracking configuration changes. AWS Config is the appropriate service for monitoring configuration changes.\n\n\n\n\nUse AWS Config to monitor configuration changes and Amazon CloudWatch to log API calls.\n\nAmazon CloudWatch is not designed for logging API calls. AWS CloudTrail should be used for recording API call history.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/config\n\nhttps://aws.amazon.com/cloudtrail",
      "correctAnswerExplanations": [
        {
          "answer": "Use AWS Config to monitor configuration changes and AWS CloudTrail to log API calls.",
          "explanation": "AWS Config is a service that provides resource inventory, configuration history, and configuration change notifications for AWS resources. AWS CloudTrail is a service that records API calls made in AWS services and delivers the resulting log files to Amazon S3 or CloudWatch Logs. By combining these services, the institution can monitor configuration changes and maintain a record of API calls made to resources, which is necessary for auditing purposes. Thus, the recommended solution is to use AWS Config to monitor configuration changes and AWS CloudTrail to log API calls."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use Amazon CloudWatch to monitor configuration changes and AWS CloudTrail to log API calls.",
          "explanation": "Amazon CloudWatch primarily focuses on monitoring performance metrics and logging, not tracking configuration changes. AWS Config is better suited for this purpose."
        },
        {
          "answer": "Use AWS CloudTrail to monitor configuration changes and Amazon CloudWatch to log API calls.",
          "explanation": "AWS CloudTrail is designed for logging API calls, not tracking configuration changes. AWS Config is the appropriate service for monitoring configuration changes."
        },
        {
          "answer": "Use AWS Config to monitor configuration changes and Amazon CloudWatch to log API calls.",
          "explanation": "Amazon CloudWatch is not designed for logging API calls. AWS CloudTrail should be used for recording API call history."
        }
      ],
      "references": [
        "https://aws.amazon.com/config",
        "https://aws.amazon.com/cloudtrail"
      ]
    },
    {
      "id": 53,
      "question": "A company has acquired a domain name using Amazon Route 53 and utilizes Amazon API Gateway in the us-west-1 Region to provide a public interface for their backend microservices. The company wants to ensure that external consumers of their APIs can access them securely using the company's domain name and an SSL certificate.\n\nWhat is the appropriate solution to meet these requirements?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Import the SSL certificate into AWS Certificate Manager (ACM) in the us-west-1 Region. Create an alias record in Route 53 for the company's domain name and point it to the edge-optimized API Gateway endpoint.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an edge-optimized API Gateway endpoint and associate it with the company's domain name. Import the SSL certificate into AWS Certificate Manager (ACM) in the us-east-1 Region. Configure Route 53 to route traffic to the API Gateway custom domain name.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Create a private API Gateway endpoint and associate it with the company's domain name. Import the SSL certificate into AWS Certificate Manager (ACM) in the us-west-1 Region. Configure Route 53 to route traffic to the API Gateway custom domain name.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Create a custom domain name in API Gateway, associate it with the company's domain name, and import the SSL certificate into AWS Certificate Manager (ACM) in the same Region. Configure Route 53 to route traffic to the API Gateway custom domain name.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nCreate a custom domain name in API Gateway, associate it with the company's domain name, and import the SSL certificate into AWS Certificate Manager (ACM) in the same Region. Configure Route 53 to route traffic to the API Gateway custom domain name.\n\nThis option provides a secure and scalable solution for external consumers of the company's APIs to access them using the company's domain name and an SSL certificate. By creating a custom domain name in API Gateway, the company can associate it with their domain name, and then import the SSL certificate into AWS Certificate Manager (ACM) in the same region. This provides a central location for managing and renewing SSL certificates. The company can then configure Route 53 to route traffic to the API Gateway custom domain name, ensuring that external consumers can access the APIs securely.\n\n\n\n\n\n\n\nIncorrect Options:\n\nCreate an edge-optimized API Gateway endpoint and associate it with the company's domain name. Import the SSL certificate into AWS Certificate Manager (ACM) in the us-east-1 Region. Configure Route 53 to route traffic to the API Gateway custom domain name.\n\nThe edge-optimized API Gateway endpoint is designed for global access and higher latency reduction. The scenario specifies a regional API Gateway.\n\n\n\n\nImport the SSL certificate into AWS Certificate Manager (ACM) in the us-west-1 Region. Create an alias record in Route 53 for the company's domain name and point it to the edge-optimized API Gateway endpoint.\n\nIt does not create a custom domain name in API Gateway and associates it with the company's domain name.\n\n\n\n\nCreate a private API Gateway endpoint and associate it with the company's domain name. Import the SSL certificate into AWS Certificate Manager (ACM) in the us-west-1 Region. Configure Route 53 to route traffic to the API Gateway custom domain name.\n\nThe private API Gateway endpoint is designed for internal access within a VPC, not for external access.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html",
      "correctAnswerExplanations": [
        {
          "answer": "Create a custom domain name in API Gateway, associate it with the company's domain name, and import the SSL certificate into AWS Certificate Manager (ACM) in the same Region. Configure Route 53 to route traffic to the API Gateway custom domain name.",
          "explanation": "This option provides a secure and scalable solution for external consumers of the company's APIs to access them using the company's domain name and an SSL certificate. By creating a custom domain name in API Gateway, the company can associate it with their domain name, and then import the SSL certificate into AWS Certificate Manager (ACM) in the same region. This provides a central location for managing and renewing SSL certificates. The company can then configure Route 53 to route traffic to the API Gateway custom domain name, ensuring that external consumers can access the APIs securely."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Create an edge-optimized API Gateway endpoint and associate it with the company's domain name. Import the SSL certificate into AWS Certificate Manager (ACM) in the us-east-1 Region. Configure Route 53 to route traffic to the API Gateway custom domain name.",
          "explanation": "The edge-optimized API Gateway endpoint is designed for global access and higher latency reduction. The scenario specifies a regional API Gateway."
        },
        {
          "answer": "Import the SSL certificate into AWS Certificate Manager (ACM) in the us-west-1 Region. Create an alias record in Route 53 for the company's domain name and point it to the edge-optimized API Gateway endpoint.",
          "explanation": "It does not create a custom domain name in API Gateway and associates it with the company's domain name."
        },
        {
          "answer": "Create a private API Gateway endpoint and associate it with the company's domain name. Import the SSL certificate into AWS Certificate Manager (ACM) in the us-west-1 Region. Configure Route 53 to route traffic to the API Gateway custom domain name.",
          "explanation": "The private API Gateway endpoint is designed for internal access within a VPC, not for external access."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html"
      ]
    },
    {
      "id": 54,
      "question": "A company operates an e-commerce platform on AWS that experiences high traffic during peak hours. They require a scalable, near-real-time solution for sharing order data with multiple internal systems. Order data must be processed to remove sensitive information before being stored in a document database for low-latency access.\n\nWhat should a solutions architect recommend to fulfill these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Store the order data in Amazon DocumentDB. Configure a trigger in DocumentDB to remove sensitive data from every order upon write. Use DocumentDB change streams to share the order data with other systems.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Stream the order data into Amazon Kinesis Data Streams. Utilize AWS Lambda integration to remove sensitive data from each order and store the processed data in Amazon DocumentDB. Other systems can consume the order data from the Kinesis data stream.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Stream the order data into Amazon Kinesis Data Firehose to store data in Amazon DocumentDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other systems can consume the data stored in Amazon S3.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Store the batched order data in Amazon S3 as files. Use AWS Lambda to process each file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DocumentDB. Other systems can consume order files stored in Amazon S3.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nStream the order data into Amazon Kinesis Data Streams. Utilize AWS Lambda integration to remove sensitive data from each order and store the processed data in Amazon DocumentDB. Other systems can consume the order data from the Kinesis data stream.\n\nAmazon Kinesis Data Firehose is a fully managed service that can capture and load streaming data in real-time and can handle high traffic during peak hours. AWS Lambda integration with Kinesis Data Firehose can be used to process the streaming data to remove sensitive information. Amazon DocumentDB is a fully managed document database that provides fast and scalable access to data. The solution uses DocumentDB to store processed data for low-latency access. Amazon S3 is used as a data store for the data that is not required for low-latency access.\n\nThe solution is highly scalable and cost-effective. It can handle high traffic during peak hours and share order data with multiple internal systems in near real-time. The use of AWS Lambda with Kinesis Data Firehose reduces the operational overhead of processing the data.\n\n\n\n\n\n\n\nIncorrect Options:\n\nStore the order data in Amazon DocumentDB. Configure a trigger in DocumentDB to remove sensitive data from every order upon write. Use DocumentDB change streams to share the order data with other systems.\n\nAmazon DocumentDB does not support triggers, making this option invalid.\n\n\n\n\nStream the order data into Amazon Kinesis Data Firehose to store data in Amazon DocumentDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other systems can consume the data stored in Amazon S3.\n\nAmazon Kinesis Data Firehose does not support direct integration with Amazon DocumentDB, making this option invalid.\n\n\n\n\nStore the batched order data in Amazon S3 as files. Use AWS Lambda to process each file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DocumentDB. Other systems can consume order files stored in Amazon S3.\n\nThis option does not provide a near-real-time solution and would be less efficient compared to using Amazon Kinesis Data Streams.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/en_us/streams/latest/dev/tutorial-stock-data-lambda.html",
      "correctAnswerExplanations": [
        {
          "answer": "Stream the order data into Amazon Kinesis Data Streams. Utilize AWS Lambda integration to remove sensitive data from each order and store the processed data in Amazon DocumentDB. Other systems can consume the order data from the Kinesis data stream.",
          "explanation": "Amazon Kinesis Data Firehose is a fully managed service that can capture and load streaming data in real-time and can handle high traffic during peak hours. AWS Lambda integration with Kinesis Data Firehose can be used to process the streaming data to remove sensitive information. Amazon DocumentDB is a fully managed document database that provides fast and scalable access to data. The solution uses DocumentDB to store processed data for low-latency access. Amazon S3 is used as a data store for the data that is not required for low-latency access."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Store the order data in Amazon DocumentDB. Configure a trigger in DocumentDB to remove sensitive data from every order upon write. Use DocumentDB change streams to share the order data with other systems.",
          "explanation": "Amazon DocumentDB does not support triggers, making this option invalid."
        },
        {
          "answer": "Stream the order data into Amazon Kinesis Data Firehose to store data in Amazon DocumentDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other systems can consume the data stored in Amazon S3.",
          "explanation": "Amazon Kinesis Data Firehose does not support direct integration with Amazon DocumentDB, making this option invalid."
        },
        {
          "answer": "Store the batched order data in Amazon S3 as files. Use AWS Lambda to process each file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DocumentDB. Other systems can consume order files stored in Amazon S3.",
          "explanation": "This option does not provide a near-real-time solution and would be less efficient compared to using Amazon Kinesis Data Streams."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/en_us/streams/latest/dev/tutorial-stock-data-lambda.html"
      ]
    },
    {
      "id": 55,
      "question": "A company has an on-premises data center hosting an application that generates a significant amount of time-sensitive data. This data must be regularly backed up to Amazon S3 with minimal latency while ensuring minimal disruption to the company's internet connectivity. A solutions architect needs to design a long-term, scalable solution to meet these requirements.\n\nWhich solution best meets these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use daily AWS Snowball devices to load the data and send the devices back to AWS each day.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Establish a dedicated AWS Direct Connect connection and route backup traffic through this connection.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Configure AWS VPN connections and direct all traffic through a VPC gateway endpoint.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Open a support ticket through the AWS Management Console and request the removal of S3 service limits from the account.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nEstablish a dedicated AWS Direct Connect connection and route backup traffic through this connection.\n\nEstablishing a dedicated AWS Direct Connect connection is the most suitable solution for this scenario. AWS Direct Connect provides a dedicated, private network connection between the company's on-premises data center and AWS. This direct connection allows for faster data transfer, lower latency, and more consistent network performance compared to transferring data over the public internet. By routing backup traffic through this dedicated connection, the company can ensure minimal disruption to its internet connectivity and maintain timely backups to Amazon S3.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure AWS VPN connections and direct all traffic through a VPC gateway endpoint.\n\nAlthough AWS VPN connections can provide secure communication between the on-premises data center and AWS, they still rely on the public internet for data transfer. This option does not address the company's concerns about internet bandwidth limitations.\n\n\n\n\nUse daily AWS Snowball devices to load the data and send the devices back to AWS each day.\n\nAWS Snowball devices are designed for large-scale data transfers, but they are not an ideal solution for daily, time-sensitive backups. The process of ordering, loading, and returning Snowball devices each day would be cumbersome and inefficient.\n\n\n\n\nOpen a support ticket through the AWS Management Console and request the removal of S3 service limits from the account.\n\nRequesting the removal of S3 service limits does not address the company's concerns about internet bandwidth limitations or the need for timely backups.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html",
      "correctAnswerExplanations": [
        {
          "answer": "Establish a dedicated AWS Direct Connect connection and route backup traffic through this connection.",
          "explanation": "Establishing a dedicated AWS Direct Connect connection is the most suitable solution for this scenario. AWS Direct Connect provides a dedicated, private network connection between the company's on-premises data center and AWS. This direct connection allows for faster data transfer, lower latency, and more consistent network performance compared to transferring data over the public internet. By routing backup traffic through this dedicated connection, the company can ensure minimal disruption to its internet connectivity and maintain timely backups to Amazon S3."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Configure AWS VPN connections and direct all traffic through a VPC gateway endpoint.",
          "explanation": "Although AWS VPN connections can provide secure communication between the on-premises data center and AWS, they still rely on the public internet for data transfer. This option does not address the company's concerns about internet bandwidth limitations."
        },
        {
          "answer": "Use daily AWS Snowball devices to load the data and send the devices back to AWS each day.",
          "explanation": "AWS Snowball devices are designed for large-scale data transfers, but they are not an ideal solution for daily, time-sensitive backups. The process of ordering, loading, and returning Snowball devices each day would be cumbersome and inefficient."
        },
        {
          "answer": "Open a support ticket through the AWS Management Console and request the removal of S3 service limits from the account.",
          "explanation": "Requesting the removal of S3 service limits does not address the company's concerns about internet bandwidth limitations or the need for timely backups."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html"
      ]
    },
    {
      "id": 56,
      "question": "A media streaming platform stores data in a PostgreSQL 11 database running on a high-capacity EC2 instance. The platform uses a fleet of EC2 instances behind an Application Load Balancer and leverages an EC2 Auto Scaling group distributed across multiple Availability Zones. The Auto Scaling group dynamically adjusts based on the platform's user load. The platform experiences a higher volume of read requests compared to write operations. The company wants to ensure the database scales automatically to accommodate unpredictable read workloads while maintaining high availability.\n\nWhat solution will best address these requirements?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use Amazon RDS with a Single-AZ deployment and configure RDS to add read replicas in another Availability Zone.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use Amazon Aurora with a Multi-AZ deployment and configure Aurora Auto Scaling with Aurora Replicas.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Use Amazon ElastiCache for Redis using EC2 Spot Instances.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use Amazon Neptune with a single node for both leader and compute operations.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nUse Amazon Aurora with a Multi-AZ deployment and configure Aurora Auto Scaling with Aurora Replicas.\n\nAmazon Aurora is a fully managed relational database service designed for high-performance workloads. It is compatible with PostgreSQL and MySQL, making it suitable for the media streaming platform's PostgreSQL 11 database. Aurora Multi-AZ deployment ensures high availability by replicating data across multiple Availability Zones, which automatically failover to a standby replica in case of an outage. Aurora Replicas handle read traffic, enhancing the read performance of the database. By configuring Aurora Auto Scaling, the platform can automatically adjust the number of replicas based on actual read traffic. As the read workload increases, Aurora Auto Scaling adds more read replicas to distribute the traffic, improving overall performance. Conversely, it removes replicas when the read traffic decreases, optimizing resource utilization. This solution meets the company's requirements of scaling the database automatically to handle unpredictable read workloads and maintain high availability.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse Amazon Neptune with a single node for both leader and compute operations.\n\nAmazon Neptune is a managed graph database service designed for storing and querying relationship data, not suitable for the media streaming platform's relational PostgreSQL 11 database.\n\n\n\n\nUse Amazon RDS with a Single-AZ deployment and configure RDS to add read replicas in another Availability Zone.\n\nAmazon RDS Single-AZ deployment does not offer high availability, as it does not automatically failover to a standby replica during an outage, making it an unsuitable choice.\n\n\n\n\nUse Amazon ElastiCache for Redis using EC2 Spot Instances.\n\nAmazon ElastiCache for Redis is a caching layer and not a relational database service. While it can improve performance, it is not a direct replacement for the PostgreSQL 11 database. Additionally, using EC2 Spot Instances does not guarantee high availability.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use Amazon Aurora with a Multi-AZ deployment and configure Aurora Auto Scaling with Aurora Replicas.",
          "explanation": "Amazon Aurora is a fully managed relational database service designed for high-performance workloads. It is compatible with PostgreSQL and MySQL, making it suitable for the media streaming platform's PostgreSQL 11 database. Aurora Multi-AZ deployment ensures high availability by replicating data across multiple Availability Zones, which automatically failover to a standby replica in case of an outage. Aurora Replicas handle read traffic, enhancing the read performance of the database. By configuring Aurora Auto Scaling, the platform can automatically adjust the number of replicas based on actual read traffic. As the read workload increases, Aurora Auto Scaling adds more read replicas to distribute the traffic, improving overall performance. Conversely, it removes replicas when the read traffic decreases, optimizing resource utilization. This solution meets the company's requirements of scaling the database automatically to handle unpredictable read workloads and maintain high availability."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use Amazon Neptune with a single node for both leader and compute operations.",
          "explanation": "Amazon Neptune is a managed graph database service designed for storing and querying relationship data, not suitable for the media streaming platform's relational PostgreSQL 11 database."
        },
        {
          "answer": "Use Amazon RDS with a Single-AZ deployment and configure RDS to add read replicas in another Availability Zone.",
          "explanation": "Amazon RDS Single-AZ deployment does not offer high availability, as it does not automatically failover to a standby replica during an outage, making it an unsuitable choice."
        },
        {
          "answer": "Use Amazon ElastiCache for Redis using EC2 Spot Instances.",
          "explanation": "Amazon ElastiCache for Redis is a caching layer and not a relational database service. While it can improve performance, it is not a direct replacement for the PostgreSQL 11 database. Additionally, using EC2 Spot Instances does not guarantee high availability."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html"
      ]
    },
    {
      "id": 57,
      "question": "A social media company has deployed an application on AWS that uses Amazon DynamoDB to store user information. The company requires a disaster recovery solution that can meet a recovery point objective (RPO) of 30 minutes and a recovery time objective (RTO) of 1 hour in case of data loss or corruption.\n\nWhich solution should a solutions architect recommend?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Configure DynamoDB global tables with a cross-Region replication feature. For RPO recovery, promote the replica table to the primary table.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use Amazon DynamoDB Accelerator (DAX) to cache frequently accessed data. For RPO recovery, restore the table by using DAX cache data.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Set up DynamoDB backups to Amazon S3 daily. For RPO recovery, restore the table by importing the backups from Amazon S3.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Use DynamoDB point-in-time recovery. For RPO recovery, restore the table to the desired point in time.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nUse DynamoDB point-in-time recovery. For RPO recovery, restore the table to the desired point in time.\n\nDynamoDB point-in-time recovery allows restoring tables to any point within the retention period of up to 35 days. It also automates backups for the table, which ensures that data is backed up continuously without the need for manual intervention. Therefore, in case of data corruption, the table can be easily restored to a previous point in time, meeting the RPO and RTO requirements.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure DynamoDB global tables with a cross-Region replication feature. For RPO recovery, promote the replica table to the primary table.\n\nWhile it fulfills the RPO requirement, it fails to meet the RTO requirement since promoting a replica to the primary table can take longer than an hour.\n\n\n\n\nSet up DynamoDB backups to Amazon S3 daily. For RPO recovery, restore the table by importing the backups from Amazon S3.\n\nWhile it meets the RPO requirement, restoring a table from Amazon S3 backups can take longer than an hour, failing to meet the RTO requirement.\n\n\n\n\nUse Amazon DynamoDB Accelerator (DAX) to cache frequently accessed data. For RPO recovery, restore the table by using DAX cache data.\n\nWhile using DAX can improve read performance, it does not provide a disaster recovery solution that can meet the RPO and RTO requirements.\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use DynamoDB point-in-time recovery. For RPO recovery, restore the table to the desired point in time.",
          "explanation": "DynamoDB point-in-time recovery allows restoring tables to any point within the retention period of up to 35 days. It also automates backups for the table, which ensures that data is backed up continuously without the need for manual intervention. Therefore, in case of data corruption, the table can be easily restored to a previous point in time, meeting the RPO and RTO requirements."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Configure DynamoDB global tables with a cross-Region replication feature. For RPO recovery, promote the replica table to the primary table.",
          "explanation": "While it fulfills the RPO requirement, it fails to meet the RTO requirement since promoting a replica to the primary table can take longer than an hour."
        },
        {
          "answer": "Set up DynamoDB backups to Amazon S3 daily. For RPO recovery, restore the table by importing the backups from Amazon S3.",
          "explanation": "While it meets the RPO requirement, restoring a table from Amazon S3 backups can take longer than an hour, failing to meet the RTO requirement."
        },
        {
          "answer": "Use Amazon DynamoDB Accelerator (DAX) to cache frequently accessed data. For RPO recovery, restore the table by using DAX cache data.",
          "explanation": "While using DAX can improve read performance, it does not provide a disaster recovery solution that can meet the RPO and RTO requirements."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html"
      ]
    },
    {
      "id": 58,
      "question": "A video streaming company runs a platform that frequently uploads and downloads videos from Amazon S3 buckets within the same AWS Region. The company's solutions architect has observed a rise in data transfer costs and needs to develop a strategy to minimize these expenses.\n\nWhat should the solutions architect recommend to achieve this goal?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Deploy the platform in a public subnet and allow it to access the S3 buckets via an internet gateway.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an S3 VPC gateway endpoint in the VPC and establish an endpoint policy granting access to the S3 buckets.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Implement a NAT gateway in a public subnet and create an endpoint policy allowing access to the S3 buckets.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Set up an Application Load Balancer (ALB) in a public subnet and configure it to route S3 requests through the ALB.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nCreate an S3 VPC gateway endpoint in the VPC and establish an endpoint policy granting access to the S3 buckets.\n\nBy creating an Amazon S3 VPC gateway endpoint, the company can reduce data transfer costs, as traffic between the VPC and S3 buckets will remain within the AWS network. This eliminates data transfer fees associated with accessing S3 buckets over the internet. The endpoint policy allows the company to control access to the S3 buckets by specifying which actions are allowed and which resources can be accessed. This solution effectively reduces data transfer costs while maintaining secure access to the necessary S3 resources.\n\n\n\n\n\n\n\nIncorrect Options:\n\nSet up an Application Load Balancer (ALB) in a public subnet and configure it to route S3 requests through the ALB.\n\nAn ALB is designed for distributing traffic among multiple targets, not for reducing data transfer costs.\n\n\n\n\nDeploy the platform in a public subnet and allow it to access the S3 buckets via an internet gateway.\n\nIt does not reduce data transfer costs, as the traffic between the platform and the S3 buckets would still traverse the public internet.\n\n\n\n\nImplement a NAT gateway in a public subnet and create an endpoint policy allowing access to the S3 buckets.\n\nA NAT gateway is designed for allowing instances in a private subnet to access the internet, not for reducing data transfer costs between VPC and S3 buckets.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html",
      "correctAnswerExplanations": [
        {
          "answer": "Create an S3 VPC gateway endpoint in the VPC and establish an endpoint policy granting access to the S3 buckets.",
          "explanation": "By creating an Amazon S3 VPC gateway endpoint, the company can reduce data transfer costs, as traffic between the VPC and S3 buckets will remain within the AWS network. This eliminates data transfer fees associated with accessing S3 buckets over the internet. The endpoint policy allows the company to control access to the S3 buckets by specifying which actions are allowed and which resources can be accessed. This solution effectively reduces data transfer costs while maintaining secure access to the necessary S3 resources."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Set up an Application Load Balancer (ALB) in a public subnet and configure it to route S3 requests through the ALB.",
          "explanation": "An ALB is designed for distributing traffic among multiple targets, not for reducing data transfer costs."
        },
        {
          "answer": "Deploy the platform in a public subnet and allow it to access the S3 buckets via an internet gateway.",
          "explanation": "It does not reduce data transfer costs, as the traffic between the platform and the S3 buckets would still traverse the public internet."
        },
        {
          "answer": "Implement a NAT gateway in a public subnet and create an endpoint policy allowing access to the S3 buckets.",
          "explanation": "A NAT gateway is designed for allowing instances in a private subnet to access the internet, not for reducing data transfer costs between VPC and S3 buckets."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"
      ]
    },
    {
      "id": 59,
      "question": "An e-commerce company's web application is hosted on AWS and utilizes an Application Load Balancer (ALB) to manage incoming traffic. The ALB is configured to handle both HTTP and HTTPS traffic separately. To enhance security, the company wants to ensure that all incoming traffic uses HTTPS.\n\nWhat should a solutions architect recommend to achieve this goal?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Modify the security group associated with the ALB to only allow HTTPS traffic.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an ALB listener rule to redirect HTTP traffic to HTTPS.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Use Amazon Route 53 to configure a routing policy that redirects HTTP traffic to HTTPS.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Replace the ALB with a Network Load Balancer (NLB) that employs Server Name Indication (SNI).",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Secure Architectures",
      "explanation": "Correct Option:\n\nCreate an ALB listener rule to redirect HTTP traffic to HTTPS.\n\nBy creating a listener rule on the ALB, all incoming HTTP traffic will be redirected to HTTPS, ensuring secure communication. This approach satisfies the company's security requirements without needing to modify the existing infrastructure significantly.\n\n\n\n\n\n\n\nIncorrect Options:\n\nModify the security group associated with the ALB to only allow HTTPS traffic.\n\nIt will block HTTP traffic, causing requests to fail instead of redirecting them to HTTPS. The goal is to redirect HTTP traffic to HTTPS, not to block it.\n\n\n\n\nUse Amazon Route 53 to configure a routing policy that redirects HTTP traffic to HTTPS.\n\nAmazon Route 53 is a DNS service, and it doesn't handle redirection of HTTP to HTTPS at the application level. This functionality is better suited to an Application Load Balancer.\n\n\n\n\nReplace the ALB with a Network Load Balancer (NLB) that employs Server Name Indication (SNI).\n\nA Network Load Balancer operates at the transport layer (Layer 4) and does not have the ability to redirect HTTP to HTTPS. Application Load Balancers, which operate at the application layer (Layer 7), are better suited for this task.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/fr_fr/elasticloadbalancing/latest/application/create-https-listener.html\n\nhttps://aws.amazon.com/fr/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb",
      "correctAnswerExplanations": [
        {
          "answer": "Create an ALB listener rule to redirect HTTP traffic to HTTPS.",
          "explanation": "By creating a listener rule on the ALB, all incoming HTTP traffic will be redirected to HTTPS, ensuring secure communication. This approach satisfies the company's security requirements without needing to modify the existing infrastructure significantly."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Modify the security group associated with the ALB to only allow HTTPS traffic.",
          "explanation": "It will block HTTP traffic, causing requests to fail instead of redirecting them to HTTPS. The goal is to redirect HTTP traffic to HTTPS, not to block it."
        },
        {
          "answer": "Use Amazon Route 53 to configure a routing policy that redirects HTTP traffic to HTTPS.",
          "explanation": "Amazon Route 53 is a DNS service, and it doesn't handle redirection of HTTP to HTTPS at the application level. This functionality is better suited to an Application Load Balancer."
        },
        {
          "answer": "Replace the ALB with a Network Load Balancer (NLB) that employs Server Name Indication (SNI).",
          "explanation": "A Network Load Balancer operates at the transport layer (Layer 4) and does not have the ability to redirect HTTP to HTTPS. Application Load Balancers, which operate at the application layer (Layer 7), are better suited for this task."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/fr_fr/elasticloadbalancing/latest/application/create-https-listener.html",
        "https://aws.amazon.com/fr/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb"
      ]
    },
    {
      "id": 60,
      "question": "A streaming service company records user engagement data from its global user base. The average volume of data collected from each site daily is 500 GB. Each site has a high-speed internet connection. The company wants to consolidate the data from all global sites as quickly as possible in a single Amazon S3 bucket while minimizing operational complexity.\n\nWhich solution meets these requirements?",
      "corrects": [
        4
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Upload the data from each site to an S3 bucket in the nearest Region, then use S3 Cross-Region Replication to copy objects to the target S3 bucket. Finally, remove the data from the origin S3 bucket.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Upload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Periodically, take an EBS snapshot and copy it to the Region containing the target S3 bucket. Restore the EBS volume in that Region.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Schedule daily AWS Snowball Edge Storage Optimized device jobs to transfer data from each site to the closest Region. Use S3 Cross-Region Replication to copy objects to the target S3 bucket.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Enable S3 Transfer Acceleration on the target S3 bucket and use multipart uploads to directly upload site data to the target S3 bucket.",
          "correct": true
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nEnable S3 Transfer Acceleration on the target S3 bucket and use multipart uploads to directly upload site data to the target S3 bucket.\n\nS3 Transfer Acceleration is an Amazon S3 feature that enables fast, easy, and secure transfers of files over the Internet using globally distributed edge locations. By enabling Transfer Acceleration on the target S3 bucket and using multipart uploads, the company can quickly and directly upload site data to the target S3 bucket. This approach reduces operational complexity and ensures the fastest possible data aggregation from global sites.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUpload the data from each site to an S3 bucket in the nearest Region, then use S3 Cross-Region Replication to copy objects to the target S3 bucket. Finally, remove the data from the origin S3 bucket.\n\nIt introduces additional operational complexity and latency due to the replication process.\n\n\n\n\nSchedule daily AWS Snowball Edge Storage Optimized device jobs to transfer data from each site to the closest Region. Use S3 Cross-Region Replication to copy objects to the target S3 bucket.\n\nIt introduces operational complexity and is not practical for daily data transfers, considering the high-speed internet connections available at each site.\n\n\n\n\nUpload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Periodically, take an EBS snapshot and copy it to the Region containing the target S3 bucket. Restore the EBS volume in that Region.\n\nIt adds unnecessary operational complexity and latency by using EBS snapshots and multiple EC2 instances.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/s3/transfer-acceleration\n\nhttps://aws.amazon.com/s3/replication",
      "correctAnswerExplanations": [
        {
          "answer": "Enable S3 Transfer Acceleration on the target S3 bucket and use multipart uploads to directly upload site data to the target S3 bucket.",
          "explanation": "S3 Transfer Acceleration is an Amazon S3 feature that enables fast, easy, and secure transfers of files over the Internet using globally distributed edge locations. By enabling Transfer Acceleration on the target S3 bucket and using multipart uploads, the company can quickly and directly upload site data to the target S3 bucket. This approach reduces operational complexity and ensures the fastest possible data aggregation from global sites."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Upload the data from each site to an S3 bucket in the nearest Region, then use S3 Cross-Region Replication to copy objects to the target S3 bucket. Finally, remove the data from the origin S3 bucket.",
          "explanation": "It introduces additional operational complexity and latency due to the replication process."
        },
        {
          "answer": "Schedule daily AWS Snowball Edge Storage Optimized device jobs to transfer data from each site to the closest Region. Use S3 Cross-Region Replication to copy objects to the target S3 bucket.",
          "explanation": "It introduces operational complexity and is not practical for daily data transfers, considering the high-speed internet connections available at each site."
        },
        {
          "answer": "Upload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Periodically, take an EBS snapshot and copy it to the Region containing the target S3 bucket. Restore the EBS volume in that Region.",
          "explanation": "It adds unnecessary operational complexity and latency by using EBS snapshots and multiple EC2 instances."
        }
      ],
      "references": [
        "https://aws.amazon.com/s3/transfer-acceleration",
        "https://aws.amazon.com/s3/replication"
      ]
    },
    {
      "id": 61,
      "question": "A business utilizing AWS for its infrastructure wants to ensure that all Amazon S3 buckets, Amazon DynamoDB tables, and Amazon Elasticache clusters have appropriate tags. The business aims to minimize the effort in configuring and maintaining this verification process.\n\nWhat should a solutions architect do to achieve this?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Use AWS Resource Groups to display untagged resources and manually apply appropriate tags.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use AWS Config rules to identify resources that lack proper tags.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Write API calls to examine all resources for correct tag assignment. Schedule an AWS Lambda function through Amazon EventBridge to run the code periodically.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Write  API calls to inspect all resources for correct tag assignment. Run the code periodically on an Amazon EC2 instance.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nUse AWS Config rules to identify resources that lack proper tags.\n\nAWS Config is a fully managed service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It provides a set of predefined rules, including rules for tag compliance, that you can use to ensure that your resources are correctly configured.\n\nBy using AWS Config rules, the solutions architect can automatically identify all S3 buckets, DynamoDB tables, and Elasticache clusters that do not have appropriate tags. This process is automated and does not require any manual intervention, which helps minimize the effort in configuring and maintaining this verification process.\n\n\n\n\n\n\n\nIncorrect Options:\n\nUse AWS Resource Groups to display untagged resources and manually apply appropriate tags.\n\nWhile this option can help identify untagged resources, it requires manual intervention and is less efficient compared to using AWS Config rules.\n\n\n\n\nWrite API calls to inspect all resources for correct tag assignment. Run the code periodically on an Amazon EC2 instance.\n\nThis would require manual maintenance and monitoring of the EC2 instance, making it less efficient than using AWS Config rules.\n\n\n\n\nWrite API calls to examine all resources for correct tag assignment. Schedule an AWS Lambda function through Amazon EventBridge to run the code periodically.\n\nAlthough this is more efficient than option AWS Config rules, it still requires more effort in configuring and maintaining the solution compared to using AWS Config rules. So this is not suitable for our case.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/tagging.html",
      "correctAnswerExplanations": [
        {
          "answer": "Use AWS Config rules to identify resources that lack proper tags.",
          "explanation": "AWS Config is a fully managed service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It provides a set of predefined rules, including rules for tag compliance, that you can use to ensure that your resources are correctly configured."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Use AWS Resource Groups to display untagged resources and manually apply appropriate tags.",
          "explanation": "While this option can help identify untagged resources, it requires manual intervention and is less efficient compared to using AWS Config rules."
        },
        {
          "answer": "Write API calls to inspect all resources for correct tag assignment. Run the code periodically on an Amazon EC2 instance.",
          "explanation": "This would require manual maintenance and monitoring of the EC2 instance, making it less efficient than using AWS Config rules."
        },
        {
          "answer": "Write API calls to examine all resources for correct tag assignment. Schedule an AWS Lambda function through Amazon EventBridge to run the code periodically.",
          "explanation": "Although this is more efficient than option AWS Config rules, it still requires more effort in configuring and maintaining the solution compared to using AWS Config rules. So this is not suitable for our case."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html",
        "https://docs.aws.amazon.com/config/latest/developerguide/tagging.html"
      ]
    },
    {
      "id": 62,
      "question": "A development team is designing a microservice that will convert large video files to smaller, compressed formats. When a user uploads a video through the web interface, the microservice should store the video in an Amazon S3 bucket, process and compress the video using an AWS Lambda function, and store the video in its compressed form in a different S3 bucket.\n\nA solutions architect needs to design a solution that uses reliable, stateless components to process the videos automatically.\n\nWhich combination of actions will meet these requirements? (Select TWO.)",
      "corrects": [
        3,
        5
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded video is detected, write the file name to a text file in memory and use the text file to keep track of the videos that were processed.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Configure the S3 bucket to send a notification to an Amazon Simple Queue Service (Amazon SQS) queue when a video is uploaded to the S3 bucket.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When a video is uploaded, send an alert to an Amazon Simple Notification Service (Amazon SNS) topic with the application owner's email address for further processing.",
          "correct": false
        },
        {
          "id": 5,
          "answer": "Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.",
          "correct": true
        }
      ],
      "multiple": true,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Options:\n\nConfigure the S3 bucket to send a notification to an Amazon Simple Queue Service (Amazon SQS) queue when a video is uploaded to the S3 bucket.\n\nConfigure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.\n\nThis combination is the most suitable for meeting the requirements. When a video is uploaded to the S3 bucket, it sends a notification to the Amazon SQS queue. This acts as a buffer, ensuring that no messages are lost even if there are temporary issues with the Lambda function. The Lambda function uses the SQS queue as its invocation source, processing and compressing the videos one by one. After a video is processed successfully, the Lambda function deletes the corresponding message from the queue to prevent it from being processed again.\n\n\n\n\n\n\n\nIncorrect Options:\n\nConfigure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded video is detected, write the file name to a text file in memory and use the text file to keep track of the videos that were processed.\n\nUsing a text file in memory is not a reliable, stateless approach and could lead to losing track of processed videos in case of Lambda function failures or issues.\n\n\n\n\nLaunch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function.\n\nThis introduces unnecessary complexity and operational overhead with the EC2 instance. Directly using the SQS queue as the invocation source for the Lambda function is more efficient and reliable.\n\n\n\n\nConfigure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When a video is uploaded, send an alert to an Amazon Simple Notification Service (Amazon SNS) topic with the application owner's email address for further processing.\n\nThis does not automate the processing of videos, as it only sends an email alert for further manual action.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ways-to-add-notification-config-to-bucket.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
      "correctAnswerExplanations": [
        {
          "answer": "Configure the S3 bucket to send a notification to an Amazon Simple Queue Service (Amazon SQS) queue when a video is uploaded to the S3 bucket.",
          "explanation": "<strong>Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.</strong>"
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded video is detected, write the file name to a text file in memory and use the text file to keep track of the videos that were processed.",
          "explanation": "Using a text file in memory is not a reliable, stateless approach and could lead to losing track of processed videos in case of Lambda function failures or issues."
        },
        {
          "answer": "Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function.",
          "explanation": "This introduces unnecessary complexity and operational overhead with the EC2 instance. Directly using the SQS queue as the invocation source for the Lambda function is more efficient and reliable."
        },
        {
          "answer": "Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When a video is uploaded, send an alert to an Amazon Simple Notification Service (Amazon SNS) topic with the application owner's email address for further processing.",
          "explanation": "This does not automate the processing of videos, as it only sends an email alert for further manual action."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/ways-to-add-notification-config-to-bucket.html",
        "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"
      ]
    },
    {
      "id": 63,
      "question": "A company is hosting a global online conference that will last for 1 week and expects a significant increase in traffic. The company needs to ensure that they have sufficient Amazon EC2 capacity in three specific Availability Zones within a particular AWS Region.\n\nWhat should the company do to secure the necessary EC2 capacity?",
      "corrects": [
        2
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Create an On-Demand Capacity Reservation that specifies the Region needed.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed.",
          "correct": true
        },
        {
          "id": 3,
          "answer": "Purchase Reserved Instances that specify the Region needed.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Purchase Reserved Instances that specify the Region and three Availability Zones needed.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Resilient Architectures",
      "explanation": "Correct Option:\n\nCreate an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed.\n\nCreating an On-Demand Capacity Reservation enables the company to reserve the required Amazon EC2 capacity in the specific Availability Zones within the desired AWS Region. This ensures that they have the necessary resources to handle the expected increase in traffic during the week-long event. On-Demand Capacity Reservations are flexible, allowing the company to reserve capacity for a short duration, which aligns with their one-week requirement.\n\n\n\n\n\n\n\nIncorrect Options:\n\nPurchase Reserved Instances that specify the Region and three Availability Zones needed.\n\nReserved Instances are more suitable for long-term commitments and typically last for one or three years. Since the company only needs guaranteed capacity for one week, Reserved Instances would not be the most cost-effective or flexible solution.\n\n\n\n\nCreate an On-Demand Capacity Reservation that specifies the Region needed.\n\nBy only specifying the Region and not the Availability Zones, the company will not have guaranteed capacity in the three specific Availability Zones they require. This option does not fully meet the company's requirements.\n\n\n\n\nPurchase Reserved Instances that specify the Region needed.\n\nReserved Instances are designed for long-term commitments and do not provide the flexibility needed for the company's one-week event. Additionally, only specifying the Region would not guarantee capacity in the three specific Availability Zones required.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/\n\nhttps://aws.amazon.com/ec2/pricing/on-demand/",
      "correctAnswerExplanations": [
        {
          "answer": "Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed.",
          "explanation": "Creating an On-Demand Capacity Reservation enables the company to reserve the required Amazon EC2 capacity in the specific Availability Zones within the desired AWS Region. This ensures that they have the necessary resources to handle the expected increase in traffic during the week-long event. On-Demand Capacity Reservations are flexible, allowing the company to reserve capacity for a short duration, which aligns with their one-week requirement."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Purchase Reserved Instances that specify the Region and three Availability Zones needed.",
          "explanation": "Reserved Instances are more suitable for long-term commitments and typically last for one or three years. Since the company only needs guaranteed capacity for one week, Reserved Instances would not be the most cost-effective or flexible solution."
        },
        {
          "answer": "Create an On-Demand Capacity Reservation that specifies the Region needed.",
          "explanation": "By only specifying the Region and not the Availability Zones, the company will not have guaranteed capacity in the three specific Availability Zones they require. This option does not fully meet the company's requirements."
        },
        {
          "answer": "Purchase Reserved Instances that specify the Region needed.",
          "explanation": "Reserved Instances are designed for long-term commitments and do not provide the flexibility needed for the company's one-week event. Additionally, only specifying the Region would not guarantee capacity in the three specific Availability Zones required."
        }
      ],
      "references": [
        "https://aws.amazon.com/ec2/pricing/reserved-instances/",
        "https://aws.amazon.com/ec2/pricing/on-demand/"
      ]
    },
    {
      "id": 64,
      "question": "A startup is operating a mission-critical e-commerce application on AWS, utilizing Amazon EC2 instances within an Auto Scaling group, situated behind an Application Load Balancer. The application relies on an Amazon Aurora PostgreSQL database, which is deployed in a single Availability Zone. The startup aims to achieve high availability, minimize downtime, and ensure minimal data loss.\n\nWhich of the following solutions will fulfill these requirements with the LEAST operational effort?",
      "corrects": [
        1
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Distribute the Auto Scaling group across multiple Availability Zones, set up the database as Multi-AZ, and employ an Amazon RDS Proxy instance for the database.",
          "correct": true
        },
        {
          "id": 2,
          "answer": "Set the Auto Scaling group to use a single Availability Zone, create hourly snapshots of the database, and restore the database from snapshots in case of failure.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Deploy the EC2 instances in various AWS Regions, use Amazon Route 53 health checks to redirect traffic, and apply Aurora PostgreSQL Cross-Region Replication.",
          "correct": false
        },
        {
          "id": 4,
          "answer": "Configure the Auto Scaling group to use multiple AWS Regions, write application data to Amazon S3, and employ S3 Event Notifications to trigger an AWS Lambda function that writes data to the database.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design High-Performing Architectures",
      "explanation": "Correct Option:\n\nDistribute the Auto Scaling group across multiple Availability Zones, set up the database as Multi-AZ, and employ an Amazon RDS Proxy instance for the database.\n\nBy configuring the Auto Scaling group to span multiple Availability Zones, the e-commerce application can maintain high availability even in the event of an AZ failure. Additionally, implementing a Multi-AZ database enhances the application's reliability by automatically replicating the database to a standby instance in a different AZ, minimizing data loss and downtime. Using an Amazon RDS Proxy instance for the database helps to manage connections, improving the application's performance and scalability, while reducing operational effort.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDeploy the EC2 instances in various AWS Regions, use Amazon Route 53 health checks to redirect traffic, and apply Aurora PostgreSQL Cross-Region Replication.\n\nIt increases operational complexity and may not guarantee high availability. Cross-region replication could introduce latency, and Route 53 health checks might not provide the desired failover performance.\n\n\n\n\nSet the Auto Scaling group to use a single Availability Zone, create hourly snapshots of the database, and restore the database from snapshots in case of failure.\n\nRelying on a single Availability Zone increases the risk of downtime and data loss. Hourly snapshots may not be sufficient to prevent significant data loss during an outage, and recovery time might be prolonged.\n\n\n\n\nConfigure the Auto Scaling group to use multiple AWS Regions, write application data to Amazon S3, and employ S3 Event Notifications to trigger an AWS Lambda function that writes data to the database.\n\nWriting application data to S3 and utilizing Lambda functions adds unnecessary complexity and potential latency issues. The solution does not address high availability for the database or minimize downtime.\n\n\n\n\n\n\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/multi-az\n\nhttps://aws.amazon.com/rds/proxy",
      "correctAnswerExplanations": [
        {
          "answer": "Distribute the Auto Scaling group across multiple Availability Zones, set up the database as Multi-AZ, and employ an Amazon RDS Proxy instance for the database.",
          "explanation": "By configuring the Auto Scaling group to span multiple Availability Zones, the e-commerce application can maintain high availability even in the event of an AZ failure. Additionally, implementing a Multi-AZ database enhances the application's reliability by automatically replicating the database to a standby instance in a different AZ, minimizing data loss and downtime. Using an Amazon RDS Proxy instance for the database helps to manage connections, improving the application's performance and scalability, while reducing operational effort."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Deploy the EC2 instances in various AWS Regions, use Amazon Route 53 health checks to redirect traffic, and apply Aurora PostgreSQL Cross-Region Replication.",
          "explanation": "It increases operational complexity and may not guarantee high availability. Cross-region replication could introduce latency, and Route 53 health checks might not provide the desired failover performance."
        },
        {
          "answer": "Set the Auto Scaling group to use a single Availability Zone, create hourly snapshots of the database, and restore the database from snapshots in case of failure.",
          "explanation": "Relying on a single Availability Zone increases the risk of downtime and data loss. Hourly snapshots may not be sufficient to prevent significant data loss during an outage, and recovery time might be prolonged."
        },
        {
          "answer": "Configure the Auto Scaling group to use multiple AWS Regions, write application data to Amazon S3, and employ S3 Event Notifications to trigger an AWS Lambda function that writes data to the database.",
          "explanation": "Writing application data to S3 and utilizing Lambda functions adds unnecessary complexity and potential latency issues. The solution does not address high availability for the database or minimize downtime."
        }
      ],
      "references": [
        "https://aws.amazon.com/rds/features/multi-az",
        "https://aws.amazon.com/rds/proxy"
      ]
    },
    {
      "id": 65,
      "question": "A company operates a fault-tolerant file-processing application on Amazon EC2 instances within a single VPC. The EC2 instances are distributed across multiple subnets and Availability Zones. While the EC2 instances do not interact with each other, they retrieve files from Amazon S3 and store processed files back in Amazon S3 through a single NAT gateway. The company wants to minimize regional data transfer costs.\n\nWhat is the MOST cost-effective method for the company to reduce regional data transfer charges?",
      "corrects": [
        3
      ],
      "answers": [
        {
          "id": 1,
          "answer": "Replace the NAT gateway with a NAT instance.",
          "correct": false
        },
        {
          "id": 2,
          "answer": "Use an EC2 Dedicated Host to run the EC2 instances.",
          "correct": false
        },
        {
          "id": 3,
          "answer": "Implement a gateway VPC endpoint for Amazon S3.",
          "correct": true
        },
        {
          "id": 4,
          "answer": "Deploy a NAT gateway in each Availability Zone.",
          "correct": false
        }
      ],
      "multiple": false,
      "domain": "Design Cost-Optimized Architectures",
      "explanation": "Correct Option:\n\nImplement a gateway VPC endpoint for Amazon S3.\n\nBy implementing a gateway VPC endpoint for Amazon S3, the company can minimize data transfer charges. Gateway VPC endpoints enable private connections between the VPC and supported AWS services, using an elastic network interface with a private IP address. This means that traffic between the VPC and S3 will stay within the AWS network, avoiding data transfer costs. Moreover, the company can ensure secure and direct communication between the EC2 instances and Amazon S3 without the need for an internet gateway, NAT device, or VPN connection.\n\n\n\n\n\n\n\nIncorrect Options:\n\nDeploy a NAT gateway in each Availability Zone.\n\nWhile this option might improve the redundancy of the NAT gateway, it does not reduce data transfer charges, as traffic still traverses the public internet when accessing Amazon S3.\n\n\n\n\nReplace the NAT gateway with a NAT instance.\n\nNAT instances can be more cost-effective than NAT gateways in some cases, but replacing the NAT gateway with a NAT instance does not address the issue of data transfer charges between the VPC and Amazon S3.\n\n\n\n\nUse an EC2 Dedicated Host to run the EC2 instances.\n\nUtilizing an EC2 Dedicated Host might provide additional control over the underlying infrastructure, but it does not directly impact data transfer charges between the VPC and Amazon S3.\n\n\n\n\n\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html",
      "correctAnswerExplanations": [
        {
          "answer": "Implement a gateway VPC endpoint for Amazon S3.",
          "explanation": "By implementing a gateway VPC endpoint for Amazon S3, the company can minimize data transfer charges. Gateway VPC endpoints enable private connections between the VPC and supported AWS services, using an elastic network interface with a private IP address. This means that traffic between the VPC and S3 will stay within the AWS network, avoiding data transfer costs. Moreover, the company can ensure secure and direct communication between the EC2 instances and Amazon S3 without the need for an internet gateway, NAT device, or VPN connection."
        }
      ],
      "incorrectAnswerExplanations": [
        {
          "answer": "Deploy a NAT gateway in each Availability Zone.",
          "explanation": "While this option might improve the redundancy of the NAT gateway, it does not reduce data transfer charges, as traffic still traverses the public internet when accessing Amazon S3."
        },
        {
          "answer": "Replace the NAT gateway with a NAT instance.",
          "explanation": "NAT instances can be more cost-effective than NAT gateways in some cases, but replacing the NAT gateway with a NAT instance does not address the issue of data transfer charges between the VPC and Amazon S3."
        },
        {
          "answer": "Use an EC2 Dedicated Host to run the EC2 instances.",
          "explanation": "Utilizing an EC2 Dedicated Host might provide additional control over the underlying infrastructure, but it does not directly impact data transfer charges between the VPC and Amazon S3."
        }
      ],
      "references": [
        "https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html",
        "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html"
      ]
    }
  ]